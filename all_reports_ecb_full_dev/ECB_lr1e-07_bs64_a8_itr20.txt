1: 6464: loss: 0.7091455102:
1: 12864: loss: 0.7078535208:
1: 19264: loss: 0.7071288131:
1: 25664: loss: 0.7061560208:
1: 32064: loss: 0.7049977994:
1: 38464: loss: 0.7042242361:
1: 44864: loss: 0.7032092799:
1: 51264: loss: 0.7020726295:
1: 57664: loss: 0.7010649659:
1: 64064: loss: 0.7000903817:
1: 70464: loss: 0.6992018746:
1: 76864: loss: 0.6982577465:
1: 83264: loss: 0.6972899607:
1: 89664: loss: 0.6962435857:
1: 96064: loss: 0.6952681825:
1: 102464: loss: 0.6942009767:
1: 108864: loss: 0.6931697343:
1: 115264: loss: 0.6921933950:
1: 121664: loss: 0.6912575718:
1: 128064: loss: 0.6902406621:
1: 134464: loss: 0.6892954784:
Dev-Acc: 1: Accuracy: 0.7947689891: precision: 0.0670877932: recall: 0.1950348580: f1: 0.0998346244
Train-Acc: 1: Accuracy: 0.7847099304: precision: 0.1787837838: recall: 0.2609295904: f1: 0.2121835824
2: 6464: loss: 0.6680539399:
2: 12864: loss: 0.6671790999:
2: 19264: loss: 0.6657379089:
2: 25664: loss: 0.6646523610:
2: 32064: loss: 0.6638044178:
2: 38464: loss: 0.6629935027:
2: 44864: loss: 0.6620018855:
2: 51264: loss: 0.6610089535:
2: 57664: loss: 0.6601945784:
2: 64064: loss: 0.6591007107:
2: 70464: loss: 0.6582710061:
2: 76864: loss: 0.6573073683:
2: 83264: loss: 0.6564527984:
2: 89664: loss: 0.6555728542:
2: 96064: loss: 0.6547215006:
2: 102464: loss: 0.6538734148:
2: 108864: loss: 0.6530329443:
2: 115264: loss: 0.6521810122:
2: 121664: loss: 0.6513574473:
2: 128064: loss: 0.6504650080:
2: 134464: loss: 0.6495097150:
Dev-Acc: 2: Accuracy: 0.9342752695: precision: 0.0695249131: recall: 0.0102023465: f1: 0.0177935943
Train-Acc: 2: Accuracy: 0.8852730989: precision: 0.3146067416: recall: 0.0276115969: f1: 0.0507675571
3: 6464: loss: 0.6291857368:
3: 12864: loss: 0.6290178719:
3: 19264: loss: 0.6281222663:
3: 25664: loss: 0.6268165740:
3: 32064: loss: 0.6258733698:
3: 38464: loss: 0.6250304861:
3: 44864: loss: 0.6240102527:
3: 51264: loss: 0.6233768450:
3: 57664: loss: 0.6226275151:
3: 64064: loss: 0.6218082444:
3: 70464: loss: 0.6210852726:
3: 76864: loss: 0.6200552899:
3: 83264: loss: 0.6190355426:
3: 89664: loss: 0.6182277450:
3: 96064: loss: 0.6172573637:
3: 102464: loss: 0.6164584582:
3: 108864: loss: 0.6156614293:
3: 115264: loss: 0.6148056492:
3: 121664: loss: 0.6139613172:
3: 128064: loss: 0.6132156451:
3: 134464: loss: 0.6124793841:
Dev-Acc: 3: Accuracy: 0.9415482283: precision: 0.1428571429: recall: 0.0003400782: f1: 0.0006785411
Train-Acc: 3: Accuracy: 0.8888888955: precision: 0.5000000000: recall: 0.0011833542: f1: 0.0023611202
4: 6464: loss: 0.5920706999:
4: 12864: loss: 0.5927626044:
4: 19264: loss: 0.5912908246:
4: 25664: loss: 0.5906903139:
4: 32064: loss: 0.5901068709:
4: 38464: loss: 0.5889445975:
4: 44864: loss: 0.5883282527:
4: 51264: loss: 0.5875306150:
4: 57664: loss: 0.5867082319:
4: 64064: loss: 0.5860355014:
4: 70464: loss: 0.5854289673:
4: 76864: loss: 0.5847288486:
4: 83264: loss: 0.5840260362:
4: 89664: loss: 0.5832555223:
4: 96064: loss: 0.5824533488:
4: 102464: loss: 0.5815224466:
4: 108864: loss: 0.5808140955:
4: 115264: loss: 0.5798645310:
4: 121664: loss: 0.5789912675:
4: 128064: loss: 0.5781667782:
4: 134464: loss: 0.5774563209:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8888962269: precision: 1.0000000000: recall: 0.0000657419: f1: 0.0001314752
5: 6464: loss: 0.5623875070:
5: 12864: loss: 0.5603812805:
5: 19264: loss: 0.5588080808:
5: 25664: loss: 0.5584734905:
5: 32064: loss: 0.5577179341:
5: 38464: loss: 0.5562007833:
5: 44864: loss: 0.5550115239:
5: 51264: loss: 0.5543368044:
5: 57664: loss: 0.5535986965:
5: 64064: loss: 0.5529353904:
5: 70464: loss: 0.5521064383:
5: 76864: loss: 0.5510667749:
5: 83264: loss: 0.5503362270:
5: 89664: loss: 0.5495990614:
5: 96064: loss: 0.5489629853:
5: 102464: loss: 0.5481314967:
5: 108864: loss: 0.5472429545:
5: 115264: loss: 0.5463310603:
5: 121664: loss: 0.5456750453:
5: 128064: loss: 0.5451145182:
5: 134464: loss: 0.5443957422:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 6464: loss: 0.5286873072:
6: 12864: loss: 0.5252942005:
6: 19264: loss: 0.5253368693:
6: 25664: loss: 0.5245248687:
6: 32064: loss: 0.5235100973:
6: 38464: loss: 0.5226006225:
6: 44864: loss: 0.5222447211:
6: 51264: loss: 0.5213496886:
6: 57664: loss: 0.5208970585:
6: 64064: loss: 0.5197046672:
6: 70464: loss: 0.5194419503:
6: 76864: loss: 0.5190673089:
6: 83264: loss: 0.5187088981:
6: 89664: loss: 0.5180705137:
6: 96064: loss: 0.5172915715:
6: 102464: loss: 0.5164579731:
6: 108864: loss: 0.5156910067:
6: 115264: loss: 0.5150055794:
6: 121664: loss: 0.5144169098:
6: 128064: loss: 0.5137505365:
6: 134464: loss: 0.5130865724:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 6464: loss: 0.4988280156:
7: 12864: loss: 0.4959943947:
7: 19264: loss: 0.4950547446:
7: 25664: loss: 0.4943125635:
7: 32064: loss: 0.4942117543:
7: 38464: loss: 0.4937325171:
7: 44864: loss: 0.4928466314:
7: 51264: loss: 0.4921233205:
7: 57664: loss: 0.4918786444:
7: 64064: loss: 0.4911559803:
7: 70464: loss: 0.4912221342:
7: 76864: loss: 0.4905702268:
7: 83264: loss: 0.4896092155:
7: 89664: loss: 0.4890013853:
7: 96064: loss: 0.4880603063:
7: 102464: loss: 0.4874797269:
7: 108864: loss: 0.4869355162:
7: 115264: loss: 0.4860071879:
7: 121664: loss: 0.4854726246:
7: 128064: loss: 0.4846140961:
7: 134464: loss: 0.4840619082:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.4716462320:
8: 12864: loss: 0.4699198921:
8: 19264: loss: 0.4678994788:
8: 25664: loss: 0.4676467419:
8: 32064: loss: 0.4672681132:
8: 38464: loss: 0.4662859011:
8: 44864: loss: 0.4656194183:
8: 51264: loss: 0.4650698551:
8: 57664: loss: 0.4648609113:
8: 64064: loss: 0.4641257088:
8: 70464: loss: 0.4638365359:
8: 76864: loss: 0.4630786118:
8: 83264: loss: 0.4621131556:
8: 89664: loss: 0.4615528678:
8: 96064: loss: 0.4608392401:
8: 102464: loss: 0.4602727656:
8: 108864: loss: 0.4595010593:
8: 115264: loss: 0.4588724658:
8: 121664: loss: 0.4578625607:
8: 128064: loss: 0.4570692639:
8: 134464: loss: 0.4567278370:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.4435446033:
9: 12864: loss: 0.4408281215:
9: 19264: loss: 0.4401400273:
9: 25664: loss: 0.4411220462:
9: 32064: loss: 0.4394879835:
9: 38464: loss: 0.4401703950:
9: 44864: loss: 0.4398858104:
9: 51264: loss: 0.4405063839:
9: 57664: loss: 0.4403476709:
9: 64064: loss: 0.4399756192:
9: 70464: loss: 0.4388913945:
9: 76864: loss: 0.4385416203:
9: 83264: loss: 0.4375412390:
9: 89664: loss: 0.4364499222:
9: 96064: loss: 0.4357294114:
9: 102464: loss: 0.4351305484:
9: 108864: loss: 0.4345852688:
9: 115264: loss: 0.4342191308:
9: 121664: loss: 0.4338763120:
9: 128064: loss: 0.4332623942:
9: 134464: loss: 0.4326536099:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.4269763443:
10: 12864: loss: 0.4204795133:
10: 19264: loss: 0.4189156978:
10: 25664: loss: 0.4157433888:
10: 32064: loss: 0.4158666009:
10: 38464: loss: 0.4146282448:
10: 44864: loss: 0.4142111955:
10: 51264: loss: 0.4150944282:
10: 57664: loss: 0.4148406617:
10: 64064: loss: 0.4147169746:
10: 70464: loss: 0.4143988801:
10: 76864: loss: 0.4139228571:
10: 83264: loss: 0.4131063872:
10: 89664: loss: 0.4126052393:
10: 96064: loss: 0.4118581464:
10: 102464: loss: 0.4114456789:
10: 108864: loss: 0.4111299259:
10: 115264: loss: 0.4112054452:
10: 121664: loss: 0.4107875502:
10: 128064: loss: 0.4106366986:
10: 134464: loss: 0.4108791063:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.4022956485:
11: 12864: loss: 0.4001580241:
11: 19264: loss: 0.3988722143:
11: 25664: loss: 0.3990645934:
11: 32064: loss: 0.3976764030:
11: 38464: loss: 0.3981968920:
11: 44864: loss: 0.3982462684:
11: 51264: loss: 0.3972928729:
11: 57664: loss: 0.3971456493:
11: 64064: loss: 0.3973714432:
11: 70464: loss: 0.3969971238:
11: 76864: loss: 0.3961435159:
11: 83264: loss: 0.3961990698:
11: 89664: loss: 0.3957824537:
11: 96064: loss: 0.3953629770:
11: 102464: loss: 0.3946727331:
11: 108864: loss: 0.3937975855:
11: 115264: loss: 0.3933899000:
11: 121664: loss: 0.3930407759:
11: 128064: loss: 0.3921939112:
11: 134464: loss: 0.3916818687:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.3856241739:
12: 12864: loss: 0.3857906277:
12: 19264: loss: 0.3824805628:
12: 25664: loss: 0.3812256685:
12: 32064: loss: 0.3809116845:
12: 38464: loss: 0.3800119894:
12: 44864: loss: 0.3800242599:
12: 51264: loss: 0.3794991552:
12: 57664: loss: 0.3790815691:
12: 64064: loss: 0.3784357375:
12: 70464: loss: 0.3785938679:
12: 76864: loss: 0.3780632083:
12: 83264: loss: 0.3772803923:
12: 89664: loss: 0.3767504721:
12: 96064: loss: 0.3766519256:
12: 102464: loss: 0.3758651537:
12: 108864: loss: 0.3758896374:
12: 115264: loss: 0.3760451596:
12: 121664: loss: 0.3757444269:
12: 128064: loss: 0.3752075922:
12: 134464: loss: 0.3751254806:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.3648613086:
13: 12864: loss: 0.3671869431:
13: 19264: loss: 0.3683731343:
13: 25664: loss: 0.3653779506:
13: 32064: loss: 0.3659505655:
13: 38464: loss: 0.3637586735:
13: 44864: loss: 0.3637051013:
13: 51264: loss: 0.3631306706:
13: 57664: loss: 0.3633238525:
13: 64064: loss: 0.3628073221:
13: 70464: loss: 0.3623934264:
13: 76864: loss: 0.3625476511:
13: 83264: loss: 0.3626130135:
13: 89664: loss: 0.3618264290:
13: 96064: loss: 0.3625655433:
13: 102464: loss: 0.3622783235:
13: 108864: loss: 0.3618855764:
13: 115264: loss: 0.3615644697:
13: 121664: loss: 0.3610069548:
13: 128064: loss: 0.3608934636:
13: 134464: loss: 0.3606380558:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.3655274183:
14: 12864: loss: 0.3558663589:
14: 19264: loss: 0.3571487959:
14: 25664: loss: 0.3551375873:
14: 32064: loss: 0.3546810888:
14: 38464: loss: 0.3538952333:
14: 44864: loss: 0.3532015953:
14: 51264: loss: 0.3522896136:
14: 57664: loss: 0.3517913985:
14: 64064: loss: 0.3509539659:
14: 70464: loss: 0.3510200064:
14: 76864: loss: 0.3505099262:
14: 83264: loss: 0.3502692729:
14: 89664: loss: 0.3499600997:
14: 96064: loss: 0.3494838838:
14: 102464: loss: 0.3497863706:
14: 108864: loss: 0.3493679409:
14: 115264: loss: 0.3488999957:
14: 121664: loss: 0.3489345992:
14: 128064: loss: 0.3486257410:
14: 134464: loss: 0.3480799326:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.3426301686:
15: 12864: loss: 0.3451271242:
15: 19264: loss: 0.3456188015:
15: 25664: loss: 0.3478431070:
15: 32064: loss: 0.3489133821:
15: 38464: loss: 0.3458707456:
15: 44864: loss: 0.3439431919:
15: 51264: loss: 0.3422572529:
15: 57664: loss: 0.3413591397:
15: 64064: loss: 0.3409328534:
15: 70464: loss: 0.3409456364:
15: 76864: loss: 0.3406516292:
15: 83264: loss: 0.3409692545:
15: 89664: loss: 0.3398490934:
15: 96064: loss: 0.3393780131:
15: 102464: loss: 0.3389923744:
15: 108864: loss: 0.3383708576:
15: 115264: loss: 0.3382834252:
15: 121664: loss: 0.3376138754:
15: 128064: loss: 0.3371507149:
15: 134464: loss: 0.3372270296:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.3264473848:
16: 12864: loss: 0.3279945745:
16: 19264: loss: 0.3310009405:
16: 25664: loss: 0.3321905309:
16: 32064: loss: 0.3314235903:
16: 38464: loss: 0.3322148752:
16: 44864: loss: 0.3314000012:
16: 51264: loss: 0.3303349566:
16: 57664: loss: 0.3306832868:
16: 64064: loss: 0.3302536537:
16: 70464: loss: 0.3302354347:
16: 76864: loss: 0.3303273918:
16: 83264: loss: 0.3303512014:
16: 89664: loss: 0.3307643839:
16: 96064: loss: 0.3302883326:
16: 102464: loss: 0.3305158211:
16: 108864: loss: 0.3297351380:
16: 115264: loss: 0.3293322163:
16: 121664: loss: 0.3286778441:
16: 128064: loss: 0.3285852257:
16: 134464: loss: 0.3276860628:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.3166003193:
17: 12864: loss: 0.3167038832:
17: 19264: loss: 0.3193284281:
17: 25664: loss: 0.3203487490:
17: 32064: loss: 0.3206420761:
17: 38464: loss: 0.3213324792:
17: 44864: loss: 0.3211952958:
17: 51264: loss: 0.3217887647:
17: 57664: loss: 0.3208257245:
17: 64064: loss: 0.3208352486:
17: 70464: loss: 0.3202590769:
17: 76864: loss: 0.3204746208:
17: 83264: loss: 0.3204398895:
17: 89664: loss: 0.3200421772:
17: 96064: loss: 0.3204593861:
17: 102464: loss: 0.3194431014:
17: 108864: loss: 0.3193448006:
17: 115264: loss: 0.3187926452:
17: 121664: loss: 0.3186516690:
17: 128064: loss: 0.3184077979:
17: 134464: loss: 0.3185696196:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.3019551082:
18: 12864: loss: 0.3102494026:
18: 19264: loss: 0.3137312604:
18: 25664: loss: 0.3122252101:
18: 32064: loss: 0.3133504186:
18: 38464: loss: 0.3121874174:
18: 44864: loss: 0.3102628087:
18: 51264: loss: 0.3109347929:
18: 57664: loss: 0.3106866173:
18: 64064: loss: 0.3116459591:
18: 70464: loss: 0.3120546766:
18: 76864: loss: 0.3117952468:
18: 83264: loss: 0.3110852072:
18: 89664: loss: 0.3107104727:
18: 96064: loss: 0.3114547512:
18: 102464: loss: 0.3115503055:
18: 108864: loss: 0.3114892206:
18: 115264: loss: 0.3113965096:
18: 121664: loss: 0.3112826289:
18: 128064: loss: 0.3110400343:
18: 134464: loss: 0.3110066148:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 6464: loss: 0.3165351123:
19: 12864: loss: 0.3143764468:
19: 19264: loss: 0.3142271416:
19: 25664: loss: 0.3127722209:
19: 32064: loss: 0.3095571428:
19: 38464: loss: 0.3081310400:
19: 44864: loss: 0.3081832603:
19: 51264: loss: 0.3077260494:
19: 57664: loss: 0.3061326916:
19: 64064: loss: 0.3068648350:
19: 70464: loss: 0.3067001848:
19: 76864: loss: 0.3063498739:
19: 83264: loss: 0.3067202288:
19: 89664: loss: 0.3062749321:
19: 96064: loss: 0.3053373438:
19: 102464: loss: 0.3048560744:
19: 108864: loss: 0.3041406436:
19: 115264: loss: 0.3039219556:
19: 121664: loss: 0.3042314127:
19: 128064: loss: 0.3040126158:
19: 134464: loss: 0.3041499372:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
20: 6464: loss: 0.3015151663:
20: 12864: loss: 0.2983665348:
20: 19264: loss: 0.2970606459:
20: 25664: loss: 0.2970639945:
20: 32064: loss: 0.2966184504:
20: 38464: loss: 0.2957106322:
20: 44864: loss: 0.2974136820:
20: 51264: loss: 0.2962507501:
20: 57664: loss: 0.2973960830:
20: 64064: loss: 0.2965235567:
20: 70464: loss: 0.2979388972:
20: 76864: loss: 0.2981689423:
20: 83264: loss: 0.2972686774:
20: 89664: loss: 0.2976268700:
20: 96064: loss: 0.2979933511:
20: 102464: loss: 0.2980614696:
20: 108864: loss: 0.2983983070:
20: 115264: loss: 0.2985594179:
20: 121664: loss: 0.2982250234:
20: 128064: loss: 0.2974193164:
20: 134464: loss: 0.2973735438:
Dev-Acc: 20: Accuracy: 0.9416673183: precision: 0.7500000000: recall: 0.0005101173: f1: 0.0010195412
Train-Acc: 20: Accuracy: 0.8890934587: precision: 1.0000000000: recall: 0.0018407731: f1: 0.0036747818
