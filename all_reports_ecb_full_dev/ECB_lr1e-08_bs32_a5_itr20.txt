1: 3232: loss: 0.7064524788:
1: 6432: loss: 0.7056283581:
1: 9632: loss: 0.7056308045:
1: 12832: loss: 0.7059923697:
1: 16032: loss: 0.7057897516:
1: 19232: loss: 0.7055971489:
1: 22432: loss: 0.7053665380:
1: 25632: loss: 0.7053156489:
1: 28832: loss: 0.7053233274:
1: 32032: loss: 0.7049581740:
1: 35232: loss: 0.7048377286:
1: 38432: loss: 0.7049272818:
1: 41632: loss: 0.7048855864:
1: 44832: loss: 0.7047493750:
1: 48032: loss: 0.7047722772:
1: 51232: loss: 0.7046585655:
1: 54432: loss: 0.7045359137:
1: 57632: loss: 0.7044684655:
1: 60832: loss: 0.7044057881:
1: 64032: loss: 0.7043516217:
1: 67232: loss: 0.7043308385:
1: 70432: loss: 0.7043368503:
1: 73632: loss: 0.7042296751:
1: 76832: loss: 0.7041178357:
1: 80032: loss: 0.7040322986:
1: 83232: loss: 0.7039915200:
1: 86432: loss: 0.7038354352:
1: 89632: loss: 0.7037899509:
Dev-Acc: 1: Accuracy: 0.3022106588: precision: 0.0642840723: recall: 0.8083659242: f1: 0.1190971265
Train-Acc: 1: Accuracy: 0.3817303181: precision: 0.1872078198: recall: 0.8108605614: f1: 0.3041864480
2: 3232: loss: 0.7034352559:
2: 6432: loss: 0.7027358750:
2: 9632: loss: 0.7021632169:
2: 12832: loss: 0.7020753294:
2: 16032: loss: 0.7019802966:
2: 19232: loss: 0.7016340635:
2: 22432: loss: 0.7016514298:
2: 25632: loss: 0.7014834934:
2: 28832: loss: 0.7012719164:
2: 32032: loss: 0.7011795501:
2: 35232: loss: 0.7011977137:
2: 38432: loss: 0.7010930186:
2: 41632: loss: 0.7010104162:
2: 44832: loss: 0.7008815229:
2: 48032: loss: 0.7008118808:
2: 51232: loss: 0.7007179816:
2: 54432: loss: 0.7006721449:
2: 57632: loss: 0.7005606820:
2: 60832: loss: 0.7004914494:
2: 64032: loss: 0.7004221763:
2: 67232: loss: 0.7003997478:
2: 70432: loss: 0.7002946890:
2: 73632: loss: 0.7002092778:
2: 76832: loss: 0.7001901013:
2: 80032: loss: 0.7001171014:
2: 83232: loss: 0.7000370664:
2: 86432: loss: 0.6999226436:
2: 89632: loss: 0.6998812975:
Dev-Acc: 2: Accuracy: 0.3589161038: precision: 0.0661199764: recall: 0.7609250128: f1: 0.1216677315
Train-Acc: 2: Accuracy: 0.4298533797: precision: 0.1934296846: recall: 0.7637236211: f1: 0.3086795361
3: 3232: loss: 0.6963548118:
3: 6432: loss: 0.6968751493:
3: 9632: loss: 0.6970644220:
3: 12832: loss: 0.6974967825:
3: 16032: loss: 0.6972940263:
3: 19232: loss: 0.6972937429:
3: 22432: loss: 0.6973253208:
3: 25632: loss: 0.6972199807:
3: 28832: loss: 0.6971069652:
3: 32032: loss: 0.6969081824:
3: 35232: loss: 0.6968533947:
3: 38432: loss: 0.6967874953:
3: 41632: loss: 0.6967570787:
3: 44832: loss: 0.6967365083:
3: 48032: loss: 0.6967018364:
3: 51232: loss: 0.6965771958:
3: 54432: loss: 0.6965482558:
3: 57632: loss: 0.6964631943:
3: 60832: loss: 0.6964777328:
3: 64032: loss: 0.6963667977:
3: 67232: loss: 0.6962974544:
3: 70432: loss: 0.6962637763:
3: 73632: loss: 0.6962311499:
3: 76832: loss: 0.6961573451:
3: 80032: loss: 0.6960517849:
3: 83232: loss: 0.6959596653:
3: 86432: loss: 0.6958683438:
3: 89632: loss: 0.6957960188:
Dev-Acc: 3: Accuracy: 0.4202254415: precision: 0.0670324781: recall: 0.6917190954: f1: 0.1222208869
Train-Acc: 3: Accuracy: 0.4813073874: precision: 0.1999738523: recall: 0.7038984945: f1: 0.3114627725
4: 3232: loss: 0.6926982373:
4: 6432: loss: 0.6944693080:
4: 9632: loss: 0.6942201062:
4: 12832: loss: 0.6940346949:
4: 16032: loss: 0.6937075458:
4: 19232: loss: 0.6936044949:
4: 22432: loss: 0.6935407993:
4: 25632: loss: 0.6934422620:
4: 28832: loss: 0.6933187252:
4: 32032: loss: 0.6931151856:
4: 35232: loss: 0.6928729776:
4: 38432: loss: 0.6928834621:
4: 41632: loss: 0.6927027271:
4: 44832: loss: 0.6925893554:
4: 48032: loss: 0.6925142182:
4: 51232: loss: 0.6924396007:
4: 54432: loss: 0.6923448624:
4: 57632: loss: 0.6922922986:
4: 60832: loss: 0.6922835978:
4: 64032: loss: 0.6922339529:
4: 67232: loss: 0.6922103014:
4: 70432: loss: 0.6921596115:
4: 73632: loss: 0.6921033825:
4: 76832: loss: 0.6919703800:
4: 80032: loss: 0.6918494803:
4: 83232: loss: 0.6917703182:
4: 86432: loss: 0.6916849447:
4: 89632: loss: 0.6915966239:
Dev-Acc: 4: Accuracy: 0.4833306670: precision: 0.0669241876: recall: 0.6068695800: f1: 0.1205539605
Train-Acc: 4: Accuracy: 0.5377358198: precision: 0.2079797368: recall: 0.6315824075: f1: 0.3129163070
5: 3232: loss: 0.6891869843:
5: 6432: loss: 0.6891230947:
5: 9632: loss: 0.6890850643:
5: 12832: loss: 0.6891883618:
5: 16032: loss: 0.6891191847:
5: 19232: loss: 0.6892598590:
5: 22432: loss: 0.6891706084:
5: 25632: loss: 0.6893540958:
5: 28832: loss: 0.6889673128:
5: 32032: loss: 0.6890660399:
5: 35232: loss: 0.6891100065:
5: 38432: loss: 0.6890759897:
5: 41632: loss: 0.6888600474:
5: 44832: loss: 0.6887462249:
5: 48032: loss: 0.6887174742:
5: 51232: loss: 0.6886887491:
5: 54432: loss: 0.6885853695:
5: 57632: loss: 0.6884799316:
5: 60832: loss: 0.6883380776:
5: 64032: loss: 0.6882965568:
5: 67232: loss: 0.6882013745:
5: 70432: loss: 0.6881408641:
5: 73632: loss: 0.6880935542:
5: 76832: loss: 0.6880148658:
5: 80032: loss: 0.6879885205:
5: 83232: loss: 0.6879591542:
5: 86432: loss: 0.6878694106:
5: 89632: loss: 0.6877619766:
Dev-Acc: 5: Accuracy: 0.5488867164: precision: 0.0659458748: recall: 0.5113076007: f1: 0.1168243361
Train-Acc: 5: Accuracy: 0.5931781530: precision: 0.2182776350: recall: 0.5582144501: f1: 0.3138363734
6: 3232: loss: 0.6851919854:
6: 6432: loss: 0.6854542014:
6: 9632: loss: 0.6854539126:
6: 12832: loss: 0.6850719005:
6: 16032: loss: 0.6850366510:
6: 19232: loss: 0.6852282179:
6: 22432: loss: 0.6854489279:
6: 25632: loss: 0.6851916192:
6: 28832: loss: 0.6851414849:
6: 32032: loss: 0.6849445171:
6: 35232: loss: 0.6848909363:
6: 38432: loss: 0.6848843830:
6: 41632: loss: 0.6849425034:
6: 44832: loss: 0.6848373325:
6: 48032: loss: 0.6847523508:
6: 51232: loss: 0.6847418067:
6: 54432: loss: 0.6846430291:
6: 57632: loss: 0.6847689552:
6: 60832: loss: 0.6846168644:
6: 64032: loss: 0.6845599497:
6: 67232: loss: 0.6845156534:
6: 70432: loss: 0.6844529742:
6: 73632: loss: 0.6843823936:
6: 76832: loss: 0.6842953932:
6: 80032: loss: 0.6842388084:
6: 83232: loss: 0.6841639046:
6: 86432: loss: 0.6841243251:
6: 89632: loss: 0.6840069704:
Dev-Acc: 6: Accuracy: 0.6136986017: precision: 0.0669287212: recall: 0.4342798844: f1: 0.1159828342
Train-Acc: 6: Accuracy: 0.6472070813: precision: 0.2338211790: recall: 0.4905002958: f1: 0.3166808149
7: 3232: loss: 0.6818920869:
7: 6432: loss: 0.6815816143:
7: 9632: loss: 0.6819603572:
7: 12832: loss: 0.6817665347:
7: 16032: loss: 0.6818809491:
7: 19232: loss: 0.6816569095:
7: 22432: loss: 0.6816387952:
7: 25632: loss: 0.6815729866:
7: 28832: loss: 0.6814992908:
7: 32032: loss: 0.6813876743:
7: 35232: loss: 0.6813517913:
7: 38432: loss: 0.6811955766:
7: 41632: loss: 0.6811678148:
7: 44832: loss: 0.6811136028:
7: 48032: loss: 0.6810184990:
7: 51232: loss: 0.6809287143:
7: 54432: loss: 0.6809386576:
7: 57632: loss: 0.6808474977:
7: 60832: loss: 0.6808226641:
7: 64032: loss: 0.6807597573:
7: 67232: loss: 0.6806588247:
7: 70432: loss: 0.6805804683:
7: 73632: loss: 0.6804823967:
7: 76832: loss: 0.6803873007:
7: 80032: loss: 0.6802924216:
7: 83232: loss: 0.6801954147:
7: 86432: loss: 0.6801242224:
7: 89632: loss: 0.6800823511:
Dev-Acc: 7: Accuracy: 0.6685088873: precision: 0.0690670006: recall: 0.3751062744: f1: 0.1166547685
Train-Acc: 7: Accuracy: 0.6932811737: precision: 0.2531288628: recall: 0.4308066531: f1: 0.3188885374
8: 3232: loss: 0.6784006095:
8: 6432: loss: 0.6783334479:
8: 9632: loss: 0.6782857863:
8: 12832: loss: 0.6780722390:
8: 16032: loss: 0.6780176712:
8: 19232: loss: 0.6778530677:
8: 22432: loss: 0.6776515365:
8: 25632: loss: 0.6775596640:
8: 28832: loss: 0.6774696107:
8: 32032: loss: 0.6774298224:
8: 35232: loss: 0.6773756264:
8: 38432: loss: 0.6774195079:
8: 41632: loss: 0.6773170004:
8: 44832: loss: 0.6773110973:
8: 48032: loss: 0.6772181090:
8: 51232: loss: 0.6771807911:
8: 54432: loss: 0.6771466687:
8: 57632: loss: 0.6770494629:
8: 60832: loss: 0.6769601751:
8: 64032: loss: 0.6769696716:
8: 67232: loss: 0.6768870158:
8: 70432: loss: 0.6768135670:
8: 73632: loss: 0.6767810400:
8: 76832: loss: 0.6767245293:
8: 80032: loss: 0.6766491530:
8: 83232: loss: 0.6765806930:
8: 86432: loss: 0.6765256638:
8: 89632: loss: 0.6764217299:
Dev-Acc: 8: Accuracy: 0.7213049531: precision: 0.0723666474: recall: 0.3195034858: f1: 0.1180054010
Train-Acc: 8: Accuracy: 0.7320688963: precision: 0.2773226677: recall: 0.3783446190: f1: 0.3200511637
9: 3232: loss: 0.6745602393:
9: 6432: loss: 0.6743689066:
9: 9632: loss: 0.6742606576:
9: 12832: loss: 0.6740176833:
9: 16032: loss: 0.6742537352:
9: 19232: loss: 0.6739754751:
9: 22432: loss: 0.6739328744:
9: 25632: loss: 0.6738797221:
9: 28832: loss: 0.6739396304:
9: 32032: loss: 0.6738680283:
9: 35232: loss: 0.6737093122:
9: 38432: loss: 0.6736388974:
9: 41632: loss: 0.6736019878:
9: 44832: loss: 0.6735974363:
9: 48032: loss: 0.6734885138:
9: 51232: loss: 0.6734231655:
9: 54432: loss: 0.6733594762:
9: 57632: loss: 0.6732609301:
9: 60832: loss: 0.6731858514:
9: 64032: loss: 0.6731446287:
9: 67232: loss: 0.6730774850:
9: 70432: loss: 0.6729438182:
9: 73632: loss: 0.6728709939:
9: 76832: loss: 0.6728099864:
9: 80032: loss: 0.6727394078:
9: 83232: loss: 0.6726612988:
9: 86432: loss: 0.6726030335:
9: 89632: loss: 0.6725135743:
Dev-Acc: 9: Accuracy: 0.7640895247: precision: 0.0754045461: recall: 0.2701921442: f1: 0.1179045782
Train-Acc: 9: Accuracy: 0.7648521662: precision: 0.3094744543: recall: 0.3337058708: f1: 0.3211337108
10: 3232: loss: 0.6703590727:
10: 6432: loss: 0.6704025707:
10: 9632: loss: 0.6707262017:
10: 12832: loss: 0.6706419271:
10: 16032: loss: 0.6704901409:
10: 19232: loss: 0.6701500059:
10: 22432: loss: 0.6699324311:
10: 25632: loss: 0.6699969175:
10: 28832: loss: 0.6700258960:
10: 32032: loss: 0.6699229226:
10: 35232: loss: 0.6698905272:
10: 38432: loss: 0.6698018844:
10: 41632: loss: 0.6697292793:
10: 44832: loss: 0.6696720457:
10: 48032: loss: 0.6695899167:
10: 51232: loss: 0.6695123583:
10: 54432: loss: 0.6694023393:
10: 57632: loss: 0.6693685175:
10: 60832: loss: 0.6693263746:
10: 64032: loss: 0.6692123755:
10: 67232: loss: 0.6691336591:
10: 70432: loss: 0.6690971425:
10: 73632: loss: 0.6689755405:
10: 76832: loss: 0.6689544672:
10: 80032: loss: 0.6688641872:
10: 83232: loss: 0.6688024067:
10: 86432: loss: 0.6687074234:
10: 89632: loss: 0.6686710206:
Dev-Acc: 10: Accuracy: 0.7985394597: precision: 0.0800186361: recall: 0.2336337358: f1: 0.1192087454
Train-Acc: 10: Accuracy: 0.7873249650: precision: 0.3391311011: recall: 0.2909736375: f1: 0.3132120869
11: 3232: loss: 0.6659754694:
11: 6432: loss: 0.6658377534:
11: 9632: loss: 0.6660586119:
11: 12832: loss: 0.6663258812:
11: 16032: loss: 0.6664422615:
11: 19232: loss: 0.6660442912:
11: 22432: loss: 0.6661181013:
11: 25632: loss: 0.6662893739:
11: 28832: loss: 0.6664482507:
11: 32032: loss: 0.6662043155:
11: 35232: loss: 0.6661878307:
11: 38432: loss: 0.6661141942:
11: 41632: loss: 0.6660457910:
11: 44832: loss: 0.6659442840:
11: 48032: loss: 0.6658714584:
11: 51232: loss: 0.6658326696:
11: 54432: loss: 0.6658446166:
11: 57632: loss: 0.6657191003:
11: 60832: loss: 0.6656524086:
11: 64032: loss: 0.6655557419:
11: 67232: loss: 0.6655692480:
11: 70432: loss: 0.6654984124:
11: 73632: loss: 0.6654568455:
11: 76832: loss: 0.6654153032:
11: 80032: loss: 0.6653582476:
11: 83232: loss: 0.6653001408:
11: 86432: loss: 0.6652084840:
11: 89632: loss: 0.6651153707:
Dev-Acc: 11: Accuracy: 0.8269268870: precision: 0.0838612151: recall: 0.1980955620: f1: 0.1178374551
Train-Acc: 11: Accuracy: 0.8045821786: precision: 0.3737976145: recall: 0.2554730130: f1: 0.3035107588
12: 3232: loss: 0.6651339346:
12: 6432: loss: 0.6644777527:
12: 9632: loss: 0.6643697705:
12: 12832: loss: 0.6639927867:
12: 16032: loss: 0.6635511855:
12: 19232: loss: 0.6632165361:
12: 22432: loss: 0.6631366538:
12: 25632: loss: 0.6628278048:
12: 28832: loss: 0.6627525910:
12: 32032: loss: 0.6627634992:
12: 35232: loss: 0.6627554598:
12: 38432: loss: 0.6625572628:
12: 41632: loss: 0.6624735177:
12: 44832: loss: 0.6624031038:
12: 48032: loss: 0.6622925427:
12: 51232: loss: 0.6622343861:
12: 54432: loss: 0.6621190443:
12: 57632: loss: 0.6619998448:
12: 60832: loss: 0.6619145291:
12: 64032: loss: 0.6618545168:
12: 67232: loss: 0.6616986242:
12: 70432: loss: 0.6617228351:
12: 73632: loss: 0.6616705040:
12: 76832: loss: 0.6616347623:
12: 80032: loss: 0.6615740927:
12: 83232: loss: 0.6615218359:
12: 86432: loss: 0.6614933663:
12: 89632: loss: 0.6614499636:
Dev-Acc: 12: Accuracy: 0.8525857329: precision: 0.0926665457: recall: 0.1736099303: f1: 0.1208355524
Train-Acc: 12: Accuracy: 0.8178182244: precision: 0.4135953136: recall: 0.2227992900: f1: 0.2895962401
13: 3232: loss: 0.6591843456:
13: 6432: loss: 0.6595903549:
13: 9632: loss: 0.6598249165:
13: 12832: loss: 0.6595611203:
13: 16032: loss: 0.6594525094:
13: 19232: loss: 0.6593222946:
13: 22432: loss: 0.6592513540:
13: 25632: loss: 0.6592951353:
13: 28832: loss: 0.6593405703:
13: 32032: loss: 0.6592736697:
13: 35232: loss: 0.6591437921:
13: 38432: loss: 0.6591806891:
13: 41632: loss: 0.6589854918:
13: 44832: loss: 0.6587131363:
13: 48032: loss: 0.6586916938:
13: 51232: loss: 0.6586306067:
13: 54432: loss: 0.6584870274:
13: 57632: loss: 0.6585031136:
13: 60832: loss: 0.6584335844:
13: 64032: loss: 0.6583652173:
13: 67232: loss: 0.6582712994:
13: 70432: loss: 0.6582430321:
13: 73632: loss: 0.6581263748:
13: 76832: loss: 0.6580792965:
13: 80032: loss: 0.6580834984:
13: 83232: loss: 0.6580171223:
13: 86432: loss: 0.6579567386:
13: 89632: loss: 0.6578826138:
Dev-Acc: 13: Accuracy: 0.8731941581: precision: 0.1017203556: recall: 0.1498044550: f1: 0.1211662770
Train-Acc: 13: Accuracy: 0.8272522092: precision: 0.4569567241: recall: 0.1936756295: f1: 0.2720472805
14: 3232: loss: 0.6533182043:
14: 6432: loss: 0.6544787636:
14: 9632: loss: 0.6547992514:
14: 12832: loss: 0.6547747195:
14: 16032: loss: 0.6548829006:
14: 19232: loss: 0.6548501518:
14: 22432: loss: 0.6551791922:
14: 25632: loss: 0.6550982203:
14: 28832: loss: 0.6551821405:
14: 32032: loss: 0.6551439039:
14: 35232: loss: 0.6551080067:
14: 38432: loss: 0.6550396582:
14: 41632: loss: 0.6549435165:
14: 44832: loss: 0.6549014804:
14: 48032: loss: 0.6547529316:
14: 51232: loss: 0.6548100338:
14: 54432: loss: 0.6547182141:
14: 57632: loss: 0.6547071818:
14: 60832: loss: 0.6547592096:
14: 64032: loss: 0.6547416160:
14: 67232: loss: 0.6545726353:
14: 70432: loss: 0.6545142556:
14: 73632: loss: 0.6544548534:
14: 76832: loss: 0.6544344067:
14: 80032: loss: 0.6544064280:
14: 83232: loss: 0.6543016778:
14: 86432: loss: 0.6542228317:
14: 89632: loss: 0.6541542237:
Dev-Acc: 14: Accuracy: 0.8897741437: precision: 0.1131991714: recall: 0.1300799184: f1: 0.1210538808
Train-Acc: 14: Accuracy: 0.8333880901: precision: 0.5004973145: recall: 0.1654066136: f1: 0.2486411701
15: 3232: loss: 0.6526090115:
15: 6432: loss: 0.6523962232:
15: 9632: loss: 0.6520564628:
15: 12832: loss: 0.6520491736:
15: 16032: loss: 0.6519644164:
15: 19232: loss: 0.6519635160:
15: 22432: loss: 0.6519121902:
15: 25632: loss: 0.6518908383:
15: 28832: loss: 0.6518053485:
15: 32032: loss: 0.6518871616:
15: 35232: loss: 0.6519529968:
15: 38432: loss: 0.6518858648:
15: 41632: loss: 0.6516472695:
15: 44832: loss: 0.6516527776:
15: 48032: loss: 0.6516286642:
15: 51232: loss: 0.6514684103:
15: 54432: loss: 0.6514051984:
15: 57632: loss: 0.6513306466:
15: 60832: loss: 0.6513373779:
15: 64032: loss: 0.6512798609:
15: 67232: loss: 0.6512169699:
15: 70432: loss: 0.6511793788:
15: 73632: loss: 0.6510961214:
15: 76832: loss: 0.6510784548:
15: 80032: loss: 0.6510382028:
15: 83232: loss: 0.6509887191:
15: 86432: loss: 0.6509433724:
15: 89632: loss: 0.6508380439:
Dev-Acc: 15: Accuracy: 0.9038140774: precision: 0.1285797779: recall: 0.1122258119: f1: 0.1198474669
Train-Acc: 15: Accuracy: 0.8369710445: precision: 0.5407461954: recall: 0.1448293998: f1: 0.2284677210
16: 3232: loss: 0.6488416201:
16: 6432: loss: 0.6491555041:
16: 9632: loss: 0.6493087097:
16: 12832: loss: 0.6490113133:
16: 16032: loss: 0.6489461451:
16: 19232: loss: 0.6488875724:
16: 22432: loss: 0.6488034730:
16: 25632: loss: 0.6488139617:
16: 28832: loss: 0.6485506606:
16: 32032: loss: 0.6484210842:
16: 35232: loss: 0.6482844631:
16: 38432: loss: 0.6480778697:
16: 41632: loss: 0.6481422263:
16: 44832: loss: 0.6480616645:
16: 48032: loss: 0.6480167479:
16: 51232: loss: 0.6479427808:
16: 54432: loss: 0.6479510504:
16: 57632: loss: 0.6477204254:
16: 60832: loss: 0.6475364027:
16: 64032: loss: 0.6475138375:
16: 67232: loss: 0.6475139145:
16: 70432: loss: 0.6474856925:
16: 73632: loss: 0.6474361947:
16: 76832: loss: 0.6474038991:
16: 80032: loss: 0.6472647398:
16: 83232: loss: 0.6472307273:
16: 86432: loss: 0.6471685034:
16: 89632: loss: 0.6470997625:
Dev-Acc: 16: Accuracy: 0.9144308567: precision: 0.1429315282: recall: 0.0933514708: f1: 0.1129397243
Train-Acc: 16: Accuracy: 0.8391515017: precision: 0.5805277525: recall: 0.1258299915: f1: 0.2068294791
17: 3232: loss: 0.6458969396:
17: 6432: loss: 0.6451864910:
17: 9632: loss: 0.6447341569:
17: 12832: loss: 0.6449542989:
17: 16032: loss: 0.6449171969:
17: 19232: loss: 0.6452741345:
17: 22432: loss: 0.6450600162:
17: 25632: loss: 0.6450174934:
17: 28832: loss: 0.6450551371:
17: 32032: loss: 0.6449637476:
17: 35232: loss: 0.6447553307:
17: 38432: loss: 0.6448309565:
17: 41632: loss: 0.6448095598:
17: 44832: loss: 0.6447352472:
17: 48032: loss: 0.6447053423:
17: 51232: loss: 0.6446852300:
17: 54432: loss: 0.6445660128:
17: 57632: loss: 0.6444783529:
17: 60832: loss: 0.6442752745:
17: 64032: loss: 0.6442160050:
17: 67232: loss: 0.6441549085:
17: 70432: loss: 0.6440457807:
17: 73632: loss: 0.6439783972:
17: 76832: loss: 0.6438825180:
17: 80032: loss: 0.6437763913:
17: 83232: loss: 0.6437542595:
17: 86432: loss: 0.6436424905:
17: 89632: loss: 0.6435635643:
Dev-Acc: 17: Accuracy: 0.9222594500: precision: 0.1598189415: recall: 0.0780479510: f1: 0.1048783274
Train-Acc: 17: Accuracy: 0.8397650719: precision: 0.6112163698: recall: 0.1060416804: f1: 0.1807282913
18: 3232: loss: 0.6427013201:
18: 6432: loss: 0.6422689164:
18: 9632: loss: 0.6417334535:
18: 12832: loss: 0.6414412607:
18: 16032: loss: 0.6412794859:
18: 19232: loss: 0.6413373333:
18: 22432: loss: 0.6412262371:
18: 25632: loss: 0.6410537801:
18: 28832: loss: 0.6407584325:
18: 32032: loss: 0.6409393260:
18: 35232: loss: 0.6409488685:
18: 38432: loss: 0.6409513040:
18: 41632: loss: 0.6407179298:
18: 44832: loss: 0.6407179586:
18: 48032: loss: 0.6406922632:
18: 51232: loss: 0.6407533868:
18: 54432: loss: 0.6406734669:
18: 57632: loss: 0.6407123863:
18: 60832: loss: 0.6406610145:
18: 64032: loss: 0.6405879344:
18: 67232: loss: 0.6405270003:
18: 70432: loss: 0.6404565535:
18: 73632: loss: 0.6404014210:
18: 76832: loss: 0.6403702068:
18: 80032: loss: 0.6403526843:
18: 83232: loss: 0.6402694891:
18: 86432: loss: 0.6401682530:
18: 89632: loss: 0.6401548092:
Dev-Acc: 18: Accuracy: 0.9278853536: precision: 0.1689737470: recall: 0.0601938446: f1: 0.0887662989
Train-Acc: 18: Accuracy: 0.8399294019: precision: 0.6403917910: recall: 0.0902636250: f1: 0.1582252953
19: 3232: loss: 0.6380985892:
19: 6432: loss: 0.6384464514:
19: 9632: loss: 0.6386836942:
19: 12832: loss: 0.6382880861:
19: 16032: loss: 0.6379745512:
19: 19232: loss: 0.6380825081:
19: 22432: loss: 0.6382886284:
19: 25632: loss: 0.6384125659:
19: 28832: loss: 0.6381424922:
19: 32032: loss: 0.6381400690:
19: 35232: loss: 0.6380116588:
19: 38432: loss: 0.6379786165:
19: 41632: loss: 0.6378142101:
19: 44832: loss: 0.6378153600:
19: 48032: loss: 0.6378084355:
19: 51232: loss: 0.6376615023:
19: 54432: loss: 0.6375518469:
19: 57632: loss: 0.6374877949:
19: 60832: loss: 0.6374270490:
19: 64032: loss: 0.6373862213:
19: 67232: loss: 0.6373442271:
19: 70432: loss: 0.6372344070:
19: 73632: loss: 0.6372225842:
19: 76832: loss: 0.6371990300:
19: 80032: loss: 0.6371811727:
19: 83232: loss: 0.6370020376:
19: 86432: loss: 0.6368705846:
19: 89632: loss: 0.6367954703:
Dev-Acc: 19: Accuracy: 0.9319633842: precision: 0.1693766938: recall: 0.0425097772: f1: 0.0679624847
Train-Acc: 19: Accuracy: 0.8392610550: precision: 0.6544831525: recall: 0.0753402143: f1: 0.1351255748
20: 3232: loss: 0.6369333839:
20: 6432: loss: 0.6361085880:
20: 9632: loss: 0.6360801591:
20: 12832: loss: 0.6354996596:
20: 16032: loss: 0.6353284351:
20: 19232: loss: 0.6349088453:
20: 22432: loss: 0.6351142676:
20: 25632: loss: 0.6347047585:
20: 28832: loss: 0.6346219656:
20: 32032: loss: 0.6346766416:
20: 35232: loss: 0.6345641035:
20: 38432: loss: 0.6341086137:
20: 41632: loss: 0.6341150409:
20: 44832: loss: 0.6339528426:
20: 48032: loss: 0.6337422159:
20: 51232: loss: 0.6337832893:
20: 54432: loss: 0.6336931722:
20: 57632: loss: 0.6337243697:
20: 60832: loss: 0.6338017596:
20: 64032: loss: 0.6337630827:
20: 67232: loss: 0.6336844701:
20: 70432: loss: 0.6336575648:
20: 73632: loss: 0.6335769282:
20: 76832: loss: 0.6335599609:
20: 80032: loss: 0.6334183402:
20: 83232: loss: 0.6333196415:
20: 86432: loss: 0.6332480827:
20: 89632: loss: 0.6331558858:
Dev-Acc: 20: Accuracy: 0.9347515106: precision: 0.1549155909: recall: 0.0265261010: f1: 0.0452961672
Train-Acc: 20: Accuracy: 0.8392938972: precision: 0.6926345609: recall: 0.0642955756: f1: 0.1176682909
