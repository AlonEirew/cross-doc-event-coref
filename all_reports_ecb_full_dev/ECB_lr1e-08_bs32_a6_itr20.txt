1: 3232: loss: 0.7075732249:
1: 6432: loss: 0.7077536342:
1: 9632: loss: 0.7072969542:
1: 12832: loss: 0.7072042619:
1: 16032: loss: 0.7068847362:
1: 19232: loss: 0.7067551628:
1: 22432: loss: 0.7068582615:
1: 25632: loss: 0.7068713292:
1: 28832: loss: 0.7067916116:
1: 32032: loss: 0.7065266407:
1: 35232: loss: 0.7064424167:
1: 38432: loss: 0.7064412778:
1: 41632: loss: 0.7062851976:
1: 44832: loss: 0.7062137880:
1: 48032: loss: 0.7060474991:
1: 51232: loss: 0.7060350944:
1: 54432: loss: 0.7059099738:
1: 57632: loss: 0.7059236844:
1: 60832: loss: 0.7058473303:
1: 64032: loss: 0.7057985997:
1: 67232: loss: 0.7057947859:
1: 70432: loss: 0.7056959733:
1: 73632: loss: 0.7056496971:
1: 76832: loss: 0.7055585069:
1: 80032: loss: 0.7054700579:
1: 83232: loss: 0.7053928326:
1: 86432: loss: 0.7053655209:
1: 89632: loss: 0.7052924117:
1: 92832: loss: 0.7052273541:
1: 96032: loss: 0.7050980343:
1: 99232: loss: 0.7049679400:
1: 102432: loss: 0.7048617451:
1: 105632: loss: 0.7048201337:
Dev-Acc: 1: Accuracy: 0.3148416281: precision: 0.0644511859: recall: 0.7947627954: f1: 0.1192331730
Train-Acc: 1: Accuracy: 0.3782976568: precision: 0.1613665949: recall: 0.7986325685: f1: 0.2684848552
2: 3232: loss: 0.7031524056:
2: 6432: loss: 0.7029442182:
2: 9632: loss: 0.7025769248:
2: 12832: loss: 0.7024378371:
2: 16032: loss: 0.7023710139:
2: 19232: loss: 0.7022222203:
2: 22432: loss: 0.7021632128:
2: 25632: loss: 0.7020082053:
2: 28832: loss: 0.7018119703:
2: 32032: loss: 0.7016524225:
2: 35232: loss: 0.7016321873:
2: 38432: loss: 0.7015343058:
2: 41632: loss: 0.7014582454:
2: 44832: loss: 0.7012970363:
2: 48032: loss: 0.7011472830:
2: 51232: loss: 0.7011600789:
2: 54432: loss: 0.7010934595:
2: 57632: loss: 0.7010010681:
2: 60832: loss: 0.7009219210:
2: 64032: loss: 0.7008134876:
2: 67232: loss: 0.7006840886:
2: 70432: loss: 0.7006348201:
2: 73632: loss: 0.7005293073:
2: 76832: loss: 0.7004575001:
2: 80032: loss: 0.7003861717:
2: 83232: loss: 0.7003077264:
2: 86432: loss: 0.7001579374:
2: 89632: loss: 0.7000353777:
2: 92832: loss: 0.6999406447:
2: 96032: loss: 0.6998854252:
2: 99232: loss: 0.6998293575:
2: 102432: loss: 0.6997576645:
2: 105632: loss: 0.6996994083:
Dev-Acc: 2: Accuracy: 0.3880675435: precision: 0.0665091994: recall: 0.7277673865: f1: 0.1218800279
Train-Acc: 2: Accuracy: 0.4396724105: precision: 0.1667391402: recall: 0.7310498981: f1: 0.2715440404
3: 3232: loss: 0.6969066429:
3: 6432: loss: 0.6968079323:
3: 9632: loss: 0.6967757752:
3: 12832: loss: 0.6965471508:
3: 16032: loss: 0.6964166071:
3: 19232: loss: 0.6964073006:
3: 22432: loss: 0.6963095179:
3: 25632: loss: 0.6964384677:
3: 28832: loss: 0.6964559713:
3: 32032: loss: 0.6963523440:
3: 35232: loss: 0.6963296144:
3: 38432: loss: 0.6961772323:
3: 41632: loss: 0.6961339088:
3: 44832: loss: 0.6960413959:
3: 48032: loss: 0.6958814748:
3: 51232: loss: 0.6957760311:
3: 54432: loss: 0.6957314126:
3: 57632: loss: 0.6956239996:
3: 60832: loss: 0.6955419454:
3: 64032: loss: 0.6954473847:
3: 67232: loss: 0.6953767116:
3: 70432: loss: 0.6953293781:
3: 73632: loss: 0.6952731509:
3: 76832: loss: 0.6951680830:
3: 80032: loss: 0.6951018262:
3: 83232: loss: 0.6950624589:
3: 86432: loss: 0.6950382316:
3: 89632: loss: 0.6950021305:
3: 92832: loss: 0.6949532779:
3: 96032: loss: 0.6949017908:
3: 99232: loss: 0.6948140392:
3: 102432: loss: 0.6947387100:
3: 105632: loss: 0.6946607997:
Dev-Acc: 3: Accuracy: 0.4661454260: precision: 0.0657811282: recall: 0.6172419657: f1: 0.1188916547
Train-Acc: 3: Accuracy: 0.5077152848: precision: 0.1722631338: recall: 0.6428242719: f1: 0.2717130035
4: 3232: loss: 0.6914215446:
4: 6432: loss: 0.6914752609:
4: 9632: loss: 0.6916869374:
4: 12832: loss: 0.6918915538:
4: 16032: loss: 0.6917681448:
4: 19232: loss: 0.6916048681:
4: 22432: loss: 0.6914813811:
4: 25632: loss: 0.6914853868:
4: 28832: loss: 0.6914567259:
4: 32032: loss: 0.6913967100:
4: 35232: loss: 0.6911800502:
4: 38432: loss: 0.6911067808:
4: 41632: loss: 0.6910271143:
4: 44832: loss: 0.6909268008:
4: 48032: loss: 0.6907934939:
4: 51232: loss: 0.6906978496:
4: 54432: loss: 0.6906254929:
4: 57632: loss: 0.6905560874:
4: 60832: loss: 0.6904377598:
4: 64032: loss: 0.6903994624:
4: 67232: loss: 0.6903737881:
4: 70432: loss: 0.6903023548:
4: 73632: loss: 0.6901585199:
4: 76832: loss: 0.6900608618:
4: 80032: loss: 0.6900065152:
4: 83232: loss: 0.6899206127:
4: 86432: loss: 0.6898486639:
4: 89632: loss: 0.6897581138:
4: 92832: loss: 0.6896898192:
4: 96032: loss: 0.6895681869:
4: 99232: loss: 0.6894941282:
4: 102432: loss: 0.6894149580:
4: 105632: loss: 0.6893524652:
Dev-Acc: 4: Accuracy: 0.5467236638: precision: 0.0639761623: recall: 0.4965141983: f1: 0.1133474371
Train-Acc: 4: Accuracy: 0.5776458979: precision: 0.1794623239: recall: 0.5476957465: f1: 0.2703421868
5: 3232: loss: 0.6862489581:
5: 6432: loss: 0.6866798225:
5: 9632: loss: 0.6865498712:
5: 12832: loss: 0.6865510133:
5: 16032: loss: 0.6865963849:
5: 19232: loss: 0.6869398690:
5: 22432: loss: 0.6869392943:
5: 25632: loss: 0.6867530200:
5: 28832: loss: 0.6867037294:
5: 32032: loss: 0.6866286309:
5: 35232: loss: 0.6866300950:
5: 38432: loss: 0.6864895010:
5: 41632: loss: 0.6862754060:
5: 44832: loss: 0.6861234122:
5: 48032: loss: 0.6860525268:
5: 51232: loss: 0.6860130171:
5: 54432: loss: 0.6859443798:
5: 57632: loss: 0.6858893108:
5: 60832: loss: 0.6858417924:
5: 64032: loss: 0.6857734614:
5: 67232: loss: 0.6857175555:
5: 70432: loss: 0.6856094840:
5: 73632: loss: 0.6855536938:
5: 76832: loss: 0.6854822758:
5: 80032: loss: 0.6854090753:
5: 83232: loss: 0.6853272637:
5: 86432: loss: 0.6852402976:
5: 89632: loss: 0.6851797465:
5: 92832: loss: 0.6851284664:
5: 96032: loss: 0.6850390082:
5: 99232: loss: 0.6849341656:
5: 102432: loss: 0.6848647717:
5: 105632: loss: 0.6847595571:
Dev-Acc: 5: Accuracy: 0.6249504089: precision: 0.0651972537: recall: 0.4069035878: f1: 0.1123869907
Train-Acc: 5: Accuracy: 0.6432938576: precision: 0.1921808252: recall: 0.4672934061: f1: 0.2723528172
6: 3232: loss: 0.6824801177:
6: 6432: loss: 0.6827361426:
6: 9632: loss: 0.6828021417:
6: 12832: loss: 0.6827460346:
6: 16032: loss: 0.6825832953:
6: 19232: loss: 0.6822470218:
6: 22432: loss: 0.6820594658:
6: 25632: loss: 0.6819342454:
6: 28832: loss: 0.6817457472:
6: 32032: loss: 0.6817706974:
6: 35232: loss: 0.6817547070:
6: 38432: loss: 0.6816448048:
6: 41632: loss: 0.6815298714:
6: 44832: loss: 0.6813761839:
6: 48032: loss: 0.6812657419:
6: 51232: loss: 0.6812070752:
6: 54432: loss: 0.6811450891:
6: 57632: loss: 0.6810074236:
6: 60832: loss: 0.6809088247:
6: 64032: loss: 0.6808460457:
6: 67232: loss: 0.6808323983:
6: 70432: loss: 0.6806807488:
6: 73632: loss: 0.6806042041:
6: 76832: loss: 0.6805554574:
6: 80032: loss: 0.6804833866:
6: 83232: loss: 0.6803955745:
6: 86432: loss: 0.6803483733:
6: 89632: loss: 0.6802773736:
6: 92832: loss: 0.6802130419:
6: 96032: loss: 0.6801517477:
6: 99232: loss: 0.6800843078:
6: 102432: loss: 0.6800027189:
6: 105632: loss: 0.6799612017:
Dev-Acc: 6: Accuracy: 0.6924313307: precision: 0.0679167383: recall: 0.3356572012: f1: 0.1129743032
Train-Acc: 6: Accuracy: 0.7005081177: precision: 0.2098469033: recall: 0.3964893827: f1: 0.2744419922
7: 3232: loss: 0.6784815502:
7: 6432: loss: 0.6774251977:
7: 9632: loss: 0.6780376995:
7: 12832: loss: 0.6775567460:
7: 16032: loss: 0.6774297935:
7: 19232: loss: 0.6773250475:
7: 22432: loss: 0.6773917426:
7: 25632: loss: 0.6772436201:
7: 28832: loss: 0.6771429064:
7: 32032: loss: 0.6769285133:
7: 35232: loss: 0.6769802827:
7: 38432: loss: 0.6768344966:
7: 41632: loss: 0.6767424321:
7: 44832: loss: 0.6767442682:
7: 48032: loss: 0.6765940926:
7: 51232: loss: 0.6765104919:
7: 54432: loss: 0.6764292512:
7: 57632: loss: 0.6763379200:
7: 60832: loss: 0.6762466978:
7: 64032: loss: 0.6761568383:
7: 67232: loss: 0.6761041818:
7: 70432: loss: 0.6760289783:
7: 73632: loss: 0.6759264616:
7: 76832: loss: 0.6758588003:
7: 80032: loss: 0.6757314235:
7: 83232: loss: 0.6756459808:
7: 86432: loss: 0.6755281968:
7: 89632: loss: 0.6754519315:
7: 92832: loss: 0.6753781291:
7: 96032: loss: 0.6753038715:
7: 99232: loss: 0.6752557568:
7: 102432: loss: 0.6751815077:
7: 105632: loss: 0.6750907760:
Dev-Acc: 7: Accuracy: 0.7514684796: precision: 0.0704585182: recall: 0.2673014793: f1: 0.1115209989
Train-Acc: 7: Accuracy: 0.7484245300: precision: 0.2339829028: recall: 0.3346919992: f1: 0.2754199464
8: 3232: loss: 0.6727884370:
8: 6432: loss: 0.6727503759:
8: 9632: loss: 0.6725288200:
8: 12832: loss: 0.6725443642:
8: 16032: loss: 0.6724577696:
8: 19232: loss: 0.6723480858:
8: 22432: loss: 0.6724087152:
8: 25632: loss: 0.6723996469:
8: 28832: loss: 0.6721509847:
8: 32032: loss: 0.6721047319:
8: 35232: loss: 0.6719206744:
8: 38432: loss: 0.6717611532:
8: 41632: loss: 0.6717648138:
8: 44832: loss: 0.6716662653:
8: 48032: loss: 0.6716646731:
8: 51232: loss: 0.6715492376:
8: 54432: loss: 0.6714403840:
8: 57632: loss: 0.6713729372:
8: 60832: loss: 0.6712689392:
8: 64032: loss: 0.6712305562:
8: 67232: loss: 0.6711651257:
8: 70432: loss: 0.6710928675:
8: 73632: loss: 0.6710304195:
8: 76832: loss: 0.6708919037:
8: 80032: loss: 0.6708898510:
8: 83232: loss: 0.6708542947:
8: 86432: loss: 0.6707784976:
8: 89632: loss: 0.6707601984:
8: 92832: loss: 0.6706760398:
8: 96032: loss: 0.6705722559:
8: 99232: loss: 0.6705109090:
8: 102432: loss: 0.6704300837:
8: 105632: loss: 0.6703515778:
Dev-Acc: 8: Accuracy: 0.7958406210: precision: 0.0751170994: recall: 0.2208808026: f1: 0.1121083973
Train-Acc: 8: Accuracy: 0.7843666077: precision: 0.2613489375: recall: 0.2789428703: f1: 0.2698594416
9: 3232: loss: 0.6682517076:
9: 6432: loss: 0.6673113662:
9: 9632: loss: 0.6674968831:
9: 12832: loss: 0.6676909073:
9: 16032: loss: 0.6676179365:
9: 19232: loss: 0.6675129433:
9: 22432: loss: 0.6674068516:
9: 25632: loss: 0.6675296586:
9: 28832: loss: 0.6674657167:
9: 32032: loss: 0.6673754612:
9: 35232: loss: 0.6673849630:
9: 38432: loss: 0.6673510566:
9: 41632: loss: 0.6671516359:
9: 44832: loss: 0.6671109022:
9: 48032: loss: 0.6669993908:
9: 51232: loss: 0.6669700627:
9: 54432: loss: 0.6669821342:
9: 57632: loss: 0.6668339784:
9: 60832: loss: 0.6668143670:
9: 64032: loss: 0.6666888410:
9: 67232: loss: 0.6666002899:
9: 70432: loss: 0.6665146488:
9: 73632: loss: 0.6663802728:
9: 76832: loss: 0.6663130294:
9: 80032: loss: 0.6661866461:
9: 83232: loss: 0.6661354461:
9: 86432: loss: 0.6661066233:
9: 89632: loss: 0.6660303221:
9: 92832: loss: 0.6659503703:
9: 96032: loss: 0.6658902414:
9: 99232: loss: 0.6658197535:
9: 102432: loss: 0.6657848123:
9: 105632: loss: 0.6656876697:
Dev-Acc: 9: Accuracy: 0.8317689300: precision: 0.0791273943: recall: 0.1770107125: f1: 0.1093659715
Train-Acc: 9: Accuracy: 0.8108323812: precision: 0.2945587868: recall: 0.2323976070: f1: 0.2598118477
10: 3232: loss: 0.6642858809:
10: 6432: loss: 0.6634679094:
10: 9632: loss: 0.6641358864:
10: 12832: loss: 0.6637395221:
10: 16032: loss: 0.6634417608:
10: 19232: loss: 0.6633022813:
10: 22432: loss: 0.6632277390:
10: 25632: loss: 0.6629941181:
10: 28832: loss: 0.6629750127:
10: 32032: loss: 0.6630123577:
10: 35232: loss: 0.6629136462:
10: 38432: loss: 0.6627323226:
10: 41632: loss: 0.6627682975:
10: 44832: loss: 0.6626464100:
10: 48032: loss: 0.6624404511:
10: 51232: loss: 0.6623378614:
10: 54432: loss: 0.6621812633:
10: 57632: loss: 0.6620932369:
10: 60832: loss: 0.6621163159:
10: 64032: loss: 0.6621347328:
10: 67232: loss: 0.6620827682:
10: 70432: loss: 0.6619731487:
10: 73632: loss: 0.6619346855:
10: 76832: loss: 0.6618384829:
10: 80032: loss: 0.6618526629:
10: 83232: loss: 0.6618040401:
10: 86432: loss: 0.6617327662:
10: 89632: loss: 0.6616818145:
10: 92832: loss: 0.6615685830:
10: 96032: loss: 0.6614727655:
10: 99232: loss: 0.6614128511:
10: 102432: loss: 0.6613224360:
10: 105632: loss: 0.6612360892:
Dev-Acc: 10: Accuracy: 0.8621904254: precision: 0.0887428102: recall: 0.1469137902: f1: 0.1106486521
Train-Acc: 10: Accuracy: 0.8287235498: precision: 0.3284580499: recall: 0.1904542765: f1: 0.2411052391
11: 3232: loss: 0.6583462822:
11: 6432: loss: 0.6571832427:
11: 9632: loss: 0.6583015503:
11: 12832: loss: 0.6580681616:
11: 16032: loss: 0.6579343810:
11: 19232: loss: 0.6578951851:
11: 22432: loss: 0.6580707063:
11: 25632: loss: 0.6582114349:
11: 28832: loss: 0.6581634138:
11: 32032: loss: 0.6581389011:
11: 35232: loss: 0.6580811640:
11: 38432: loss: 0.6581366945:
11: 41632: loss: 0.6580289222:
11: 44832: loss: 0.6578553195:
11: 48032: loss: 0.6577673561:
11: 51232: loss: 0.6577265069:
11: 54432: loss: 0.6577153166:
11: 57632: loss: 0.6576628669:
11: 60832: loss: 0.6576036534:
11: 64032: loss: 0.6575183589:
11: 67232: loss: 0.6574530385:
11: 70432: loss: 0.6573820801:
11: 73632: loss: 0.6572869314:
11: 76832: loss: 0.6572106052:
11: 80032: loss: 0.6571705412:
11: 83232: loss: 0.6571525583:
11: 86432: loss: 0.6571126060:
11: 89632: loss: 0.6569904841:
11: 92832: loss: 0.6569369521:
11: 96032: loss: 0.6568193856:
11: 99232: loss: 0.6567499225:
11: 102432: loss: 0.6566620513:
11: 105632: loss: 0.6566012591:
Dev-Acc: 11: Accuracy: 0.8854976892: precision: 0.0995046001: recall: 0.1195374936: f1: 0.1086049745
Train-Acc: 11: Accuracy: 0.8417686224: precision: 0.3719693415: recall: 0.1563342318: f1: 0.2201444177
12: 3232: loss: 0.6531448656:
12: 6432: loss: 0.6535274002:
12: 9632: loss: 0.6540143216:
12: 12832: loss: 0.6540705147:
12: 16032: loss: 0.6540552768:
12: 19232: loss: 0.6538060480:
12: 22432: loss: 0.6536377882:
12: 25632: loss: 0.6536942916:
12: 28832: loss: 0.6536771407:
12: 32032: loss: 0.6537510606:
12: 35232: loss: 0.6536889313:
12: 38432: loss: 0.6535277865:
12: 41632: loss: 0.6534460675:
12: 44832: loss: 0.6533866836:
12: 48032: loss: 0.6532937123:
12: 51232: loss: 0.6531647155:
12: 54432: loss: 0.6530403545:
12: 57632: loss: 0.6529717245:
12: 60832: loss: 0.6528966546:
12: 64032: loss: 0.6528566189:
12: 67232: loss: 0.6527581419:
12: 70432: loss: 0.6526846184:
12: 73632: loss: 0.6526341991:
12: 76832: loss: 0.6526146241:
12: 80032: loss: 0.6525902692:
12: 83232: loss: 0.6524448039:
12: 86432: loss: 0.6524430962:
12: 89632: loss: 0.6523007366:
12: 92832: loss: 0.6522283740:
12: 96032: loss: 0.6522116990:
12: 99232: loss: 0.6521628950:
12: 102432: loss: 0.6521646234:
12: 105632: loss: 0.6521417472:
Dev-Acc: 12: Accuracy: 0.9032385945: precision: 0.1103281659: recall: 0.0931814317: f1: 0.1010324484
Train-Acc: 12: Accuracy: 0.8500615358: precision: 0.4168137688: recall: 0.1241864440: f1: 0.1913589627
13: 3232: loss: 0.6492598003:
13: 6432: loss: 0.6497149524:
13: 9632: loss: 0.6498901643:
13: 12832: loss: 0.6500527096:
13: 16032: loss: 0.6499006969:
13: 19232: loss: 0.6500266791:
13: 22432: loss: 0.6495414243:
13: 25632: loss: 0.6493805467:
13: 28832: loss: 0.6493740250:
13: 32032: loss: 0.6492638640:
13: 35232: loss: 0.6492117665:
13: 38432: loss: 0.6490432119:
13: 41632: loss: 0.6488731187:
13: 44832: loss: 0.6488581792:
13: 48032: loss: 0.6488250421:
13: 51232: loss: 0.6487612082:
13: 54432: loss: 0.6485906669:
13: 57632: loss: 0.6485679986:
13: 60832: loss: 0.6485708256:
13: 64032: loss: 0.6485120232:
13: 67232: loss: 0.6484266607:
13: 70432: loss: 0.6484178518:
13: 73632: loss: 0.6482961794:
13: 76832: loss: 0.6482641693:
13: 80032: loss: 0.6481821847:
13: 83232: loss: 0.6481281647:
13: 86432: loss: 0.6480240404:
13: 89632: loss: 0.6479533612:
13: 92832: loss: 0.6478447391:
13: 96032: loss: 0.6477762513:
13: 99232: loss: 0.6477092014:
13: 102432: loss: 0.6476674919:
13: 105632: loss: 0.6476236014:
Dev-Acc: 13: Accuracy: 0.9172685742: precision: 0.1239669421: recall: 0.0688658391: f1: 0.0885439440
Train-Acc: 13: Accuracy: 0.8546071053: precision: 0.4582043344: recall: 0.0972980080: f1: 0.1605119028
14: 3232: loss: 0.6447378778:
14: 6432: loss: 0.6446366143:
14: 9632: loss: 0.6451547297:
14: 12832: loss: 0.6449740674:
14: 16032: loss: 0.6450369229:
14: 19232: loss: 0.6449139694:
14: 22432: loss: 0.6450377818:
14: 25632: loss: 0.6449801952:
14: 28832: loss: 0.6449073142:
14: 32032: loss: 0.6447850143:
14: 35232: loss: 0.6446213518:
14: 38432: loss: 0.6444402057:
14: 41632: loss: 0.6443773295:
14: 44832: loss: 0.6442282424:
14: 48032: loss: 0.6440694137:
14: 51232: loss: 0.6440682702:
14: 54432: loss: 0.6441010689:
14: 57632: loss: 0.6440351285:
14: 60832: loss: 0.6439557242:
14: 64032: loss: 0.6439843155:
14: 67232: loss: 0.6440057424:
14: 70432: loss: 0.6439612955:
14: 73632: loss: 0.6438529125:
14: 76832: loss: 0.6437795798:
14: 80032: loss: 0.6437252254:
14: 83232: loss: 0.6436304599:
14: 86432: loss: 0.6436361488:
14: 89632: loss: 0.6435933890:
14: 92832: loss: 0.6435127876:
14: 96032: loss: 0.6433777991:
14: 99232: loss: 0.6433361655:
14: 102432: loss: 0.6432477270:
14: 105632: loss: 0.6431593253:
Dev-Acc: 14: Accuracy: 0.9260696173: precision: 0.1276091082: recall: 0.0457405203: f1: 0.0673425961
Train-Acc: 14: Accuracy: 0.8563915491: precision: 0.4830938292: recall: 0.0751429886: f1: 0.1300563236
15: 3232: loss: 0.6407706898:
15: 6432: loss: 0.6413396958:
15: 9632: loss: 0.6418395307:
15: 12832: loss: 0.6416705622:
15: 16032: loss: 0.6405573307:
15: 19232: loss: 0.6406830938:
15: 22432: loss: 0.6405120644:
15: 25632: loss: 0.6403033973:
15: 28832: loss: 0.6402812093:
15: 32032: loss: 0.6403215926:
15: 35232: loss: 0.6403939925:
15: 38432: loss: 0.6403961594:
15: 41632: loss: 0.6401751095:
15: 44832: loss: 0.6402458736:
15: 48032: loss: 0.6401143800:
15: 51232: loss: 0.6399343996:
15: 54432: loss: 0.6398805760:
15: 57632: loss: 0.6398371991:
15: 60832: loss: 0.6396829154:
15: 64032: loss: 0.6396448809:
15: 67232: loss: 0.6394571813:
15: 70432: loss: 0.6393836817:
15: 73632: loss: 0.6393824347:
15: 76832: loss: 0.6393041197:
15: 80032: loss: 0.6392720153:
15: 83232: loss: 0.6392531931:
15: 86432: loss: 0.6391714339:
15: 89632: loss: 0.6390553468:
15: 92832: loss: 0.6389703103:
15: 96032: loss: 0.6388902654:
15: 99232: loss: 0.6388343696:
15: 102432: loss: 0.6387888873:
15: 105632: loss: 0.6387361214:
Dev-Acc: 15: Accuracy: 0.9314970374: precision: 0.1122062168: recall: 0.0251657881: f1: 0.0411111111
Train-Acc: 15: Accuracy: 0.8572931290: precision: 0.5048192771: recall: 0.0550917099: f1: 0.0993420663
16: 3232: loss: 0.6355822414:
16: 6432: loss: 0.6358474481:
16: 9632: loss: 0.6359373317:
16: 12832: loss: 0.6366259101:
16: 16032: loss: 0.6364346367:
16: 19232: loss: 0.6362729433:
16: 22432: loss: 0.6361806447:
16: 25632: loss: 0.6362011522:
16: 28832: loss: 0.6361480206:
16: 32032: loss: 0.6362100522:
16: 35232: loss: 0.6360752833:
16: 38432: loss: 0.6359708221:
16: 41632: loss: 0.6357592977:
16: 44832: loss: 0.6357066596:
16: 48032: loss: 0.6356136848:
16: 51232: loss: 0.6353856956:
16: 54432: loss: 0.6353806374:
16: 57632: loss: 0.6353441253:
16: 60832: loss: 0.6352440459:
16: 64032: loss: 0.6352778046:
16: 67232: loss: 0.6352361653:
16: 70432: loss: 0.6352491027:
16: 73632: loss: 0.6350980021:
16: 76832: loss: 0.6349658445:
16: 80032: loss: 0.6349031066:
16: 83232: loss: 0.6348617301:
16: 86432: loss: 0.6347893434:
16: 89632: loss: 0.6347792619:
16: 92832: loss: 0.6347889581:
16: 96032: loss: 0.6346506566:
16: 99232: loss: 0.6346754118:
16: 102432: loss: 0.6346239807:
16: 105632: loss: 0.6345694321:
Dev-Acc: 16: Accuracy: 0.9354659319: precision: 0.1061946903: recall: 0.0142832852: f1: 0.0251798561
Train-Acc: 16: Accuracy: 0.8579317927: precision: 0.5363321799: recall: 0.0407599763: f1: 0.0757622044
17: 3232: loss: 0.6321919602:
17: 6432: loss: 0.6326495698:
17: 9632: loss: 0.6320167327:
17: 12832: loss: 0.6321552870:
17: 16032: loss: 0.6321017662:
17: 19232: loss: 0.6319124227:
17: 22432: loss: 0.6317863427:
17: 25632: loss: 0.6318812868:
17: 28832: loss: 0.6318078468:
17: 32032: loss: 0.6318961245:
17: 35232: loss: 0.6315707790:
17: 38432: loss: 0.6314770656:
17: 41632: loss: 0.6315367431:
17: 44832: loss: 0.6314394898:
17: 48032: loss: 0.6314210943:
17: 51232: loss: 0.6313759251:
17: 54432: loss: 0.6313063423:
17: 57632: loss: 0.6312608146:
17: 60832: loss: 0.6311543942:
17: 64032: loss: 0.6310755898:
17: 67232: loss: 0.6309580099:
17: 70432: loss: 0.6309055426:
17: 73632: loss: 0.6307810262:
17: 76832: loss: 0.6308048961:
17: 80032: loss: 0.6307854610:
17: 83232: loss: 0.6306998644:
17: 86432: loss: 0.6305979865:
17: 89632: loss: 0.6305274775:
17: 92832: loss: 0.6304675791:
17: 96032: loss: 0.6304487304:
17: 99232: loss: 0.6304159691:
17: 102432: loss: 0.6303018250:
17: 105632: loss: 0.6302544719:
Dev-Acc: 17: Accuracy: 0.9376388788: precision: 0.1159695817: recall: 0.0103723856: f1: 0.0190416732
Train-Acc: 17: Accuracy: 0.8585516214: precision: 0.5894988067: recall: 0.0324764973: f1: 0.0615614680
18: 3232: loss: 0.6268228972:
18: 6432: loss: 0.6283439583:
18: 9632: loss: 0.6274789131:
18: 12832: loss: 0.6276819448:
18: 16032: loss: 0.6273853183:
18: 19232: loss: 0.6272412808:
18: 22432: loss: 0.6267960544:
18: 25632: loss: 0.6266519834:
18: 28832: loss: 0.6266256182:
18: 32032: loss: 0.6266579855:
18: 35232: loss: 0.6267362142:
18: 38432: loss: 0.6266336887:
18: 41632: loss: 0.6266565862:
18: 44832: loss: 0.6266568245:
18: 48032: loss: 0.6265101562:
18: 51232: loss: 0.6266443045:
18: 54432: loss: 0.6265576641:
18: 57632: loss: 0.6266027228:
18: 60832: loss: 0.6265271107:
18: 64032: loss: 0.6264732293:
18: 67232: loss: 0.6263832324:
18: 70432: loss: 0.6262109120:
18: 73632: loss: 0.6261646689:
18: 76832: loss: 0.6261789190:
18: 80032: loss: 0.6261001518:
18: 83232: loss: 0.6259697385:
18: 86432: loss: 0.6259168338:
18: 89632: loss: 0.6259286247:
18: 92832: loss: 0.6258946568:
18: 96032: loss: 0.6259175187:
18: 99232: loss: 0.6259417368:
18: 102432: loss: 0.6258070053:
18: 105632: loss: 0.6257177171:
Dev-Acc: 18: Accuracy: 0.9392958879: precision: 0.1331269350: recall: 0.0073116817: f1: 0.0138620245
Train-Acc: 18: Accuracy: 0.8584483266: precision: 0.6221441125: recall: 0.0232726316: f1: 0.0448669202
19: 3232: loss: 0.6253465080:
19: 6432: loss: 0.6239736369:
19: 9632: loss: 0.6237493978:
19: 12832: loss: 0.6234797741:
19: 16032: loss: 0.6226489089:
19: 19232: loss: 0.6226841486:
19: 22432: loss: 0.6226600434:
19: 25632: loss: 0.6228319748:
19: 28832: loss: 0.6229828388:
19: 32032: loss: 0.6229736689:
19: 35232: loss: 0.6228198042:
19: 38432: loss: 0.6228879115:
19: 41632: loss: 0.6227090798:
19: 44832: loss: 0.6227877462:
19: 48032: loss: 0.6227182068:
19: 51232: loss: 0.6226499801:
19: 54432: loss: 0.6224472551:
19: 57632: loss: 0.6224022487:
19: 60832: loss: 0.6224082561:
19: 64032: loss: 0.6222800905:
19: 67232: loss: 0.6222141256:
19: 70432: loss: 0.6222140080:
19: 73632: loss: 0.6222353160:
19: 76832: loss: 0.6221450519:
19: 80032: loss: 0.6221314660:
19: 83232: loss: 0.6220976891:
19: 86432: loss: 0.6221379389:
19: 89632: loss: 0.6220276528:
19: 92832: loss: 0.6219294029:
19: 96032: loss: 0.6219088946:
19: 99232: loss: 0.6218394085:
19: 102432: loss: 0.6217009042:
19: 105632: loss: 0.6216718544:
Dev-Acc: 19: Accuracy: 0.9403377175: precision: 0.1597938144: recall: 0.0052712124: f1: 0.0102057613
Train-Acc: 19: Accuracy: 0.8580632210: precision: 0.6331521739: recall: 0.0153178621: f1: 0.0299120611
20: 3232: loss: 0.6195793724:
20: 6432: loss: 0.6188861775:
20: 9632: loss: 0.6188972596:
20: 12832: loss: 0.6190015274:
20: 16032: loss: 0.6186800871:
20: 19232: loss: 0.6189178723:
20: 22432: loss: 0.6190594660:
20: 25632: loss: 0.6193804540:
20: 28832: loss: 0.6190733691:
20: 32032: loss: 0.6190226970:
20: 35232: loss: 0.6188997863:
20: 38432: loss: 0.6190282452:
20: 41632: loss: 0.6189360517:
20: 44832: loss: 0.6187289286:
20: 48032: loss: 0.6185816603:
20: 51232: loss: 0.6183655056:
20: 54432: loss: 0.6183487203:
20: 57632: loss: 0.6181562406:
20: 60832: loss: 0.6181918855:
20: 64032: loss: 0.6180580354:
20: 67232: loss: 0.6181220971:
20: 70432: loss: 0.6180802326:
20: 73632: loss: 0.6179968726:
20: 76832: loss: 0.6178991003:
20: 80032: loss: 0.6177409130:
20: 83232: loss: 0.6177536218:
20: 86432: loss: 0.6176168325:
20: 89632: loss: 0.6175653667:
20: 92832: loss: 0.6175396663:
20: 96032: loss: 0.6174451168:
20: 99232: loss: 0.6173324468:
20: 102432: loss: 0.6172354360:
20: 105632: loss: 0.6171320570:
Dev-Acc: 20: Accuracy: 0.9411612749: precision: 0.2048192771: recall: 0.0028906649: f1: 0.0057008719
Train-Acc: 20: Accuracy: 0.8579317927: precision: 0.6764705882: recall: 0.0105844455: f1: 0.0208427730
