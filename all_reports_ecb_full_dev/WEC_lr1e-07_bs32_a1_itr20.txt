1: 3232: loss: 0.6906687427:
1: 6432: loss: 0.6899029922:
1: 9632: loss: 0.6893851278:
1: 12832: loss: 0.6890831104:
1: 16032: loss: 0.6886963482:
1: 19232: loss: 0.6885527574:
1: 22432: loss: 0.6882949838:
1: 25632: loss: 0.6880549941:
1: 28832: loss: 0.6879184248:
1: 32032: loss: 0.6878165686:
1: 35232: loss: 0.6876517979:
1: 38432: loss: 0.6874085416:
1: 41632: loss: 0.6871216137:
1: 44832: loss: 0.6868711050:
1: 48032: loss: 0.6866486709:
1: 51232: loss: 0.6864294591:
1: 54432: loss: 0.6861748999:
1: 57632: loss: 0.6859769865:
1: 60832: loss: 0.6857096922:
1: 64032: loss: 0.6855132639:
1: 67232: loss: 0.6852941881:
1: 70432: loss: 0.6850202694:
1: 73632: loss: 0.6847609142:
1: 76832: loss: 0.6845215456:
1: 80032: loss: 0.6842918708:
1: 83232: loss: 0.6840379619:
1: 86432: loss: 0.6838220647:
1: 89632: loss: 0.6835801024:
1: 92832: loss: 0.6833456144:
1: 96032: loss: 0.6831475238:
1: 99232: loss: 0.6829634156:
1: 102432: loss: 0.6827531863:
1: 105632: loss: 0.6824885451:
1: 108832: loss: 0.6822861456:
1: 112032: loss: 0.6820711540:
1: 115232: loss: 0.6818244051:
1: 118432: loss: 0.6816075626:
1: 121632: loss: 0.6814025629:
1: 124832: loss: 0.6811638900:
1: 128032: loss: 0.6809535767:
1: 131232: loss: 0.6807477497:
1: 134432: loss: 0.6805280070:
1: 137632: loss: 0.6803079333:
1: 140832: loss: 0.6800859323:
1: 144032: loss: 0.6798671812:
1: 147232: loss: 0.6796672923:
1: 150432: loss: 0.6794555796:
1: 153632: loss: 0.6792178908:
1: 156832: loss: 0.6790174118:
1: 160032: loss: 0.6787959695:
1: 163232: loss: 0.6785585550:
1: 166432: loss: 0.6783341575:
1: 169632: loss: 0.6780973018:
1: 172832: loss: 0.6779001985:
1: 176032: loss: 0.6776714872:
1: 179232: loss: 0.6774346968:
1: 182432: loss: 0.6771748038:
1: 185632: loss: 0.6769524618:
1: 188832: loss: 0.6767347897:
1: 192032: loss: 0.6765197955:
1: 195232: loss: 0.6762766734:
1: 198432: loss: 0.6760380088:
1: 201632: loss: 0.6758048793:
1: 204832: loss: 0.6755879637:
1: 208032: loss: 0.6753501829:
1: 211232: loss: 0.6751327003:
1: 214432: loss: 0.6749179445:
1: 217632: loss: 0.6746848853:
1: 220832: loss: 0.6745046664:
1: 224032: loss: 0.6742666330:
1: 227232: loss: 0.6740552818:
1: 230432: loss: 0.6738297313:
1: 233632: loss: 0.6736074763:
1: 236832: loss: 0.6733804192:
1: 240032: loss: 0.6731699934:
1: 243232: loss: 0.6729722349:
1: 246432: loss: 0.6727555369:
1: 249632: loss: 0.6725280420:
1: 252832: loss: 0.6722988880:
1: 256032: loss: 0.6720671542:
1: 259232: loss: 0.6718233983:
1: 262432: loss: 0.6715818682:
1: 265632: loss: 0.6713484985:
1: 268832: loss: 0.6711200587:
1: 272032: loss: 0.6708908909:
1: 275232: loss: 0.6706762330:
1: 278432: loss: 0.6704349450:
1: 281632: loss: 0.6701930129:
1: 284832: loss: 0.6699514897:
1: 288032: loss: 0.6697239481:
1: 291232: loss: 0.6695013282:
1: 294432: loss: 0.6692604841:
1: 297632: loss: 0.6690225123:
1: 300832: loss: 0.6687774917:
1: 304032: loss: 0.6685240641:
1: 307232: loss: 0.6683384217:
1: 310432: loss: 0.6681082406:
1: 313632: loss: 0.6678758328:
1: 316832: loss: 0.6676527067:
1: 320032: loss: 0.6674160973:
1: 323232: loss: 0.6671877475:
1: 326432: loss: 0.6669330727:
1: 329632: loss: 0.6666804195:
Dev-Acc: 1: Accuracy: 0.2906016707: precision: 0.0592581646: recall: 0.7500425098: f1: 0.1098383924
Train-Acc: 1: Accuracy: 0.8211877942: precision: 0.9423009807: recall: 0.6725557355: f1: 0.7848992703
2: 3232: loss: 0.6434032053:
2: 6432: loss: 0.6420859405:
2: 9632: loss: 0.6417354486:
2: 12832: loss: 0.6412815672:
2: 16032: loss: 0.6412719170:
2: 19232: loss: 0.6407698602:
2: 22432: loss: 0.6408674922:
2: 25632: loss: 0.6405350708:
2: 28832: loss: 0.6402166139:
2: 32032: loss: 0.6399227604:
2: 35232: loss: 0.6397151972:
2: 38432: loss: 0.6395046183:
2: 41632: loss: 0.6392355075:
2: 44832: loss: 0.6389688826:
2: 48032: loss: 0.6387395954:
2: 51232: loss: 0.6385922239:
2: 54432: loss: 0.6383037747:
2: 57632: loss: 0.6380606524:
2: 60832: loss: 0.6378533380:
2: 64032: loss: 0.6376045257:
2: 67232: loss: 0.6373199534:
2: 70432: loss: 0.6370838092:
2: 73632: loss: 0.6367865262:
2: 76832: loss: 0.6365123909:
2: 80032: loss: 0.6362080295:
2: 83232: loss: 0.6360096688:
2: 86432: loss: 0.6357267947:
2: 89632: loss: 0.6353652248:
2: 92832: loss: 0.6350606106:
2: 96032: loss: 0.6347091002:
2: 99232: loss: 0.6344477878:
2: 102432: loss: 0.6342604309:
2: 105632: loss: 0.6339465191:
2: 108832: loss: 0.6337701731:
2: 112032: loss: 0.6335419513:
2: 115232: loss: 0.6333073441:
2: 118432: loss: 0.6330581669:
2: 121632: loss: 0.6326979747:
2: 124832: loss: 0.6324978580:
2: 128032: loss: 0.6322104844:
2: 131232: loss: 0.6319265635:
2: 134432: loss: 0.6316565371:
2: 137632: loss: 0.6313915947:
2: 140832: loss: 0.6311565908:
2: 144032: loss: 0.6309565119:
2: 147232: loss: 0.6306822793:
2: 150432: loss: 0.6303853820:
2: 153632: loss: 0.6301299367:
2: 156832: loss: 0.6297738978:
2: 160032: loss: 0.6294959005:
2: 163232: loss: 0.6292369267:
2: 166432: loss: 0.6289621084:
2: 169632: loss: 0.6286962865:
2: 172832: loss: 0.6284846881:
2: 176032: loss: 0.6282606088:
2: 179232: loss: 0.6279886209:
2: 182432: loss: 0.6277284165:
2: 185632: loss: 0.6274961714:
2: 188832: loss: 0.6272474097:
2: 192032: loss: 0.6269759358:
2: 195232: loss: 0.6267458192:
2: 198432: loss: 0.6264830835:
2: 201632: loss: 0.6262204664:
2: 204832: loss: 0.6259502734:
2: 208032: loss: 0.6257065863:
2: 211232: loss: 0.6254069431:
2: 214432: loss: 0.6251174630:
2: 217632: loss: 0.6248498225:
2: 220832: loss: 0.6246184762:
2: 224032: loss: 0.6243643330:
2: 227232: loss: 0.6241239265:
2: 230432: loss: 0.6238315913:
2: 233632: loss: 0.6235829653:
2: 236832: loss: 0.6233030080:
2: 240032: loss: 0.6231020554:
2: 243232: loss: 0.6228677525:
2: 246432: loss: 0.6225910619:
2: 249632: loss: 0.6223645066:
2: 252832: loss: 0.6220725199:
2: 256032: loss: 0.6217764290:
2: 259232: loss: 0.6215069199:
2: 262432: loss: 0.6211767078:
2: 265632: loss: 0.6208526874:
2: 268832: loss: 0.6205519408:
2: 272032: loss: 0.6203153822:
2: 275232: loss: 0.6200673409:
2: 278432: loss: 0.6197701933:
2: 281632: loss: 0.6194866399:
2: 284832: loss: 0.6191995786:
2: 288032: loss: 0.6189414158:
2: 291232: loss: 0.6186560814:
2: 294432: loss: 0.6183839762:
2: 297632: loss: 0.6181136900:
2: 300832: loss: 0.6177937731:
2: 304032: loss: 0.6175121092:
2: 307232: loss: 0.6172313882:
2: 310432: loss: 0.6169518551:
2: 313632: loss: 0.6166878770:
2: 316832: loss: 0.6164040897:
2: 320032: loss: 0.6161014835:
2: 323232: loss: 0.6158052107:
2: 326432: loss: 0.6154815956:
2: 329632: loss: 0.6152183527:
Dev-Acc: 2: Accuracy: 0.2270697653: precision: 0.0563427135: recall: 0.7775888454: f1: 0.1050720891
Train-Acc: 2: Accuracy: 0.8686110377: precision: 0.9562049484: recall: 0.7641362561: f1: 0.8494487807
3: 3232: loss: 0.5841406846:
3: 6432: loss: 0.5852619910:
3: 9632: loss: 0.5839527076:
3: 12832: loss: 0.5831699072:
3: 16032: loss: 0.5839659184:
3: 19232: loss: 0.5837894069:
3: 22432: loss: 0.5836041241:
3: 25632: loss: 0.5828005674:
3: 28832: loss: 0.5822076213:
3: 32032: loss: 0.5821788137:
3: 35232: loss: 0.5822652485:
3: 38432: loss: 0.5816385947:
3: 41632: loss: 0.5815405074:
3: 44832: loss: 0.5812196432:
3: 48032: loss: 0.5811176486:
3: 51232: loss: 0.5806913000:
3: 54432: loss: 0.5805146876:
3: 57632: loss: 0.5802953637:
3: 60832: loss: 0.5800036698:
3: 64032: loss: 0.5798121205:
3: 67232: loss: 0.5795228188:
3: 70432: loss: 0.5792566430:
3: 73632: loss: 0.5791570649:
3: 76832: loss: 0.5788955561:
3: 80032: loss: 0.5785567950:
3: 83232: loss: 0.5780274639:
3: 86432: loss: 0.5775208971:
3: 89632: loss: 0.5772476848:
3: 92832: loss: 0.5769376776:
3: 96032: loss: 0.5765616225:
3: 99232: loss: 0.5763232585:
3: 102432: loss: 0.5760603465:
3: 105632: loss: 0.5756770762:
3: 108832: loss: 0.5754739869:
3: 112032: loss: 0.5751884712:
3: 115232: loss: 0.5748686506:
3: 118432: loss: 0.5745583474:
3: 121632: loss: 0.5742326887:
3: 124832: loss: 0.5739166390:
3: 128032: loss: 0.5737073814:
3: 131232: loss: 0.5733623928:
3: 134432: loss: 0.5729453310:
3: 137632: loss: 0.5726705653:
3: 140832: loss: 0.5723165369:
3: 144032: loss: 0.5719900548:
3: 147232: loss: 0.5715847931:
3: 150432: loss: 0.5712980191:
3: 153632: loss: 0.5709904476:
3: 156832: loss: 0.5706105334:
3: 160032: loss: 0.5702434194:
3: 163232: loss: 0.5699379791:
3: 166432: loss: 0.5695493709:
3: 169632: loss: 0.5692559418:
3: 172832: loss: 0.5689602143:
3: 176032: loss: 0.5685267023:
3: 179232: loss: 0.5681312180:
3: 182432: loss: 0.5678648821:
3: 185632: loss: 0.5675716924:
3: 188832: loss: 0.5672787851:
3: 192032: loss: 0.5669133699:
3: 195232: loss: 0.5665933217:
3: 198432: loss: 0.5663336343:
3: 201632: loss: 0.5660051569:
3: 204832: loss: 0.5657231247:
3: 208032: loss: 0.5653496111:
3: 211232: loss: 0.5650192066:
3: 214432: loss: 0.5646869010:
3: 217632: loss: 0.5644027103:
3: 220832: loss: 0.5640093092:
3: 224032: loss: 0.5637596212:
3: 227232: loss: 0.5634193641:
3: 230432: loss: 0.5630996292:
3: 233632: loss: 0.5627950217:
3: 236832: loss: 0.5625275864:
3: 240032: loss: 0.5622025128:
3: 243232: loss: 0.5618632354:
3: 246432: loss: 0.5615947363:
3: 249632: loss: 0.5613330558:
3: 252832: loss: 0.5610535958:
3: 256032: loss: 0.5607548539:
3: 259232: loss: 0.5604121498:
3: 262432: loss: 0.5600378521:
3: 265632: loss: 0.5596358702:
3: 268832: loss: 0.5593001107:
3: 272032: loss: 0.5590009456:
3: 275232: loss: 0.5586925455:
3: 278432: loss: 0.5583631173:
3: 281632: loss: 0.5580156747:
3: 284832: loss: 0.5577184489:
3: 288032: loss: 0.5574375593:
3: 291232: loss: 0.5570935668:
3: 294432: loss: 0.5567826533:
3: 297632: loss: 0.5564408662:
3: 300832: loss: 0.5561338666:
3: 304032: loss: 0.5557162601:
3: 307232: loss: 0.5554225267:
3: 310432: loss: 0.5551349487:
3: 313632: loss: 0.5547622810:
3: 316832: loss: 0.5544150380:
3: 320032: loss: 0.5540620198:
3: 323232: loss: 0.5537216894:
3: 326432: loss: 0.5533811511:
3: 329632: loss: 0.5530272980:
Dev-Acc: 3: Accuracy: 0.1980869919: precision: 0.0579289515: recall: 0.8348920252: f1: 0.1083406884
Train-Acc: 3: Accuracy: 0.9018335938: precision: 0.9638100480: recall: 0.8287457965: f1: 0.8911895586
4: 3232: loss: 0.5150575492:
4: 6432: loss: 0.5157446785:
4: 9632: loss: 0.5168803695:
4: 12832: loss: 0.5171120336:
4: 16032: loss: 0.5161998325:
4: 19232: loss: 0.5166747391:
4: 22432: loss: 0.5157712397:
4: 25632: loss: 0.5154851527:
4: 28832: loss: 0.5154194685:
4: 32032: loss: 0.5149516868:
4: 35232: loss: 0.5149792433:
4: 38432: loss: 0.5147176047:
4: 41632: loss: 0.5144386099:
4: 44832: loss: 0.5142803052:
4: 48032: loss: 0.5140560857:
4: 51232: loss: 0.5137819067:
4: 54432: loss: 0.5134732138:
4: 57632: loss: 0.5131107156:
4: 60832: loss: 0.5126501750:
4: 64032: loss: 0.5119631089:
4: 67232: loss: 0.5111013842:
4: 70432: loss: 0.5109009062:
4: 73632: loss: 0.5107810281:
4: 76832: loss: 0.5103509620:
4: 80032: loss: 0.5099014868:
4: 83232: loss: 0.5093764490:
4: 86432: loss: 0.5090448326:
4: 89632: loss: 0.5084902928:
4: 92832: loss: 0.5081931161:
4: 96032: loss: 0.5080649894:
4: 99232: loss: 0.5077331166:
4: 102432: loss: 0.5075239722:
4: 105632: loss: 0.5071502877:
4: 108832: loss: 0.5066925958:
4: 112032: loss: 0.5065214151:
4: 115232: loss: 0.5061127219:
4: 118432: loss: 0.5058054493:
4: 121632: loss: 0.5053279043:
4: 124832: loss: 0.5050229936:
4: 128032: loss: 0.5045498533:
4: 131232: loss: 0.5042861709:
4: 134432: loss: 0.5041282163:
4: 137632: loss: 0.5038324835:
4: 140832: loss: 0.5034341460:
4: 144032: loss: 0.5030238743:
4: 147232: loss: 0.5026771085:
4: 150432: loss: 0.5022194942:
4: 153632: loss: 0.5019475571:
4: 156832: loss: 0.5015773002:
4: 160032: loss: 0.5012002211:
4: 163232: loss: 0.5008388786:
4: 166432: loss: 0.5004806131:
4: 169632: loss: 0.5001555767:
4: 172832: loss: 0.4998189785:
4: 176032: loss: 0.4995922130:
4: 179232: loss: 0.4992718531:
4: 182432: loss: 0.4989459634:
4: 185632: loss: 0.4987493580:
4: 188832: loss: 0.4983686292:
4: 192032: loss: 0.4979797721:
4: 195232: loss: 0.4975766042:
4: 198432: loss: 0.4972632039:
4: 201632: loss: 0.4968537738:
4: 204832: loss: 0.4964057156:
4: 208032: loss: 0.4960995981:
4: 211232: loss: 0.4958158443:
4: 214432: loss: 0.4954812580:
4: 217632: loss: 0.4951111686:
4: 220832: loss: 0.4947424515:
4: 224032: loss: 0.4943760931:
4: 227232: loss: 0.4939867155:
4: 230432: loss: 0.4936091967:
4: 233632: loss: 0.4931896851:
4: 236832: loss: 0.4928691159:
4: 240032: loss: 0.4924993807:
4: 243232: loss: 0.4921780907:
4: 246432: loss: 0.4918588854:
4: 249632: loss: 0.4915263167:
4: 252832: loss: 0.4911864835:
4: 256032: loss: 0.4907369586:
4: 259232: loss: 0.4903598071:
4: 262432: loss: 0.4900064563:
4: 265632: loss: 0.4896019779:
4: 268832: loss: 0.4892568013:
4: 272032: loss: 0.4889008572:
4: 275232: loss: 0.4885859625:
4: 278432: loss: 0.4881798297:
4: 281632: loss: 0.4878994627:
4: 284832: loss: 0.4875455731:
4: 288032: loss: 0.4872241360:
4: 291232: loss: 0.4868742423:
4: 294432: loss: 0.4864598204:
4: 297632: loss: 0.4861920099:
4: 300832: loss: 0.4858333792:
4: 304032: loss: 0.4855386113:
4: 307232: loss: 0.4852793263:
4: 310432: loss: 0.4849381509:
4: 313632: loss: 0.4846272751:
4: 316832: loss: 0.4843624805:
4: 320032: loss: 0.4840146643:
4: 323232: loss: 0.4836119288:
4: 326432: loss: 0.4832904133:
4: 329632: loss: 0.4830263850:
Dev-Acc: 4: Accuracy: 0.1846424043: precision: 0.0604374078: recall: 0.8918551267: f1: 0.1132034749
Train-Acc: 4: Accuracy: 0.9169768095: precision: 0.9600558582: recall: 0.8648275003: f1: 0.9099570163
5: 3232: loss: 0.4427602285:
5: 6432: loss: 0.4473885269:
5: 9632: loss: 0.4483210084:
5: 12832: loss: 0.4469543283:
5: 16032: loss: 0.4457805810:
5: 19232: loss: 0.4454411239:
5: 22432: loss: 0.4452245741:
5: 25632: loss: 0.4451475390:
5: 28832: loss: 0.4443669955:
5: 32032: loss: 0.4442153645:
5: 35232: loss: 0.4432218580:
5: 38432: loss: 0.4425328548:
5: 41632: loss: 0.4423070383:
5: 44832: loss: 0.4422021559:
5: 48032: loss: 0.4416579556:
5: 51232: loss: 0.4411648231:
5: 54432: loss: 0.4405237234:
5: 57632: loss: 0.4402251182:
5: 60832: loss: 0.4396208399:
5: 64032: loss: 0.4395544407:
5: 67232: loss: 0.4392314587:
5: 70432: loss: 0.4389037574:
5: 73632: loss: 0.4384846584:
5: 76832: loss: 0.4380424883:
5: 80032: loss: 0.4375821428:
5: 83232: loss: 0.4372847474:
5: 86432: loss: 0.4371267284:
5: 89632: loss: 0.4370302596:
5: 92832: loss: 0.4365949307:
5: 96032: loss: 0.4362727509:
5: 99232: loss: 0.4359845705:
5: 102432: loss: 0.4354953544:
5: 105632: loss: 0.4350358594:
5: 108832: loss: 0.4345127429:
5: 112032: loss: 0.4341077275:
5: 115232: loss: 0.4338689645:
5: 118432: loss: 0.4336687085:
5: 121632: loss: 0.4332430081:
5: 124832: loss: 0.4329356344:
5: 128032: loss: 0.4326979704:
5: 131232: loss: 0.4323779487:
5: 134432: loss: 0.4320321181:
5: 137632: loss: 0.4316442221:
5: 140832: loss: 0.4312577108:
5: 144032: loss: 0.4310911896:
5: 147232: loss: 0.4306501561:
5: 150432: loss: 0.4302746647:
5: 153632: loss: 0.4301034869:
5: 156832: loss: 0.4299202314:
5: 160032: loss: 0.4296111068:
5: 163232: loss: 0.4290800150:
5: 166432: loss: 0.4288773700:
5: 169632: loss: 0.4285421214:
5: 172832: loss: 0.4283600861:
5: 176032: loss: 0.4282096952:
5: 179232: loss: 0.4277579411:
5: 182432: loss: 0.4273815048:
5: 185632: loss: 0.4270789540:
5: 188832: loss: 0.4266607241:
5: 192032: loss: 0.4263464425:
5: 195232: loss: 0.4260801265:
5: 198432: loss: 0.4258254554:
5: 201632: loss: 0.4254182716:
5: 204832: loss: 0.4251358270:
5: 208032: loss: 0.4247720341:
5: 211232: loss: 0.4245810469:
5: 214432: loss: 0.4242066775:
5: 217632: loss: 0.4237909111:
5: 220832: loss: 0.4235682907:
5: 224032: loss: 0.4233975709:
5: 227232: loss: 0.4230827363:
5: 230432: loss: 0.4227918100:
5: 233632: loss: 0.4224671040:
5: 236832: loss: 0.4221369979:
5: 240032: loss: 0.4218389351:
5: 243232: loss: 0.4216156127:
5: 246432: loss: 0.4212900647:
5: 249632: loss: 0.4210161083:
5: 252832: loss: 0.4206701952:
5: 256032: loss: 0.4203174959:
5: 259232: loss: 0.4198408352:
5: 262432: loss: 0.4195415953:
5: 265632: loss: 0.4193139267:
5: 268832: loss: 0.4189914184:
5: 272032: loss: 0.4187218109:
5: 275232: loss: 0.4183125929:
5: 278432: loss: 0.4179660107:
5: 281632: loss: 0.4176953026:
5: 284832: loss: 0.4173854521:
5: 288032: loss: 0.4170438526:
5: 291232: loss: 0.4167306084:
5: 294432: loss: 0.4163596495:
5: 297632: loss: 0.4160504921:
5: 300832: loss: 0.4157836369:
5: 304032: loss: 0.4154056010:
5: 307232: loss: 0.4150638564:
5: 310432: loss: 0.4147764443:
5: 313632: loss: 0.4144708916:
5: 316832: loss: 0.4141931639:
5: 320032: loss: 0.4137944183:
5: 323232: loss: 0.4134829412:
5: 326432: loss: 0.4132078581:
5: 329632: loss: 0.4129584917:
Dev-Acc: 5: Accuracy: 0.1785898507: precision: 0.0602570847: recall: 0.8959360653: f1: 0.1129196447
Train-Acc: 5: Accuracy: 0.9264801741: precision: 0.9641596599: recall: 0.8811931747: f1: 0.9208113437
6: 3232: loss: 0.3757603773:
6: 6432: loss: 0.3779161957:
6: 9632: loss: 0.3764550893:
6: 12832: loss: 0.3754280186:
6: 16032: loss: 0.3736274092:
6: 19232: loss: 0.3729251782:
6: 22432: loss: 0.3740598242:
6: 25632: loss: 0.3741248273:
6: 28832: loss: 0.3729791260:
6: 32032: loss: 0.3733077283:
6: 35232: loss: 0.3730393585:
6: 38432: loss: 0.3734316261:
6: 41632: loss: 0.3735375237:
6: 44832: loss: 0.3731390668:
6: 48032: loss: 0.3726334174:
6: 51232: loss: 0.3728647253:
6: 54432: loss: 0.3727014886:
6: 57632: loss: 0.3724933755:
6: 60832: loss: 0.3722424371:
6: 64032: loss: 0.3721075869:
6: 67232: loss: 0.3718109769:
6: 70432: loss: 0.3718026923:
6: 73632: loss: 0.3712808811:
6: 76832: loss: 0.3707460211:
6: 80032: loss: 0.3706641294:
6: 83232: loss: 0.3705809413:
6: 86432: loss: 0.3703828096:
6: 89632: loss: 0.3700275739:
6: 92832: loss: 0.3694460924:
6: 96032: loss: 0.3693614026:
6: 99232: loss: 0.3693008770:
6: 102432: loss: 0.3689140867:
6: 105632: loss: 0.3685386337:
6: 108832: loss: 0.3683619863:
6: 112032: loss: 0.3679369893:
6: 115232: loss: 0.3676406153:
6: 118432: loss: 0.3672077455:
6: 121632: loss: 0.3668253867:
6: 124832: loss: 0.3664879484:
6: 128032: loss: 0.3662008895:
6: 131232: loss: 0.3658942650:
6: 134432: loss: 0.3657237083:
6: 137632: loss: 0.3655361213:
6: 140832: loss: 0.3653305367:
6: 144032: loss: 0.3651189425:
6: 147232: loss: 0.3647976703:
6: 150432: loss: 0.3645522761:
6: 153632: loss: 0.3642163578:
6: 156832: loss: 0.3638977981:
6: 160032: loss: 0.3636044052:
6: 163232: loss: 0.3634603352:
6: 166432: loss: 0.3631337998:
6: 169632: loss: 0.3627920066:
6: 172832: loss: 0.3625463738:
6: 176032: loss: 0.3624320836:
6: 179232: loss: 0.3622060051:
6: 182432: loss: 0.3620331866:
6: 185632: loss: 0.3617483538:
6: 188832: loss: 0.3613785015:
6: 192032: loss: 0.3610390245:
6: 195232: loss: 0.3607335381:
6: 198432: loss: 0.3605178662:
6: 201632: loss: 0.3603791125:
6: 204832: loss: 0.3601191073:
6: 208032: loss: 0.3597802842:
6: 211232: loss: 0.3594412896:
6: 214432: loss: 0.3591406104:
6: 217632: loss: 0.3587437000:
6: 220832: loss: 0.3584236131:
6: 224032: loss: 0.3581313057:
6: 227232: loss: 0.3578010746:
6: 230432: loss: 0.3575293260:
6: 233632: loss: 0.3572197848:
6: 236832: loss: 0.3569014848:
6: 240032: loss: 0.3565833155:
6: 243232: loss: 0.3562239564:
6: 246432: loss: 0.3557833828:
6: 249632: loss: 0.3554350093:
6: 252832: loss: 0.3552191042:
6: 256032: loss: 0.3548610721:
6: 259232: loss: 0.3545827367:
6: 262432: loss: 0.3543261335:
6: 265632: loss: 0.3540121143:
6: 268832: loss: 0.3537274031:
6: 272032: loss: 0.3534706556:
6: 275232: loss: 0.3531758919:
6: 278432: loss: 0.3529491915:
6: 281632: loss: 0.3526359645:
6: 284832: loss: 0.3523564618:
6: 288032: loss: 0.3520862520:
6: 291232: loss: 0.3518325521:
6: 294432: loss: 0.3515994553:
6: 297632: loss: 0.3512912424:
6: 300832: loss: 0.3511260206:
6: 304032: loss: 0.3509250483:
6: 307232: loss: 0.3506525079:
6: 310432: loss: 0.3503992251:
6: 313632: loss: 0.3500417068:
6: 316832: loss: 0.3496929792:
6: 320032: loss: 0.3493755911:
6: 323232: loss: 0.3490982011:
6: 326432: loss: 0.3488732680:
6: 329632: loss: 0.3486450597:
Dev-Acc: 6: Accuracy: 0.1736287475: precision: 0.0598544297: recall: 0.8949158306: f1: 0.1122043257
Train-Acc: 6: Accuracy: 0.9335518479: precision: 0.9692670274: recall: 0.8912753768: f1: 0.9286365450
7: 3232: loss: 0.3153006086:
7: 6432: loss: 0.3185578382:
7: 9632: loss: 0.3209535325:
7: 12832: loss: 0.3200116118:
7: 16032: loss: 0.3200690893:
7: 19232: loss: 0.3197693912:
7: 22432: loss: 0.3186241036:
7: 25632: loss: 0.3183502100:
7: 28832: loss: 0.3171296922:
7: 32032: loss: 0.3168233579:
7: 35232: loss: 0.3161220432:
7: 38432: loss: 0.3159681173:
7: 41632: loss: 0.3151738480:
7: 44832: loss: 0.3155155179:
7: 48032: loss: 0.3156844610:
7: 51232: loss: 0.3155807518:
7: 54432: loss: 0.3147427474:
7: 57632: loss: 0.3142857628:
7: 60832: loss: 0.3141517604:
7: 64032: loss: 0.3139593896:
7: 67232: loss: 0.3133227380:
7: 70432: loss: 0.3129871944:
7: 73632: loss: 0.3129590422:
7: 76832: loss: 0.3128797370:
7: 80032: loss: 0.3125211834:
7: 83232: loss: 0.3122926706:
7: 86432: loss: 0.3121902038:
7: 89632: loss: 0.3117371948:
7: 92832: loss: 0.3114539674:
7: 96032: loss: 0.3112778367:
7: 99232: loss: 0.3109475564:
7: 102432: loss: 0.3109865885:
7: 105632: loss: 0.3103126939:
7: 108832: loss: 0.3101105707:
7: 112032: loss: 0.3097368747:
7: 115232: loss: 0.3091516695:
7: 118432: loss: 0.3087706055:
7: 121632: loss: 0.3086784306:
7: 124832: loss: 0.3083451263:
7: 128032: loss: 0.3082347749:
7: 131232: loss: 0.3079672952:
7: 134432: loss: 0.3077243090:
7: 137632: loss: 0.3075501507:
7: 140832: loss: 0.3074097046:
7: 144032: loss: 0.3072112337:
7: 147232: loss: 0.3070240188:
7: 150432: loss: 0.3068124725:
7: 153632: loss: 0.3066330296:
7: 156832: loss: 0.3065296272:
7: 160032: loss: 0.3062314824:
7: 163232: loss: 0.3058826590:
7: 166432: loss: 0.3056698474:
7: 169632: loss: 0.3054447217:
7: 172832: loss: 0.3051567669:
7: 176032: loss: 0.3049381320:
7: 179232: loss: 0.3047055796:
7: 182432: loss: 0.3044534270:
7: 185632: loss: 0.3040780747:
7: 188832: loss: 0.3039663294:
7: 192032: loss: 0.3037307586:
7: 195232: loss: 0.3034848938:
7: 198432: loss: 0.3032127043:
7: 201632: loss: 0.3029330398:
7: 204832: loss: 0.3026574512:
7: 208032: loss: 0.3024308519:
7: 211232: loss: 0.3021983339:
7: 214432: loss: 0.3020116620:
7: 217632: loss: 0.3017289338:
7: 220832: loss: 0.3013649375:
7: 224032: loss: 0.3010444065:
7: 227232: loss: 0.3009106239:
7: 230432: loss: 0.3007452936:
7: 233632: loss: 0.3006104845:
7: 236832: loss: 0.3003938661:
7: 240032: loss: 0.3002195247:
7: 243232: loss: 0.3000655334:
7: 246432: loss: 0.2998317942:
7: 249632: loss: 0.2995927809:
7: 252832: loss: 0.2992841788:
7: 256032: loss: 0.2990503696:
7: 259232: loss: 0.2987873221:
7: 262432: loss: 0.2986249377:
7: 265632: loss: 0.2984045494:
7: 268832: loss: 0.2981733671:
7: 272032: loss: 0.2979807988:
7: 275232: loss: 0.2977676330:
7: 278432: loss: 0.2975249883:
7: 281632: loss: 0.2972702676:
7: 284832: loss: 0.2969975248:
7: 288032: loss: 0.2967797665:
7: 291232: loss: 0.2965523145:
7: 294432: loss: 0.2963405306:
7: 297632: loss: 0.2960535967:
7: 300832: loss: 0.2958646584:
7: 304032: loss: 0.2957231960:
7: 307232: loss: 0.2955215835:
7: 310432: loss: 0.2953211494:
7: 313632: loss: 0.2951299616:
7: 316832: loss: 0.2949447038:
7: 320032: loss: 0.2948039355:
7: 323232: loss: 0.2945902737:
7: 326432: loss: 0.2943669059:
7: 329632: loss: 0.2942172681:
Dev-Acc: 7: Accuracy: 0.1697690040: precision: 0.0597503141: recall: 0.8976364564: f1: 0.1120426182
Train-Acc: 7: Accuracy: 0.9390375614: precision: 0.9745999337: recall: 0.8977207622: f1: 0.9345819840
8: 3232: loss: 0.2670952000:
8: 6432: loss: 0.2685983954:
8: 9632: loss: 0.2696555444:
8: 12832: loss: 0.2692642071:
8: 16032: loss: 0.2701063470:
8: 19232: loss: 0.2695382721:
8: 22432: loss: 0.2687788118:
8: 25632: loss: 0.2693599671:
8: 28832: loss: 0.2691095178:
8: 32032: loss: 0.2688807329:
8: 35232: loss: 0.2684100947:
8: 38432: loss: 0.2684461374:
8: 41632: loss: 0.2684803087:
8: 44832: loss: 0.2685434411:
8: 48032: loss: 0.2682151904:
8: 51232: loss: 0.2676871530:
8: 54432: loss: 0.2671943705:
8: 57632: loss: 0.2669742095:
8: 60832: loss: 0.2666649438:
8: 64032: loss: 0.2665187077:
8: 67232: loss: 0.2664049340:
8: 70432: loss: 0.2660028047:
8: 73632: loss: 0.2657890846:
8: 76832: loss: 0.2653730491:
8: 80032: loss: 0.2651143034:
8: 83232: loss: 0.2644472290:
8: 86432: loss: 0.2642168783:
8: 89632: loss: 0.2642409883:
8: 92832: loss: 0.2638492392:
8: 96032: loss: 0.2637163693:
8: 99232: loss: 0.2633982648:
8: 102432: loss: 0.2633156648:
8: 105632: loss: 0.2628523275:
8: 108832: loss: 0.2627506164:
8: 112032: loss: 0.2624633608:
8: 115232: loss: 0.2625109229:
8: 118432: loss: 0.2623208572:
8: 121632: loss: 0.2622681705:
8: 124832: loss: 0.2620739562:
8: 128032: loss: 0.2615646245:
8: 131232: loss: 0.2616510692:
8: 134432: loss: 0.2614237208:
8: 137632: loss: 0.2611061994:
8: 140832: loss: 0.2607658382:
8: 144032: loss: 0.2605522622:
8: 147232: loss: 0.2603614831:
8: 150432: loss: 0.2602723182:
8: 153632: loss: 0.2600266106:
8: 156832: loss: 0.2597006388:
8: 160032: loss: 0.2595741524:
8: 163232: loss: 0.2596326890:
8: 166432: loss: 0.2596828619:
8: 169632: loss: 0.2594583500:
8: 172832: loss: 0.2593632490:
8: 176032: loss: 0.2590492365:
8: 179232: loss: 0.2586334513:
8: 182432: loss: 0.2583453198:
8: 185632: loss: 0.2579712949:
8: 188832: loss: 0.2578417461:
8: 192032: loss: 0.2576178653:
8: 195232: loss: 0.2575007595:
8: 198432: loss: 0.2572777157:
8: 201632: loss: 0.2570494529:
8: 204832: loss: 0.2569334450:
8: 208032: loss: 0.2568054174:
8: 211232: loss: 0.2568250233:
8: 214432: loss: 0.2565832002:
8: 217632: loss: 0.2563122759:
8: 220832: loss: 0.2560929830:
8: 224032: loss: 0.2560299141:
8: 227232: loss: 0.2557125124:
8: 230432: loss: 0.2555613055:
8: 233632: loss: 0.2553661710:
8: 236832: loss: 0.2551757945:
8: 240032: loss: 0.2548979256:
8: 243232: loss: 0.2548393873:
8: 246432: loss: 0.2546196384:
8: 249632: loss: 0.2544560711:
8: 252832: loss: 0.2541399285:
8: 256032: loss: 0.2541228304:
8: 259232: loss: 0.2539783561:
8: 262432: loss: 0.2538073552:
8: 265632: loss: 0.2536141916:
8: 268832: loss: 0.2535010118:
8: 272032: loss: 0.2533748850:
8: 275232: loss: 0.2532167366:
8: 278432: loss: 0.2529957943:
8: 281632: loss: 0.2527837092:
8: 284832: loss: 0.2525889482:
8: 288032: loss: 0.2525032356:
8: 291232: loss: 0.2522891543:
8: 294432: loss: 0.2522035638:
8: 297632: loss: 0.2520739878:
8: 300832: loss: 0.2518373392:
8: 304032: loss: 0.2516102545:
8: 307232: loss: 0.2514616322:
8: 310432: loss: 0.2513346331:
8: 313632: loss: 0.2511743001:
8: 316832: loss: 0.2509619926:
8: 320032: loss: 0.2508067053:
8: 323232: loss: 0.2506040162:
8: 326432: loss: 0.2503803133:
8: 329632: loss: 0.2503064845:
Dev-Acc: 8: Accuracy: 0.1649170518: precision: 0.0596395302: recall: 0.9013773168: f1: 0.1118767477
Train-Acc: 8: Accuracy: 0.9441185594: precision: 0.9782744808: recall: 0.9048947565: f1: 0.9401549585
9: 3232: loss: 0.2304978123:
9: 6432: loss: 0.2281560794:
9: 9632: loss: 0.2279441576:
9: 12832: loss: 0.2255155395:
9: 16032: loss: 0.2256028089:
9: 19232: loss: 0.2263363312:
9: 22432: loss: 0.2254085663:
9: 25632: loss: 0.2260005360:
9: 28832: loss: 0.2263476059:
9: 32032: loss: 0.2275866578:
9: 35232: loss: 0.2273678260:
9: 38432: loss: 0.2269321192:
9: 41632: loss: 0.2273350562:
9: 44832: loss: 0.2271130168:
9: 48032: loss: 0.2270559550:
9: 51232: loss: 0.2271466906:
9: 54432: loss: 0.2278195760:
9: 57632: loss: 0.2277163339:
9: 60832: loss: 0.2276954765:
9: 64032: loss: 0.2271544125:
9: 67232: loss: 0.2269876397:
9: 70432: loss: 0.2270795008:
9: 73632: loss: 0.2269860901:
9: 76832: loss: 0.2268905316:
9: 80032: loss: 0.2266922131:
9: 83232: loss: 0.2265417212:
9: 86432: loss: 0.2264107881:
9: 89632: loss: 0.2260067718:
9: 92832: loss: 0.2258376488:
9: 96032: loss: 0.2254608488:
9: 99232: loss: 0.2256736495:
9: 102432: loss: 0.2255218016:
9: 105632: loss: 0.2254528405:
9: 108832: loss: 0.2251548446:
9: 112032: loss: 0.2252819654:
9: 115232: loss: 0.2252187732:
9: 118432: loss: 0.2250711136:
9: 121632: loss: 0.2249149728:
9: 124832: loss: 0.2247129422:
9: 128032: loss: 0.2246393677:
9: 131232: loss: 0.2246080728:
9: 134432: loss: 0.2244858003:
9: 137632: loss: 0.2242502534:
9: 140832: loss: 0.2243589115:
9: 144032: loss: 0.2242371535:
9: 147232: loss: 0.2241275699:
9: 150432: loss: 0.2240844724:
9: 153632: loss: 0.2240205584:
9: 156832: loss: 0.2238326140:
9: 160032: loss: 0.2234842008:
9: 163232: loss: 0.2233643054:
9: 166432: loss: 0.2232373400:
9: 169632: loss: 0.2231817867:
9: 172832: loss: 0.2231666995:
9: 176032: loss: 0.2229634876:
9: 179232: loss: 0.2227714854:
9: 182432: loss: 0.2226741594:
9: 185632: loss: 0.2224763467:
9: 188832: loss: 0.2224931136:
9: 192032: loss: 0.2224585857:
9: 195232: loss: 0.2222947338:
9: 198432: loss: 0.2220434303:
9: 201632: loss: 0.2218987568:
9: 204832: loss: 0.2217276033:
9: 208032: loss: 0.2215448626:
9: 211232: loss: 0.2215888895:
9: 214432: loss: 0.2214357800:
9: 217632: loss: 0.2212802735:
9: 220832: loss: 0.2210623905:
9: 224032: loss: 0.2209756901:
9: 227232: loss: 0.2207199451:
9: 230432: loss: 0.2205152816:
9: 233632: loss: 0.2203124215:
9: 236832: loss: 0.2201758350:
9: 240032: loss: 0.2201069182:
9: 243232: loss: 0.2199293259:
9: 246432: loss: 0.2199208794:
9: 249632: loss: 0.2198215387:
9: 252832: loss: 0.2196535038:
9: 256032: loss: 0.2194776689:
9: 259232: loss: 0.2192881514:
9: 262432: loss: 0.2190543676:
9: 265632: loss: 0.2188980071:
9: 268832: loss: 0.2187570323:
9: 272032: loss: 0.2186503287:
9: 275232: loss: 0.2185255660:
9: 278432: loss: 0.2184332077:
9: 281632: loss: 0.2182689191:
9: 284832: loss: 0.2182040994:
9: 288032: loss: 0.2180360768:
9: 291232: loss: 0.2178479195:
9: 294432: loss: 0.2176711141:
9: 297632: loss: 0.2175160787:
9: 300832: loss: 0.2174561033:
9: 304032: loss: 0.2173890145:
9: 307232: loss: 0.2172784032:
9: 310432: loss: 0.2171099946:
9: 313632: loss: 0.2169808654:
9: 316832: loss: 0.2167918779:
9: 320032: loss: 0.2166263564:
9: 323232: loss: 0.2164631469:
9: 326432: loss: 0.2162675790:
9: 329632: loss: 0.2161016821:
Dev-Acc: 9: Accuracy: 0.1599658728: precision: 0.0596317455: recall: 0.9069886074: f1: 0.1119060107
Train-Acc: 9: Accuracy: 0.9474262595: precision: 0.9799603095: recall: 0.9102316602: f1: 0.9438098497
10: 3232: loss: 0.1907281115:
10: 6432: loss: 0.1935971492:
10: 9632: loss: 0.1970097201:
10: 12832: loss: 0.2002264885:
10: 16032: loss: 0.2010494588:
10: 19232: loss: 0.2012848077:
10: 22432: loss: 0.2002802562:
10: 25632: loss: 0.2003091052:
10: 28832: loss: 0.2006595428:
10: 32032: loss: 0.2005529918:
10: 35232: loss: 0.2006466676:
10: 38432: loss: 0.2004075271:
10: 41632: loss: 0.2005317242:
10: 44832: loss: 0.2005868683:
10: 48032: loss: 0.2004998877:
10: 51232: loss: 0.1996212263:
10: 54432: loss: 0.1994345434:
10: 57632: loss: 0.1992513962:
10: 60832: loss: 0.1990145489:
10: 64032: loss: 0.1993499760:
10: 67232: loss: 0.1992080406:
10: 70432: loss: 0.1984500121:
10: 73632: loss: 0.1982400870:
10: 76832: loss: 0.1979446580:
10: 80032: loss: 0.1978288423:
10: 83232: loss: 0.1979677784:
10: 86432: loss: 0.1977735992:
10: 89632: loss: 0.1974513410:
10: 92832: loss: 0.1972814065:
10: 96032: loss: 0.1969278294:
10: 99232: loss: 0.1970919948:
10: 102432: loss: 0.1969501600:
10: 105632: loss: 0.1968718670:
10: 108832: loss: 0.1968074998:
10: 112032: loss: 0.1963933208:
10: 115232: loss: 0.1964269550:
10: 118432: loss: 0.1964230028:
10: 121632: loss: 0.1961616005:
10: 124832: loss: 0.1962023929:
10: 128032: loss: 0.1961429466:
10: 131232: loss: 0.1963225785:
10: 134432: loss: 0.1962450803:
10: 137632: loss: 0.1963798241:
10: 140832: loss: 0.1961411297:
10: 144032: loss: 0.1959244046:
10: 147232: loss: 0.1960814436:
10: 150432: loss: 0.1960813310:
10: 153632: loss: 0.1959029575:
10: 156832: loss: 0.1959187211:
10: 160032: loss: 0.1959966602:
10: 163232: loss: 0.1957576511:
10: 166432: loss: 0.1955220035:
10: 169632: loss: 0.1953640300:
10: 172832: loss: 0.1951738849:
10: 176032: loss: 0.1950807525:
10: 179232: loss: 0.1948124033:
10: 182432: loss: 0.1946642100:
10: 185632: loss: 0.1945255390:
10: 188832: loss: 0.1943663616:
10: 192032: loss: 0.1943566338:
10: 195232: loss: 0.1943134259:
10: 198432: loss: 0.1943457779:
10: 201632: loss: 0.1942069026:
10: 204832: loss: 0.1939625498:
10: 208032: loss: 0.1939218171:
10: 211232: loss: 0.1938022663:
10: 214432: loss: 0.1936907626:
10: 217632: loss: 0.1934410752:
10: 220832: loss: 0.1933061638:
10: 224032: loss: 0.1930233708:
10: 227232: loss: 0.1930065133:
10: 230432: loss: 0.1928036981:
10: 233632: loss: 0.1927621108:
10: 236832: loss: 0.1924649897:
10: 240032: loss: 0.1923171247:
10: 243232: loss: 0.1922603964:
10: 246432: loss: 0.1922473114:
10: 249632: loss: 0.1921805588:
10: 252832: loss: 0.1921348436:
10: 256032: loss: 0.1920567391:
10: 259232: loss: 0.1919873151:
10: 262432: loss: 0.1918598845:
10: 265632: loss: 0.1917137502:
10: 268832: loss: 0.1916106413:
10: 272032: loss: 0.1915256637:
10: 275232: loss: 0.1914377523:
10: 278432: loss: 0.1912637962:
10: 281632: loss: 0.1912158014:
10: 284832: loss: 0.1912158770:
10: 288032: loss: 0.1911406425:
10: 291232: loss: 0.1909863627:
10: 294432: loss: 0.1909120117:
10: 297632: loss: 0.1908603314:
10: 300832: loss: 0.1907893926:
10: 304032: loss: 0.1909061294:
10: 307232: loss: 0.1908115504:
10: 310432: loss: 0.1905614137:
10: 313632: loss: 0.1904762173:
10: 316832: loss: 0.1903601338:
10: 320032: loss: 0.1901880993:
10: 323232: loss: 0.1900184730:
10: 326432: loss: 0.1899606186:
10: 329632: loss: 0.1898773230:
Dev-Acc: 10: Accuracy: 0.1547765434: precision: 0.0594124314: recall: 0.9091991158: f1: 0.1115364157
Train-Acc: 10: Accuracy: 0.9504168630: precision: 0.9815935755: recall: 0.9149395940: f1: 0.9470953019
11: 3232: loss: 0.1726826181:
11: 6432: loss: 0.1741350756:
11: 9632: loss: 0.1786214126:
11: 12832: loss: 0.1780642763:
11: 16032: loss: 0.1796082000:
11: 19232: loss: 0.1805839774:
11: 22432: loss: 0.1804438605:
11: 25632: loss: 0.1801000023:
11: 28832: loss: 0.1802833065:
11: 32032: loss: 0.1799267196:
11: 35232: loss: 0.1797663522:
11: 38432: loss: 0.1795209331:
11: 41632: loss: 0.1792660454:
11: 44832: loss: 0.1788490843:
11: 48032: loss: 0.1788611712:
11: 51232: loss: 0.1788059395:
11: 54432: loss: 0.1791846435:
11: 57632: loss: 0.1788331079:
11: 60832: loss: 0.1784357854:
11: 64032: loss: 0.1783680026:
11: 67232: loss: 0.1784493640:
11: 70432: loss: 0.1781392573:
11: 73632: loss: 0.1778474669:
11: 76832: loss: 0.1776074015:
11: 80032: loss: 0.1773312844:
11: 83232: loss: 0.1773003344:
11: 86432: loss: 0.1773457097:
11: 89632: loss: 0.1773492884:
11: 92832: loss: 0.1771213544:
11: 96032: loss: 0.1767674857:
11: 99232: loss: 0.1768422812:
11: 102432: loss: 0.1768735561:
11: 105632: loss: 0.1766990826:
11: 108832: loss: 0.1763231911:
11: 112032: loss: 0.1763800215:
11: 115232: loss: 0.1764067453:
11: 118432: loss: 0.1762327728:
11: 121632: loss: 0.1761713201:
11: 124832: loss: 0.1759779101:
11: 128032: loss: 0.1759332393:
11: 131232: loss: 0.1758761671:
11: 134432: loss: 0.1758632563:
11: 137632: loss: 0.1756445878:
11: 140832: loss: 0.1754223654:
11: 144032: loss: 0.1754824105:
11: 147232: loss: 0.1753412441:
11: 150432: loss: 0.1751100227:
11: 153632: loss: 0.1749252691:
11: 156832: loss: 0.1748382335:
11: 160032: loss: 0.1747342294:
11: 163232: loss: 0.1747066948:
11: 166432: loss: 0.1745295392:
11: 169632: loss: 0.1743855880:
11: 172832: loss: 0.1742592312:
11: 176032: loss: 0.1740259796:
11: 179232: loss: 0.1740191358:
11: 182432: loss: 0.1738517139:
11: 185632: loss: 0.1738943455:
11: 188832: loss: 0.1737569573:
11: 192032: loss: 0.1735365782:
11: 195232: loss: 0.1735667131:
11: 198432: loss: 0.1734445019:
11: 201632: loss: 0.1734993408:
11: 204832: loss: 0.1732607172:
11: 208032: loss: 0.1731524462:
11: 211232: loss: 0.1730320532:
11: 214432: loss: 0.1728866432:
11: 217632: loss: 0.1726900333:
11: 220832: loss: 0.1725183445:
11: 224032: loss: 0.1724057080:
11: 227232: loss: 0.1724282452:
11: 230432: loss: 0.1723200249:
11: 233632: loss: 0.1721802280:
11: 236832: loss: 0.1721224064:
11: 240032: loss: 0.1720136469:
11: 243232: loss: 0.1718711693:
11: 246432: loss: 0.1718311918:
11: 249632: loss: 0.1717324745:
11: 252832: loss: 0.1716013504:
11: 256032: loss: 0.1714650046:
11: 259232: loss: 0.1714185607:
11: 262432: loss: 0.1713389035:
11: 265632: loss: 0.1711288491:
11: 268832: loss: 0.1710978211:
11: 272032: loss: 0.1709206154:
11: 275232: loss: 0.1708007547:
11: 278432: loss: 0.1707365937:
11: 281632: loss: 0.1706503468:
11: 284832: loss: 0.1704810983:
11: 288032: loss: 0.1704302963:
11: 291232: loss: 0.1704013150:
11: 294432: loss: 0.1703479598:
11: 297632: loss: 0.1702022690:
11: 300832: loss: 0.1702072888:
11: 304032: loss: 0.1702161162:
11: 307232: loss: 0.1701232915:
11: 310432: loss: 0.1700438159:
11: 313632: loss: 0.1699424138:
11: 316832: loss: 0.1698590143:
11: 320032: loss: 0.1698261908:
11: 323232: loss: 0.1696831061:
11: 326432: loss: 0.1695880068:
11: 329632: loss: 0.1694690470:
Dev-Acc: 11: Accuracy: 0.1524547487: precision: 0.0590628881: recall: 0.9057983336: f1: 0.1108948404
Train-Acc: 11: Accuracy: 0.9543166757: precision: 0.9871071015: recall: 0.9178104372: f1: 0.9511983400
12: 3232: loss: 0.1656615580:
12: 6432: loss: 0.1600045338:
12: 9632: loss: 0.1596548545:
12: 12832: loss: 0.1612580060:
12: 16032: loss: 0.1622688970:
12: 19232: loss: 0.1630447399:
12: 22432: loss: 0.1622845767:
12: 25632: loss: 0.1625488751:
12: 28832: loss: 0.1619360771:
12: 32032: loss: 0.1610893872:
12: 35232: loss: 0.1608497426:
12: 38432: loss: 0.1610387974:
12: 41632: loss: 0.1608259074:
12: 44832: loss: 0.1601715453:
12: 48032: loss: 0.1606211331:
12: 51232: loss: 0.1601390262:
12: 54432: loss: 0.1599637315:
12: 57632: loss: 0.1601878504:
12: 60832: loss: 0.1605932694:
12: 64032: loss: 0.1609116054:
12: 67232: loss: 0.1608670775:
12: 70432: loss: 0.1609096382:
12: 73632: loss: 0.1604534167:
12: 76832: loss: 0.1603584521:
12: 80032: loss: 0.1605670649:
12: 83232: loss: 0.1601514359:
12: 86432: loss: 0.1602513473:
12: 89632: loss: 0.1602622065:
12: 92832: loss: 0.1601520755:
12: 96032: loss: 0.1599646975:
12: 99232: loss: 0.1597580606:
12: 102432: loss: 0.1594991561:
12: 105632: loss: 0.1592276986:
12: 108832: loss: 0.1589582183:
12: 112032: loss: 0.1589628180:
12: 115232: loss: 0.1587857448:
12: 118432: loss: 0.1587527471:
12: 121632: loss: 0.1585748382:
12: 124832: loss: 0.1584258167:
12: 128032: loss: 0.1581662311:
12: 131232: loss: 0.1580490772:
12: 134432: loss: 0.1579502489:
12: 137632: loss: 0.1579342459:
12: 140832: loss: 0.1578887071:
12: 144032: loss: 0.1579609555:
12: 147232: loss: 0.1580017149:
12: 150432: loss: 0.1579197861:
12: 153632: loss: 0.1578721816:
12: 156832: loss: 0.1577197927:
12: 160032: loss: 0.1577331501:
12: 163232: loss: 0.1574693799:
12: 166432: loss: 0.1573175276:
12: 169632: loss: 0.1573413559:
12: 172832: loss: 0.1573974970:
12: 176032: loss: 0.1572759860:
12: 179232: loss: 0.1570802288:
12: 182432: loss: 0.1570010372:
12: 185632: loss: 0.1568880114:
12: 188832: loss: 0.1568628534:
12: 192032: loss: 0.1568281453:
12: 195232: loss: 0.1567040623:
12: 198432: loss: 0.1565825153:
12: 201632: loss: 0.1566704860:
12: 204832: loss: 0.1565755521:
12: 208032: loss: 0.1565032796:
12: 211232: loss: 0.1564322240:
12: 214432: loss: 0.1563493263:
12: 217632: loss: 0.1563163765:
12: 220832: loss: 0.1564953807:
12: 224032: loss: 0.1564087747:
12: 227232: loss: 0.1564339317:
12: 230432: loss: 0.1562664594:
12: 233632: loss: 0.1560771937:
12: 236832: loss: 0.1559644957:
12: 240032: loss: 0.1558857988:
12: 243232: loss: 0.1558385977:
12: 246432: loss: 0.1558650595:
12: 249632: loss: 0.1557307648:
12: 252832: loss: 0.1556391944:
12: 256032: loss: 0.1555892990:
12: 259232: loss: 0.1555914213:
12: 262432: loss: 0.1555111938:
12: 265632: loss: 0.1554027810:
12: 268832: loss: 0.1554394870:
12: 272032: loss: 0.1553973147:
12: 275232: loss: 0.1553264562:
12: 278432: loss: 0.1553248664:
12: 281632: loss: 0.1551874147:
12: 284832: loss: 0.1550957307:
12: 288032: loss: 0.1548679385:
12: 291232: loss: 0.1547813503:
12: 294432: loss: 0.1546833618:
12: 297632: loss: 0.1545728923:
12: 300832: loss: 0.1544305226:
12: 304032: loss: 0.1543239114:
12: 307232: loss: 0.1541913790:
12: 310432: loss: 0.1541237195:
12: 313632: loss: 0.1539781712:
12: 316832: loss: 0.1538950846:
12: 320032: loss: 0.1537303565:
12: 323232: loss: 0.1536997188:
12: 326432: loss: 0.1536794475:
12: 329632: loss: 0.1536162712:
Dev-Acc: 12: Accuracy: 0.1479302198: precision: 0.0589117537: recall: 0.9083489203: f1: 0.1106473762
Train-Acc: 12: Accuracy: 0.9573435187: precision: 0.9877966734: recall: 0.9234711670: f1: 0.9545514527
13: 3232: loss: 0.1515066990:
13: 6432: loss: 0.1474134038:
13: 9632: loss: 0.1484199771:
13: 12832: loss: 0.1464668368:
13: 16032: loss: 0.1456243949:
13: 19232: loss: 0.1452692147:
13: 22432: loss: 0.1466821207:
13: 25632: loss: 0.1479234799:
13: 28832: loss: 0.1478657453:
13: 32032: loss: 0.1485199947:
13: 35232: loss: 0.1478476298:
13: 38432: loss: 0.1476134139:
13: 41632: loss: 0.1470961883:
13: 44832: loss: 0.1475072190:
13: 48032: loss: 0.1474954133:
13: 51232: loss: 0.1467934366:
13: 54432: loss: 0.1469976273:
13: 57632: loss: 0.1469711421:
13: 60832: loss: 0.1473870038:
13: 64032: loss: 0.1469891070:
13: 67232: loss: 0.1468360770:
13: 70432: loss: 0.1467187675:
13: 73632: loss: 0.1465161733:
13: 76832: loss: 0.1462147503:
13: 80032: loss: 0.1465296760:
13: 83232: loss: 0.1465520471:
13: 86432: loss: 0.1465751488:
13: 89632: loss: 0.1463590378:
13: 92832: loss: 0.1460950381:
13: 96032: loss: 0.1460303871:
13: 99232: loss: 0.1459806413:
13: 102432: loss: 0.1457079712:
13: 105632: loss: 0.1457908986:
13: 108832: loss: 0.1456734676:
13: 112032: loss: 0.1455192008:
13: 115232: loss: 0.1454208505:
13: 118432: loss: 0.1449873257:
13: 121632: loss: 0.1448798129:
13: 124832: loss: 0.1447152729:
13: 128032: loss: 0.1445138616:
13: 131232: loss: 0.1443789883:
13: 134432: loss: 0.1443812074:
13: 137632: loss: 0.1442941529:
13: 140832: loss: 0.1441969889:
13: 144032: loss: 0.1441640004:
13: 147232: loss: 0.1440512982:
13: 150432: loss: 0.1438484836:
13: 153632: loss: 0.1439813324:
13: 156832: loss: 0.1438786694:
13: 160032: loss: 0.1437091728:
13: 163232: loss: 0.1437289314:
13: 166432: loss: 0.1436018363:
13: 169632: loss: 0.1435365716:
13: 172832: loss: 0.1435014894:
13: 176032: loss: 0.1433663696:
13: 179232: loss: 0.1433465623:
13: 182432: loss: 0.1432361411:
13: 185632: loss: 0.1431875412:
13: 188832: loss: 0.1432626167:
13: 192032: loss: 0.1432441869:
13: 195232: loss: 0.1431953405:
13: 198432: loss: 0.1430899012:
13: 201632: loss: 0.1429878658:
13: 204832: loss: 0.1427964145:
13: 208032: loss: 0.1426005562:
13: 211232: loss: 0.1426653217:
13: 214432: loss: 0.1427615348:
13: 217632: loss: 0.1426839348:
13: 220832: loss: 0.1426224024:
13: 224032: loss: 0.1428448426:
13: 227232: loss: 0.1428260321:
13: 230432: loss: 0.1427251140:
13: 233632: loss: 0.1427430114:
13: 236832: loss: 0.1426779138:
13: 240032: loss: 0.1427107739:
13: 243232: loss: 0.1426709232:
13: 246432: loss: 0.1425821532:
13: 249632: loss: 0.1425797791:
13: 252832: loss: 0.1423481853:
13: 256032: loss: 0.1422537624:
13: 259232: loss: 0.1421317498:
13: 262432: loss: 0.1422533226:
13: 265632: loss: 0.1421442473:
13: 268832: loss: 0.1421225954:
13: 272032: loss: 0.1421717295:
13: 275232: loss: 0.1422339216:
13: 278432: loss: 0.1420700442:
13: 281632: loss: 0.1420586622:
13: 284832: loss: 0.1419768957:
13: 288032: loss: 0.1418648642:
13: 291232: loss: 0.1418395982:
13: 294432: loss: 0.1417972900:
13: 297632: loss: 0.1417172332:
13: 300832: loss: 0.1416930868:
13: 304032: loss: 0.1415820684:
13: 307232: loss: 0.1415732468:
13: 310432: loss: 0.1415028478:
13: 313632: loss: 0.1414063219:
13: 316832: loss: 0.1412984585:
13: 320032: loss: 0.1412551021:
13: 323232: loss: 0.1412686342:
13: 326432: loss: 0.1412244215:
13: 329632: loss: 0.1411084374:
Dev-Acc: 13: Accuracy: 0.1451619267: precision: 0.0584883452: recall: 0.9040979425: f1: 0.1098689920
Train-Acc: 13: Accuracy: 0.9605697393: precision: 0.9916747209: recall: 0.9264914684: f1: 0.9579755638
14: 3232: loss: 0.1335332413:
14: 6432: loss: 0.1330717184:
14: 9632: loss: 0.1352255167:
14: 12832: loss: 0.1339530232:
14: 16032: loss: 0.1329469423:
14: 19232: loss: 0.1342051972:
14: 22432: loss: 0.1355445889:
14: 25632: loss: 0.1352859645:
14: 28832: loss: 0.1356641246:
14: 32032: loss: 0.1361047166:
14: 35232: loss: 0.1360657072:
14: 38432: loss: 0.1359220950:
14: 41632: loss: 0.1358791692:
14: 44832: loss: 0.1354116101:
14: 48032: loss: 0.1354864305:
14: 51232: loss: 0.1355590900:
14: 54432: loss: 0.1355429896:
14: 57632: loss: 0.1353366522:
14: 60832: loss: 0.1355309518:
14: 64032: loss: 0.1353367608:
14: 67232: loss: 0.1348257356:
14: 70432: loss: 0.1348482283:
14: 73632: loss: 0.1348213448:
14: 76832: loss: 0.1346244677:
14: 80032: loss: 0.1344893145:
14: 83232: loss: 0.1343097072:
14: 86432: loss: 0.1342695807:
14: 89632: loss: 0.1341649560:
14: 92832: loss: 0.1343273922:
14: 96032: loss: 0.1342835621:
14: 99232: loss: 0.1343125512:
14: 102432: loss: 0.1341921486:
14: 105632: loss: 0.1340744808:
14: 108832: loss: 0.1340778153:
14: 112032: loss: 0.1340749959:
14: 115232: loss: 0.1340547528:
14: 118432: loss: 0.1343086135:
14: 121632: loss: 0.1341804240:
14: 124832: loss: 0.1340723871:
14: 128032: loss: 0.1338928101:
14: 131232: loss: 0.1335634914:
14: 134432: loss: 0.1335197929:
14: 137632: loss: 0.1332748199:
14: 140832: loss: 0.1334452606:
14: 144032: loss: 0.1334846915:
14: 147232: loss: 0.1333072776:
14: 150432: loss: 0.1332641813:
14: 153632: loss: 0.1331894199:
14: 156832: loss: 0.1331074227:
14: 160032: loss: 0.1330447546:
14: 163232: loss: 0.1328832504:
14: 166432: loss: 0.1329122119:
14: 169632: loss: 0.1327701683:
14: 172832: loss: 0.1327598119:
14: 176032: loss: 0.1325936894:
14: 179232: loss: 0.1323355590:
14: 182432: loss: 0.1322693116:
14: 185632: loss: 0.1321705584:
14: 188832: loss: 0.1322533426:
14: 192032: loss: 0.1321274989:
14: 195232: loss: 0.1321445252:
14: 198432: loss: 0.1323175156:
14: 201632: loss: 0.1322436795:
14: 204832: loss: 0.1320668817:
14: 208032: loss: 0.1320962780:
14: 211232: loss: 0.1322206630:
14: 214432: loss: 0.1321299068:
14: 217632: loss: 0.1319161538:
14: 220832: loss: 0.1318387617:
14: 224032: loss: 0.1318353183:
14: 227232: loss: 0.1318391760:
14: 230432: loss: 0.1318361918:
14: 233632: loss: 0.1317500995:
14: 236832: loss: 0.1318497005:
14: 240032: loss: 0.1318396286:
14: 243232: loss: 0.1317066538:
14: 246432: loss: 0.1316483789:
14: 249632: loss: 0.1316075551:
14: 252832: loss: 0.1315685544:
14: 256032: loss: 0.1315195822:
14: 259232: loss: 0.1314295070:
14: 262432: loss: 0.1313172683:
14: 265632: loss: 0.1312747216:
14: 268832: loss: 0.1313018578:
14: 272032: loss: 0.1312812889:
14: 275232: loss: 0.1312196934:
14: 278432: loss: 0.1312133813:
14: 281632: loss: 0.1311537619:
14: 284832: loss: 0.1311227263:
14: 288032: loss: 0.1311171308:
14: 291232: loss: 0.1311652520:
14: 294432: loss: 0.1311392786:
14: 297632: loss: 0.1310648870:
14: 300832: loss: 0.1309926119:
14: 304032: loss: 0.1309461055:
14: 307232: loss: 0.1308637322:
14: 310432: loss: 0.1308874550:
14: 313632: loss: 0.1309687954:
14: 316832: loss: 0.1308503813:
14: 320032: loss: 0.1307925898:
14: 323232: loss: 0.1307069949:
14: 326432: loss: 0.1306208903:
14: 329632: loss: 0.1305889198:
Dev-Acc: 14: Accuracy: 0.1405977160: precision: 0.0582712321: recall: 0.9054582554: f1: 0.1094958052
Train-Acc: 14: Accuracy: 0.9635512233: precision: 0.9932381269: recall: 0.9311994022: f1: 0.9612187831
15: 3232: loss: 0.1165905794:
15: 6432: loss: 0.1197991655:
15: 9632: loss: 0.1209290870:
15: 12832: loss: 0.1229880698:
15: 16032: loss: 0.1238782296:
15: 19232: loss: 0.1266110329:
15: 22432: loss: 0.1256819101:
15: 25632: loss: 0.1247913421:
15: 28832: loss: 0.1251884996:
15: 32032: loss: 0.1258098089:
15: 35232: loss: 0.1255562838:
15: 38432: loss: 0.1256221064:
15: 41632: loss: 0.1260054260:
15: 44832: loss: 0.1253786082:
15: 48032: loss: 0.1250057646:
15: 51232: loss: 0.1251534810:
15: 54432: loss: 0.1253433050:
15: 57632: loss: 0.1249562359:
15: 60832: loss: 0.1253756096:
15: 64032: loss: 0.1252507884:
15: 67232: loss: 0.1255442692:
15: 70432: loss: 0.1253313904:
15: 73632: loss: 0.1252302209:
15: 76832: loss: 0.1254259724:
15: 80032: loss: 0.1252426474:
15: 83232: loss: 0.1249188356:
15: 86432: loss: 0.1247741478:
15: 89632: loss: 0.1247412118:
15: 92832: loss: 0.1249255007:
15: 96032: loss: 0.1252285428:
15: 99232: loss: 0.1251516383:
15: 102432: loss: 0.1251421472:
15: 105632: loss: 0.1247990058:
15: 108832: loss: 0.1247791743:
15: 112032: loss: 0.1246256882:
15: 115232: loss: 0.1245505404:
15: 118432: loss: 0.1246412173:
15: 121632: loss: 0.1243868749:
15: 124832: loss: 0.1245542488:
15: 128032: loss: 0.1243786871:
15: 131232: loss: 0.1243231060:
15: 134432: loss: 0.1244292222:
15: 137632: loss: 0.1244432780:
15: 140832: loss: 0.1243983517:
15: 144032: loss: 0.1242992158:
15: 147232: loss: 0.1243213165:
15: 150432: loss: 0.1241831036:
15: 153632: loss: 0.1242654738:
15: 156832: loss: 0.1243084520:
15: 160032: loss: 0.1244241463:
15: 163232: loss: 0.1244135424:
15: 166432: loss: 0.1244133575:
15: 169632: loss: 0.1243624548:
15: 172832: loss: 0.1242159518:
15: 176032: loss: 0.1242210436:
15: 179232: loss: 0.1241683202:
15: 182432: loss: 0.1242290696:
15: 185632: loss: 0.1240931644:
15: 188832: loss: 0.1240004153:
15: 192032: loss: 0.1240100277:
15: 195232: loss: 0.1239881318:
15: 198432: loss: 0.1238889362:
15: 201632: loss: 0.1237995100:
15: 204832: loss: 0.1237265599:
15: 208032: loss: 0.1235930210:
15: 211232: loss: 0.1235005773:
15: 214432: loss: 0.1234466879:
15: 217632: loss: 0.1235246177:
15: 220832: loss: 0.1236001723:
15: 224032: loss: 0.1235071764:
15: 227232: loss: 0.1235232787:
15: 230432: loss: 0.1234411365:
15: 233632: loss: 0.1232743358:
15: 236832: loss: 0.1232941992:
15: 240032: loss: 0.1233958152:
15: 243232: loss: 0.1232517903:
15: 246432: loss: 0.1232348538:
15: 249632: loss: 0.1232002724:
15: 252832: loss: 0.1231436265:
15: 256032: loss: 0.1230581168:
15: 259232: loss: 0.1230757905:
15: 262432: loss: 0.1230749394:
15: 265632: loss: 0.1231391433:
15: 268832: loss: 0.1231838172:
15: 272032: loss: 0.1231751033:
15: 275232: loss: 0.1231353686:
15: 278432: loss: 0.1230916845:
15: 281632: loss: 0.1231307584:
15: 284832: loss: 0.1230840223:
15: 288032: loss: 0.1230187486:
15: 291232: loss: 0.1229329479:
15: 294432: loss: 0.1229127694:
15: 297632: loss: 0.1228866776:
15: 300832: loss: 0.1228126035:
15: 304032: loss: 0.1227247586:
15: 307232: loss: 0.1226862597:
15: 310432: loss: 0.1226152369:
15: 313632: loss: 0.1225079716:
15: 316832: loss: 0.1224531477:
15: 320032: loss: 0.1223327144:
15: 323232: loss: 0.1223188571:
15: 326432: loss: 0.1223426875:
15: 329632: loss: 0.1223401081:
Dev-Acc: 15: Accuracy: 0.1373432279: precision: 0.0578506987: recall: 0.9017173950: f1: 0.1087259606
Train-Acc: 15: Accuracy: 0.9655691385: precision: 0.9948781232: recall: 0.9338273758: f1: 0.9633865072
16: 3232: loss: 0.1191716800:
16: 6432: loss: 0.1159464455:
16: 9632: loss: 0.1160085164:
16: 12832: loss: 0.1163258278:
16: 16032: loss: 0.1184957705:
16: 19232: loss: 0.1178038339:
16: 22432: loss: 0.1171706575:
16: 25632: loss: 0.1173778686:
16: 28832: loss: 0.1170339686:
16: 32032: loss: 0.1167204314:
16: 35232: loss: 0.1158459590:
16: 38432: loss: 0.1164823997:
16: 41632: loss: 0.1168821388:
16: 44832: loss: 0.1170788505:
16: 48032: loss: 0.1171632630:
16: 51232: loss: 0.1173013518:
16: 54432: loss: 0.1174018043:
16: 57632: loss: 0.1172774177:
16: 60832: loss: 0.1176604448:
16: 64032: loss: 0.1176272326:
16: 67232: loss: 0.1178702494:
16: 70432: loss: 0.1176424447:
16: 73632: loss: 0.1176063647:
16: 76832: loss: 0.1177080227:
16: 80032: loss: 0.1176435582:
16: 83232: loss: 0.1176099701:
16: 86432: loss: 0.1173555023:
16: 89632: loss: 0.1170516001:
16: 92832: loss: 0.1175267724:
16: 96032: loss: 0.1175893141:
16: 99232: loss: 0.1178411194:
16: 102432: loss: 0.1178570278:
16: 105632: loss: 0.1178524038:
16: 108832: loss: 0.1180320998:
16: 112032: loss: 0.1180251575:
16: 115232: loss: 0.1180709193:
16: 118432: loss: 0.1179151805:
16: 121632: loss: 0.1180248066:
16: 124832: loss: 0.1179443489:
16: 128032: loss: 0.1178256286:
16: 131232: loss: 0.1178882132:
16: 134432: loss: 0.1176404391:
16: 137632: loss: 0.1176320978:
16: 140832: loss: 0.1174068414:
16: 144032: loss: 0.1172429285:
16: 147232: loss: 0.1173099532:
16: 150432: loss: 0.1172443939:
16: 153632: loss: 0.1171539418:
16: 156832: loss: 0.1169286025:
16: 160032: loss: 0.1169903600:
16: 163232: loss: 0.1169439929:
16: 166432: loss: 0.1168620862:
16: 169632: loss: 0.1170169179:
16: 172832: loss: 0.1169497594:
16: 176032: loss: 0.1167998483:
16: 179232: loss: 0.1167491136:
16: 182432: loss: 0.1164848267:
16: 185632: loss: 0.1165484892:
16: 188832: loss: 0.1165388038:
16: 192032: loss: 0.1164753488:
16: 195232: loss: 0.1164327643:
16: 198432: loss: 0.1162784627:
16: 201632: loss: 0.1163017421:
16: 204832: loss: 0.1162837929:
16: 208032: loss: 0.1163798354:
16: 211232: loss: 0.1163511046:
16: 214432: loss: 0.1162696258:
16: 217632: loss: 0.1161796181:
16: 220832: loss: 0.1160317752:
16: 224032: loss: 0.1159585340:
16: 227232: loss: 0.1159515601:
16: 230432: loss: 0.1159588596:
16: 233632: loss: 0.1158275374:
16: 236832: loss: 0.1158451358:
16: 240032: loss: 0.1158261811:
16: 243232: loss: 0.1156728198:
16: 246432: loss: 0.1156587274:
16: 249632: loss: 0.1156322658:
16: 252832: loss: 0.1155506654:
16: 256032: loss: 0.1154864665:
16: 259232: loss: 0.1154561605:
16: 262432: loss: 0.1154808218:
16: 265632: loss: 0.1154374572:
16: 268832: loss: 0.1155676188:
16: 272032: loss: 0.1155919721:
16: 275232: loss: 0.1155952620:
16: 278432: loss: 0.1155617522:
16: 281632: loss: 0.1155625990:
16: 284832: loss: 0.1154875249:
16: 288032: loss: 0.1154981422:
16: 291232: loss: 0.1154863148:
16: 294432: loss: 0.1155155029:
16: 297632: loss: 0.1154514096:
16: 300832: loss: 0.1154573277:
16: 304032: loss: 0.1154039262:
16: 307232: loss: 0.1153401031:
16: 310432: loss: 0.1153152025:
16: 313632: loss: 0.1153213346:
16: 316832: loss: 0.1151976076:
16: 320032: loss: 0.1151494811:
16: 323232: loss: 0.1150981408:
16: 326432: loss: 0.1150880032:
16: 329632: loss: 0.1150939335:
Dev-Acc: 16: Accuracy: 0.1349519789: precision: 0.0574678859: recall: 0.8976364564: f1: 0.1080201758
Train-Acc: 16: Accuracy: 0.9674208760: precision: 0.9963024564: recall: 0.9363121186: f1: 0.9653762067
17: 3232: loss: 0.1112759596:
17: 6432: loss: 0.1146109345:
17: 9632: loss: 0.1137602697:
17: 12832: loss: 0.1126616963:
17: 16032: loss: 0.1112604249:
17: 19232: loss: 0.1095909314:
17: 22432: loss: 0.1098809268:
17: 25632: loss: 0.1104376001:
17: 28832: loss: 0.1106524999:
17: 32032: loss: 0.1108928847:
17: 35232: loss: 0.1106443225:
17: 38432: loss: 0.1107619668:
17: 41632: loss: 0.1104463699:
17: 44832: loss: 0.1104662540:
17: 48032: loss: 0.1106746413:
17: 51232: loss: 0.1106080491:
17: 54432: loss: 0.1100436040:
17: 57632: loss: 0.1101147234:
17: 60832: loss: 0.1104851863:
17: 64032: loss: 0.1103970307:
17: 67232: loss: 0.1103971592:
17: 70432: loss: 0.1105281686:
17: 73632: loss: 0.1105926821:
17: 76832: loss: 0.1104577086:
17: 80032: loss: 0.1107105289:
17: 83232: loss: 0.1106623275:
17: 86432: loss: 0.1108297554:
17: 89632: loss: 0.1106783866:
17: 92832: loss: 0.1106795860:
17: 96032: loss: 0.1105684020:
17: 99232: loss: 0.1104297664:
17: 102432: loss: 0.1103778510:
17: 105632: loss: 0.1105209882:
17: 108832: loss: 0.1103824925:
17: 112032: loss: 0.1103324586:
17: 115232: loss: 0.1101778649:
17: 118432: loss: 0.1101529473:
17: 121632: loss: 0.1102509204:
17: 124832: loss: 0.1104644484:
17: 128032: loss: 0.1102550270:
17: 131232: loss: 0.1102044862:
17: 134432: loss: 0.1100077299:
17: 137632: loss: 0.1100660618:
17: 140832: loss: 0.1099485107:
17: 144032: loss: 0.1098199876:
17: 147232: loss: 0.1098581116:
17: 150432: loss: 0.1097248556:
17: 153632: loss: 0.1096947009:
17: 156832: loss: 0.1096354936:
17: 160032: loss: 0.1097464183:
17: 163232: loss: 0.1097364310:
17: 166432: loss: 0.1096206942:
17: 169632: loss: 0.1095592200:
17: 172832: loss: 0.1094780638:
17: 176032: loss: 0.1094856968:
17: 179232: loss: 0.1095122080:
17: 182432: loss: 0.1094881599:
17: 185632: loss: 0.1093927208:
17: 188832: loss: 0.1093454322:
17: 192032: loss: 0.1092362803:
17: 195232: loss: 0.1091715427:
17: 198432: loss: 0.1091913587:
17: 201632: loss: 0.1093315957:
17: 204832: loss: 0.1093301652:
17: 208032: loss: 0.1093909948:
17: 211232: loss: 0.1092212617:
17: 214432: loss: 0.1093221967:
17: 217632: loss: 0.1093442444:
17: 220832: loss: 0.1093088294:
17: 224032: loss: 0.1093087166:
17: 227232: loss: 0.1093351760:
17: 230432: loss: 0.1092019109:
17: 233632: loss: 0.1091938458:
17: 236832: loss: 0.1091320221:
17: 240032: loss: 0.1090495789:
17: 243232: loss: 0.1091208989:
17: 246432: loss: 0.1090610413:
17: 249632: loss: 0.1090870449:
17: 252832: loss: 0.1090133460:
17: 256032: loss: 0.1090773895:
17: 259232: loss: 0.1091422725:
17: 262432: loss: 0.1090182831:
17: 265632: loss: 0.1090081118:
17: 268832: loss: 0.1089851521:
17: 272032: loss: 0.1089825404:
17: 275232: loss: 0.1090125307:
17: 278432: loss: 0.1090530460:
17: 281632: loss: 0.1090245753:
17: 284832: loss: 0.1090939128:
17: 288032: loss: 0.1091194714:
17: 291232: loss: 0.1090648134:
17: 294432: loss: 0.1090582374:
17: 297632: loss: 0.1090270705:
17: 300832: loss: 0.1089853375:
17: 304032: loss: 0.1090742233:
17: 307232: loss: 0.1090298065:
17: 310432: loss: 0.1089780740:
17: 313632: loss: 0.1089088945:
17: 316832: loss: 0.1088589808:
17: 320032: loss: 0.1088709257:
17: 323232: loss: 0.1088646159:
17: 326432: loss: 0.1088190431:
17: 329632: loss: 0.1088821925:
Dev-Acc: 17: Accuracy: 0.1305961311: precision: 0.0575971770: recall: 0.9047780990: f1: 0.1083000896
Train-Acc: 17: Accuracy: 0.9697800875: precision: 0.9960860283: recall: 0.9413999253: f1: 0.9679712112
18: 3232: loss: 0.1031886675:
18: 6432: loss: 0.1033158223:
18: 9632: loss: 0.1032486920:
18: 12832: loss: 0.1030864892:
18: 16032: loss: 0.1038788749:
18: 19232: loss: 0.1043033156:
18: 22432: loss: 0.1044355295:
18: 25632: loss: 0.1046187962:
18: 28832: loss: 0.1049139509:
18: 32032: loss: 0.1041588212:
18: 35232: loss: 0.1033260515:
18: 38432: loss: 0.1037869523:
18: 41632: loss: 0.1039914897:
18: 44832: loss: 0.1036908029:
18: 48032: loss: 0.1039570570:
18: 51232: loss: 0.1039984970:
18: 54432: loss: 0.1043760316:
18: 57632: loss: 0.1039566831:
18: 60832: loss: 0.1038808917:
18: 64032: loss: 0.1038189407:
18: 67232: loss: 0.1038455322:
18: 70432: loss: 0.1036232586:
18: 73632: loss: 0.1038341141:
18: 76832: loss: 0.1039211719:
18: 80032: loss: 0.1039342798:
18: 83232: loss: 0.1042997616:
18: 86432: loss: 0.1043422145:
18: 89632: loss: 0.1046073559:
18: 92832: loss: 0.1048228828:
18: 96032: loss: 0.1048880808:
18: 99232: loss: 0.1047649131:
18: 102432: loss: 0.1046690651:
18: 105632: loss: 0.1045004626:
18: 108832: loss: 0.1046041796:
18: 112032: loss: 0.1045317482:
18: 115232: loss: 0.1044960394:
18: 118432: loss: 0.1045806661:
18: 121632: loss: 0.1046431731:
18: 124832: loss: 0.1044442191:
18: 128032: loss: 0.1044906048:
18: 131232: loss: 0.1043648371:
18: 134432: loss: 0.1046217496:
18: 137632: loss: 0.1045306270:
18: 140832: loss: 0.1045171566:
18: 144032: loss: 0.1047199851:
18: 147232: loss: 0.1045158393:
18: 150432: loss: 0.1045252890:
18: 153632: loss: 0.1044028271:
18: 156832: loss: 0.1044460518:
18: 160032: loss: 0.1042210417:
18: 163232: loss: 0.1041047558:
18: 166432: loss: 0.1040026494:
18: 169632: loss: 0.1040392608:
18: 172832: loss: 0.1040096006:
18: 176032: loss: 0.1039490714:
18: 179232: loss: 0.1039139982:
18: 182432: loss: 0.1039824932:
18: 185632: loss: 0.1038056593:
18: 188832: loss: 0.1038825378:
18: 192032: loss: 0.1039140885:
18: 195232: loss: 0.1038275834:
18: 198432: loss: 0.1037415960:
18: 201632: loss: 0.1037891482:
18: 204832: loss: 0.1037621561:
18: 208032: loss: 0.1038251195:
18: 211232: loss: 0.1038447068:
18: 214432: loss: 0.1038229758:
18: 217632: loss: 0.1038150644:
18: 220832: loss: 0.1037019584:
18: 224032: loss: 0.1037149302:
18: 227232: loss: 0.1037407412:
18: 230432: loss: 0.1035464563:
18: 233632: loss: 0.1034813522:
18: 236832: loss: 0.1034091606:
18: 240032: loss: 0.1033957122:
18: 243232: loss: 0.1034123340:
18: 246432: loss: 0.1033958434:
18: 249632: loss: 0.1034283862:
18: 252832: loss: 0.1033772650:
18: 256032: loss: 0.1032764277:
18: 259232: loss: 0.1032700095:
18: 262432: loss: 0.1033983185:
18: 265632: loss: 0.1034135802:
18: 268832: loss: 0.1034811633:
18: 272032: loss: 0.1034414626:
18: 275232: loss: 0.1034182549:
18: 278432: loss: 0.1033992246:
18: 281632: loss: 0.1033687627:
18: 284832: loss: 0.1033821397:
18: 288032: loss: 0.1034351251:
18: 291232: loss: 0.1033057235:
18: 294432: loss: 0.1034163969:
18: 297632: loss: 0.1034955525:
18: 300832: loss: 0.1034763712:
18: 304032: loss: 0.1033809166:
18: 307232: loss: 0.1034382273:
18: 310432: loss: 0.1033879143:
18: 313632: loss: 0.1034371538:
18: 316832: loss: 0.1033621449:
18: 320032: loss: 0.1033970819:
18: 323232: loss: 0.1033508589:
18: 326432: loss: 0.1034610512:
18: 329632: loss: 0.1034409155:
Dev-Acc: 18: Accuracy: 0.1271035075: precision: 0.0574262763: recall: 0.9056282945: f1: 0.1080039746
Train-Acc: 18: Accuracy: 0.9713901877: precision: 0.9962887302: recall: 0.9445385478: f1: 0.9697237060
19: 3232: loss: 0.0927092604:
19: 6432: loss: 0.0979869543:
19: 9632: loss: 0.0993844779:
19: 12832: loss: 0.0991131336:
19: 16032: loss: 0.0994258250:
19: 19232: loss: 0.0997997386:
19: 22432: loss: 0.0995746228:
19: 25632: loss: 0.0994179852:
19: 28832: loss: 0.1002536581:
19: 32032: loss: 0.0994593224:
19: 35232: loss: 0.0999416648:
19: 38432: loss: 0.1009353901:
19: 41632: loss: 0.1011903388:
19: 44832: loss: 0.1009594967:
19: 48032: loss: 0.1010975138:
19: 51232: loss: 0.1009517825:
19: 54432: loss: 0.1011652903:
19: 57632: loss: 0.1016063860:
19: 60832: loss: 0.1011189234:
19: 64032: loss: 0.1014567561:
19: 67232: loss: 0.1007777695:
19: 70432: loss: 0.1009079099:
19: 73632: loss: 0.1011357520:
19: 76832: loss: 0.1010627568:
19: 80032: loss: 0.1010050175:
19: 83232: loss: 0.1010295082:
19: 86432: loss: 0.1010006780:
19: 89632: loss: 0.1013691639:
19: 92832: loss: 0.1013990680:
19: 96032: loss: 0.1014833342:
19: 99232: loss: 0.1016835151:
19: 102432: loss: 0.1017143269:
19: 105632: loss: 0.1014719676:
19: 108832: loss: 0.1015528546:
19: 112032: loss: 0.1015592838:
19: 115232: loss: 0.1015685538:
19: 118432: loss: 0.1016020523:
19: 121632: loss: 0.1015057012:
19: 124832: loss: 0.1014177329:
19: 128032: loss: 0.1013438167:
19: 131232: loss: 0.1013403215:
19: 134432: loss: 0.1012333066:
19: 137632: loss: 0.1011706645:
19: 140832: loss: 0.1011844952:
19: 144032: loss: 0.1010107595:
19: 147232: loss: 0.1009179750:
19: 150432: loss: 0.1007403569:
19: 153632: loss: 0.1005399701:
19: 156832: loss: 0.1007044479:
19: 160032: loss: 0.1005036015:
19: 163232: loss: 0.1004531254:
19: 166432: loss: 0.1004213951:
19: 169632: loss: 0.1004304440:
19: 172832: loss: 0.1003900380:
19: 176032: loss: 0.1003319611:
19: 179232: loss: 0.1004209895:
19: 182432: loss: 0.1004477365:
19: 185632: loss: 0.1004869708:
19: 188832: loss: 0.1003786632:
19: 192032: loss: 0.1003759568:
19: 195232: loss: 0.1001653019:
19: 198432: loss: 0.1001624459:
19: 201632: loss: 0.1003013916:
19: 204832: loss: 0.1002504829:
19: 208032: loss: 0.1002131479:
19: 211232: loss: 0.1002227863:
19: 214432: loss: 0.1002147032:
19: 217632: loss: 0.1000555731:
19: 220832: loss: 0.0999321189:
19: 224032: loss: 0.0999147034:
19: 227232: loss: 0.0998015975:
19: 230432: loss: 0.0997722411:
19: 233632: loss: 0.0996267419:
19: 236832: loss: 0.0997294146:
19: 240032: loss: 0.0996775459:
19: 243232: loss: 0.0995734085:
19: 246432: loss: 0.0996639925:
19: 249632: loss: 0.0995837518:
19: 252832: loss: 0.0994258600:
19: 256032: loss: 0.0994747115:
19: 259232: loss: 0.0994058541:
19: 262432: loss: 0.0994306047:
19: 265632: loss: 0.0993246691:
19: 268832: loss: 0.0991910137:
19: 272032: loss: 0.0991777352:
19: 275232: loss: 0.0992535183:
19: 278432: loss: 0.0991883612:
19: 281632: loss: 0.0992488282:
19: 284832: loss: 0.0992818862:
19: 288032: loss: 0.0992311428:
19: 291232: loss: 0.0991961248:
19: 294432: loss: 0.0990833926:
19: 297632: loss: 0.0991002663:
19: 300832: loss: 0.0990196566:
19: 304032: loss: 0.0990143760:
19: 307232: loss: 0.0989900880:
19: 310432: loss: 0.0989272466:
19: 313632: loss: 0.0989100199:
19: 316832: loss: 0.0988846038:
19: 320032: loss: 0.0989785742:
19: 323232: loss: 0.0989717928:
19: 326432: loss: 0.0989299700:
19: 329632: loss: 0.0989649729:
Dev-Acc: 19: Accuracy: 0.1256151795: precision: 0.0572477578: recall: 0.9040979425: f1: 0.1076773527
Train-Acc: 19: Accuracy: 0.9723779559: precision: 0.9966482572: recall: 0.9462386349: f1: 0.9707894888
20: 3232: loss: 0.0971898470:
20: 6432: loss: 0.0967446759:
20: 9632: loss: 0.0981627511:
20: 12832: loss: 0.0975405115:
20: 16032: loss: 0.0962092199:
20: 19232: loss: 0.0957655918:
20: 22432: loss: 0.0958662286:
20: 25632: loss: 0.0962962324:
20: 28832: loss: 0.0972546130:
20: 32032: loss: 0.0964741947:
20: 35232: loss: 0.0967498562:
20: 38432: loss: 0.0968276573:
20: 41632: loss: 0.0968834726:
20: 44832: loss: 0.0958273039:
20: 48032: loss: 0.0958216834:
20: 51232: loss: 0.0950573285:
20: 54432: loss: 0.0953317922:
20: 57632: loss: 0.0952582355:
20: 60832: loss: 0.0952200282:
20: 64032: loss: 0.0954290948:
20: 67232: loss: 0.0953283060:
20: 70432: loss: 0.0951454488:
20: 73632: loss: 0.0947729414:
20: 76832: loss: 0.0945218615:
20: 80032: loss: 0.0944977416:
20: 83232: loss: 0.0945587361:
20: 86432: loss: 0.0942874547:
20: 89632: loss: 0.0942217474:
20: 92832: loss: 0.0940755522:
20: 96032: loss: 0.0942877149:
20: 99232: loss: 0.0944626097:
20: 102432: loss: 0.0946010730:
20: 105632: loss: 0.0951035263:
20: 108832: loss: 0.0952166219:
20: 112032: loss: 0.0953211512:
20: 115232: loss: 0.0954432198:
20: 118432: loss: 0.0954878589:
20: 121632: loss: 0.0952858864:
20: 124832: loss: 0.0954602081:
20: 128032: loss: 0.0954049091:
20: 131232: loss: 0.0955690073:
20: 134432: loss: 0.0953990310:
20: 137632: loss: 0.0954359361:
20: 140832: loss: 0.0956689352:
20: 144032: loss: 0.0957806790:
20: 147232: loss: 0.0957518924:
20: 150432: loss: 0.0958094327:
20: 153632: loss: 0.0960264429:
20: 156832: loss: 0.0960085979:
20: 160032: loss: 0.0958703228:
20: 163232: loss: 0.0958008181:
20: 166432: loss: 0.0958497410:
20: 169632: loss: 0.0958396412:
20: 172832: loss: 0.0957678924:
20: 176032: loss: 0.0958050813:
20: 179232: loss: 0.0955961656:
20: 182432: loss: 0.0956828536:
20: 185632: loss: 0.0956500340:
20: 188832: loss: 0.0956088889:
20: 192032: loss: 0.0955487551:
20: 195232: loss: 0.0955006334:
20: 198432: loss: 0.0954952257:
20: 201632: loss: 0.0954409565:
20: 204832: loss: 0.0954703455:
20: 208032: loss: 0.0953467567:
20: 211232: loss: 0.0953057586:
20: 214432: loss: 0.0951547780:
20: 217632: loss: 0.0952363273:
20: 220832: loss: 0.0952617286:
20: 224032: loss: 0.0952747595:
20: 227232: loss: 0.0952985022:
20: 230432: loss: 0.0954264360:
20: 233632: loss: 0.0953317473:
20: 236832: loss: 0.0953068744:
20: 240032: loss: 0.0952941013:
20: 243232: loss: 0.0952165940:
20: 246432: loss: 0.0951503575:
20: 249632: loss: 0.0952476616:
20: 252832: loss: 0.0952363259:
20: 256032: loss: 0.0953353573:
20: 259232: loss: 0.0953654954:
20: 262432: loss: 0.0954084856:
20: 265632: loss: 0.0955113958:
20: 268832: loss: 0.0955172762:
20: 272032: loss: 0.0954764882:
20: 275232: loss: 0.0955128547:
20: 278432: loss: 0.0955545752:
20: 281632: loss: 0.0955412567:
20: 284832: loss: 0.0955127413:
20: 288032: loss: 0.0954989123:
20: 291232: loss: 0.0954895354:
20: 294432: loss: 0.0953866291:
20: 297632: loss: 0.0953844255:
20: 300832: loss: 0.0953169522:
20: 304032: loss: 0.0952470185:
20: 307232: loss: 0.0951448904:
20: 310432: loss: 0.0951653002:
20: 313632: loss: 0.0951220374:
20: 316832: loss: 0.0950082958:
20: 320032: loss: 0.0950672126:
20: 323232: loss: 0.0949911601:
20: 326432: loss: 0.0949506831:
20: 329632: loss: 0.0949354277:
Dev-Acc: 20: Accuracy: 0.1243748963: precision: 0.0569516761: recall: 0.9001870430: f1: 0.1071258739
Train-Acc: 20: Accuracy: 0.9730516076: precision: 0.9970372895: recall: 0.9472599327: f1: 0.9715114181
