1: 6464: loss: 0.7091455102:
1: 12864: loss: 0.7078535208:
1: 19264: loss: 0.7071288131:
1: 25664: loss: 0.7061560208:
1: 32064: loss: 0.7049977994:
1: 38464: loss: 0.7042242361:
1: 44864: loss: 0.7032092799:
1: 51264: loss: 0.7020726295:
1: 57664: loss: 0.7010649659:
1: 64064: loss: 0.7000903817:
1: 70464: loss: 0.6992018746:
1: 76864: loss: 0.6982577465:
1: 83264: loss: 0.6972899607:
1: 89664: loss: 0.6962435857:
1: 96064: loss: 0.6952681825:
1: 102464: loss: 0.6942009767:
1: 108864: loss: 0.6931697343:
1: 115264: loss: 0.6921933950:
1: 121664: loss: 0.6912575718:
1: 128064: loss: 0.6902406621:
1: 134464: loss: 0.6892954784:
Dev-Acc: 1: Accuracy: 0.7947689891: precision: 0.0670877932: recall: 0.1950348580: f1: 0.0998346244
Train-Acc: 1: Accuracy: 0.7847099304: precision: 0.1787837838: recall: 0.2609295904: f1: 0.2121835824
2: 6464: loss: 0.6680539399:
2: 12864: loss: 0.6671790999:
2: 19264: loss: 0.6657379089:
2: 25664: loss: 0.6646523610:
2: 32064: loss: 0.6638044178:
2: 38464: loss: 0.6629935027:
2: 44864: loss: 0.6620018855:
2: 51264: loss: 0.6610089535:
2: 57664: loss: 0.6601945784:
2: 64064: loss: 0.6591007107:
2: 70464: loss: 0.6582710061:
2: 76864: loss: 0.6573073683:
2: 83264: loss: 0.6564527984:
2: 89664: loss: 0.6555728542:
2: 96064: loss: 0.6547215006:
2: 102464: loss: 0.6538734148:
2: 108864: loss: 0.6530329443:
2: 115264: loss: 0.6521810122:
2: 121664: loss: 0.6513574473:
2: 128064: loss: 0.6504650080:
2: 134464: loss: 0.6495097150:
Dev-Acc: 2: Accuracy: 0.9342752695: precision: 0.0695249131: recall: 0.0102023465: f1: 0.0177935943
Train-Acc: 2: Accuracy: 0.8852730989: precision: 0.3146067416: recall: 0.0276115969: f1: 0.0507675571
3: 6464: loss: 0.6291857368:
3: 12864: loss: 0.6290178719:
3: 19264: loss: 0.6281222663:
3: 25664: loss: 0.6268165740:
3: 32064: loss: 0.6258733698:
3: 38464: loss: 0.6250304861:
3: 44864: loss: 0.6240102527:
3: 51264: loss: 0.6233768450:
3: 57664: loss: 0.6226275151:
3: 64064: loss: 0.6218082444:
3: 70464: loss: 0.6210852726:
3: 76864: loss: 0.6200552899:
3: 83264: loss: 0.6190355426:
3: 89664: loss: 0.6182277450:
3: 96064: loss: 0.6172573637:
3: 102464: loss: 0.6164584582:
3: 108864: loss: 0.6156614293:
3: 115264: loss: 0.6148056492:
3: 121664: loss: 0.6139613172:
3: 128064: loss: 0.6132156451:
3: 134464: loss: 0.6124793841:
Dev-Acc: 3: Accuracy: 0.9415482283: precision: 0.1428571429: recall: 0.0003400782: f1: 0.0006785411
Train-Acc: 3: Accuracy: 0.8888888955: precision: 0.5000000000: recall: 0.0011833542: f1: 0.0023611202
4: 6464: loss: 0.5920706999:
4: 12864: loss: 0.5927626044:
4: 19264: loss: 0.5912908246:
4: 25664: loss: 0.5906903139:
4: 32064: loss: 0.5901068709:
4: 38464: loss: 0.5889445975:
4: 44864: loss: 0.5883282527:
4: 51264: loss: 0.5875306150:
4: 57664: loss: 0.5867082319:
4: 64064: loss: 0.5860355014:
4: 70464: loss: 0.5854289673:
4: 76864: loss: 0.5847288486:
4: 83264: loss: 0.5840260362:
4: 89664: loss: 0.5832555223:
4: 96064: loss: 0.5824533488:
4: 102464: loss: 0.5815224466:
4: 108864: loss: 0.5808140955:
4: 115264: loss: 0.5798645310:
4: 121664: loss: 0.5789912675:
4: 128064: loss: 0.5781667782:
4: 134464: loss: 0.5774563209:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8888962269: precision: 1.0000000000: recall: 0.0000657419: f1: 0.0001314752
5: 6464: loss: 0.5623875070:
5: 12864: loss: 0.5603812805:
5: 19264: loss: 0.5588080808:
5: 25664: loss: 0.5584734905:
5: 32064: loss: 0.5577179341:
5: 38464: loss: 0.5562007833:
5: 44864: loss: 0.5550115239:
5: 51264: loss: 0.5543368044:
5: 57664: loss: 0.5535986965:
5: 64064: loss: 0.5529353904:
5: 70464: loss: 0.5521064383:
5: 76864: loss: 0.5510667749:
5: 83264: loss: 0.5503362270:
5: 89664: loss: 0.5495990614:
5: 96064: loss: 0.5489629853:
5: 102464: loss: 0.5481314967:
5: 108864: loss: 0.5472429545:
5: 115264: loss: 0.5463310603:
5: 121664: loss: 0.5456750453:
5: 128064: loss: 0.5451145182:
5: 134464: loss: 0.5443957422:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 6464: loss: 0.5286873072:
6: 12864: loss: 0.5252942005:
6: 19264: loss: 0.5253368693:
6: 25664: loss: 0.5245248687:
6: 32064: loss: 0.5235100973:
6: 38464: loss: 0.5226006225:
6: 44864: loss: 0.5222447211:
6: 51264: loss: 0.5213496886:
6: 57664: loss: 0.5208970585:
6: 64064: loss: 0.5197046672:
6: 70464: loss: 0.5194419503:
6: 76864: loss: 0.5190673089:
6: 83264: loss: 0.5187088981:
6: 89664: loss: 0.5180705137:
6: 96064: loss: 0.5172915715:
6: 102464: loss: 0.5164579731:
6: 108864: loss: 0.5156910067:
6: 115264: loss: 0.5150055794:
6: 121664: loss: 0.5144169098:
6: 128064: loss: 0.5137505365:
6: 134464: loss: 0.5130865724:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 6464: loss: 0.4988280156:
7: 12864: loss: 0.4959943947:
7: 19264: loss: 0.4950547446:
7: 25664: loss: 0.4943125635:
7: 32064: loss: 0.4942117543:
7: 38464: loss: 0.4937325171:
7: 44864: loss: 0.4928466314:
7: 51264: loss: 0.4921233205:
7: 57664: loss: 0.4918786444:
7: 64064: loss: 0.4911559803:
7: 70464: loss: 0.4912221342:
7: 76864: loss: 0.4905702268:
7: 83264: loss: 0.4896092155:
7: 89664: loss: 0.4890013853:
7: 96064: loss: 0.4880603063:
7: 102464: loss: 0.4874797269:
7: 108864: loss: 0.4869355162:
7: 115264: loss: 0.4860071879:
7: 121664: loss: 0.4854726246:
7: 128064: loss: 0.4846140961:
7: 134464: loss: 0.4840619082:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.4716462320:
8: 12864: loss: 0.4699198921:
8: 19264: loss: 0.4678994788:
8: 25664: loss: 0.4676467419:
8: 32064: loss: 0.4672681132:
8: 38464: loss: 0.4662859011:
8: 44864: loss: 0.4656194183:
8: 51264: loss: 0.4650698551:
8: 57664: loss: 0.4648609113:
8: 64064: loss: 0.4641257088:
8: 70464: loss: 0.4638365359:
8: 76864: loss: 0.4630786118:
8: 83264: loss: 0.4621131556:
8: 89664: loss: 0.4615528678:
8: 96064: loss: 0.4608392401:
8: 102464: loss: 0.4602727656:
8: 108864: loss: 0.4595010593:
8: 115264: loss: 0.4588724658:
8: 121664: loss: 0.4578625607:
8: 128064: loss: 0.4570692639:
8: 134464: loss: 0.4567278370:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.4435446033:
9: 12864: loss: 0.4408281215:
9: 19264: loss: 0.4401400273:
9: 25664: loss: 0.4411220462:
9: 32064: loss: 0.4394879835:
9: 38464: loss: 0.4401703950:
9: 44864: loss: 0.4398858104:
9: 51264: loss: 0.4405063839:
9: 57664: loss: 0.4403476709:
9: 64064: loss: 0.4399756192:
9: 70464: loss: 0.4388913945:
9: 76864: loss: 0.4385416203:
9: 83264: loss: 0.4375412390:
9: 89664: loss: 0.4364499222:
9: 96064: loss: 0.4357294114:
9: 102464: loss: 0.4351305484:
9: 108864: loss: 0.4345852688:
9: 115264: loss: 0.4342191308:
9: 121664: loss: 0.4338763120:
9: 128064: loss: 0.4332623942:
9: 134464: loss: 0.4326536099:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.4269763443:
10: 12864: loss: 0.4204795133:
10: 19264: loss: 0.4189156978:
10: 25664: loss: 0.4157433888:
10: 32064: loss: 0.4158666009:
10: 38464: loss: 0.4146282448:
10: 44864: loss: 0.4142111955:
10: 51264: loss: 0.4150944282:
10: 57664: loss: 0.4148406617:
10: 64064: loss: 0.4147169746:
10: 70464: loss: 0.4143988801:
10: 76864: loss: 0.4139228571:
10: 83264: loss: 0.4131063872:
10: 89664: loss: 0.4126052393:
10: 96064: loss: 0.4118581464:
10: 102464: loss: 0.4114456789:
10: 108864: loss: 0.4111299259:
10: 115264: loss: 0.4112054452:
10: 121664: loss: 0.4107875502:
10: 128064: loss: 0.4106366986:
10: 134464: loss: 0.4108791063:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.4022956485:
11: 12864: loss: 0.4001580241:
11: 19264: loss: 0.3988722143:
11: 25664: loss: 0.3990645934:
11: 32064: loss: 0.3976764030:
11: 38464: loss: 0.3981968920:
11: 44864: loss: 0.3982462684:
11: 51264: loss: 0.3972928729:
11: 57664: loss: 0.3971456493:
11: 64064: loss: 0.3973714432:
11: 70464: loss: 0.3969971238:
11: 76864: loss: 0.3961435159:
11: 83264: loss: 0.3961990698:
11: 89664: loss: 0.3957824537:
11: 96064: loss: 0.3953629770:
11: 102464: loss: 0.3946727331:
11: 108864: loss: 0.3937975855:
11: 115264: loss: 0.3933899000:
11: 121664: loss: 0.3930407759:
11: 128064: loss: 0.3921939112:
11: 134464: loss: 0.3916818687:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.3856241739:
12: 12864: loss: 0.3857906277:
12: 19264: loss: 0.3824805628:
12: 25664: loss: 0.3812256685:
12: 32064: loss: 0.3809116845:
12: 38464: loss: 0.3800119894:
12: 44864: loss: 0.3800242599:
12: 51264: loss: 0.3794991552:
12: 57664: loss: 0.3790815691:
12: 64064: loss: 0.3784357375:
12: 70464: loss: 0.3785938679:
12: 76864: loss: 0.3780632083:
12: 83264: loss: 0.3772803923:
12: 89664: loss: 0.3767504721:
12: 96064: loss: 0.3766519256:
12: 102464: loss: 0.3758651537:
12: 108864: loss: 0.3758896374:
12: 115264: loss: 0.3760451596:
12: 121664: loss: 0.3757444269:
12: 128064: loss: 0.3752075922:
12: 134464: loss: 0.3751254806:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.3648613086:
13: 12864: loss: 0.3671869431:
13: 19264: loss: 0.3683731343:
13: 25664: loss: 0.3653779506:
13: 32064: loss: 0.3659505655:
13: 38464: loss: 0.3637586735:
13: 44864: loss: 0.3637051013:
13: 51264: loss: 0.3631306706:
13: 57664: loss: 0.3633238525:
13: 64064: loss: 0.3628073221:
13: 70464: loss: 0.3623934264:
13: 76864: loss: 0.3625476511:
13: 83264: loss: 0.3626130135:
13: 89664: loss: 0.3618264290:
13: 96064: loss: 0.3625655433:
13: 102464: loss: 0.3622783235:
13: 108864: loss: 0.3618855764:
13: 115264: loss: 0.3615644697:
13: 121664: loss: 0.3610069548:
13: 128064: loss: 0.3608934636:
13: 134464: loss: 0.3606380558:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.3655274183:
14: 12864: loss: 0.3558663589:
14: 19264: loss: 0.3571487959:
14: 25664: loss: 0.3551375873:
14: 32064: loss: 0.3546810888:
14: 38464: loss: 0.3538952333:
14: 44864: loss: 0.3532015953:
14: 51264: loss: 0.3522896136:
14: 57664: loss: 0.3517913985:
14: 64064: loss: 0.3509539659:
14: 70464: loss: 0.3510200064:
14: 76864: loss: 0.3505099262:
14: 83264: loss: 0.3502692729:
14: 89664: loss: 0.3499600997:
14: 96064: loss: 0.3494838838:
14: 102464: loss: 0.3497863706:
14: 108864: loss: 0.3493679409:
14: 115264: loss: 0.3488999957:
14: 121664: loss: 0.3489345992:
14: 128064: loss: 0.3486257410:
14: 134464: loss: 0.3480799326:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.3426301686:
15: 12864: loss: 0.3451271242:
15: 19264: loss: 0.3456188015:
15: 25664: loss: 0.3478431070:
15: 32064: loss: 0.3489133821:
15: 38464: loss: 0.3458707456:
15: 44864: loss: 0.3439431919:
15: 51264: loss: 0.3422572529:
15: 57664: loss: 0.3413591397:
15: 64064: loss: 0.3409328534:
15: 70464: loss: 0.3409456364:
15: 76864: loss: 0.3406516292:
15: 83264: loss: 0.3409692545:
15: 89664: loss: 0.3398490934:
15: 96064: loss: 0.3393780131:
15: 102464: loss: 0.3389923744:
15: 108864: loss: 0.3383708576:
15: 115264: loss: 0.3382834252:
15: 121664: loss: 0.3376138754:
15: 128064: loss: 0.3371507149:
15: 134464: loss: 0.3372270296:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.3264473848:
16: 12864: loss: 0.3279945745:
16: 19264: loss: 0.3310009405:
16: 25664: loss: 0.3321905309:
16: 32064: loss: 0.3314235903:
16: 38464: loss: 0.3322148752:
16: 44864: loss: 0.3314000012:
16: 51264: loss: 0.3303349566:
16: 57664: loss: 0.3306832868:
16: 64064: loss: 0.3302536537:
16: 70464: loss: 0.3302354347:
16: 76864: loss: 0.3303273918:
16: 83264: loss: 0.3303512014:
16: 89664: loss: 0.3307643839:
16: 96064: loss: 0.3302883326:
16: 102464: loss: 0.3305158211:
16: 108864: loss: 0.3297351380:
16: 115264: loss: 0.3293322163:
16: 121664: loss: 0.3286778441:
16: 128064: loss: 0.3285852257:
16: 134464: loss: 0.3276860628:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.3166003193:
17: 12864: loss: 0.3167038832:
17: 19264: loss: 0.3193284281:
17: 25664: loss: 0.3203487490:
17: 32064: loss: 0.3206420761:
17: 38464: loss: 0.3213324792:
17: 44864: loss: 0.3211952958:
17: 51264: loss: 0.3217887647:
17: 57664: loss: 0.3208257245:
17: 64064: loss: 0.3208352486:
17: 70464: loss: 0.3202590769:
17: 76864: loss: 0.3204746208:
17: 83264: loss: 0.3204398895:
17: 89664: loss: 0.3200421772:
17: 96064: loss: 0.3204593861:
17: 102464: loss: 0.3194431014:
17: 108864: loss: 0.3193448006:
17: 115264: loss: 0.3187926452:
17: 121664: loss: 0.3186516690:
17: 128064: loss: 0.3184077979:
17: 134464: loss: 0.3185696196:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.3019551082:
18: 12864: loss: 0.3102494026:
18: 19264: loss: 0.3137312604:
18: 25664: loss: 0.3122252101:
18: 32064: loss: 0.3133504186:
18: 38464: loss: 0.3121874174:
18: 44864: loss: 0.3102628087:
18: 51264: loss: 0.3109347929:
18: 57664: loss: 0.3106866173:
18: 64064: loss: 0.3116459591:
18: 70464: loss: 0.3120546766:
18: 76864: loss: 0.3117952468:
18: 83264: loss: 0.3110852072:
18: 89664: loss: 0.3107104727:
18: 96064: loss: 0.3114547512:
18: 102464: loss: 0.3115503055:
18: 108864: loss: 0.3114892206:
18: 115264: loss: 0.3113965096:
18: 121664: loss: 0.3112826289:
18: 128064: loss: 0.3110400343:
18: 134464: loss: 0.3110066148:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 6464: loss: 0.3165351123:
19: 12864: loss: 0.3143764468:
19: 19264: loss: 0.3142271416:
19: 25664: loss: 0.3127722209:
19: 32064: loss: 0.3095571428:
19: 38464: loss: 0.3081310400:
19: 44864: loss: 0.3081832603:
19: 51264: loss: 0.3077260494:
19: 57664: loss: 0.3061326916:
19: 64064: loss: 0.3068648350:
19: 70464: loss: 0.3067001848:
19: 76864: loss: 0.3063498739:
19: 83264: loss: 0.3067202288:
19: 89664: loss: 0.3062749321:
19: 96064: loss: 0.3053373438:
19: 102464: loss: 0.3048560744:
19: 108864: loss: 0.3041406436:
19: 115264: loss: 0.3039219556:
19: 121664: loss: 0.3042314127:
19: 128064: loss: 0.3040126158:
19: 134464: loss: 0.3041499372:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
20: 6464: loss: 0.3015151663:
20: 12864: loss: 0.2983665348:
20: 19264: loss: 0.2970606459:
20: 25664: loss: 0.2970639945:
20: 32064: loss: 0.2966184504:
20: 38464: loss: 0.2957106322:
20: 44864: loss: 0.2974136820:
20: 51264: loss: 0.2962507501:
20: 57664: loss: 0.2973960830:
20: 64064: loss: 0.2965235567:
20: 70464: loss: 0.2979388972:
20: 76864: loss: 0.2981689423:
20: 83264: loss: 0.2972686774:
20: 89664: loss: 0.2976268700:
20: 96064: loss: 0.2979933511:
20: 102464: loss: 0.2980614696:
20: 108864: loss: 0.2983983070:
20: 115264: loss: 0.2985594179:
20: 121664: loss: 0.2982250234:
20: 128064: loss: 0.2974193164:
20: 134464: loss: 0.2973735438:
Dev-Acc: 20: Accuracy: 0.9416673183: precision: 0.7500000000: recall: 0.0005101173: f1: 0.0010195412
Train-Acc: 20: Accuracy: 0.8890934587: precision: 1.0000000000: recall: 0.0018407731: f1: 0.0036747818
21: 6464: loss: 0.2827431583:
21: 12864: loss: 0.2882873242:
21: 19264: loss: 0.2890021632:
21: 25664: loss: 0.2922675817:
21: 32064: loss: 0.2923159797:
21: 38464: loss: 0.2906187036:
21: 44864: loss: 0.2905021627:
21: 51264: loss: 0.2916150608:
21: 57664: loss: 0.2910113533:
21: 64064: loss: 0.2916349420:
21: 70464: loss: 0.2917470277:
21: 76864: loss: 0.2915982985:
21: 83264: loss: 0.2912745310:
21: 89664: loss: 0.2917351215:
21: 96064: loss: 0.2917989526:
21: 102464: loss: 0.2917396051:
21: 108864: loss: 0.2915527605:
21: 115264: loss: 0.2916425851:
21: 121664: loss: 0.2918037914:
21: 128064: loss: 0.2919209099:
21: 134464: loss: 0.2913931428:
Dev-Acc: 21: Accuracy: 0.9416871667: precision: 0.8333333333: recall: 0.0008501955: f1: 0.0016986581
Train-Acc: 21: Accuracy: 0.8894513845: precision: 1.0000000000: recall: 0.0050621261: f1: 0.0100732601
22: 6464: loss: 0.2944273582:
22: 12864: loss: 0.2913995484:
22: 19264: loss: 0.2918226535:
22: 25664: loss: 0.2902169364:
22: 32064: loss: 0.2889409696:
22: 38464: loss: 0.2888031898:
22: 44864: loss: 0.2873178349:
22: 51264: loss: 0.2864328949:
22: 57664: loss: 0.2863660006:
22: 64064: loss: 0.2866771798:
22: 70464: loss: 0.2863980635:
22: 76864: loss: 0.2864409486:
22: 83264: loss: 0.2864707595:
22: 89664: loss: 0.2861475244:
22: 96064: loss: 0.2861562698:
22: 102464: loss: 0.2858726815:
22: 108864: loss: 0.2865508929:
22: 115264: loss: 0.2869155997:
22: 121664: loss: 0.2872454889:
22: 128064: loss: 0.2872100295:
22: 134464: loss: 0.2866588918:
Dev-Acc: 22: Accuracy: 0.9416375756: precision: 0.4782608696: recall: 0.0018704302: f1: 0.0037262873
Train-Acc: 22: Accuracy: 0.8906858563: precision: 0.8575581395: recall: 0.0193938597: f1: 0.0379299261
23: 6464: loss: 0.2763516080:
23: 12864: loss: 0.2829996864:
23: 19264: loss: 0.2845201462:
23: 25664: loss: 0.2834502958:
23: 32064: loss: 0.2837147755:
23: 38464: loss: 0.2852679680:
23: 44864: loss: 0.2843630137:
23: 51264: loss: 0.2840111385:
23: 57664: loss: 0.2813498878:
23: 64064: loss: 0.2811449617:
23: 70464: loss: 0.2812055277:
23: 76864: loss: 0.2812163020:
23: 83264: loss: 0.2821202437:
23: 89664: loss: 0.2817972010:
23: 96064: loss: 0.2818354985:
23: 102464: loss: 0.2811634207:
23: 108864: loss: 0.2809035608:
23: 115264: loss: 0.2802385139:
23: 121664: loss: 0.2804035436:
23: 128064: loss: 0.2809785572:
23: 134464: loss: 0.2811520186:
Dev-Acc: 23: Accuracy: 0.9420542717: precision: 0.7157894737: recall: 0.0115626594: f1: 0.0227576975
Train-Acc: 23: Accuracy: 0.8916719556: precision: 0.8067632850: recall: 0.0329366906: f1: 0.0632895402
24: 6464: loss: 0.2891964227:
24: 12864: loss: 0.2833787516:
24: 19264: loss: 0.2822047529:
24: 25664: loss: 0.2785616374:
24: 32064: loss: 0.2783684787:
24: 38464: loss: 0.2798190021:
24: 44864: loss: 0.2793471842:
24: 51264: loss: 0.2778537081:
24: 57664: loss: 0.2764270707:
24: 64064: loss: 0.2774281705:
24: 70464: loss: 0.2764942255:
24: 76864: loss: 0.2763689603:
24: 83264: loss: 0.2769485375:
24: 89664: loss: 0.2779144714:
24: 96064: loss: 0.2772788538:
24: 102464: loss: 0.2769594627:
24: 108864: loss: 0.2766308543:
24: 115264: loss: 0.2768210534:
24: 121664: loss: 0.2766770180:
24: 128064: loss: 0.2763512011:
24: 134464: loss: 0.2765204960:
Dev-Acc: 24: Accuracy: 0.9424213767: precision: 0.6989795918: recall: 0.0232953579: f1: 0.0450880369
Train-Acc: 24: Accuracy: 0.8924170732: precision: 0.8340248963: recall: 0.0396423641: f1: 0.0756872097
25: 6464: loss: 0.2697023860:
25: 12864: loss: 0.2735090046:
25: 19264: loss: 0.2733597503:
25: 25664: loss: 0.2728270727:
25: 32064: loss: 0.2730439540:
25: 38464: loss: 0.2731197136:
25: 44864: loss: 0.2736444112:
25: 51264: loss: 0.2735399752:
25: 57664: loss: 0.2734373032:
25: 64064: loss: 0.2736220079:
25: 70464: loss: 0.2736645475:
25: 76864: loss: 0.2732633432:
25: 83264: loss: 0.2739703293:
25: 89664: loss: 0.2722838960:
25: 96064: loss: 0.2721479362:
25: 102464: loss: 0.2721293756:
25: 108864: loss: 0.2721684575:
25: 115264: loss: 0.2729636004:
25: 121664: loss: 0.2725722649:
25: 128064: loss: 0.2723961228:
25: 134464: loss: 0.2723670130:
Dev-Acc: 25: Accuracy: 0.9423817396: precision: 0.6045197740: recall: 0.0363883693: f1: 0.0686447474
Train-Acc: 25: Accuracy: 0.8938195705: precision: 0.8617363344: recall: 0.0528564854: f1: 0.0996035679
26: 6464: loss: 0.2683939759:
26: 12864: loss: 0.2677585746:
26: 19264: loss: 0.2676302649:
26: 25664: loss: 0.2699399446:
26: 32064: loss: 0.2689534315:
26: 38464: loss: 0.2705842313:
26: 44864: loss: 0.2694084025:
26: 51264: loss: 0.2684921555:
26: 57664: loss: 0.2667379948:
26: 64064: loss: 0.2671682622:
26: 70464: loss: 0.2672926004:
26: 76864: loss: 0.2675983278:
26: 83264: loss: 0.2672417183:
26: 89664: loss: 0.2675329587:
26: 96064: loss: 0.2674116927:
26: 102464: loss: 0.2676025046:
26: 108864: loss: 0.2674988085:
26: 115264: loss: 0.2673734020:
26: 121664: loss: 0.2678231752:
26: 128064: loss: 0.2680269196:
26: 134464: loss: 0.2678744272:
Dev-Acc: 26: Accuracy: 0.9420245290: precision: 0.5429864253: recall: 0.0408093862: f1: 0.0759133323
Train-Acc: 26: Accuracy: 0.8951343894: precision: 0.8632115548: recall: 0.0667937677: f1: 0.1239931657
27: 6464: loss: 0.2745508936:
27: 12864: loss: 0.2691377956:
27: 19264: loss: 0.2712250002:
27: 25664: loss: 0.2700791952:
27: 32064: loss: 0.2684040617:
27: 38464: loss: 0.2687396890:
27: 44864: loss: 0.2685438961:
27: 51264: loss: 0.2689668864:
27: 57664: loss: 0.2676370670:
27: 64064: loss: 0.2682521221:
27: 70464: loss: 0.2686086500:
27: 76864: loss: 0.2678855350:
27: 83264: loss: 0.2671840421:
27: 89664: loss: 0.2664016746:
27: 96064: loss: 0.2651868749:
27: 102464: loss: 0.2642791022:
27: 108864: loss: 0.2643381157:
27: 115264: loss: 0.2643596792:
27: 121664: loss: 0.2646562983:
27: 128064: loss: 0.2643018669:
27: 134464: loss: 0.2641251165:
Dev-Acc: 27: Accuracy: 0.9422130585: precision: 0.5528756957: recall: 0.0506716545: f1: 0.0928348910
Train-Acc: 27: Accuracy: 0.8969605565: precision: 0.8627708470: recall: 0.0863848531: f1: 0.1570455360
28: 6464: loss: 0.2676977862:
28: 12864: loss: 0.2664166509:
28: 19264: loss: 0.2643883149:
28: 25664: loss: 0.2615955880:
28: 32064: loss: 0.2636826427:
28: 38464: loss: 0.2618362207:
28: 44864: loss: 0.2632739367:
28: 51264: loss: 0.2631150667:
28: 57664: loss: 0.2633461159:
28: 64064: loss: 0.2623440588:
28: 70464: loss: 0.2626628264:
28: 76864: loss: 0.2619727048:
28: 83264: loss: 0.2615934598:
28: 89664: loss: 0.2612060191:
28: 96064: loss: 0.2609530780:
28: 102464: loss: 0.2613084538:
28: 108864: loss: 0.2609672941:
28: 115264: loss: 0.2611396742:
28: 121664: loss: 0.2613219625:
28: 128064: loss: 0.2611978084:
28: 134464: loss: 0.2609637845:
Dev-Acc: 28: Accuracy: 0.9431357980: precision: 0.5850340136: recall: 0.0877401802: f1: 0.1525950022
Train-Acc: 28: Accuracy: 0.8988378644: precision: 0.8606991525: recall: 0.1068305831: f1: 0.1900695947
29: 6464: loss: 0.2580657241:
29: 12864: loss: 0.2600547136:
29: 19264: loss: 0.2593000436:
29: 25664: loss: 0.2597298993:
29: 32064: loss: 0.2589669125:
29: 38464: loss: 0.2590240987:
29: 44864: loss: 0.2593620625:
29: 51264: loss: 0.2610882480:
29: 57664: loss: 0.2601377869:
29: 64064: loss: 0.2607297209:
29: 70464: loss: 0.2600238447:
29: 76864: loss: 0.2591013451:
29: 83264: loss: 0.2590390008:
29: 89664: loss: 0.2583633329:
29: 96064: loss: 0.2586756213:
29: 102464: loss: 0.2585628928:
29: 108864: loss: 0.2583869966:
29: 115264: loss: 0.2579646593:
29: 121664: loss: 0.2570180272:
29: 128064: loss: 0.2574060016:
29: 134464: loss: 0.2575771281:
Dev-Acc: 29: Accuracy: 0.9435425997: precision: 0.5842894969: recall: 0.1125658902: f1: 0.1887653265
Train-Acc: 29: Accuracy: 0.9012702703: precision: 0.8689595124: recall: 0.1312208270: f1: 0.2280100525
30: 6464: loss: 0.2531623757:
30: 12864: loss: 0.2576266347:
30: 19264: loss: 0.2565700504:
30: 25664: loss: 0.2552891544:
30: 32064: loss: 0.2547745865:
30: 38464: loss: 0.2529839547:
30: 44864: loss: 0.2533629020:
30: 51264: loss: 0.2527662068:
30: 57664: loss: 0.2541089377:
30: 64064: loss: 0.2538943704:
30: 70464: loss: 0.2536331251:
30: 76864: loss: 0.2533562773:
30: 83264: loss: 0.2527255655:
30: 89664: loss: 0.2525991000:
30: 96064: loss: 0.2522928516:
30: 102464: loss: 0.2531266505:
30: 108864: loss: 0.2528824002:
30: 115264: loss: 0.2533935652:
30: 121664: loss: 0.2541402468:
30: 128064: loss: 0.2540192914:
30: 134464: loss: 0.2540584578:
Dev-Acc: 30: Accuracy: 0.9435822964: precision: 0.5735849057: recall: 0.1292297228: f1: 0.2109353317
Train-Acc: 30: Accuracy: 0.9034397602: precision: 0.8710879285: recall: 0.1537045559: f1: 0.2613020397
31: 6464: loss: 0.2553277568:
31: 12864: loss: 0.2539703677:
31: 19264: loss: 0.2561804449:
31: 25664: loss: 0.2558451167:
31: 32064: loss: 0.2553590122:
31: 38464: loss: 0.2562040541:
31: 44864: loss: 0.2550424015:
31: 51264: loss: 0.2542894385:
31: 57664: loss: 0.2550606024:
31: 64064: loss: 0.2565145209:
31: 70464: loss: 0.2556047938:
31: 76864: loss: 0.2554767122:
31: 83264: loss: 0.2549603269:
31: 89664: loss: 0.2539935060:
31: 96064: loss: 0.2535228829:
31: 102464: loss: 0.2529487094:
31: 108864: loss: 0.2526934477:
31: 115264: loss: 0.2524959283:
31: 121664: loss: 0.2521753201:
31: 128064: loss: 0.2519041562:
31: 134464: loss: 0.2516705911:
Dev-Acc: 31: Accuracy: 0.9440585971: precision: 0.5817081372: recall: 0.1470838293: f1: 0.2347991314
Train-Acc: 31: Accuracy: 0.9054704905: precision: 0.8564698492: recall: 0.1792781540: f1: 0.2964936124
32: 6464: loss: 0.2422385334:
32: 12864: loss: 0.2412653269:
32: 19264: loss: 0.2455053683:
32: 25664: loss: 0.2479617487:
32: 32064: loss: 0.2483486893:
32: 38464: loss: 0.2453915101:
32: 44864: loss: 0.2441135543:
32: 51264: loss: 0.2447196241:
32: 57664: loss: 0.2463513159:
32: 64064: loss: 0.2473681251:
32: 70464: loss: 0.2472033139:
32: 76864: loss: 0.2470721685:
32: 83264: loss: 0.2468614405:
32: 89664: loss: 0.2464339183:
32: 96064: loss: 0.2467301803:
32: 102464: loss: 0.2472448953:
32: 108864: loss: 0.2471158577:
32: 115264: loss: 0.2482021622:
32: 121664: loss: 0.2480694106:
32: 128064: loss: 0.2482616934:
32: 134464: loss: 0.2486551660:
Dev-Acc: 32: Accuracy: 0.9434037209: precision: 0.5487603306: recall: 0.1693589526: f1: 0.2588357588
Train-Acc: 32: Accuracy: 0.9070848227: precision: 0.8379918589: recall: 0.2030109789: f1: 0.3268416596
33: 6464: loss: 0.2518923712:
33: 12864: loss: 0.2472396671:
33: 19264: loss: 0.2489082286:
33: 25664: loss: 0.2493800851:
33: 32064: loss: 0.2488827511:
33: 38464: loss: 0.2480452366:
33: 44864: loss: 0.2480688324:
33: 51264: loss: 0.2486483019:
33: 57664: loss: 0.2481262590:
33: 64064: loss: 0.2478291931:
33: 70464: loss: 0.2476016365:
33: 76864: loss: 0.2477287226:
33: 83264: loss: 0.2477604821:
33: 89664: loss: 0.2467862946:
33: 96064: loss: 0.2468699449:
33: 102464: loss: 0.2467076579:
33: 108864: loss: 0.2467276655:
33: 115264: loss: 0.2466767905:
33: 121664: loss: 0.2459691469:
33: 128064: loss: 0.2458330759:
33: 134464: loss: 0.2455327432:
Dev-Acc: 33: Accuracy: 0.9424610734: precision: 0.5168170632: recall: 0.2142492773: f1: 0.3029210242
Train-Acc: 33: Accuracy: 0.9082316160: precision: 0.8237163814: recall: 0.2214844520: f1: 0.3491010828
34: 6464: loss: 0.2414867663:
34: 12864: loss: 0.2426797345:
34: 19264: loss: 0.2430310689:
34: 25664: loss: 0.2416701685:
34: 32064: loss: 0.2411694112:
34: 38464: loss: 0.2436330921:
34: 44864: loss: 0.2426075109:
34: 51264: loss: 0.2433841551:
34: 57664: loss: 0.2443301929:
34: 64064: loss: 0.2449086799:
34: 70464: loss: 0.2433876267:
34: 76864: loss: 0.2426996463:
34: 83264: loss: 0.2428356247:
34: 89664: loss: 0.2429279735:
34: 96064: loss: 0.2428635249:
34: 102464: loss: 0.2429234354:
34: 108864: loss: 0.2429330653:
34: 115264: loss: 0.2427859163:
34: 121664: loss: 0.2428034109:
34: 128064: loss: 0.2430571013:
34: 134464: loss: 0.2433759568:
Dev-Acc: 34: Accuracy: 0.9413993955: precision: 0.4955752212: recall: 0.2380547526: f1: 0.3216172754
Train-Acc: 34: Accuracy: 0.9096925855: precision: 0.8117338004: recall: 0.2437709552: f1: 0.3749431215
35: 6464: loss: 0.2431381562:
35: 12864: loss: 0.2376823818:
35: 19264: loss: 0.2379887851:
35: 25664: loss: 0.2395452321:
35: 32064: loss: 0.2394944184:
35: 38464: loss: 0.2400640736:
35: 44864: loss: 0.2411907236:
35: 51264: loss: 0.2413304953:
35: 57664: loss: 0.2423234020:
35: 64064: loss: 0.2416057998:
35: 70464: loss: 0.2419569148:
35: 76864: loss: 0.2414923964:
35: 83264: loss: 0.2418621920:
35: 89664: loss: 0.2417972379:
35: 96064: loss: 0.2413334313:
35: 102464: loss: 0.2416947851:
35: 108864: loss: 0.2422549579:
35: 115264: loss: 0.2416573544:
35: 121664: loss: 0.2414431904:
35: 128064: loss: 0.2409815243:
35: 134464: loss: 0.2405950907:
Dev-Acc: 35: Accuracy: 0.9408338666: precision: 0.4867571059: recall: 0.2562489373: f1: 0.3357469088
Train-Acc: 35: Accuracy: 0.9110658169: precision: 0.8080357143: recall: 0.2617842351: f1: 0.3954516113
36: 6464: loss: 0.2458262292:
36: 12864: loss: 0.2435408609:
36: 19264: loss: 0.2435740346:
36: 25664: loss: 0.2455371217:
36: 32064: loss: 0.2438145754:
36: 38464: loss: 0.2432111460:
36: 44864: loss: 0.2434544329:
36: 51264: loss: 0.2427789737:
36: 57664: loss: 0.2417623496:
36: 64064: loss: 0.2412710298:
36: 70464: loss: 0.2407676257:
36: 76864: loss: 0.2406063907:
36: 83264: loss: 0.2407749253:
36: 89664: loss: 0.2399678581:
36: 96064: loss: 0.2392613835:
36: 102464: loss: 0.2395682805:
36: 108864: loss: 0.2395150373:
36: 115264: loss: 0.2394323300:
36: 121664: loss: 0.2394455191:
36: 128064: loss: 0.2391382684:
36: 134464: loss: 0.2390259384:
Dev-Acc: 36: Accuracy: 0.9403178692: precision: 0.4798192771: recall: 0.2708723006: f1: 0.3462667101
Train-Acc: 36: Accuracy: 0.9120300412: precision: 0.8030998852: recall: 0.2759187430: f1: 0.4107256447
37: 6464: loss: 0.2367700258:
37: 12864: loss: 0.2329368643:
37: 19264: loss: 0.2356127747:
37: 25664: loss: 0.2351829829:
37: 32064: loss: 0.2368737562:
37: 38464: loss: 0.2373215925:
37: 44864: loss: 0.2368504259:
37: 51264: loss: 0.2376736212:
37: 57664: loss: 0.2380573534:
37: 64064: loss: 0.2373289850:
37: 70464: loss: 0.2373039373:
37: 76864: loss: 0.2368602299:
37: 83264: loss: 0.2368896767:
37: 89664: loss: 0.2360828433:
37: 96064: loss: 0.2357019941:
37: 102464: loss: 0.2350724181:
37: 108864: loss: 0.2354344483:
37: 115264: loss: 0.2352891162:
37: 121664: loss: 0.2359387117:
37: 128064: loss: 0.2357724363:
37: 134464: loss: 0.2361847962:
Dev-Acc: 37: Accuracy: 0.9400004148: precision: 0.4767115600: recall: 0.2888964462: f1: 0.3597670725
Train-Acc: 37: Accuracy: 0.9132572412: precision: 0.8060550459: recall: 0.2888041549: f1: 0.4252456319
38: 6464: loss: 0.2394818704:
38: 12864: loss: 0.2374443971:
38: 19264: loss: 0.2358216338:
38: 25664: loss: 0.2347374999:
38: 32064: loss: 0.2333617184:
38: 38464: loss: 0.2345954034:
38: 44864: loss: 0.2342078854:
38: 51264: loss: 0.2348425314:
38: 57664: loss: 0.2349803239:
38: 64064: loss: 0.2345368757:
38: 70464: loss: 0.2353236630:
38: 76864: loss: 0.2344438187:
38: 83264: loss: 0.2345438443:
38: 89664: loss: 0.2343754559:
38: 96064: loss: 0.2349690219:
38: 102464: loss: 0.2350557924:
38: 108864: loss: 0.2351913954:
38: 115264: loss: 0.2350637839:
38: 121664: loss: 0.2347281794:
38: 128064: loss: 0.2341032232:
38: 134464: loss: 0.2338643661:
Dev-Acc: 38: Accuracy: 0.9394645691: precision: 0.4707135250: recall: 0.3006291447: f1: 0.3669191657
Train-Acc: 38: Accuracy: 0.9141995311: precision: 0.8056094549: recall: 0.3002432450: f1: 0.4374521073
39: 6464: loss: 0.2250670999:
39: 12864: loss: 0.2282232694:
39: 19264: loss: 0.2295529414:
39: 25664: loss: 0.2294946610:
39: 32064: loss: 0.2290711559:
39: 38464: loss: 0.2304696435:
39: 44864: loss: 0.2312384838:
39: 51264: loss: 0.2301591226:
39: 57664: loss: 0.2302799525:
39: 64064: loss: 0.2307541854:
39: 70464: loss: 0.2308299334:
39: 76864: loss: 0.2317260424:
39: 83264: loss: 0.2321893443:
39: 89664: loss: 0.2315002676:
39: 96064: loss: 0.2311116352:
39: 102464: loss: 0.2310103662:
39: 108864: loss: 0.2312653403:
39: 115264: loss: 0.2315297858:
39: 121664: loss: 0.2316252517:
39: 128064: loss: 0.2319558046:
39: 134464: loss: 0.2324190518:
Dev-Acc: 39: Accuracy: 0.9386807084: precision: 0.4621039290: recall: 0.3099812957: f1: 0.3710563810
Train-Acc: 39: Accuracy: 0.9147619605: precision: 0.8025281859: recall: 0.3088554336: f1: 0.4460479468
40: 6464: loss: 0.2335081380:
40: 12864: loss: 0.2329671327:
40: 19264: loss: 0.2321189568:
40: 25664: loss: 0.2311817873:
40: 32064: loss: 0.2295660291:
40: 38464: loss: 0.2298306314:
40: 44864: loss: 0.2288344427:
40: 51264: loss: 0.2302228968:
40: 57664: loss: 0.2312070343:
40: 64064: loss: 0.2316028076:
40: 70464: loss: 0.2314312678:
40: 76864: loss: 0.2309000587:
40: 83264: loss: 0.2311860382:
40: 89664: loss: 0.2310128096:
40: 96064: loss: 0.2310946888:
40: 102464: loss: 0.2302341973:
40: 108864: loss: 0.2304568719:
40: 115264: loss: 0.2305589737:
40: 121664: loss: 0.2306589734:
40: 128064: loss: 0.2302559582:
40: 134464: loss: 0.2303140230:
Dev-Acc: 40: Accuracy: 0.9381052256: precision: 0.4563035496: recall: 0.3169528992: f1: 0.3740718443
Train-Acc: 40: Accuracy: 0.9154193997: precision: 0.8005627276: recall: 0.3179935573: f1: 0.4551827977
41: 6464: loss: 0.2360422359:
41: 12864: loss: 0.2322156664:
41: 19264: loss: 0.2312355625:
41: 25664: loss: 0.2322171627:
41: 32064: loss: 0.2304787256:
41: 38464: loss: 0.2284686890:
41: 44864: loss: 0.2298085031:
41: 51264: loss: 0.2290194478:
41: 57664: loss: 0.2285010077:
41: 64064: loss: 0.2283230993:
41: 70464: loss: 0.2287280324:
41: 76864: loss: 0.2299419074:
41: 83264: loss: 0.2294145961:
41: 89664: loss: 0.2294705642:
41: 96064: loss: 0.2291284211:
41: 102464: loss: 0.2289438472:
41: 108864: loss: 0.2285593477:
41: 115264: loss: 0.2289991973:
41: 121664: loss: 0.2284382227:
41: 128064: loss: 0.2280243576:
41: 134464: loss: 0.2284012057:
Dev-Acc: 41: Accuracy: 0.9379166961: precision: 0.4549376798: recall: 0.3227342289: f1: 0.3775987267
Train-Acc: 41: Accuracy: 0.9159599543: precision: 0.7982936252: recall: 0.3260140688: f1: 0.4629603697
42: 6464: loss: 0.2169354644:
42: 12864: loss: 0.2266277909:
42: 19264: loss: 0.2256313690:
42: 25664: loss: 0.2253068624:
42: 32064: loss: 0.2286938476:
42: 38464: loss: 0.2271253942:
42: 44864: loss: 0.2276937389:
42: 51264: loss: 0.2286479581:
42: 57664: loss: 0.2286011580:
42: 64064: loss: 0.2290095672:
42: 70464: loss: 0.2282899454:
42: 76864: loss: 0.2284023611:
42: 83264: loss: 0.2274414003:
42: 89664: loss: 0.2273856339:
42: 96064: loss: 0.2267816902:
42: 102464: loss: 0.2267645124:
42: 108864: loss: 0.2264156689:
42: 115264: loss: 0.2263428775:
42: 121664: loss: 0.2264498307:
42: 128064: loss: 0.2266763595:
42: 134464: loss: 0.2265412472:
Dev-Acc: 42: Accuracy: 0.9376190305: precision: 0.4521000472: recall: 0.3257949328: f1: 0.3786935468
Train-Acc: 42: Accuracy: 0.9166246653: precision: 0.7985532316: recall: 0.3338373545: f1: 0.4708391284
43: 6464: loss: 0.2329580760:
43: 12864: loss: 0.2290485086:
43: 19264: loss: 0.2285443208:
43: 25664: loss: 0.2271981527:
43: 32064: loss: 0.2277347369:
43: 38464: loss: 0.2275447277:
43: 44864: loss: 0.2275343777:
43: 51264: loss: 0.2264636629:
43: 57664: loss: 0.2255293504:
43: 64064: loss: 0.2253969502:
43: 70464: loss: 0.2256060400:
43: 76864: loss: 0.2256061121:
43: 83264: loss: 0.2245477908:
43: 89664: loss: 0.2248318044:
43: 96064: loss: 0.2241779545:
43: 102464: loss: 0.2247415624:
43: 108864: loss: 0.2249273503:
43: 115264: loss: 0.2244811020:
43: 121664: loss: 0.2247293745:
43: 128064: loss: 0.2247842128:
43: 134464: loss: 0.2247489785:
Dev-Acc: 43: Accuracy: 0.9375000000: precision: 0.4513047530: recall: 0.3293657541: f1: 0.3808119532
Train-Acc: 43: Accuracy: 0.9173040390: precision: 0.7974006116: recall: 0.3428439945: f1: 0.4795181831
44: 6464: loss: 0.2177654850:
44: 12864: loss: 0.2234335040:
44: 19264: loss: 0.2256149667:
44: 25664: loss: 0.2259420578:
44: 32064: loss: 0.2256055476:
44: 38464: loss: 0.2239946879:
44: 44864: loss: 0.2231926394:
44: 51264: loss: 0.2231801620:
44: 57664: loss: 0.2234299049:
44: 64064: loss: 0.2235165000:
44: 70464: loss: 0.2230607830:
44: 76864: loss: 0.2237698254:
44: 83264: loss: 0.2227763779:
44: 89664: loss: 0.2231421213:
44: 96064: loss: 0.2229323813:
44: 102464: loss: 0.2234789682:
44: 108864: loss: 0.2234765385:
44: 115264: loss: 0.2236050392:
44: 121664: loss: 0.2233928878:
44: 128064: loss: 0.2232486323:
44: 134464: loss: 0.2232027996:
Dev-Acc: 44: Accuracy: 0.9375000000: precision: 0.4519761029: recall: 0.3344669274: f1: 0.3844424900
Train-Acc: 44: Accuracy: 0.9178007245: precision: 0.7969687875: recall: 0.3491552166: f1: 0.4855771429
45: 6464: loss: 0.2144376541:
45: 12864: loss: 0.2222723047:
45: 19264: loss: 0.2241381214:
45: 25664: loss: 0.2225151934:
45: 32064: loss: 0.2239048328:
45: 38464: loss: 0.2228920688:
45: 44864: loss: 0.2217746860:
45: 51264: loss: 0.2212397820:
45: 57664: loss: 0.2216077235:
45: 64064: loss: 0.2220209125:
45: 70464: loss: 0.2226115780:
45: 76864: loss: 0.2222744292:
45: 83264: loss: 0.2227896906:
45: 89664: loss: 0.2230171103:
45: 96064: loss: 0.2225862846:
45: 102464: loss: 0.2228355431:
45: 108864: loss: 0.2230214400:
45: 115264: loss: 0.2224020615:
45: 121664: loss: 0.2223178968:
45: 128064: loss: 0.2222211644:
45: 134464: loss: 0.2217330996:
Dev-Acc: 45: Accuracy: 0.9374404550: precision: 0.4523595506: recall: 0.3422887264: f1: 0.3897009002
Train-Acc: 45: Accuracy: 0.9184070230: precision: 0.7970013229: recall: 0.3564525672: f1: 0.4925956210
46: 6464: loss: 0.2232913678:
46: 12864: loss: 0.2237886957:
46: 19264: loss: 0.2221251225:
46: 25664: loss: 0.2224918809:
46: 32064: loss: 0.2199600912:
46: 38464: loss: 0.2197403096:
46: 44864: loss: 0.2194817436:
46: 51264: loss: 0.2196579902:
46: 57664: loss: 0.2210793685:
46: 64064: loss: 0.2207101229:
46: 70464: loss: 0.2199356859:
46: 76864: loss: 0.2198298805:
46: 83264: loss: 0.2193916616:
46: 89664: loss: 0.2208649049:
46: 96064: loss: 0.2208254676:
46: 102464: loss: 0.2209507826:
46: 108864: loss: 0.2202997531:
46: 115264: loss: 0.2202862084:
46: 121664: loss: 0.2199719328:
46: 128064: loss: 0.2199832133:
46: 134464: loss: 0.2197269490:
Dev-Acc: 46: Accuracy: 0.9368848205: precision: 0.4472990777: recall: 0.3463696650: f1: 0.3904168663
Train-Acc: 46: Accuracy: 0.9191520810: precision: 0.7984440282: recall: 0.3643415949: f1: 0.5003611412
47: 6464: loss: 0.2110499770:
47: 12864: loss: 0.2155104935:
47: 19264: loss: 0.2156907855:
47: 25664: loss: 0.2154687667:
47: 32064: loss: 0.2166440877:
47: 38464: loss: 0.2150839436:
47: 44864: loss: 0.2152480524:
47: 51264: loss: 0.2169658100:
47: 57664: loss: 0.2171545859:
47: 64064: loss: 0.2176238552:
47: 70464: loss: 0.2184451102:
47: 76864: loss: 0.2190000453:
47: 83264: loss: 0.2182431412:
47: 89664: loss: 0.2187692877:
47: 96064: loss: 0.2186680741:
47: 102464: loss: 0.2181265935:
47: 108864: loss: 0.2180842472:
47: 115264: loss: 0.2186137413:
47: 121664: loss: 0.2189995432:
47: 128064: loss: 0.2189995249:
47: 134464: loss: 0.2188149446:
Dev-Acc: 47: Accuracy: 0.9366268516: precision: 0.4452380952: recall: 0.3497704472: f1: 0.3917722122
Train-Acc: 47: Accuracy: 0.9198533297: precision: 0.7994913099: recall: 0.3719676550: f1: 0.5077171572
48: 6464: loss: 0.2140865333:
48: 12864: loss: 0.2147688390:
48: 19264: loss: 0.2134963620:
48: 25664: loss: 0.2132540434:
48: 32064: loss: 0.2134102050:
48: 38464: loss: 0.2140559276:
48: 44864: loss: 0.2146195128:
48: 51264: loss: 0.2163206432:
48: 57664: loss: 0.2163757700:
48: 64064: loss: 0.2168639533:
48: 70464: loss: 0.2165737328:
48: 76864: loss: 0.2170541240:
48: 83264: loss: 0.2171565203:
48: 89664: loss: 0.2169436843:
48: 96064: loss: 0.2178082411:
48: 102464: loss: 0.2178523361:
48: 108864: loss: 0.2181010736:
48: 115264: loss: 0.2176975687:
48: 121664: loss: 0.2172234292:
48: 128064: loss: 0.2167090262:
48: 134464: loss: 0.2170485651:
Dev-Acc: 48: Accuracy: 0.9362993836: precision: 0.4424514200: recall: 0.3523210338: f1: 0.3922756532
Train-Acc: 48: Accuracy: 0.9202769995: precision: 0.7992756651: recall: 0.3772270068: f1: 0.5125502456
49: 6464: loss: 0.2151400866:
49: 12864: loss: 0.2205180694:
49: 19264: loss: 0.2180496624:
49: 25664: loss: 0.2182951117:
49: 32064: loss: 0.2174631173:
49: 38464: loss: 0.2173524224:
49: 44864: loss: 0.2157092422:
49: 51264: loss: 0.2167034135:
49: 57664: loss: 0.2163470057:
49: 64064: loss: 0.2159609741:
49: 70464: loss: 0.2159532790:
49: 76864: loss: 0.2162216959:
49: 83264: loss: 0.2164998976:
49: 89664: loss: 0.2158082967:
49: 96064: loss: 0.2160574937:
49: 102464: loss: 0.2165357957:
49: 108864: loss: 0.2166621801:
49: 115264: loss: 0.2170498784:
49: 121664: loss: 0.2165174980:
49: 128064: loss: 0.2164932282:
49: 134464: loss: 0.2163091589:
Dev-Acc: 49: Accuracy: 0.9359918237: precision: 0.4400504838: recall: 0.3557218160: f1: 0.3934179596
Train-Acc: 49: Accuracy: 0.9205034375: precision: 0.7974161627: recall: 0.3814344882: f1: 0.5160314849
50: 6464: loss: 0.2141391453:
50: 12864: loss: 0.2115664639:
50: 19264: loss: 0.2161840132:
50: 25664: loss: 0.2134046812:
50: 32064: loss: 0.2141964280:
50: 38464: loss: 0.2135935067:
50: 44864: loss: 0.2146106453:
50: 51264: loss: 0.2145296483:
50: 57664: loss: 0.2144390902:
50: 64064: loss: 0.2143431060:
50: 70464: loss: 0.2136369098:
50: 76864: loss: 0.2129696955:
50: 83264: loss: 0.2127612616:
50: 89664: loss: 0.2129244373:
50: 96064: loss: 0.2126349357:
50: 102464: loss: 0.2131079361:
50: 108864: loss: 0.2132711168:
50: 115264: loss: 0.2143020828:
50: 121664: loss: 0.2142260886:
50: 128064: loss: 0.2147157674:
50: 134464: loss: 0.2146288065:
Dev-Acc: 50: Accuracy: 0.9357040524: precision: 0.4378501764: recall: 0.3587825200: f1: 0.3943925234
Train-Acc: 50: Accuracy: 0.9209198356: precision: 0.7976109678: recall: 0.3862993886: f1: 0.5205066879
