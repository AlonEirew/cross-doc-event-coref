1: 3232: loss: 0.7080008829:
1: 6432: loss: 0.7067370796:
1: 9632: loss: 0.7059941892:
1: 12832: loss: 0.7054001537:
1: 16032: loss: 0.7041897291:
1: 19232: loss: 0.7035516484:
1: 22432: loss: 0.7028332359:
1: 25632: loss: 0.7019185592:
1: 28832: loss: 0.7011548103:
1: 32032: loss: 0.7002539968:
1: 35232: loss: 0.6993748772:
1: 38432: loss: 0.6985942178:
1: 41632: loss: 0.6978656976:
1: 44832: loss: 0.6970326750:
1: 48032: loss: 0.6962567831:
1: 51232: loss: 0.6955720587:
1: 54432: loss: 0.6947807602:
1: 57632: loss: 0.6940289283:
1: 60832: loss: 0.6932546615:
1: 64032: loss: 0.6924656979:
1: 67232: loss: 0.6916678509:
1: 70432: loss: 0.6909439143:
1: 73632: loss: 0.6901980859:
1: 76832: loss: 0.6894049575:
1: 80032: loss: 0.6885904900:
1: 83232: loss: 0.6878553511:
1: 86432: loss: 0.6870985361:
1: 89632: loss: 0.6863708160:
1: 92832: loss: 0.6855749842:
1: 96032: loss: 0.6848600149:
1: 99232: loss: 0.6840841697:
1: 102432: loss: 0.6832790073:
1: 105632: loss: 0.6825314937:
1: 108832: loss: 0.6817884330:
1: 112032: loss: 0.6810045339:
1: 115232: loss: 0.6802265528:
1: 118432: loss: 0.6795180203:
1: 121632: loss: 0.6787626786:
Dev-Acc: 1: Accuracy: 0.9044888020: precision: 0.0935532885: recall: 0.0732868560: f1: 0.0821891686
Train-Acc: 1: Accuracy: 0.8589343429: precision: 0.3094170404: recall: 0.1043323910: f1: 0.1560471976
2: 3232: loss: 0.6500315952:
2: 6432: loss: 0.6489159024:
2: 9632: loss: 0.6485162914:
2: 12832: loss: 0.6476318814:
2: 16032: loss: 0.6468442281:
2: 19232: loss: 0.6459058429:
2: 22432: loss: 0.6451642331:
2: 25632: loss: 0.6445634463:
2: 28832: loss: 0.6437380003:
2: 32032: loss: 0.6430359756:
2: 35232: loss: 0.6421914347:
2: 38432: loss: 0.6414654587:
2: 41632: loss: 0.6407732251:
2: 44832: loss: 0.6398559637:
2: 48032: loss: 0.6392882407:
2: 51232: loss: 0.6384796445:
2: 54432: loss: 0.6376311725:
2: 57632: loss: 0.6367940780:
2: 60832: loss: 0.6361214715:
2: 64032: loss: 0.6353950460:
2: 67232: loss: 0.6345424520:
2: 70432: loss: 0.6339211500:
2: 73632: loss: 0.6332923159:
2: 76832: loss: 0.6326556078:
2: 80032: loss: 0.6320311865:
2: 83232: loss: 0.6314198004:
2: 86432: loss: 0.6308273958:
2: 89632: loss: 0.6301224929:
2: 92832: loss: 0.6293644729:
2: 96032: loss: 0.6286267787:
2: 99232: loss: 0.6279315456:
2: 102432: loss: 0.6272816748:
2: 105632: loss: 0.6266210024:
2: 108832: loss: 0.6259128416:
2: 112032: loss: 0.6253147221:
2: 115232: loss: 0.6245286190:
2: 118432: loss: 0.6239393896:
2: 121632: loss: 0.6232557120:
Dev-Acc: 2: Accuracy: 0.9415978789: precision: 0.1428571429: recall: 0.0001700391: f1: 0.0003396739
Train-Acc: 2: Accuracy: 0.8750411272: precision: 0.5757575758: recall: 0.0012490960: f1: 0.0024927840
3: 3232: loss: 0.5974996561:
3: 6432: loss: 0.5964575461:
3: 9632: loss: 0.5960693381:
3: 12832: loss: 0.5952170998:
3: 16032: loss: 0.5951915849:
3: 19232: loss: 0.5943828224:
3: 22432: loss: 0.5935312325:
3: 25632: loss: 0.5930022989:
3: 28832: loss: 0.5927036071:
3: 32032: loss: 0.5919678163:
3: 35232: loss: 0.5905762988:
3: 38432: loss: 0.5898363585:
3: 41632: loss: 0.5890758597:
3: 44832: loss: 0.5884172965:
3: 48032: loss: 0.5879160277:
3: 51232: loss: 0.5872686038:
3: 54432: loss: 0.5866374687:
3: 57632: loss: 0.5863008776:
3: 60832: loss: 0.5854409270:
3: 64032: loss: 0.5849938126:
3: 67232: loss: 0.5842748285:
3: 70432: loss: 0.5835070771:
3: 73632: loss: 0.5828098477:
3: 76832: loss: 0.5820613427:
3: 80032: loss: 0.5815043240:
3: 83232: loss: 0.5808154853:
3: 86432: loss: 0.5800832364:
3: 89632: loss: 0.5795360059:
3: 92832: loss: 0.5788284910:
3: 96032: loss: 0.5780822537:
3: 99232: loss: 0.5773602703:
3: 102432: loss: 0.5768198574:
3: 105632: loss: 0.5761046330:
3: 108832: loss: 0.5754465481:
3: 112032: loss: 0.5748180506:
3: 115232: loss: 0.5739774148:
3: 118432: loss: 0.5733215145:
3: 121632: loss: 0.5726793564:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.5477189240:
4: 6432: loss: 0.5475534002:
4: 9632: loss: 0.5470884130:
4: 12832: loss: 0.5465959034:
4: 16032: loss: 0.5462219269:
4: 19232: loss: 0.5449793876:
4: 22432: loss: 0.5444463368:
4: 25632: loss: 0.5439175465:
4: 28832: loss: 0.5435102072:
4: 32032: loss: 0.5430197700:
4: 35232: loss: 0.5425892184:
4: 38432: loss: 0.5422069644:
4: 41632: loss: 0.5412506672:
4: 44832: loss: 0.5405133170:
4: 48032: loss: 0.5398519861:
4: 51232: loss: 0.5394473654:
4: 54432: loss: 0.5387604448:
4: 57632: loss: 0.5380401005:
4: 60832: loss: 0.5373411122:
4: 64032: loss: 0.5368433174:
4: 67232: loss: 0.5362800609:
4: 70432: loss: 0.5357560499:
4: 73632: loss: 0.5348846960:
4: 76832: loss: 0.5344418181:
4: 80032: loss: 0.5336821803:
4: 83232: loss: 0.5332175153:
4: 86432: loss: 0.5326285464:
4: 89632: loss: 0.5322058689:
4: 92832: loss: 0.5315407149:
4: 96032: loss: 0.5309987117:
4: 99232: loss: 0.5306489232:
4: 102432: loss: 0.5299773120:
4: 105632: loss: 0.5291895024:
4: 108832: loss: 0.5286524983:
4: 112032: loss: 0.5280140656:
4: 115232: loss: 0.5272628152:
4: 118432: loss: 0.5268533355:
4: 121632: loss: 0.5263382227:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.5004494727:
5: 6432: loss: 0.5013572715:
5: 9632: loss: 0.5006152615:
5: 12832: loss: 0.5008632462:
5: 16032: loss: 0.5008214021:
5: 19232: loss: 0.5012656817:
5: 22432: loss: 0.5005599068:
5: 25632: loss: 0.4998062678:
5: 28832: loss: 0.4993159818:
5: 32032: loss: 0.4984034475:
5: 35232: loss: 0.4980479583:
5: 38432: loss: 0.4980650512:
5: 41632: loss: 0.4969978697:
5: 44832: loss: 0.4962611236:
5: 48032: loss: 0.4955149990:
5: 51232: loss: 0.4952341493:
5: 54432: loss: 0.4947010340:
5: 57632: loss: 0.4942879293:
5: 60832: loss: 0.4944096751:
5: 64032: loss: 0.4939631341:
5: 67232: loss: 0.4933971573:
5: 70432: loss: 0.4929821028:
5: 73632: loss: 0.4924060279:
5: 76832: loss: 0.4920885593:
5: 80032: loss: 0.4914823535:
5: 83232: loss: 0.4908344859:
5: 86432: loss: 0.4902159325:
5: 89632: loss: 0.4896237012:
5: 92832: loss: 0.4892700522:
5: 96032: loss: 0.4885386377:
5: 99232: loss: 0.4880861277:
5: 102432: loss: 0.4876638304:
5: 105632: loss: 0.4871536574:
5: 108832: loss: 0.4866716310:
5: 112032: loss: 0.4863295395:
5: 115232: loss: 0.4857739660:
5: 118432: loss: 0.4852839496:
5: 121632: loss: 0.4846314105:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.4657553783:
6: 6432: loss: 0.4676317115:
6: 9632: loss: 0.4662195057:
6: 12832: loss: 0.4643509446:
6: 16032: loss: 0.4626548199:
6: 19232: loss: 0.4621973169:
6: 22432: loss: 0.4617903212:
6: 25632: loss: 0.4623983758:
6: 28832: loss: 0.4624851588:
6: 32032: loss: 0.4618244720:
6: 35232: loss: 0.4616202809:
6: 38432: loss: 0.4608951113:
6: 41632: loss: 0.4602559238:
6: 44832: loss: 0.4594196634:
6: 48032: loss: 0.4592583213:
6: 51232: loss: 0.4591850041:
6: 54432: loss: 0.4582125650:
6: 57632: loss: 0.4571647793:
6: 60832: loss: 0.4568879615:
6: 64032: loss: 0.4563639925:
6: 67232: loss: 0.4557980359:
6: 70432: loss: 0.4551712666:
6: 73632: loss: 0.4548425908:
6: 76832: loss: 0.4540906577:
6: 80032: loss: 0.4541040048:
6: 83232: loss: 0.4536131129:
6: 86432: loss: 0.4529778379:
6: 89632: loss: 0.4527170681:
6: 92832: loss: 0.4523199082:
6: 96032: loss: 0.4519395883:
6: 99232: loss: 0.4515509969:
6: 102432: loss: 0.4511429297:
6: 105632: loss: 0.4504587751:
6: 108832: loss: 0.4498755993:
6: 112032: loss: 0.4494297889:
6: 115232: loss: 0.4491904403:
6: 118432: loss: 0.4487013577:
6: 121632: loss: 0.4480436282:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.4388877037:
7: 6432: loss: 0.4302075805:
7: 9632: loss: 0.4303661587:
7: 12832: loss: 0.4300377794:
7: 16032: loss: 0.4314824260:
7: 19232: loss: 0.4305281638:
7: 22432: loss: 0.4301006793:
7: 25632: loss: 0.4287410064:
7: 28832: loss: 0.4285541023:
7: 32032: loss: 0.4286567408:
7: 35232: loss: 0.4275362873:
7: 38432: loss: 0.4269494814:
7: 41632: loss: 0.4270675290:
7: 44832: loss: 0.4265626109:
7: 48032: loss: 0.4267983235:
7: 51232: loss: 0.4264000323:
7: 54432: loss: 0.4255943420:
7: 57632: loss: 0.4253209476:
7: 60832: loss: 0.4245749616:
7: 64032: loss: 0.4242953306:
7: 67232: loss: 0.4240589025:
7: 70432: loss: 0.4237700954:
7: 73632: loss: 0.4231466688:
7: 76832: loss: 0.4228617108:
7: 80032: loss: 0.4224586051:
7: 83232: loss: 0.4225716379:
7: 86432: loss: 0.4217912379:
7: 89632: loss: 0.4214609010:
7: 92832: loss: 0.4212005186:
7: 96032: loss: 0.4208274836:
7: 99232: loss: 0.4203019407:
7: 102432: loss: 0.4199227289:
7: 105632: loss: 0.4194924242:
7: 108832: loss: 0.4189179876:
7: 112032: loss: 0.4180500599:
7: 115232: loss: 0.4177453885:
7: 118432: loss: 0.4172059188:
7: 121632: loss: 0.4169586450:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.4027174580:
8: 6432: loss: 0.4028008795:
8: 9632: loss: 0.4037794051:
8: 12832: loss: 0.4034007939:
8: 16032: loss: 0.4038825932:
8: 19232: loss: 0.4043494302:
8: 22432: loss: 0.4030186392:
8: 25632: loss: 0.4025913142:
8: 28832: loss: 0.4031274087:
8: 32032: loss: 0.4021293499:
8: 35232: loss: 0.4013869852:
8: 38432: loss: 0.4013067636:
8: 41632: loss: 0.3999616398:
8: 44832: loss: 0.3999672830:
8: 48032: loss: 0.4000751480:
8: 51232: loss: 0.3995608070:
8: 54432: loss: 0.3994719319:
8: 57632: loss: 0.3980981409:
8: 60832: loss: 0.3980066945:
8: 64032: loss: 0.3975071888:
8: 67232: loss: 0.3963833352:
8: 70432: loss: 0.3964571900:
8: 73632: loss: 0.3959742896:
8: 76832: loss: 0.3953355710:
8: 80032: loss: 0.3952660079:
8: 83232: loss: 0.3950402969:
8: 86432: loss: 0.3943047922:
8: 89632: loss: 0.3937467209:
8: 92832: loss: 0.3938491904:
8: 96032: loss: 0.3935006934:
8: 99232: loss: 0.3931961380:
8: 102432: loss: 0.3932201637:
8: 105632: loss: 0.3927939936:
8: 108832: loss: 0.3923120766:
8: 112032: loss: 0.3921879078:
8: 115232: loss: 0.3920439465:
8: 118432: loss: 0.3918707383:
8: 121632: loss: 0.3915758011:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3801117969:
9: 6432: loss: 0.3758844776:
9: 9632: loss: 0.3763032397:
9: 12832: loss: 0.3783144629:
9: 16032: loss: 0.3771481662:
9: 19232: loss: 0.3760927546:
9: 22432: loss: 0.3761675124:
9: 25632: loss: 0.3768570413:
9: 28832: loss: 0.3776409992:
9: 32032: loss: 0.3766802871:
9: 35232: loss: 0.3753791233:
9: 38432: loss: 0.3760714016:
9: 41632: loss: 0.3766497965:
9: 44832: loss: 0.3763868620:
9: 48032: loss: 0.3756119710:
9: 51232: loss: 0.3756769989:
9: 54432: loss: 0.3753481199:
9: 57632: loss: 0.3755811637:
9: 60832: loss: 0.3752677718:
9: 64032: loss: 0.3760737343:
9: 67232: loss: 0.3758262224:
9: 70432: loss: 0.3755664755:
9: 73632: loss: 0.3754684730:
9: 76832: loss: 0.3752765504:
9: 80032: loss: 0.3748433001:
9: 83232: loss: 0.3743185721:
9: 86432: loss: 0.3744169015:
9: 89632: loss: 0.3736230256:
9: 92832: loss: 0.3736580842:
9: 96032: loss: 0.3731557480:
9: 99232: loss: 0.3726243568:
9: 102432: loss: 0.3721936502:
9: 105632: loss: 0.3719196591:
9: 108832: loss: 0.3717055652:
9: 112032: loss: 0.3715087455:
9: 115232: loss: 0.3715980807:
9: 118432: loss: 0.3711369357:
9: 121632: loss: 0.3709459782:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.3688473141:
10: 6432: loss: 0.3639612756:
10: 9632: loss: 0.3643459034:
10: 12832: loss: 0.3670163941:
10: 16032: loss: 0.3627119857:
10: 19232: loss: 0.3625517548:
10: 22432: loss: 0.3622293267:
10: 25632: loss: 0.3622915857:
10: 28832: loss: 0.3614240991:
10: 32032: loss: 0.3595288193:
10: 35232: loss: 0.3586772833:
10: 38432: loss: 0.3576285404:
10: 41632: loss: 0.3575672228:
10: 44832: loss: 0.3575216049:
10: 48032: loss: 0.3576193447:
10: 51232: loss: 0.3577498939:
10: 54432: loss: 0.3580185899:
10: 57632: loss: 0.3581160333:
10: 60832: loss: 0.3582466130:
10: 64032: loss: 0.3586828214:
10: 67232: loss: 0.3585674957:
10: 70432: loss: 0.3582989498:
10: 73632: loss: 0.3577578372:
10: 76832: loss: 0.3575239591:
10: 80032: loss: 0.3575135519:
10: 83232: loss: 0.3572897966:
10: 86432: loss: 0.3570886774:
10: 89632: loss: 0.3570394659:
10: 92832: loss: 0.3565384069:
10: 96032: loss: 0.3561065724:
10: 99232: loss: 0.3559399535:
10: 102432: loss: 0.3557581841:
10: 105632: loss: 0.3555265042:
10: 108832: loss: 0.3554014456:
10: 112032: loss: 0.3548910737:
10: 115232: loss: 0.3544239259:
10: 118432: loss: 0.3543708968:
10: 121632: loss: 0.3541785628:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.3537096782:
11: 6432: loss: 0.3452700478:
11: 9632: loss: 0.3460805122:
11: 12832: loss: 0.3450251526:
11: 16032: loss: 0.3466694195:
11: 19232: loss: 0.3459463515:
11: 22432: loss: 0.3448984093:
11: 25632: loss: 0.3443451882:
11: 28832: loss: 0.3445606887:
11: 32032: loss: 0.3429716795:
11: 35232: loss: 0.3428758175:
11: 38432: loss: 0.3434931951:
11: 41632: loss: 0.3432724726:
11: 44832: loss: 0.3432131776:
11: 48032: loss: 0.3431173786:
11: 51232: loss: 0.3441506132:
11: 54432: loss: 0.3440235485:
11: 57632: loss: 0.3444304057:
11: 60832: loss: 0.3437341280:
11: 64032: loss: 0.3435242090:
11: 67232: loss: 0.3438316744:
11: 70432: loss: 0.3436005451:
11: 73632: loss: 0.3431010363:
11: 76832: loss: 0.3426036499:
11: 80032: loss: 0.3420660638:
11: 83232: loss: 0.3419861057:
11: 86432: loss: 0.3417958601:
11: 89632: loss: 0.3411938091:
11: 92832: loss: 0.3411242026:
11: 96032: loss: 0.3412660094:
11: 99232: loss: 0.3413031945:
11: 102432: loss: 0.3406878951:
11: 105632: loss: 0.3404158225:
11: 108832: loss: 0.3403233777:
11: 112032: loss: 0.3400964999:
11: 115232: loss: 0.3397620718:
11: 118432: loss: 0.3399670914:
11: 121632: loss: 0.3399469764:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.3300218697:
12: 6432: loss: 0.3319852947:
12: 9632: loss: 0.3310701713:
12: 12832: loss: 0.3293474686:
12: 16032: loss: 0.3309043393:
12: 19232: loss: 0.3317950498:
12: 22432: loss: 0.3318340097:
12: 25632: loss: 0.3333913226:
12: 28832: loss: 0.3324469954:
12: 32032: loss: 0.3314508267:
12: 35232: loss: 0.3306865649:
12: 38432: loss: 0.3313280681:
12: 41632: loss: 0.3320869786:
12: 44832: loss: 0.3312545147:
12: 48032: loss: 0.3310965987:
12: 51232: loss: 0.3310451784:
12: 54432: loss: 0.3302280893:
12: 57632: loss: 0.3302221281:
12: 60832: loss: 0.3294702292:
12: 64032: loss: 0.3295562479:
12: 67232: loss: 0.3299165832:
12: 70432: loss: 0.3300884633:
12: 73632: loss: 0.3296625230:
12: 76832: loss: 0.3294591282:
12: 80032: loss: 0.3290781328:
12: 83232: loss: 0.3291626461:
12: 86432: loss: 0.3287087759:
12: 89632: loss: 0.3290778638:
12: 92832: loss: 0.3290988963:
12: 96032: loss: 0.3295830473:
12: 99232: loss: 0.3290517174:
12: 102432: loss: 0.3287959557:
12: 105632: loss: 0.3286430689:
12: 108832: loss: 0.3282535156:
12: 112032: loss: 0.3284997118:
12: 115232: loss: 0.3284489697:
12: 118432: loss: 0.3279110989:
12: 121632: loss: 0.3277945292:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.3262641540:
13: 6432: loss: 0.3240449121:
13: 9632: loss: 0.3187136309:
13: 12832: loss: 0.3210883698:
13: 16032: loss: 0.3192463502:
13: 19232: loss: 0.3203177624:
13: 22432: loss: 0.3187693319:
13: 25632: loss: 0.3217778200:
13: 28832: loss: 0.3233838893:
13: 32032: loss: 0.3221422814:
13: 35232: loss: 0.3210730511:
13: 38432: loss: 0.3203472658:
13: 41632: loss: 0.3211466619:
13: 44832: loss: 0.3201978111:
13: 48032: loss: 0.3194170777:
13: 51232: loss: 0.3191134801:
13: 54432: loss: 0.3190171566:
13: 57632: loss: 0.3188414793:
13: 60832: loss: 0.3188756779:
13: 64032: loss: 0.3185024945:
13: 67232: loss: 0.3183973703:
13: 70432: loss: 0.3176420368:
13: 73632: loss: 0.3176843507:
13: 76832: loss: 0.3175034529:
13: 80032: loss: 0.3180243360:
13: 83232: loss: 0.3179310978:
13: 86432: loss: 0.3182403688:
13: 89632: loss: 0.3182532106:
13: 92832: loss: 0.3180810096:
13: 96032: loss: 0.3177788387:
13: 99232: loss: 0.3176272785:
13: 102432: loss: 0.3174916749:
13: 105632: loss: 0.3177632504:
13: 108832: loss: 0.3175814195:
13: 112032: loss: 0.3174178228:
13: 115232: loss: 0.3172710356:
13: 118432: loss: 0.3170892004:
13: 121632: loss: 0.3170562884:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.8754191399: precision: 1.0000000000: recall: 0.0033528368: f1: 0.0066832656
14: 3232: loss: 0.3061810784:
14: 6432: loss: 0.3066966571:
14: 9632: loss: 0.3082650611:
14: 12832: loss: 0.3086553153:
14: 16032: loss: 0.3089452281:
14: 19232: loss: 0.3104877363:
14: 22432: loss: 0.3110012812:
14: 25632: loss: 0.3125384632:
14: 28832: loss: 0.3118524317:
14: 32032: loss: 0.3117658080:
14: 35232: loss: 0.3113943462:
14: 38432: loss: 0.3121233647:
14: 41632: loss: 0.3109832541:
14: 44832: loss: 0.3118473107:
14: 48032: loss: 0.3123410880:
14: 51232: loss: 0.3115987872:
14: 54432: loss: 0.3116331757:
14: 57632: loss: 0.3109685639:
14: 60832: loss: 0.3109651331:
14: 64032: loss: 0.3110181797:
14: 67232: loss: 0.3104370369:
14: 70432: loss: 0.3108094201:
14: 73632: loss: 0.3114639635:
14: 76832: loss: 0.3111062105:
14: 80032: loss: 0.3106760267:
14: 83232: loss: 0.3104257124:
14: 86432: loss: 0.3100186448:
14: 89632: loss: 0.3096649504:
14: 92832: loss: 0.3094580494:
14: 96032: loss: 0.3093619835:
14: 99232: loss: 0.3093647410:
14: 102432: loss: 0.3091500870:
14: 105632: loss: 0.3093135462:
14: 108832: loss: 0.3087682662:
14: 112032: loss: 0.3091525432:
14: 115232: loss: 0.3087000843:
14: 118432: loss: 0.3085510912:
14: 121632: loss: 0.3081538783:
Dev-Acc: 14: Accuracy: 0.9416871667: precision: 0.5588235294: recall: 0.0032307431: f1: 0.0064243449
Train-Acc: 14: Accuracy: 0.8772763610: precision: 0.8386308068: recall: 0.0225494708: f1: 0.0439180538
15: 3232: loss: 0.3170884232:
15: 6432: loss: 0.3108823457:
15: 9632: loss: 0.3083938187:
15: 12832: loss: 0.3068106234:
15: 16032: loss: 0.3044681279:
15: 19232: loss: 0.3046427896:
15: 22432: loss: 0.3060493375:
15: 25632: loss: 0.3068737416:
15: 28832: loss: 0.3062436446:
15: 32032: loss: 0.3050598511:
15: 35232: loss: 0.3041922110:
15: 38432: loss: 0.3053450579:
15: 41632: loss: 0.3042751226:
15: 44832: loss: 0.3047245529:
15: 48032: loss: 0.3037760328:
15: 51232: loss: 0.3038174127:
15: 54432: loss: 0.3037227159:
15: 57632: loss: 0.3036424343:
15: 60832: loss: 0.3033944727:
15: 64032: loss: 0.3026802285:
15: 67232: loss: 0.3027047259:
15: 70432: loss: 0.3027081268:
15: 73632: loss: 0.3024180218:
15: 76832: loss: 0.3022368444:
15: 80032: loss: 0.3023633876:
15: 83232: loss: 0.3010765114:
15: 86432: loss: 0.3007532384:
15: 89632: loss: 0.3009426620:
15: 92832: loss: 0.3010444884:
15: 96032: loss: 0.3007063823:
15: 99232: loss: 0.3000491895:
15: 102432: loss: 0.3002046184:
15: 105632: loss: 0.3000203627:
15: 108832: loss: 0.2999432302:
15: 112032: loss: 0.2996293936:
15: 115232: loss: 0.2993992215:
15: 118432: loss: 0.2993266513:
15: 121632: loss: 0.2994024481:
Dev-Acc: 15: Accuracy: 0.9423320889: precision: 0.6949152542: recall: 0.0209148104: f1: 0.0406074612
Train-Acc: 15: Accuracy: 0.8787062168: precision: 0.8263386397: recall: 0.0375386234: f1: 0.0718148661
16: 3232: loss: 0.3117953835:
16: 6432: loss: 0.3053474104:
16: 9632: loss: 0.3033000121:
16: 12832: loss: 0.3016415134:
16: 16032: loss: 0.3023219896:
16: 19232: loss: 0.3011047746:
16: 22432: loss: 0.2964201615:
16: 25632: loss: 0.2956210459:
16: 28832: loss: 0.2952596190:
16: 32032: loss: 0.2955590629:
16: 35232: loss: 0.2960096185:
16: 38432: loss: 0.2947499424:
16: 41632: loss: 0.2961860303:
16: 44832: loss: 0.2959781822:
16: 48032: loss: 0.2957835439:
16: 51232: loss: 0.2953474838:
16: 54432: loss: 0.2953420922:
16: 57632: loss: 0.2950380664:
16: 60832: loss: 0.2950429186:
16: 64032: loss: 0.2956595946:
16: 67232: loss: 0.2952987648:
16: 70432: loss: 0.2947729642:
16: 73632: loss: 0.2945741079:
16: 76832: loss: 0.2951296618:
16: 80032: loss: 0.2946181541:
16: 83232: loss: 0.2945018401:
16: 86432: loss: 0.2941369704:
16: 89632: loss: 0.2938821223:
16: 92832: loss: 0.2936567652:
16: 96032: loss: 0.2934270706:
16: 99232: loss: 0.2932734485:
16: 102432: loss: 0.2935405184:
16: 105632: loss: 0.2932891370:
16: 108832: loss: 0.2929130719:
16: 112032: loss: 0.2926676659:
16: 115232: loss: 0.2923987063:
16: 118432: loss: 0.2923361917:
16: 121632: loss: 0.2919518732:
Dev-Acc: 16: Accuracy: 0.9423916340: precision: 0.6080691643: recall: 0.0358782520: f1: 0.0677585100
Train-Acc: 16: Accuracy: 0.8814838529: precision: 0.8583106267: recall: 0.0621260930: f1: 0.1158656204
17: 3232: loss: 0.2945330848:
17: 6432: loss: 0.2870306778:
17: 9632: loss: 0.2853937437:
17: 12832: loss: 0.2894052524:
17: 16032: loss: 0.2876973585:
17: 19232: loss: 0.2898880144:
17: 22432: loss: 0.2910531221:
17: 25632: loss: 0.2907952348:
17: 28832: loss: 0.2895444699:
17: 32032: loss: 0.2890398624:
17: 35232: loss: 0.2902617192:
17: 38432: loss: 0.2893788553:
17: 41632: loss: 0.2881949549:
17: 44832: loss: 0.2885652494:
17: 48032: loss: 0.2883184082:
17: 51232: loss: 0.2880199395:
17: 54432: loss: 0.2887719403:
17: 57632: loss: 0.2893819432:
17: 60832: loss: 0.2891684904:
17: 64032: loss: 0.2885271552:
17: 67232: loss: 0.2881727822:
17: 70432: loss: 0.2877078268:
17: 73632: loss: 0.2871892793:
17: 76832: loss: 0.2871049927:
17: 80032: loss: 0.2871493537:
17: 83232: loss: 0.2878504802:
17: 86432: loss: 0.2870640506:
17: 89632: loss: 0.2871410472:
17: 92832: loss: 0.2871964073:
17: 96032: loss: 0.2872116817:
17: 99232: loss: 0.2866909196:
17: 102432: loss: 0.2862163937:
17: 105632: loss: 0.2858536460:
17: 108832: loss: 0.2855463381:
17: 112032: loss: 0.2857661054:
17: 115232: loss: 0.2856935678:
17: 118432: loss: 0.2854225915:
17: 121632: loss: 0.2852253044:
Dev-Acc: 17: Accuracy: 0.9422824979: precision: 0.5422163588: recall: 0.0698860738: f1: 0.1238138274
Train-Acc: 17: Accuracy: 0.8851736188: precision: 0.8711031175: recall: 0.0955229768: f1: 0.1721665975
18: 3232: loss: 0.2943151523:
18: 6432: loss: 0.2838540091:
18: 9632: loss: 0.2776072452:
18: 12832: loss: 0.2829873931:
18: 16032: loss: 0.2803637921:
18: 19232: loss: 0.2810313896:
18: 22432: loss: 0.2811525888:
18: 25632: loss: 0.2819835668:
18: 28832: loss: 0.2820038502:
18: 32032: loss: 0.2830561154:
18: 35232: loss: 0.2835119828:
18: 38432: loss: 0.2837524959:
18: 41632: loss: 0.2837130342:
18: 44832: loss: 0.2834077124:
18: 48032: loss: 0.2832498040:
18: 51232: loss: 0.2825335399:
18: 54432: loss: 0.2818047503:
18: 57632: loss: 0.2822821067:
18: 60832: loss: 0.2813993514:
18: 64032: loss: 0.2807134596:
18: 67232: loss: 0.2811277647:
18: 70432: loss: 0.2809959600:
18: 73632: loss: 0.2811818178:
18: 76832: loss: 0.2813367637:
18: 80032: loss: 0.2811538359:
18: 83232: loss: 0.2805382038:
18: 86432: loss: 0.2802321822:
18: 89632: loss: 0.2801781737:
18: 92832: loss: 0.2794765954:
18: 96032: loss: 0.2789941874:
18: 99232: loss: 0.2789486684:
18: 102432: loss: 0.2787075366:
18: 105632: loss: 0.2787267230:
18: 108832: loss: 0.2786415261:
18: 112032: loss: 0.2789824959:
18: 115232: loss: 0.2793251572:
18: 118432: loss: 0.2794410609:
18: 121632: loss: 0.2791631522:
Dev-Acc: 18: Accuracy: 0.9421634078: precision: 0.5243445693: recall: 0.0952219010: f1: 0.1611742697
Train-Acc: 18: Accuracy: 0.8880497813: precision: 0.8832046332: recall: 0.1203076721: f1: 0.2117687901
19: 3232: loss: 0.2695118677:
19: 6432: loss: 0.2653431945:
19: 9632: loss: 0.2703623015:
19: 12832: loss: 0.2713702206:
19: 16032: loss: 0.2732369334:
19: 19232: loss: 0.2726683140:
19: 22432: loss: 0.2732602347:
19: 25632: loss: 0.2734142571:
19: 28832: loss: 0.2732850748:
19: 32032: loss: 0.2751398822:
19: 35232: loss: 0.2742331423:
19: 38432: loss: 0.2732738695:
19: 41632: loss: 0.2727382872:
19: 44832: loss: 0.2725324839:
19: 48032: loss: 0.2726720331:
19: 51232: loss: 0.2722590558:
19: 54432: loss: 0.2717183451:
19: 57632: loss: 0.2727915083:
19: 60832: loss: 0.2726333624:
19: 64032: loss: 0.2732213630:
19: 67232: loss: 0.2738301156:
19: 70432: loss: 0.2732743960:
19: 73632: loss: 0.2736656639:
19: 76832: loss: 0.2736779116:
19: 80032: loss: 0.2733383808:
19: 83232: loss: 0.2737204604:
19: 86432: loss: 0.2735789328:
19: 89632: loss: 0.2740037740:
19: 92832: loss: 0.2736904608:
19: 96032: loss: 0.2736931787:
19: 99232: loss: 0.2735897148:
19: 102432: loss: 0.2741664229:
19: 105632: loss: 0.2735853446:
19: 108832: loss: 0.2732145051:
19: 112032: loss: 0.2732518762:
19: 115232: loss: 0.2736334441:
19: 118432: loss: 0.2730191333:
19: 121632: loss: 0.2729975613:
Dev-Acc: 19: Accuracy: 0.9434533119: precision: 0.5636363636: recall: 0.1370515219: f1: 0.2204896731
Train-Acc: 19: Accuracy: 0.8924462795: precision: 0.9010200227: recall: 0.1567944251: f1: 0.2671071789
20: 3232: loss: 0.2794553269:
20: 6432: loss: 0.2787742614:
20: 9632: loss: 0.2794547601:
20: 12832: loss: 0.2769827006:
20: 16032: loss: 0.2738631573:
20: 19232: loss: 0.2743426689:
20: 22432: loss: 0.2734929547:
20: 25632: loss: 0.2733254334:
20: 28832: loss: 0.2748639985:
20: 32032: loss: 0.2738134489:
20: 35232: loss: 0.2745286447:
20: 38432: loss: 0.2726713276:
20: 41632: loss: 0.2718740697:
20: 44832: loss: 0.2709616279:
20: 48032: loss: 0.2710061971:
20: 51232: loss: 0.2705240129:
20: 54432: loss: 0.2711449408:
20: 57632: loss: 0.2718551311:
20: 60832: loss: 0.2717133229:
20: 64032: loss: 0.2715072249:
20: 67232: loss: 0.2719711945:
20: 70432: loss: 0.2719538188:
20: 73632: loss: 0.2722030267:
20: 76832: loss: 0.2722329828:
20: 80032: loss: 0.2715021296:
20: 83232: loss: 0.2708068454:
20: 86432: loss: 0.2708477415:
20: 89632: loss: 0.2709808506:
20: 92832: loss: 0.2705716437:
20: 96032: loss: 0.2704580893:
20: 99232: loss: 0.2706168305:
20: 102432: loss: 0.2701688990:
20: 105632: loss: 0.2699785205:
20: 108832: loss: 0.2692550795:
20: 112032: loss: 0.2691845194:
20: 115232: loss: 0.2686783523:
20: 118432: loss: 0.2677727869:
20: 121632: loss: 0.2679990967:
Dev-Acc: 20: Accuracy: 0.9429274201: precision: 0.5321695761: recall: 0.1814317293: f1: 0.2706061375
Train-Acc: 20: Accuracy: 0.8958237767: precision: 0.8869883934: recall: 0.1909144698: f1: 0.3142007033
