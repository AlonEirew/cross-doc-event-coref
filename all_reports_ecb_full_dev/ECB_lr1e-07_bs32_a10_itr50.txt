1: 3232: loss: 0.7112611806:
1: 6432: loss: 0.7097432712:
1: 9632: loss: 0.7084852558:
1: 12832: loss: 0.7071429537:
1: 16032: loss: 0.7061985493:
1: 19232: loss: 0.7053276958:
1: 22432: loss: 0.7043118258:
1: 25632: loss: 0.7035348297:
1: 28832: loss: 0.7026002464:
1: 32032: loss: 0.7015433731:
1: 35232: loss: 0.7005933997:
1: 38432: loss: 0.6997306822:
1: 41632: loss: 0.6988281319:
1: 44832: loss: 0.6978609248:
1: 48032: loss: 0.6969125580:
1: 51232: loss: 0.6959290968:
1: 54432: loss: 0.6949804320:
1: 57632: loss: 0.6940334151:
1: 60832: loss: 0.6931383503:
1: 64032: loss: 0.6921623945:
1: 67232: loss: 0.6911905331:
1: 70432: loss: 0.6902488118:
1: 73632: loss: 0.6893569468:
1: 76832: loss: 0.6884352072:
1: 80032: loss: 0.6875398663:
1: 83232: loss: 0.6866327399:
1: 86432: loss: 0.6857047199:
1: 89632: loss: 0.6848422311:
1: 92832: loss: 0.6838995577:
1: 96032: loss: 0.6830377509:
1: 99232: loss: 0.6821181798:
1: 102432: loss: 0.6812077560:
1: 105632: loss: 0.6802884248:
1: 108832: loss: 0.6793707551:
1: 112032: loss: 0.6784982296:
1: 115232: loss: 0.6775766188:
1: 118432: loss: 0.6767018696:
1: 121632: loss: 0.6758170935:
1: 124832: loss: 0.6749233205:
1: 128032: loss: 0.6740477829:
1: 131232: loss: 0.6731839255:
1: 134432: loss: 0.6722877227:
1: 137632: loss: 0.6714026846:
1: 140832: loss: 0.6704848356:
1: 144032: loss: 0.6695820709:
1: 147232: loss: 0.6686933795:
1: 150432: loss: 0.6678274344:
1: 153632: loss: 0.6669630949:
1: 156832: loss: 0.6661089480:
1: 160032: loss: 0.6652039704:
1: 163232: loss: 0.6643022808:
1: 166432: loss: 0.6634888526:
Dev-Acc: 1: Accuracy: 0.9392264485: precision: 0.0960264901: recall: 0.0049311342: f1: 0.0093805596
Train-Acc: 1: Accuracy: 0.9069274068: precision: 0.2189440994: recall: 0.0092696075: f1: 0.0177861873
2: 3232: loss: 0.6176242077:
2: 6432: loss: 0.6177148038:
2: 9632: loss: 0.6159027708:
2: 12832: loss: 0.6150143029:
2: 16032: loss: 0.6140921875:
2: 19232: loss: 0.6135586536:
2: 22432: loss: 0.6128076198:
2: 25632: loss: 0.6119092467:
2: 28832: loss: 0.6109855837:
2: 32032: loss: 0.6100474994:
2: 35232: loss: 0.6091333382:
2: 38432: loss: 0.6082783175:
2: 41632: loss: 0.6076051418:
2: 44832: loss: 0.6066463295:
2: 48032: loss: 0.6058010809:
2: 51232: loss: 0.6049322071:
2: 54432: loss: 0.6041742979:
2: 57632: loss: 0.6034443866:
2: 60832: loss: 0.6028583143:
2: 64032: loss: 0.6020060028:
2: 67232: loss: 0.6012304881:
2: 70432: loss: 0.6005009481:
2: 73632: loss: 0.5996030311:
2: 76832: loss: 0.5988984134:
2: 80032: loss: 0.5980345382:
2: 83232: loss: 0.5971754073:
2: 86432: loss: 0.5963806740:
2: 89632: loss: 0.5955419912:
2: 92832: loss: 0.5947722282:
2: 96032: loss: 0.5939438224:
2: 99232: loss: 0.5931671056:
2: 102432: loss: 0.5923183392:
2: 105632: loss: 0.5915165666:
2: 108832: loss: 0.5908496621:
2: 112032: loss: 0.5900473552:
2: 115232: loss: 0.5893132832:
2: 118432: loss: 0.5884049184:
2: 121632: loss: 0.5876413628:
2: 124832: loss: 0.5868272346:
2: 128032: loss: 0.5861696402:
2: 131232: loss: 0.5854245764:
2: 134432: loss: 0.5845809420:
2: 137632: loss: 0.5838180621:
2: 140832: loss: 0.5829572519:
2: 144032: loss: 0.5821003547:
2: 147232: loss: 0.5812381383:
2: 150432: loss: 0.5804245246:
2: 153632: loss: 0.5797014088:
2: 156832: loss: 0.5789030584:
2: 160032: loss: 0.5782045595:
2: 163232: loss: 0.5774016899:
2: 166432: loss: 0.5765848769:
Dev-Acc: 2: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 2: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
3: 3232: loss: 0.5341464862:
3: 6432: loss: 0.5343040407:
3: 9632: loss: 0.5343658143:
3: 12832: loss: 0.5337754379:
3: 16032: loss: 0.5323493320:
3: 19232: loss: 0.5313974428:
3: 22432: loss: 0.5304170208:
3: 25632: loss: 0.5303221112:
3: 28832: loss: 0.5291613202:
3: 32032: loss: 0.5287497957:
3: 35232: loss: 0.5280997976:
3: 38432: loss: 0.5269104557:
3: 41632: loss: 0.5261724131:
3: 44832: loss: 0.5256210800:
3: 48032: loss: 0.5249332897:
3: 51232: loss: 0.5239647226:
3: 54432: loss: 0.5231733070:
3: 57632: loss: 0.5224424581:
3: 60832: loss: 0.5217967190:
3: 64032: loss: 0.5208504600:
3: 67232: loss: 0.5204852483:
3: 70432: loss: 0.5198394993:
3: 73632: loss: 0.5190364264:
3: 76832: loss: 0.5185264436:
3: 80032: loss: 0.5175790011:
3: 83232: loss: 0.5169506656:
3: 86432: loss: 0.5163878940:
3: 89632: loss: 0.5155827367:
3: 92832: loss: 0.5149445214:
3: 96032: loss: 0.5142974510:
3: 99232: loss: 0.5134588433:
3: 102432: loss: 0.5127009202:
3: 105632: loss: 0.5121334364:
3: 108832: loss: 0.5116281989:
3: 112032: loss: 0.5107553037:
3: 115232: loss: 0.5101268708:
3: 118432: loss: 0.5095982552:
3: 121632: loss: 0.5089623982:
3: 124832: loss: 0.5082976334:
3: 128032: loss: 0.5075667909:
3: 131232: loss: 0.5066501293:
3: 134432: loss: 0.5061427723:
3: 137632: loss: 0.5054974408:
3: 140832: loss: 0.5049403416:
3: 144032: loss: 0.5042719564:
3: 147232: loss: 0.5037450700:
3: 150432: loss: 0.5031214105:
3: 153632: loss: 0.5024302365:
3: 156832: loss: 0.5015943032:
3: 160032: loss: 0.5009512543:
3: 163232: loss: 0.5003062068:
3: 166432: loss: 0.4994418406:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.4602072278:
4: 6432: loss: 0.4598050711:
4: 9632: loss: 0.4598430326:
4: 12832: loss: 0.4605297347:
4: 16032: loss: 0.4593037827:
4: 19232: loss: 0.4584557288:
4: 22432: loss: 0.4574457143:
4: 25632: loss: 0.4576212151:
4: 28832: loss: 0.4576276073:
4: 32032: loss: 0.4574535792:
4: 35232: loss: 0.4569069166:
4: 38432: loss: 0.4560141691:
4: 41632: loss: 0.4553661878:
4: 44832: loss: 0.4550986308:
4: 48032: loss: 0.4545019643:
4: 51232: loss: 0.4537484303:
4: 54432: loss: 0.4532521909:
4: 57632: loss: 0.4525564096:
4: 60832: loss: 0.4523782824:
4: 64032: loss: 0.4519194218:
4: 67232: loss: 0.4516233653:
4: 70432: loss: 0.4510376940:
4: 73632: loss: 0.4505070895:
4: 76832: loss: 0.4500115030:
4: 80032: loss: 0.4496177770:
4: 83232: loss: 0.4491033691:
4: 86432: loss: 0.4483117442:
4: 89632: loss: 0.4474864620:
4: 92832: loss: 0.4465819996:
4: 96032: loss: 0.4460987840:
4: 99232: loss: 0.4454716956:
4: 102432: loss: 0.4447480482:
4: 105632: loss: 0.4442535437:
4: 108832: loss: 0.4441200189:
4: 112032: loss: 0.4435527752:
4: 115232: loss: 0.4432046033:
4: 118432: loss: 0.4427656743:
4: 121632: loss: 0.4423146684:
4: 124832: loss: 0.4416178055:
4: 128032: loss: 0.4408180117:
4: 131232: loss: 0.4403779233:
4: 134432: loss: 0.4397539541:
4: 137632: loss: 0.4389259609:
4: 140832: loss: 0.4384094129:
4: 144032: loss: 0.4380420940:
4: 147232: loss: 0.4375269289:
4: 150432: loss: 0.4368666131:
4: 153632: loss: 0.4360739636:
4: 156832: loss: 0.4354246147:
4: 160032: loss: 0.4347791512:
4: 163232: loss: 0.4341479198:
4: 166432: loss: 0.4335591441:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4008821326:
5: 6432: loss: 0.4050012395:
5: 9632: loss: 0.4045754439:
5: 12832: loss: 0.4019764914:
5: 16032: loss: 0.4019102100:
5: 19232: loss: 0.4001309077:
5: 22432: loss: 0.4004462555:
5: 25632: loss: 0.3998069543:
5: 28832: loss: 0.3993936720:
5: 32032: loss: 0.3982922009:
5: 35232: loss: 0.3973008976:
5: 38432: loss: 0.3973520795:
5: 41632: loss: 0.3963136141:
5: 44832: loss: 0.3958038843:
5: 48032: loss: 0.3961067354:
5: 51232: loss: 0.3957867972:
5: 54432: loss: 0.3957672196:
5: 57632: loss: 0.3953736485:
5: 60832: loss: 0.3948121879:
5: 64032: loss: 0.3944971442:
5: 67232: loss: 0.3936662471:
5: 70432: loss: 0.3934120740:
5: 73632: loss: 0.3933198259:
5: 76832: loss: 0.3931707590:
5: 80032: loss: 0.3927099455:
5: 83232: loss: 0.3921504242:
5: 86432: loss: 0.3916761024:
5: 89632: loss: 0.3914402977:
5: 92832: loss: 0.3909145886:
5: 96032: loss: 0.3906513781:
5: 99232: loss: 0.3901687390:
5: 102432: loss: 0.3899117822:
5: 105632: loss: 0.3897101518:
5: 108832: loss: 0.3893161281:
5: 112032: loss: 0.3889562348:
5: 115232: loss: 0.3884032610:
5: 118432: loss: 0.3878997896:
5: 121632: loss: 0.3872442271:
5: 124832: loss: 0.3869525471:
5: 128032: loss: 0.3866387366:
5: 131232: loss: 0.3865116455:
5: 134432: loss: 0.3863191281:
5: 137632: loss: 0.3856557172:
5: 140832: loss: 0.3850498585:
5: 144032: loss: 0.3844960495:
5: 147232: loss: 0.3841205153:
5: 150432: loss: 0.3836032050:
5: 153632: loss: 0.3831904639:
5: 156832: loss: 0.3828511916:
5: 160032: loss: 0.3824433122:
5: 163232: loss: 0.3820596414:
5: 166432: loss: 0.3818178630:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.3611842607:
6: 6432: loss: 0.3611181985:
6: 9632: loss: 0.3653124517:
6: 12832: loss: 0.3665210989:
6: 16032: loss: 0.3670206808:
6: 19232: loss: 0.3653385315:
6: 22432: loss: 0.3646759547:
6: 25632: loss: 0.3618423820:
6: 28832: loss: 0.3612937571:
6: 32032: loss: 0.3595066428:
6: 35232: loss: 0.3594196373:
6: 38432: loss: 0.3592826335:
6: 41632: loss: 0.3584990600:
6: 44832: loss: 0.3586640380:
6: 48032: loss: 0.3584985905:
6: 51232: loss: 0.3576664119:
6: 54432: loss: 0.3570222855:
6: 57632: loss: 0.3568387121:
6: 60832: loss: 0.3559785701:
6: 64032: loss: 0.3559642749:
6: 67232: loss: 0.3551299432:
6: 70432: loss: 0.3547059689:
6: 73632: loss: 0.3542875996:
6: 76832: loss: 0.3538148626:
6: 80032: loss: 0.3537074869:
6: 83232: loss: 0.3534686371:
6: 86432: loss: 0.3531846869:
6: 89632: loss: 0.3526395930:
6: 92832: loss: 0.3525749917:
6: 96032: loss: 0.3522818634:
6: 99232: loss: 0.3519189597:
6: 102432: loss: 0.3511604461:
6: 105632: loss: 0.3506025120:
6: 108832: loss: 0.3506198583:
6: 112032: loss: 0.3504603915:
6: 115232: loss: 0.3496106231:
6: 118432: loss: 0.3492901983:
6: 121632: loss: 0.3489797524:
6: 124832: loss: 0.3484878150:
6: 128032: loss: 0.3480126552:
6: 131232: loss: 0.3475433175:
6: 134432: loss: 0.3470349277:
6: 137632: loss: 0.3469572293:
6: 140832: loss: 0.3467092887:
6: 144032: loss: 0.3462120101:
6: 147232: loss: 0.3460500781:
6: 150432: loss: 0.3454784882:
6: 153632: loss: 0.3453652857:
6: 156832: loss: 0.3452000594:
6: 160032: loss: 0.3448262728:
6: 163232: loss: 0.3445821931:
6: 166432: loss: 0.3441817107:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.3427736610:
7: 6432: loss: 0.3379549015:
7: 9632: loss: 0.3338615339:
7: 12832: loss: 0.3296037031:
7: 16032: loss: 0.3298675950:
7: 19232: loss: 0.3343391966:
7: 22432: loss: 0.3329057540:
7: 25632: loss: 0.3324999789:
7: 28832: loss: 0.3309193875:
7: 32032: loss: 0.3285589602:
7: 35232: loss: 0.3283845429:
7: 38432: loss: 0.3277853466:
7: 41632: loss: 0.3267106535:
7: 44832: loss: 0.3253861674:
7: 48032: loss: 0.3257893903:
7: 51232: loss: 0.3263882908:
7: 54432: loss: 0.3267839297:
7: 57632: loss: 0.3269429744:
7: 60832: loss: 0.3263711209:
7: 64032: loss: 0.3262692661:
7: 67232: loss: 0.3257780617:
7: 70432: loss: 0.3253413306:
7: 73632: loss: 0.3248022486:
7: 76832: loss: 0.3243335887:
7: 80032: loss: 0.3241300441:
7: 83232: loss: 0.3241068798:
7: 86432: loss: 0.3236386954:
7: 89632: loss: 0.3236872682:
7: 92832: loss: 0.3238989298:
7: 96032: loss: 0.3238222238:
7: 99232: loss: 0.3235410789:
7: 102432: loss: 0.3233852527:
7: 105632: loss: 0.3224296856:
7: 108832: loss: 0.3222488631:
7: 112032: loss: 0.3218185801:
7: 115232: loss: 0.3219857781:
7: 118432: loss: 0.3215474099:
7: 121632: loss: 0.3209669996:
7: 124832: loss: 0.3207115288:
7: 128032: loss: 0.3204382777:
7: 131232: loss: 0.3202426589:
7: 134432: loss: 0.3200875488:
7: 137632: loss: 0.3197892522:
7: 140832: loss: 0.3194806225:
7: 144032: loss: 0.3191946712:
7: 147232: loss: 0.3191504110:
7: 150432: loss: 0.3187149853:
7: 153632: loss: 0.3184008068:
7: 156832: loss: 0.3182885645:
7: 160032: loss: 0.3185399470:
7: 163232: loss: 0.3181838765:
7: 166432: loss: 0.3181672922:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3064984752:
8: 6432: loss: 0.3086067788:
8: 9632: loss: 0.3094005804:
8: 12832: loss: 0.3063627038:
8: 16032: loss: 0.3069139097:
8: 19232: loss: 0.3066068557:
8: 22432: loss: 0.3036129140:
8: 25632: loss: 0.3036659537:
8: 28832: loss: 0.3049741214:
8: 32032: loss: 0.3048726288:
8: 35232: loss: 0.3038217919:
8: 38432: loss: 0.3043855033:
8: 41632: loss: 0.3045312056:
8: 44832: loss: 0.3041729428:
8: 48032: loss: 0.3040294000:
8: 51232: loss: 0.3048506869:
8: 54432: loss: 0.3044015731:
8: 57632: loss: 0.3044724572:
8: 60832: loss: 0.3045399051:
8: 64032: loss: 0.3043267774:
8: 67232: loss: 0.3040202749:
8: 70432: loss: 0.3037416903:
8: 73632: loss: 0.3033734244:
8: 76832: loss: 0.3036951746:
8: 80032: loss: 0.3044613354:
8: 83232: loss: 0.3042138742:
8: 86432: loss: 0.3044754513:
8: 89632: loss: 0.3040157878:
8: 92832: loss: 0.3032949097:
8: 96032: loss: 0.3028294327:
8: 99232: loss: 0.3026062908:
8: 102432: loss: 0.3020812273:
8: 105632: loss: 0.3019609552:
8: 108832: loss: 0.3016820252:
8: 112032: loss: 0.3012537943:
8: 115232: loss: 0.3010593095:
8: 118432: loss: 0.3012725667:
8: 121632: loss: 0.3008020449:
8: 124832: loss: 0.3006218304:
8: 128032: loss: 0.3005071429:
8: 131232: loss: 0.3004138810:
8: 134432: loss: 0.3002621255:
8: 137632: loss: 0.3000257389:
8: 140832: loss: 0.2998404352:
8: 144032: loss: 0.2997944262:
8: 147232: loss: 0.2996335560:
8: 150432: loss: 0.2996147188:
8: 153632: loss: 0.2996317860:
8: 156832: loss: 0.2995521791:
8: 160032: loss: 0.2998669722:
8: 163232: loss: 0.2999573001:
8: 166432: loss: 0.3000412808:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.2801035039:
9: 6432: loss: 0.2847698764:
9: 9632: loss: 0.2872940315:
9: 12832: loss: 0.2930272645:
9: 16032: loss: 0.2899470701:
9: 19232: loss: 0.2883596615:
9: 22432: loss: 0.2885406491:
9: 25632: loss: 0.2876373960:
9: 28832: loss: 0.2872359295:
9: 32032: loss: 0.2875808939:
9: 35232: loss: 0.2878397866:
9: 38432: loss: 0.2883891152:
9: 41632: loss: 0.2893360952:
9: 44832: loss: 0.2899014127:
9: 48032: loss: 0.2897637125:
9: 51232: loss: 0.2901352512:
9: 54432: loss: 0.2901742313:
9: 57632: loss: 0.2913326762:
9: 60832: loss: 0.2911581017:
9: 64032: loss: 0.2912457778:
9: 67232: loss: 0.2911130709:
9: 70432: loss: 0.2906763243:
9: 73632: loss: 0.2907733988:
9: 76832: loss: 0.2896054214:
9: 80032: loss: 0.2894745195:
9: 83232: loss: 0.2888938469:
9: 86432: loss: 0.2888875036:
9: 89632: loss: 0.2889040595:
9: 92832: loss: 0.2889207834:
9: 96032: loss: 0.2886452503:
9: 99232: loss: 0.2882045909:
9: 102432: loss: 0.2880463164:
9: 105632: loss: 0.2880593490:
9: 108832: loss: 0.2879056048:
9: 112032: loss: 0.2877641826:
9: 115232: loss: 0.2876859567:
9: 118432: loss: 0.2875002768:
9: 121632: loss: 0.2880730919:
9: 124832: loss: 0.2878989399:
9: 128032: loss: 0.2881107594:
9: 131232: loss: 0.2880528152:
9: 134432: loss: 0.2875903034:
9: 137632: loss: 0.2875429536:
9: 140832: loss: 0.2875530061:
9: 144032: loss: 0.2873137888:
9: 147232: loss: 0.2872176480:
9: 150432: loss: 0.2870657708:
9: 153632: loss: 0.2866720721:
9: 156832: loss: 0.2866068649:
9: 160032: loss: 0.2865981842:
9: 163232: loss: 0.2865917956:
9: 166432: loss: 0.2863811982:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.2617894167:
10: 6432: loss: 0.2709736247:
10: 9632: loss: 0.2753865610:
10: 12832: loss: 0.2762602125:
10: 16032: loss: 0.2778608782:
10: 19232: loss: 0.2774624698:
10: 22432: loss: 0.2758517750:
10: 25632: loss: 0.2745855886:
10: 28832: loss: 0.2775462128:
10: 32032: loss: 0.2779138966:
10: 35232: loss: 0.2783498760:
10: 38432: loss: 0.2774378879:
10: 41632: loss: 0.2780741422:
10: 44832: loss: 0.2778610584:
10: 48032: loss: 0.2777368878:
10: 51232: loss: 0.2780521162:
10: 54432: loss: 0.2774082286:
10: 57632: loss: 0.2767480382:
10: 60832: loss: 0.2775374834:
10: 64032: loss: 0.2778246050:
10: 67232: loss: 0.2774657900:
10: 70432: loss: 0.2765726796:
10: 73632: loss: 0.2764474905:
10: 76832: loss: 0.2759957783:
10: 80032: loss: 0.2762409405:
10: 83232: loss: 0.2763327671:
10: 86432: loss: 0.2760327327:
10: 89632: loss: 0.2756450263:
10: 92832: loss: 0.2758334784:
10: 96032: loss: 0.2761876235:
10: 99232: loss: 0.2763618067:
10: 102432: loss: 0.2764356824:
10: 105632: loss: 0.2771495596:
10: 108832: loss: 0.2770671148:
10: 112032: loss: 0.2773853806:
10: 115232: loss: 0.2772470056:
10: 118432: loss: 0.2769397546:
10: 121632: loss: 0.2767230697:
10: 124832: loss: 0.2767532377:
10: 128032: loss: 0.2763922406:
10: 131232: loss: 0.2762094501:
10: 134432: loss: 0.2759531054:
10: 137632: loss: 0.2761234320:
10: 140832: loss: 0.2759228382:
10: 144032: loss: 0.2758903380:
10: 147232: loss: 0.2755038699:
10: 150432: loss: 0.2754388743:
10: 153632: loss: 0.2752002214:
10: 156832: loss: 0.2751808809:
10: 160032: loss: 0.2752914447:
10: 163232: loss: 0.2754221918:
10: 166432: loss: 0.2754970449:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.2678352639:
11: 6432: loss: 0.2676925294:
11: 9632: loss: 0.2732976530:
11: 12832: loss: 0.2727909932:
11: 16032: loss: 0.2718745456:
11: 19232: loss: 0.2681961085:
11: 22432: loss: 0.2692999696:
11: 25632: loss: 0.2692136611:
11: 28832: loss: 0.2674535606:
11: 32032: loss: 0.2667483249:
11: 35232: loss: 0.2682421889:
11: 38432: loss: 0.2675239536:
11: 41632: loss: 0.2677828825:
11: 44832: loss: 0.2685134806:
11: 48032: loss: 0.2676360884:
11: 51232: loss: 0.2677720256:
11: 54432: loss: 0.2668070341:
11: 57632: loss: 0.2669502513:
11: 60832: loss: 0.2675421318:
11: 64032: loss: 0.2680689205:
11: 67232: loss: 0.2681694272:
11: 70432: loss: 0.2684972559:
11: 73632: loss: 0.2680505124:
11: 76832: loss: 0.2685529988:
11: 80032: loss: 0.2682622458:
11: 83232: loss: 0.2684991922:
11: 86432: loss: 0.2688333328:
11: 89632: loss: 0.2690013785:
11: 92832: loss: 0.2687493327:
11: 96032: loss: 0.2683582405:
11: 99232: loss: 0.2689739201:
11: 102432: loss: 0.2691084582:
11: 105632: loss: 0.2692058469:
11: 108832: loss: 0.2693028164:
11: 112032: loss: 0.2690510586:
11: 115232: loss: 0.2685703152:
11: 118432: loss: 0.2686406381:
11: 121632: loss: 0.2679634431:
11: 124832: loss: 0.2683350150:
11: 128032: loss: 0.2681491899:
11: 131232: loss: 0.2680494650:
11: 134432: loss: 0.2682468168:
11: 137632: loss: 0.2678843844:
11: 140832: loss: 0.2676996100:
11: 144032: loss: 0.2674621907:
11: 147232: loss: 0.2672881880:
11: 150432: loss: 0.2672317660:
11: 153632: loss: 0.2667867930:
11: 156832: loss: 0.2665576608:
11: 160032: loss: 0.2667426408:
11: 163232: loss: 0.2664821131:
11: 166432: loss: 0.2662222322:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.2645723457:
12: 6432: loss: 0.2569098076:
12: 9632: loss: 0.2585812088:
12: 12832: loss: 0.2585330380:
12: 16032: loss: 0.2604210625:
12: 19232: loss: 0.2606862654:
12: 22432: loss: 0.2617301480:
12: 25632: loss: 0.2592273378:
12: 28832: loss: 0.2574909506:
12: 32032: loss: 0.2576706591:
12: 35232: loss: 0.2584704391:
12: 38432: loss: 0.2580774276:
12: 41632: loss: 0.2597137654:
12: 44832: loss: 0.2581015335:
12: 48032: loss: 0.2585371958:
12: 51232: loss: 0.2582564472:
12: 54432: loss: 0.2578584371:
12: 57632: loss: 0.2579508026:
12: 60832: loss: 0.2578888088:
12: 64032: loss: 0.2574303043:
12: 67232: loss: 0.2571517474:
12: 70432: loss: 0.2575113643:
12: 73632: loss: 0.2569860378:
12: 76832: loss: 0.2575616023:
12: 80032: loss: 0.2574854981:
12: 83232: loss: 0.2579137849:
12: 86432: loss: 0.2578542133:
12: 89632: loss: 0.2574040256:
12: 92832: loss: 0.2572490955:
12: 96032: loss: 0.2571231094:
12: 99232: loss: 0.2574926790:
12: 102432: loss: 0.2575260979:
12: 105632: loss: 0.2574936693:
12: 108832: loss: 0.2578224550:
12: 112032: loss: 0.2584763817:
12: 115232: loss: 0.2592226321:
12: 118432: loss: 0.2589859549:
12: 121632: loss: 0.2586374292:
12: 124832: loss: 0.2585961583:
12: 128032: loss: 0.2584128999:
12: 131232: loss: 0.2586697418:
12: 134432: loss: 0.2584335589:
12: 137632: loss: 0.2584030721:
12: 140832: loss: 0.2587882820:
12: 144032: loss: 0.2584262783:
12: 147232: loss: 0.2581437856:
12: 150432: loss: 0.2581514375:
12: 153632: loss: 0.2580744872:
12: 156832: loss: 0.2579459740:
12: 160032: loss: 0.2579365383:
12: 163232: loss: 0.2577569632:
12: 166432: loss: 0.2578239990:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2596737207:
13: 6432: loss: 0.2619295051:
13: 9632: loss: 0.2629307402:
13: 12832: loss: 0.2587271609:
13: 16032: loss: 0.2537972225:
13: 19232: loss: 0.2516951214:
13: 22432: loss: 0.2526081262:
13: 25632: loss: 0.2516906071:
13: 28832: loss: 0.2518854222:
13: 32032: loss: 0.2524103229:
13: 35232: loss: 0.2533922183:
13: 38432: loss: 0.2542645968:
13: 41632: loss: 0.2542179005:
13: 44832: loss: 0.2534350587:
13: 48032: loss: 0.2526780865:
13: 51232: loss: 0.2526212651:
13: 54432: loss: 0.2523969403:
13: 57632: loss: 0.2517835926:
13: 60832: loss: 0.2521980379:
13: 64032: loss: 0.2513124521:
13: 67232: loss: 0.2510945500:
13: 70432: loss: 0.2511106781:
13: 73632: loss: 0.2509132222:
13: 76832: loss: 0.2505726918:
13: 80032: loss: 0.2503950505:
13: 83232: loss: 0.2499446071:
13: 86432: loss: 0.2502481657:
13: 89632: loss: 0.2503083811:
13: 92832: loss: 0.2505105970:
13: 96032: loss: 0.2512776539:
13: 99232: loss: 0.2512986110:
13: 102432: loss: 0.2516514534:
13: 105632: loss: 0.2513192086:
13: 108832: loss: 0.2508692537:
13: 112032: loss: 0.2505118401:
13: 115232: loss: 0.2508628039:
13: 118432: loss: 0.2508329039:
13: 121632: loss: 0.2507351120:
13: 124832: loss: 0.2508261572:
13: 128032: loss: 0.2507722738:
13: 131232: loss: 0.2511598996:
13: 134432: loss: 0.2514111367:
13: 137632: loss: 0.2516454977:
13: 140832: loss: 0.2517055670:
13: 144032: loss: 0.2511798207:
13: 147232: loss: 0.2508983871:
13: 150432: loss: 0.2509048589:
13: 153632: loss: 0.2509847213:
13: 156832: loss: 0.2508465549:
13: 160032: loss: 0.2510277124:
13: 163232: loss: 0.2509851062:
13: 166432: loss: 0.2509699670:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.9094973207: precision: 1.0000000000: recall: 0.0044704490: f1: 0.0089011061
14: 3232: loss: 0.2401652895:
14: 6432: loss: 0.2447462367:
14: 9632: loss: 0.2507624433:
14: 12832: loss: 0.2456395140:
14: 16032: loss: 0.2450021133:
14: 19232: loss: 0.2453274437:
14: 22432: loss: 0.2454028114:
14: 25632: loss: 0.2471409292:
14: 28832: loss: 0.2473919894:
14: 32032: loss: 0.2473140381:
14: 35232: loss: 0.2477065229:
14: 38432: loss: 0.2473612639:
14: 41632: loss: 0.2471121511:
14: 44832: loss: 0.2476015612:
14: 48032: loss: 0.2481163732:
14: 51232: loss: 0.2474176078:
14: 54432: loss: 0.2473906468:
14: 57632: loss: 0.2469295489:
14: 60832: loss: 0.2464598042:
14: 64032: loss: 0.2455362754:
14: 67232: loss: 0.2448825974:
14: 70432: loss: 0.2451180982:
14: 73632: loss: 0.2456415298:
14: 76832: loss: 0.2457689317:
14: 80032: loss: 0.2451238040:
14: 83232: loss: 0.2455410656:
14: 86432: loss: 0.2457468488:
14: 89632: loss: 0.2461707327:
14: 92832: loss: 0.2467855547:
14: 96032: loss: 0.2462494793:
14: 99232: loss: 0.2459431547:
14: 102432: loss: 0.2460804892:
14: 105632: loss: 0.2457374847:
14: 108832: loss: 0.2453396956:
14: 112032: loss: 0.2453484716:
14: 115232: loss: 0.2456287160:
14: 118432: loss: 0.2457143029:
14: 121632: loss: 0.2457434427:
14: 124832: loss: 0.2460314523:
14: 128032: loss: 0.2459775283:
14: 131232: loss: 0.2457937023:
14: 134432: loss: 0.2457811694:
14: 137632: loss: 0.2458460913:
14: 140832: loss: 0.2455891847:
14: 144032: loss: 0.2453934351:
14: 147232: loss: 0.2456444467:
14: 150432: loss: 0.2456105352:
14: 153632: loss: 0.2456632127:
14: 156832: loss: 0.2455126838:
14: 160032: loss: 0.2452235235:
14: 163232: loss: 0.2449988119:
14: 166432: loss: 0.2450181867:
Dev-Acc: 14: Accuracy: 0.9418558478: precision: 0.6567164179: recall: 0.0074817208: f1: 0.0147948890
Train-Acc: 14: Accuracy: 0.9109556079: precision: 0.8095238095: recall: 0.0268226941: f1: 0.0519249125
15: 3232: loss: 0.2405529891:
15: 6432: loss: 0.2420241298:
15: 9632: loss: 0.2387944730:
15: 12832: loss: 0.2395692375:
15: 16032: loss: 0.2386925288:
15: 19232: loss: 0.2396493886:
15: 22432: loss: 0.2419920023:
15: 25632: loss: 0.2423551980:
15: 28832: loss: 0.2413228600:
15: 32032: loss: 0.2404620721:
15: 35232: loss: 0.2401527384:
15: 38432: loss: 0.2404237185:
15: 41632: loss: 0.2398480388:
15: 44832: loss: 0.2397302793:
15: 48032: loss: 0.2393677184:
15: 51232: loss: 0.2388014026:
15: 54432: loss: 0.2395463017:
15: 57632: loss: 0.2394428401:
15: 60832: loss: 0.2393484378:
15: 64032: loss: 0.2398970482:
15: 67232: loss: 0.2394397993:
15: 70432: loss: 0.2387312761:
15: 73632: loss: 0.2390691935:
15: 76832: loss: 0.2388379225:
15: 80032: loss: 0.2392955091:
15: 83232: loss: 0.2397835491:
15: 86432: loss: 0.2395958864:
15: 89632: loss: 0.2401144842:
15: 92832: loss: 0.2404913265:
15: 96032: loss: 0.2405193238:
15: 99232: loss: 0.2404132058:
15: 102432: loss: 0.2403276513:
15: 105632: loss: 0.2400051514:
15: 108832: loss: 0.2397700630:
15: 112032: loss: 0.2396475950:
15: 115232: loss: 0.2393164538:
15: 118432: loss: 0.2396311207:
15: 121632: loss: 0.2398176908:
15: 124832: loss: 0.2396592622:
15: 128032: loss: 0.2395275848:
15: 131232: loss: 0.2395131582:
15: 134432: loss: 0.2394838309:
15: 137632: loss: 0.2394786984:
15: 140832: loss: 0.2397596691:
15: 144032: loss: 0.2398077671:
15: 147232: loss: 0.2400289141:
15: 150432: loss: 0.2398707718:
15: 153632: loss: 0.2396275821:
15: 156832: loss: 0.2395184203:
15: 160032: loss: 0.2395489752:
15: 163232: loss: 0.2393002226:
15: 166432: loss: 0.2394128242:
Dev-Acc: 15: Accuracy: 0.9420840740: precision: 0.7244897959: recall: 0.0120727767: f1: 0.0237497909
Train-Acc: 15: Accuracy: 0.9114695787: precision: 0.8119122257: recall: 0.0340543028: f1: 0.0653669001
16: 3232: loss: 0.2439832942:
16: 6432: loss: 0.2380255203:
16: 9632: loss: 0.2399846190:
16: 12832: loss: 0.2407783911:
16: 16032: loss: 0.2396249901:
16: 19232: loss: 0.2403509659:
16: 22432: loss: 0.2391644002:
16: 25632: loss: 0.2386044606:
16: 28832: loss: 0.2381735441:
16: 32032: loss: 0.2397643483:
16: 35232: loss: 0.2397146917:
16: 38432: loss: 0.2383613102:
16: 41632: loss: 0.2393765446:
16: 44832: loss: 0.2398012840:
16: 48032: loss: 0.2378117776:
16: 51232: loss: 0.2372424568:
16: 54432: loss: 0.2367368089:
16: 57632: loss: 0.2360088493:
16: 60832: loss: 0.2354713436:
16: 64032: loss: 0.2361771140:
16: 67232: loss: 0.2358092482:
16: 70432: loss: 0.2355649612:
16: 73632: loss: 0.2351086042:
16: 76832: loss: 0.2347340670:
16: 80032: loss: 0.2344384299:
16: 83232: loss: 0.2346930943:
16: 86432: loss: 0.2349274567:
16: 89632: loss: 0.2349330485:
16: 92832: loss: 0.2349256479:
16: 96032: loss: 0.2347272226:
16: 99232: loss: 0.2349809170:
16: 102432: loss: 0.2348549171:
16: 105632: loss: 0.2342328523:
16: 108832: loss: 0.2340696529:
16: 112032: loss: 0.2338212397:
16: 115232: loss: 0.2338124014:
16: 118432: loss: 0.2340890427:
16: 121632: loss: 0.2339330563:
16: 124832: loss: 0.2339868171:
16: 128032: loss: 0.2340902044:
16: 131232: loss: 0.2338203990:
16: 134432: loss: 0.2337525639:
16: 137632: loss: 0.2338243414:
16: 140832: loss: 0.2339231344:
16: 144032: loss: 0.2341351411:
16: 147232: loss: 0.2341277038:
16: 150432: loss: 0.2340723245:
16: 153632: loss: 0.2340480309:
16: 156832: loss: 0.2340782188:
16: 160032: loss: 0.2345349392:
16: 163232: loss: 0.2347478699:
16: 166432: loss: 0.2347106238:
Dev-Acc: 16: Accuracy: 0.9424114823: precision: 0.6370106762: recall: 0.0304370005: f1: 0.0580980201
Train-Acc: 16: Accuracy: 0.9130653143: precision: 0.8474399164: recall: 0.0533166787: f1: 0.1003216230
17: 3232: loss: 0.2191870544:
17: 6432: loss: 0.2376218126:
17: 9632: loss: 0.2387371298:
17: 12832: loss: 0.2339951050:
17: 16032: loss: 0.2365784193:
17: 19232: loss: 0.2358725771:
17: 22432: loss: 0.2355050646:
17: 25632: loss: 0.2351662847:
17: 28832: loss: 0.2361340353:
17: 32032: loss: 0.2361682996:
17: 35232: loss: 0.2364448918:
17: 38432: loss: 0.2351892929:
17: 41632: loss: 0.2356938114:
17: 44832: loss: 0.2359654730:
17: 48032: loss: 0.2356467005:
17: 51232: loss: 0.2354437037:
17: 54432: loss: 0.2343779726:
17: 57632: loss: 0.2335314169:
17: 60832: loss: 0.2335675290:
17: 64032: loss: 0.2329628373:
17: 67232: loss: 0.2327338368:
17: 70432: loss: 0.2325779918:
17: 73632: loss: 0.2326781193:
17: 76832: loss: 0.2325234606:
17: 80032: loss: 0.2323669648:
17: 83232: loss: 0.2321992247:
17: 86432: loss: 0.2319286863:
17: 89632: loss: 0.2314389963:
17: 92832: loss: 0.2315489698:
17: 96032: loss: 0.2310140672:
17: 99232: loss: 0.2315254242:
17: 102432: loss: 0.2311775618:
17: 105632: loss: 0.2313196312:
17: 108832: loss: 0.2317486164:
17: 112032: loss: 0.2314702679:
17: 115232: loss: 0.2316773252:
17: 118432: loss: 0.2317437045:
17: 121632: loss: 0.2321175509:
17: 124832: loss: 0.2324369265:
17: 128032: loss: 0.2322898413:
17: 131232: loss: 0.2320970423:
17: 134432: loss: 0.2316399504:
17: 137632: loss: 0.2316539036:
17: 140832: loss: 0.2312789250:
17: 144032: loss: 0.2310060183:
17: 147232: loss: 0.2309584772:
17: 150432: loss: 0.2310108338:
17: 153632: loss: 0.2305982509:
17: 156832: loss: 0.2301741534:
17: 160032: loss: 0.2303659643:
17: 163232: loss: 0.2305308578:
17: 166432: loss: 0.2303457541:
Dev-Acc: 17: Accuracy: 0.9420642257: precision: 0.5483870968: recall: 0.0404693079: f1: 0.0753760887
Train-Acc: 17: Accuracy: 0.9140813351: precision: 0.8430566968: recall: 0.0674511866: f1: 0.1249086925
18: 3232: loss: 0.2258099861:
18: 6432: loss: 0.2306106512:
18: 9632: loss: 0.2281182733:
18: 12832: loss: 0.2282480170:
18: 16032: loss: 0.2276208784:
18: 19232: loss: 0.2286417049:
18: 22432: loss: 0.2285649451:
18: 25632: loss: 0.2296080617:
18: 28832: loss: 0.2296048142:
18: 32032: loss: 0.2286727941:
18: 35232: loss: 0.2285802223:
18: 38432: loss: 0.2288191209:
18: 41632: loss: 0.2281315272:
18: 44832: loss: 0.2287198056:
18: 48032: loss: 0.2278162319:
18: 51232: loss: 0.2270401977:
18: 54432: loss: 0.2273349840:
18: 57632: loss: 0.2282514289:
18: 60832: loss: 0.2272322306:
18: 64032: loss: 0.2266876218:
18: 67232: loss: 0.2267113471:
18: 70432: loss: 0.2267594176:
18: 73632: loss: 0.2261081148:
18: 76832: loss: 0.2259301474:
18: 80032: loss: 0.2256583835:
18: 83232: loss: 0.2258624599:
18: 86432: loss: 0.2253141860:
18: 89632: loss: 0.2257959667:
18: 92832: loss: 0.2265967727:
18: 96032: loss: 0.2266267577:
18: 99232: loss: 0.2269245316:
18: 102432: loss: 0.2270810654:
18: 105632: loss: 0.2270015640:
18: 108832: loss: 0.2266216349:
18: 112032: loss: 0.2267265803:
18: 115232: loss: 0.2264996351:
18: 118432: loss: 0.2264201716:
18: 121632: loss: 0.2261216084:
18: 124832: loss: 0.2261318430:
18: 128032: loss: 0.2258370553:
18: 131232: loss: 0.2259416109:
18: 134432: loss: 0.2261330378:
18: 137632: loss: 0.2257085836:
18: 140832: loss: 0.2258786693:
18: 144032: loss: 0.2257213794:
18: 147232: loss: 0.2255057702:
18: 150432: loss: 0.2255950608:
18: 153632: loss: 0.2259034080:
18: 156832: loss: 0.2261281274:
18: 160032: loss: 0.2261178533:
18: 163232: loss: 0.2262355205:
18: 166432: loss: 0.2263574427:
Dev-Acc: 18: Accuracy: 0.9430365562: precision: 0.5940860215: recall: 0.0751572862: f1: 0.1334339623
Train-Acc: 18: Accuracy: 0.9159938097: precision: 0.8419182948: recall: 0.0934849780: f1: 0.1682840237
19: 3232: loss: 0.2359708828:
19: 6432: loss: 0.2272717504:
19: 9632: loss: 0.2252728639:
19: 12832: loss: 0.2267961530:
19: 16032: loss: 0.2265969281:
19: 19232: loss: 0.2271206375:
19: 22432: loss: 0.2270360634:
19: 25632: loss: 0.2264066234:
19: 28832: loss: 0.2259351167:
19: 32032: loss: 0.2249052984:
19: 35232: loss: 0.2242357512:
19: 38432: loss: 0.2242570504:
19: 41632: loss: 0.2234991673:
19: 44832: loss: 0.2243124122:
19: 48032: loss: 0.2239515888:
19: 51232: loss: 0.2234029831:
19: 54432: loss: 0.2239269006:
19: 57632: loss: 0.2235130134:
19: 60832: loss: 0.2230250412:
19: 64032: loss: 0.2224247093:
19: 67232: loss: 0.2227020887:
19: 70432: loss: 0.2226830764:
19: 73632: loss: 0.2223436412:
19: 76832: loss: 0.2228622404:
19: 80032: loss: 0.2233733918:
19: 83232: loss: 0.2234792446:
19: 86432: loss: 0.2239748837:
19: 89632: loss: 0.2237028652:
19: 92832: loss: 0.2231415802:
19: 96032: loss: 0.2233116687:
19: 99232: loss: 0.2229106116:
19: 102432: loss: 0.2230213567:
19: 105632: loss: 0.2228178250:
19: 108832: loss: 0.2232028127:
19: 112032: loss: 0.2230438022:
19: 115232: loss: 0.2230983421:
19: 118432: loss: 0.2229379311:
19: 121632: loss: 0.2231567018:
19: 124832: loss: 0.2232471066:
19: 128032: loss: 0.2235390797:
19: 131232: loss: 0.2235751059:
19: 134432: loss: 0.2234001775:
19: 137632: loss: 0.2237484055:
19: 140832: loss: 0.2235588138:
19: 144032: loss: 0.2235525623:
19: 147232: loss: 0.2232708574:
19: 150432: loss: 0.2231619257:
19: 153632: loss: 0.2232259453:
19: 156832: loss: 0.2232671320:
19: 160032: loss: 0.2230071651:
19: 163232: loss: 0.2229916772:
19: 166432: loss: 0.2228496757:
Dev-Acc: 19: Accuracy: 0.9433243275: precision: 0.5780240074: recall: 0.1064444822: f1: 0.1797817346
Train-Acc: 19: Accuracy: 0.9180377722: precision: 0.8354997759: recall: 0.1225428966: f1: 0.2137369568
20: 3232: loss: 0.2275374868:
20: 6432: loss: 0.2294340112:
20: 9632: loss: 0.2267297416:
20: 12832: loss: 0.2260001178:
20: 16032: loss: 0.2230294378:
20: 19232: loss: 0.2194040106:
20: 22432: loss: 0.2171917862:
20: 25632: loss: 0.2187312088:
20: 28832: loss: 0.2201694972:
20: 32032: loss: 0.2197916856:
20: 35232: loss: 0.2199367821:
20: 38432: loss: 0.2196607141:
20: 41632: loss: 0.2195713293:
20: 44832: loss: 0.2199134015:
20: 48032: loss: 0.2202058289:
20: 51232: loss: 0.2197279308:
20: 54432: loss: 0.2199380207:
20: 57632: loss: 0.2202703481:
20: 60832: loss: 0.2206214844:
20: 64032: loss: 0.2200395735:
20: 67232: loss: 0.2194654944:
20: 70432: loss: 0.2195779532:
20: 73632: loss: 0.2194722871:
20: 76832: loss: 0.2198148207:
20: 80032: loss: 0.2196341636:
20: 83232: loss: 0.2194322577:
20: 86432: loss: 0.2189354634:
20: 89632: loss: 0.2188087385:
20: 92832: loss: 0.2187726338:
20: 96032: loss: 0.2188036366:
20: 99232: loss: 0.2189957719:
20: 102432: loss: 0.2190143459:
20: 105632: loss: 0.2189548026:
20: 108832: loss: 0.2192633989:
20: 112032: loss: 0.2194550916:
20: 115232: loss: 0.2193906112:
20: 118432: loss: 0.2195881616:
20: 121632: loss: 0.2196091309:
20: 124832: loss: 0.2196460001:
20: 128032: loss: 0.2195015018:
20: 131232: loss: 0.2193059820:
20: 134432: loss: 0.2191862478:
20: 137632: loss: 0.2190677654:
20: 140832: loss: 0.2191984392:
20: 144032: loss: 0.2194483926:
20: 147232: loss: 0.2194915103:
20: 150432: loss: 0.2199071323:
20: 153632: loss: 0.2195499076:
20: 156832: loss: 0.2198533285:
20: 160032: loss: 0.2194473807:
20: 163232: loss: 0.2193975151:
20: 166432: loss: 0.2197331893:
Dev-Acc: 20: Accuracy: 0.9434037209: precision: 0.5666917860: recall: 0.1278694100: f1: 0.2086570477
Train-Acc: 20: Accuracy: 0.9204881787: precision: 0.8192166053: recall: 0.1608704227: f1: 0.2689306517
21: 3232: loss: 0.2094817438:
21: 6432: loss: 0.2196429650:
21: 9632: loss: 0.2200949730:
21: 12832: loss: 0.2183120313:
21: 16032: loss: 0.2202219672:
21: 19232: loss: 0.2207080559:
21: 22432: loss: 0.2186594927:
21: 25632: loss: 0.2185578999:
21: 28832: loss: 0.2176393083:
21: 32032: loss: 0.2164372191:
21: 35232: loss: 0.2158131395:
21: 38432: loss: 0.2158096106:
21: 41632: loss: 0.2159254421:
21: 44832: loss: 0.2168335405:
21: 48032: loss: 0.2157554848:
21: 51232: loss: 0.2153377646:
21: 54432: loss: 0.2148443832:
21: 57632: loss: 0.2145959033:
21: 60832: loss: 0.2147073962:
21: 64032: loss: 0.2145803282:
21: 67232: loss: 0.2150535257:
21: 70432: loss: 0.2151523058:
21: 73632: loss: 0.2164547887:
21: 76832: loss: 0.2169076346:
21: 80032: loss: 0.2164277668:
21: 83232: loss: 0.2163283563:
21: 86432: loss: 0.2161451616:
21: 89632: loss: 0.2168337059:
21: 92832: loss: 0.2162861568:
21: 96032: loss: 0.2157999379:
21: 99232: loss: 0.2160838330:
21: 102432: loss: 0.2158484124:
21: 105632: loss: 0.2158890491:
21: 108832: loss: 0.2162459165:
21: 112032: loss: 0.2160942392:
21: 115232: loss: 0.2164028047:
21: 118432: loss: 0.2159245396:
21: 121632: loss: 0.2159192350:
21: 124832: loss: 0.2161965852:
21: 128032: loss: 0.2160559633:
21: 131232: loss: 0.2161475800:
21: 134432: loss: 0.2163691795:
21: 137632: loss: 0.2164619378:
21: 140832: loss: 0.2161205677:
21: 144032: loss: 0.2164024724:
21: 147232: loss: 0.2164236618:
21: 150432: loss: 0.2164160580:
21: 153632: loss: 0.2166745350:
21: 156832: loss: 0.2167934120:
21: 160032: loss: 0.2167001512:
21: 163232: loss: 0.2166729518:
21: 166432: loss: 0.2167095641:
Dev-Acc: 21: Accuracy: 0.9438303709: precision: 0.5727513228: recall: 0.1472538684: f1: 0.2342756662
Train-Acc: 21: Accuracy: 0.9216476083: precision: 0.7902735562: recall: 0.1880218263: f1: 0.3037705789
22: 3232: loss: 0.2153499872:
22: 6432: loss: 0.2203971094:
22: 9632: loss: 0.2180102751:
22: 12832: loss: 0.2160810396:
22: 16032: loss: 0.2180730655:
22: 19232: loss: 0.2166865695:
22: 22432: loss: 0.2181428414:
22: 25632: loss: 0.2170171352:
22: 28832: loss: 0.2170149006:
22: 32032: loss: 0.2168232663:
22: 35232: loss: 0.2177204287:
22: 38432: loss: 0.2174279680:
22: 41632: loss: 0.2167902645:
22: 44832: loss: 0.2176861532:
22: 48032: loss: 0.2178746187:
22: 51232: loss: 0.2177758037:
22: 54432: loss: 0.2171524376:
22: 57632: loss: 0.2166481110:
22: 60832: loss: 0.2162863489:
22: 64032: loss: 0.2165452578:
22: 67232: loss: 0.2155035993:
22: 70432: loss: 0.2167878368:
22: 73632: loss: 0.2160227268:
22: 76832: loss: 0.2162985480:
22: 80032: loss: 0.2159602141:
22: 83232: loss: 0.2155631896:
22: 86432: loss: 0.2150159261:
22: 89632: loss: 0.2150952580:
22: 92832: loss: 0.2145104807:
22: 96032: loss: 0.2142608504:
22: 99232: loss: 0.2141600922:
22: 102432: loss: 0.2136888534:
22: 105632: loss: 0.2142779182:
22: 108832: loss: 0.2141757523:
22: 112032: loss: 0.2141372564:
22: 115232: loss: 0.2139234986:
22: 118432: loss: 0.2140341429:
22: 121632: loss: 0.2140374115:
22: 124832: loss: 0.2136211554:
22: 128032: loss: 0.2140038896:
22: 131232: loss: 0.2141293141:
22: 134432: loss: 0.2143145949:
22: 137632: loss: 0.2139957652:
22: 140832: loss: 0.2138794118:
22: 144032: loss: 0.2138397336:
22: 147232: loss: 0.2140448892:
22: 150432: loss: 0.2142224440:
22: 153632: loss: 0.2140204977:
22: 156832: loss: 0.2139266638:
22: 160032: loss: 0.2138219508:
22: 163232: loss: 0.2138255720:
22: 166432: loss: 0.2136836084:
Dev-Acc: 22: Accuracy: 0.9438105226: precision: 0.5613048369: recall: 0.1696990308: f1: 0.2606084345
Train-Acc: 22: Accuracy: 0.9220719337: precision: 0.7586946165: recall: 0.2093879429: f1: 0.3281982585
23: 3232: loss: 0.2217310765:
23: 6432: loss: 0.2145039892:
23: 9632: loss: 0.2104495656:
23: 12832: loss: 0.2105355815:
23: 16032: loss: 0.2090703619:
23: 19232: loss: 0.2082958807:
23: 22432: loss: 0.2103302246:
23: 25632: loss: 0.2104434208:
23: 28832: loss: 0.2114429034:
23: 32032: loss: 0.2119295902:
23: 35232: loss: 0.2100912682:
23: 38432: loss: 0.2100988928:
23: 41632: loss: 0.2100257443:
23: 44832: loss: 0.2094908289:
23: 48032: loss: 0.2089199305:
23: 51232: loss: 0.2098767819:
23: 54432: loss: 0.2099906045:
23: 57632: loss: 0.2100525597:
23: 60832: loss: 0.2099772700:
23: 64032: loss: 0.2095681670:
23: 67232: loss: 0.2092673765:
23: 70432: loss: 0.2091242043:
23: 73632: loss: 0.2096828301:
23: 76832: loss: 0.2097794232:
23: 80032: loss: 0.2096161575:
23: 83232: loss: 0.2104363761:
23: 86432: loss: 0.2108615679:
23: 89632: loss: 0.2108313596:
23: 92832: loss: 0.2104925653:
23: 96032: loss: 0.2103867917:
23: 99232: loss: 0.2106074071:
23: 102432: loss: 0.2108562038:
23: 105632: loss: 0.2106032956:
23: 108832: loss: 0.2110184364:
23: 112032: loss: 0.2108408124:
23: 115232: loss: 0.2105889562:
23: 118432: loss: 0.2105741581:
23: 121632: loss: 0.2107204253:
23: 124832: loss: 0.2105963709:
23: 128032: loss: 0.2108571989:
23: 131232: loss: 0.2110836529:
23: 134432: loss: 0.2109124666:
23: 137632: loss: 0.2106984559:
23: 140832: loss: 0.2107692778:
23: 144032: loss: 0.2109660292:
23: 147232: loss: 0.2107666365:
23: 150432: loss: 0.2107312611:
23: 153632: loss: 0.2109143858:
23: 156832: loss: 0.2108208246:
23: 160032: loss: 0.2109366632:
23: 163232: loss: 0.2110429622:
23: 166432: loss: 0.2112279523:
Dev-Acc: 23: Accuracy: 0.9428579807: precision: 0.5253955037: recall: 0.2145893556: f1: 0.3047205119
Train-Acc: 23: Accuracy: 0.9233688712: precision: 0.7548538511: recall: 0.2325948327: f1: 0.3556136295
24: 3232: loss: 0.2151787019:
24: 6432: loss: 0.2135976977:
24: 9632: loss: 0.2179185008:
24: 12832: loss: 0.2213236566:
24: 16032: loss: 0.2187078873:
24: 19232: loss: 0.2154376789:
24: 22432: loss: 0.2149735285:
24: 25632: loss: 0.2151766623:
24: 28832: loss: 0.2133815207:
24: 32032: loss: 0.2142858208:
24: 35232: loss: 0.2146757006:
24: 38432: loss: 0.2143880055:
24: 41632: loss: 0.2134944924:
24: 44832: loss: 0.2135248885:
24: 48032: loss: 0.2133817991:
24: 51232: loss: 0.2135407174:
24: 54432: loss: 0.2133128497:
24: 57632: loss: 0.2120214132:
24: 60832: loss: 0.2114630861:
24: 64032: loss: 0.2120344642:
24: 67232: loss: 0.2121612116:
24: 70432: loss: 0.2115828234:
24: 73632: loss: 0.2120208796:
24: 76832: loss: 0.2110324413:
24: 80032: loss: 0.2110296806:
24: 83232: loss: 0.2108470564:
24: 86432: loss: 0.2107162215:
24: 89632: loss: 0.2110101932:
24: 92832: loss: 0.2107362281:
24: 96032: loss: 0.2110476987:
24: 99232: loss: 0.2112689637:
24: 102432: loss: 0.2110532700:
24: 105632: loss: 0.2110158264:
24: 108832: loss: 0.2105991496:
24: 112032: loss: 0.2104557033:
24: 115232: loss: 0.2100896090:
24: 118432: loss: 0.2102444507:
24: 121632: loss: 0.2100951192:
24: 124832: loss: 0.2098954018:
24: 128032: loss: 0.2096482798:
24: 131232: loss: 0.2094268714:
24: 134432: loss: 0.2092253477:
24: 137632: loss: 0.2094019905:
24: 140832: loss: 0.2092805124:
24: 144032: loss: 0.2093810667:
24: 147232: loss: 0.2092408084:
24: 150432: loss: 0.2093116133:
24: 153632: loss: 0.2090930120:
24: 156832: loss: 0.2091557117:
24: 160032: loss: 0.2089675625:
24: 163232: loss: 0.2088744920:
24: 166432: loss: 0.2087449294:
Dev-Acc: 24: Accuracy: 0.9415680766: precision: 0.4986486486: recall: 0.2509777249: f1: 0.3338988802
Train-Acc: 24: Accuracy: 0.9248629808: precision: 0.7584720862: recall: 0.2545526264: f1: 0.3811773971
25: 3232: loss: 0.1897531580:
25: 6432: loss: 0.2033635052:
25: 9632: loss: 0.2061123737:
25: 12832: loss: 0.2040123827:
25: 16032: loss: 0.2023375278:
25: 19232: loss: 0.2055222079:
25: 22432: loss: 0.2049081623:
25: 25632: loss: 0.2041540743:
25: 28832: loss: 0.2049876028:
25: 32032: loss: 0.2058366968:
25: 35232: loss: 0.2047123107:
25: 38432: loss: 0.2047072433:
25: 41632: loss: 0.2043028190:
25: 44832: loss: 0.2049815879:
25: 48032: loss: 0.2045036967:
25: 51232: loss: 0.2049205588:
25: 54432: loss: 0.2047086900:
25: 57632: loss: 0.2049312907:
25: 60832: loss: 0.2053346743:
25: 64032: loss: 0.2052041158:
25: 67232: loss: 0.2049064909:
25: 70432: loss: 0.2050654702:
25: 73632: loss: 0.2051487453:
25: 76832: loss: 0.2055743064:
25: 80032: loss: 0.2055206627:
25: 83232: loss: 0.2056814892:
25: 86432: loss: 0.2057628151:
25: 89632: loss: 0.2053397148:
25: 92832: loss: 0.2055958299:
25: 96032: loss: 0.2060640506:
25: 99232: loss: 0.2064645182:
25: 102432: loss: 0.2061581614:
25: 105632: loss: 0.2058578137:
25: 108832: loss: 0.2057344092:
25: 112032: loss: 0.2059118888:
25: 115232: loss: 0.2062020359:
25: 118432: loss: 0.2062271612:
25: 121632: loss: 0.2064583743:
25: 124832: loss: 0.2065199672:
25: 128032: loss: 0.2065768666:
25: 131232: loss: 0.2066546164:
25: 134432: loss: 0.2066672869:
25: 137632: loss: 0.2069421225:
25: 140832: loss: 0.2066930244:
25: 144032: loss: 0.2067848520:
25: 147232: loss: 0.2067384379:
25: 150432: loss: 0.2068130032:
25: 153632: loss: 0.2066991163:
25: 156832: loss: 0.2066531765:
25: 160032: loss: 0.2066535796:
25: 163232: loss: 0.2068248812:
25: 166432: loss: 0.2064494341:
Dev-Acc: 25: Accuracy: 0.9413596988: precision: 0.4956153614: recall: 0.2786940996: f1: 0.3567696996
Train-Acc: 25: Accuracy: 0.9260881543: precision: 0.7615894040: recall: 0.2721714549: f1: 0.4010267836
26: 3232: loss: 0.2035958109:
26: 6432: loss: 0.1944961282:
26: 9632: loss: 0.1971359820:
26: 12832: loss: 0.1971356729:
26: 16032: loss: 0.1988489659:
26: 19232: loss: 0.1993122492:
26: 22432: loss: 0.2014670253:
26: 25632: loss: 0.2017305145:
26: 28832: loss: 0.2031053779:
26: 32032: loss: 0.2033481294:
26: 35232: loss: 0.2038268686:
26: 38432: loss: 0.2032346019:
26: 41632: loss: 0.2022810052:
26: 44832: loss: 0.2027132064:
26: 48032: loss: 0.2029499435:
26: 51232: loss: 0.2029101232:
26: 54432: loss: 0.2028984133:
26: 57632: loss: 0.2031241044:
26: 60832: loss: 0.2032108396:
26: 64032: loss: 0.2034572351:
26: 67232: loss: 0.2039958941:
26: 70432: loss: 0.2035648658:
26: 73632: loss: 0.2033683367:
26: 76832: loss: 0.2035213382:
26: 80032: loss: 0.2034010918:
26: 83232: loss: 0.2031670862:
26: 86432: loss: 0.2031259492:
26: 89632: loss: 0.2035325268:
26: 92832: loss: 0.2034599007:
26: 96032: loss: 0.2034688793:
26: 99232: loss: 0.2034771417:
26: 102432: loss: 0.2034260262:
26: 105632: loss: 0.2038974647:
26: 108832: loss: 0.2038467020:
26: 112032: loss: 0.2041162198:
26: 115232: loss: 0.2039491034:
26: 118432: loss: 0.2035399812:
26: 121632: loss: 0.2038446111:
26: 124832: loss: 0.2037621818:
26: 128032: loss: 0.2039817926:
26: 131232: loss: 0.2040875467:
26: 134432: loss: 0.2044108793:
26: 137632: loss: 0.2046154009:
26: 140832: loss: 0.2044207928:
26: 144032: loss: 0.2043011647:
26: 147232: loss: 0.2041715826:
26: 150432: loss: 0.2043101503:
26: 153632: loss: 0.2042707655:
26: 156832: loss: 0.2044960375:
26: 160032: loss: 0.2042568729:
26: 163232: loss: 0.2044225376:
26: 166432: loss: 0.2042644796:
Dev-Acc: 26: Accuracy: 0.9408338666: precision: 0.4883720930: recall: 0.2928073457: f1: 0.3661103434
Train-Acc: 26: Accuracy: 0.9269069433: precision: 0.7608048994: recall: 0.2858457695: f1: 0.4155595909
27: 3232: loss: 0.1979758213:
27: 6432: loss: 0.2016723461:
27: 9632: loss: 0.2077624361:
27: 12832: loss: 0.2046947982:
27: 16032: loss: 0.2060592776:
27: 19232: loss: 0.2056113843:
27: 22432: loss: 0.2050098783:
27: 25632: loss: 0.2053740001:
27: 28832: loss: 0.2056300714:
27: 32032: loss: 0.2053464792:
27: 35232: loss: 0.2053456834:
27: 38432: loss: 0.2053153181:
27: 41632: loss: 0.2052287452:
27: 44832: loss: 0.2059664371:
27: 48032: loss: 0.2051447250:
27: 51232: loss: 0.2040227002:
27: 54432: loss: 0.2040895453:
27: 57632: loss: 0.2030108012:
27: 60832: loss: 0.2030111621:
27: 64032: loss: 0.2031137425:
27: 67232: loss: 0.2030373330:
27: 70432: loss: 0.2030930625:
27: 73632: loss: 0.2022945870:
27: 76832: loss: 0.2028596394:
27: 80032: loss: 0.2030655115:
27: 83232: loss: 0.2030531628:
27: 86432: loss: 0.2033120180:
27: 89632: loss: 0.2031511633:
27: 92832: loss: 0.2026586573:
27: 96032: loss: 0.2024229990:
27: 99232: loss: 0.2024260433:
27: 102432: loss: 0.2022505402:
27: 105632: loss: 0.2018892213:
27: 108832: loss: 0.2021127807:
27: 112032: loss: 0.2019873165:
27: 115232: loss: 0.2017483884:
27: 118432: loss: 0.2017635602:
27: 121632: loss: 0.2017296709:
27: 124832: loss: 0.2016831390:
27: 128032: loss: 0.2017689586:
27: 131232: loss: 0.2015611103:
27: 134432: loss: 0.2016217358:
27: 137632: loss: 0.2019770454:
27: 140832: loss: 0.2021627360:
27: 144032: loss: 0.2024758217:
27: 147232: loss: 0.2024999115:
27: 150432: loss: 0.2024068387:
27: 153632: loss: 0.2021673291:
27: 156832: loss: 0.2022542087:
27: 160032: loss: 0.2023757387:
27: 163232: loss: 0.2025662242:
27: 166432: loss: 0.2026014907:
Dev-Acc: 27: Accuracy: 0.9402583838: precision: 0.4809679173: recall: 0.3007991838: f1: 0.3701223977
Train-Acc: 27: Accuracy: 0.9274747372: precision: 0.7591843613: recall: 0.2961672474: f1: 0.4261054623
28: 3232: loss: 0.2086992707:
28: 6432: loss: 0.2030380228:
28: 9632: loss: 0.2010441359:
28: 12832: loss: 0.1978125819:
28: 16032: loss: 0.1989240065:
28: 19232: loss: 0.1978969799:
28: 22432: loss: 0.2005251242:
28: 25632: loss: 0.2015751426:
28: 28832: loss: 0.2009141207:
28: 32032: loss: 0.2007795774:
28: 35232: loss: 0.2008059365:
28: 38432: loss: 0.2011369250:
28: 41632: loss: 0.2016764312:
28: 44832: loss: 0.2026831238:
28: 48032: loss: 0.2027844676:
28: 51232: loss: 0.2026723030:
28: 54432: loss: 0.2017704141:
28: 57632: loss: 0.2022614013:
28: 60832: loss: 0.2021045396:
28: 64032: loss: 0.2019585730:
28: 67232: loss: 0.2017406616:
28: 70432: loss: 0.2021637823:
28: 73632: loss: 0.2020710045:
28: 76832: loss: 0.2019241780:
28: 80032: loss: 0.2024385818:
28: 83232: loss: 0.2025699643:
28: 86432: loss: 0.2021393863:
28: 89632: loss: 0.2025942986:
28: 92832: loss: 0.2024991811:
28: 96032: loss: 0.2021572233:
28: 99232: loss: 0.2023082157:
28: 102432: loss: 0.2024140690:
28: 105632: loss: 0.2023807891:
28: 108832: loss: 0.2019996342:
28: 112032: loss: 0.2017787457:
28: 115232: loss: 0.2016739593:
28: 118432: loss: 0.2017417246:
28: 121632: loss: 0.2018174242:
28: 124832: loss: 0.2020648017:
28: 128032: loss: 0.2021974330:
28: 131232: loss: 0.2018946655:
28: 134432: loss: 0.2016354403:
28: 137632: loss: 0.2016286228:
28: 140832: loss: 0.2014214850:
28: 144032: loss: 0.2012705002:
28: 147232: loss: 0.2012914948:
28: 150432: loss: 0.2014487536:
28: 153632: loss: 0.2009444700:
28: 156832: loss: 0.2011953354:
28: 160032: loss: 0.2010216062:
28: 163232: loss: 0.2007928981:
28: 166432: loss: 0.2004937470:
Dev-Acc: 28: Accuracy: 0.9400301576: precision: 0.4785469860: recall: 0.3091311002: f1: 0.3756198347
Train-Acc: 28: Accuracy: 0.9279468656: precision: 0.7558799676: recall: 0.3063572415: f1: 0.4360029940
29: 3232: loss: 0.1782621109:
29: 6432: loss: 0.1902440626:
29: 9632: loss: 0.1909780293:
29: 12832: loss: 0.1917045829:
29: 16032: loss: 0.1935914270:
29: 19232: loss: 0.1922807385:
29: 22432: loss: 0.1912782762:
29: 25632: loss: 0.1915406381:
29: 28832: loss: 0.1940099000:
29: 32032: loss: 0.1940611044:
29: 35232: loss: 0.1954800796:
29: 38432: loss: 0.1957082002:
29: 41632: loss: 0.1962605722:
29: 44832: loss: 0.1957844763:
29: 48032: loss: 0.1957626547:
29: 51232: loss: 0.1965331537:
29: 54432: loss: 0.1965240242:
29: 57632: loss: 0.1976345881:
29: 60832: loss: 0.1976627650:
29: 64032: loss: 0.1979170517:
29: 67232: loss: 0.1979850179:
29: 70432: loss: 0.1977577178:
29: 73632: loss: 0.1978220967:
29: 76832: loss: 0.1979512823:
29: 80032: loss: 0.1981891341:
29: 83232: loss: 0.1978597542:
29: 86432: loss: 0.1978634826:
29: 89632: loss: 0.1982346786:
29: 92832: loss: 0.1981944278:
29: 96032: loss: 0.1983956578:
29: 99232: loss: 0.1985298547:
29: 102432: loss: 0.1987153085:
29: 105632: loss: 0.1985314565:
29: 108832: loss: 0.1987869266:
29: 112032: loss: 0.1983404152:
29: 115232: loss: 0.1982356222:
29: 118432: loss: 0.1980867526:
29: 121632: loss: 0.1981245926:
29: 124832: loss: 0.1982250963:
29: 128032: loss: 0.1981591722:
29: 131232: loss: 0.1983735923:
29: 134432: loss: 0.1983841122:
29: 137632: loss: 0.1981703884:
29: 140832: loss: 0.1980309807:
29: 144032: loss: 0.1982740676:
29: 147232: loss: 0.1981432296:
29: 150432: loss: 0.1981605539:
29: 153632: loss: 0.1982344872:
29: 156832: loss: 0.1982220688:
29: 160032: loss: 0.1983261550:
29: 163232: loss: 0.1983591060:
29: 166432: loss: 0.1983297082:
Dev-Acc: 29: Accuracy: 0.9397423863: precision: 0.4753593429: recall: 0.3149124299: f1: 0.3788483175
Train-Acc: 29: Accuracy: 0.9283174276: precision: 0.7524721394: recall: 0.3151666557: f1: 0.4442591048
30: 3232: loss: 0.2021463171:
30: 6432: loss: 0.1922608675:
30: 9632: loss: 0.1961902786:
30: 12832: loss: 0.1926981171:
30: 16032: loss: 0.1939649870:
30: 19232: loss: 0.1945685731:
30: 22432: loss: 0.1938241303:
30: 25632: loss: 0.1935718755:
30: 28832: loss: 0.1943257654:
30: 32032: loss: 0.1942869188:
30: 35232: loss: 0.1950967372:
30: 38432: loss: 0.1945593978:
30: 41632: loss: 0.1955281510:
30: 44832: loss: 0.1957318888:
30: 48032: loss: 0.1959189790:
30: 51232: loss: 0.1962656624:
30: 54432: loss: 0.1963909144:
30: 57632: loss: 0.1961050836:
30: 60832: loss: 0.1957212428:
30: 64032: loss: 0.1960439318:
30: 67232: loss: 0.1955577624:
30: 70432: loss: 0.1955991550:
30: 73632: loss: 0.1953535632:
30: 76832: loss: 0.1950422865:
30: 80032: loss: 0.1954357756:
30: 83232: loss: 0.1949558530:
30: 86432: loss: 0.1952422529:
30: 89632: loss: 0.1950365678:
30: 92832: loss: 0.1950296624:
30: 96032: loss: 0.1953016749:
30: 99232: loss: 0.1955339563:
30: 102432: loss: 0.1959042488:
30: 105632: loss: 0.1957654579:
30: 108832: loss: 0.1956384760:
30: 112032: loss: 0.1960077050:
30: 115232: loss: 0.1962517696:
30: 118432: loss: 0.1967383516:
30: 121632: loss: 0.1970259293:
30: 124832: loss: 0.1966800750:
30: 128032: loss: 0.1963084821:
30: 131232: loss: 0.1969172216:
30: 134432: loss: 0.1970722731:
30: 137632: loss: 0.1969840406:
30: 140832: loss: 0.1968043339:
30: 144032: loss: 0.1969000533:
30: 147232: loss: 0.1971726379:
30: 150432: loss: 0.1970533197:
30: 153632: loss: 0.1968747128:
30: 156832: loss: 0.1964379882:
30: 160032: loss: 0.1963338678:
30: 163232: loss: 0.1966010453:
30: 166432: loss: 0.1969017070:
Dev-Acc: 30: Accuracy: 0.9394149780: precision: 0.4719101124: recall: 0.3213739160: f1: 0.3823588914
Train-Acc: 30: Accuracy: 0.9289270639: precision: 0.7517827340: recall: 0.3257511012: f1: 0.4545454545
31: 3232: loss: 0.2110090123:
31: 6432: loss: 0.2151779409:
31: 9632: loss: 0.2105393842:
31: 12832: loss: 0.2040881314:
31: 16032: loss: 0.2044823546:
31: 19232: loss: 0.2004488562:
31: 22432: loss: 0.1997329650:
31: 25632: loss: 0.1988444220:
31: 28832: loss: 0.1983824581:
31: 32032: loss: 0.1975310080:
31: 35232: loss: 0.1977996774:
31: 38432: loss: 0.1971202360:
31: 41632: loss: 0.1974284115:
31: 44832: loss: 0.1983060948:
31: 48032: loss: 0.1971155431:
31: 51232: loss: 0.1971154742:
31: 54432: loss: 0.1968482465:
31: 57632: loss: 0.1965042650:
31: 60832: loss: 0.1965381058:
31: 64032: loss: 0.1965841392:
31: 67232: loss: 0.1968028898:
31: 70432: loss: 0.1971051646:
31: 73632: loss: 0.1970687586:
31: 76832: loss: 0.1966628606:
31: 80032: loss: 0.1965115156:
31: 83232: loss: 0.1965726282:
31: 86432: loss: 0.1966424866:
31: 89632: loss: 0.1964804444:
31: 92832: loss: 0.1962279389:
31: 96032: loss: 0.1958978846:
31: 99232: loss: 0.1954305634:
31: 102432: loss: 0.1953413541:
31: 105632: loss: 0.1949359427:
31: 108832: loss: 0.1947658174:
31: 112032: loss: 0.1949540196:
31: 115232: loss: 0.1949682992:
31: 118432: loss: 0.1949692761:
31: 121632: loss: 0.1948935383:
31: 124832: loss: 0.1948560988:
31: 128032: loss: 0.1950241353:
31: 131232: loss: 0.1952721118:
31: 134432: loss: 0.1954099638:
31: 137632: loss: 0.1950477818:
31: 140832: loss: 0.1950163640:
31: 144032: loss: 0.1949684423:
31: 147232: loss: 0.1951517258:
31: 150432: loss: 0.1951410841:
31: 153632: loss: 0.1952930253:
31: 156832: loss: 0.1950296691:
31: 160032: loss: 0.1950028129:
31: 163232: loss: 0.1951075187:
31: 166432: loss: 0.1952452799:
Dev-Acc: 31: Accuracy: 0.9392561913: precision: 0.4704439539: recall: 0.3261350111: f1: 0.3852179152
Train-Acc: 31: Accuracy: 0.9294529557: precision: 0.7504778709: recall: 0.3355466439: f1: 0.4637470471
32: 3232: loss: 0.2064570319:
32: 6432: loss: 0.1978384327:
32: 9632: loss: 0.2027864466:
32: 12832: loss: 0.2019404173:
32: 16032: loss: 0.2002732413:
32: 19232: loss: 0.1989788628:
32: 22432: loss: 0.1974039620:
32: 25632: loss: 0.1975915423:
32: 28832: loss: 0.1974805054:
32: 32032: loss: 0.1981177926:
32: 35232: loss: 0.1982800477:
32: 38432: loss: 0.1981936446:
32: 41632: loss: 0.1987899864:
32: 44832: loss: 0.1988015082:
32: 48032: loss: 0.1979874822:
32: 51232: loss: 0.1981511324:
32: 54432: loss: 0.1983888069:
32: 57632: loss: 0.1976325309:
32: 60832: loss: 0.1976644345:
32: 64032: loss: 0.1971944104:
32: 67232: loss: 0.1971368719:
32: 70432: loss: 0.1968662493:
32: 73632: loss: 0.1956541179:
32: 76832: loss: 0.1951943527:
32: 80032: loss: 0.1957993138:
32: 83232: loss: 0.1952387559:
32: 86432: loss: 0.1954239285:
32: 89632: loss: 0.1954129691:
32: 92832: loss: 0.1956582271:
32: 96032: loss: 0.1948965787:
32: 99232: loss: 0.1948523145:
32: 102432: loss: 0.1950264641:
32: 105632: loss: 0.1947439731:
32: 108832: loss: 0.1946308378:
32: 112032: loss: 0.1945042071:
32: 115232: loss: 0.1946415132:
32: 118432: loss: 0.1945452012:
32: 121632: loss: 0.1944824366:
32: 124832: loss: 0.1946762433:
32: 128032: loss: 0.1949701634:
32: 131232: loss: 0.1952048287:
32: 134432: loss: 0.1946328360:
32: 137632: loss: 0.1945763424:
32: 140832: loss: 0.1943702113:
32: 144032: loss: 0.1945607895:
32: 147232: loss: 0.1944731892:
32: 150432: loss: 0.1942927785:
32: 153632: loss: 0.1941566404:
32: 156832: loss: 0.1941280872:
32: 160032: loss: 0.1942390380:
32: 163232: loss: 0.1941483365:
32: 166432: loss: 0.1938809337:
Dev-Acc: 32: Accuracy: 0.9389783740: precision: 0.4675825500: recall: 0.3298758715: f1: 0.3868394816
Train-Acc: 32: Accuracy: 0.9297996163: precision: 0.7484583393: recall: 0.3431069621: f1: 0.4705192932
33: 3232: loss: 0.1892595658:
33: 6432: loss: 0.1906645913:
33: 9632: loss: 0.1926935890:
33: 12832: loss: 0.1903791708:
33: 16032: loss: 0.1904879648:
33: 19232: loss: 0.1902455411:
33: 22432: loss: 0.1918469385:
33: 25632: loss: 0.1918788570:
33: 28832: loss: 0.1909668336:
33: 32032: loss: 0.1920878541:
33: 35232: loss: 0.1918907855:
33: 38432: loss: 0.1918416213:
33: 41632: loss: 0.1918496116:
33: 44832: loss: 0.1916609173:
33: 48032: loss: 0.1914146612:
33: 51232: loss: 0.1924234098:
33: 54432: loss: 0.1926350999:
33: 57632: loss: 0.1922786166:
33: 60832: loss: 0.1914984863:
33: 64032: loss: 0.1913617461:
33: 67232: loss: 0.1918164812:
33: 70432: loss: 0.1915230770:
33: 73632: loss: 0.1911492722:
33: 76832: loss: 0.1905184904:
33: 80032: loss: 0.1903320660:
33: 83232: loss: 0.1916680621:
33: 86432: loss: 0.1918305938:
33: 89632: loss: 0.1915035215:
33: 92832: loss: 0.1908795946:
33: 96032: loss: 0.1909130794:
33: 99232: loss: 0.1907810356:
33: 102432: loss: 0.1910504822:
33: 105632: loss: 0.1914501565:
33: 108832: loss: 0.1916058296:
33: 112032: loss: 0.1920143830:
33: 115232: loss: 0.1922695222:
33: 118432: loss: 0.1916492274:
33: 121632: loss: 0.1920945409:
33: 124832: loss: 0.1918956349:
33: 128032: loss: 0.1920509268:
33: 131232: loss: 0.1922331873:
33: 134432: loss: 0.1923981172:
33: 137632: loss: 0.1925008173:
33: 140832: loss: 0.1927462413:
33: 144032: loss: 0.1927277922:
33: 147232: loss: 0.1924918854:
33: 150432: loss: 0.1924566609:
33: 153632: loss: 0.1925298488:
33: 156832: loss: 0.1926136756:
33: 160032: loss: 0.1927805880:
33: 163232: loss: 0.1925874806:
33: 166432: loss: 0.1926381609:
Dev-Acc: 33: Accuracy: 0.9386807084: precision: 0.4649636747: recall: 0.3373575922: f1: 0.3910130075
Train-Acc: 33: Accuracy: 0.9300386906: precision: 0.7460339744: recall: 0.3493524423: f1: 0.4758663920
34: 3232: loss: 0.1949484739:
34: 6432: loss: 0.1944626640:
34: 9632: loss: 0.1940112717:
34: 12832: loss: 0.1891501292:
34: 16032: loss: 0.1886415091:
34: 19232: loss: 0.1879789580:
34: 22432: loss: 0.1878865310:
34: 25632: loss: 0.1883096626:
34: 28832: loss: 0.1873170013:
34: 32032: loss: 0.1866669761:
34: 35232: loss: 0.1883681740:
34: 38432: loss: 0.1885852231:
34: 41632: loss: 0.1866825103:
34: 44832: loss: 0.1871165448:
34: 48032: loss: 0.1871360748:
34: 51232: loss: 0.1877444289:
34: 54432: loss: 0.1875659079:
34: 57632: loss: 0.1876273785:
34: 60832: loss: 0.1883482895:
34: 64032: loss: 0.1885990628:
34: 67232: loss: 0.1895487695:
34: 70432: loss: 0.1897241210:
34: 73632: loss: 0.1898260131:
34: 76832: loss: 0.1900049843:
34: 80032: loss: 0.1907517474:
34: 83232: loss: 0.1905612696:
34: 86432: loss: 0.1906808500:
34: 89632: loss: 0.1908262623:
34: 92832: loss: 0.1910819197:
34: 96032: loss: 0.1917343680:
34: 99232: loss: 0.1915535996:
34: 102432: loss: 0.1911067219:
34: 105632: loss: 0.1909640732:
34: 108832: loss: 0.1909517497:
34: 112032: loss: 0.1907134838:
34: 115232: loss: 0.1909830609:
34: 118432: loss: 0.1911354049:
34: 121632: loss: 0.1911206961:
34: 124832: loss: 0.1911623576:
34: 128032: loss: 0.1915827059:
34: 131232: loss: 0.1914745207:
34: 134432: loss: 0.1913320453:
34: 137632: loss: 0.1910964695:
34: 140832: loss: 0.1910632652:
34: 144032: loss: 0.1910342345:
34: 147232: loss: 0.1910816068:
34: 150432: loss: 0.1911087936:
34: 153632: loss: 0.1909748596:
34: 156832: loss: 0.1912654572:
34: 160032: loss: 0.1911560712:
34: 163232: loss: 0.1912142795:
34: 166432: loss: 0.1911679231:
Dev-Acc: 34: Accuracy: 0.9383235574: precision: 0.4613253290: recall: 0.3397381398: f1: 0.3913043478
Train-Acc: 34: Accuracy: 0.9303434491: precision: 0.7449035813: recall: 0.3555321807: f1: 0.4813314939
35: 3232: loss: 0.1803730110:
35: 6432: loss: 0.1906638095:
35: 9632: loss: 0.1959052586:
35: 12832: loss: 0.1942505685:
35: 16032: loss: 0.1934827712:
35: 19232: loss: 0.1931352370:
35: 22432: loss: 0.1926094807:
35: 25632: loss: 0.1922073051:
35: 28832: loss: 0.1909463240:
35: 32032: loss: 0.1911545778:
35: 35232: loss: 0.1911684939:
35: 38432: loss: 0.1913866951:
35: 41632: loss: 0.1902739455:
35: 44832: loss: 0.1906741806:
35: 48032: loss: 0.1912058869:
35: 51232: loss: 0.1904237442:
35: 54432: loss: 0.1912834943:
35: 57632: loss: 0.1911461166:
35: 60832: loss: 0.1915452098:
35: 64032: loss: 0.1915933864:
35: 67232: loss: 0.1920491768:
35: 70432: loss: 0.1922346290:
35: 73632: loss: 0.1926290628:
35: 76832: loss: 0.1929586947:
35: 80032: loss: 0.1932793176:
35: 83232: loss: 0.1933163287:
35: 86432: loss: 0.1929796264:
35: 89632: loss: 0.1928677405:
35: 92832: loss: 0.1928454067:
35: 96032: loss: 0.1924748069:
35: 99232: loss: 0.1922984556:
35: 102432: loss: 0.1923392459:
35: 105632: loss: 0.1921921116:
35: 108832: loss: 0.1921301114:
35: 112032: loss: 0.1920640380:
35: 115232: loss: 0.1918852367:
35: 118432: loss: 0.1918414216:
35: 121632: loss: 0.1920215614:
35: 124832: loss: 0.1920928749:
35: 128032: loss: 0.1921495063:
35: 131232: loss: 0.1916780159:
35: 134432: loss: 0.1915575309:
35: 137632: loss: 0.1915048546:
35: 140832: loss: 0.1912726840:
35: 144032: loss: 0.1910274933:
35: 147232: loss: 0.1907719410:
35: 150432: loss: 0.1906257600:
35: 153632: loss: 0.1906165762:
35: 156832: loss: 0.1905423002:
35: 160032: loss: 0.1905414614:
35: 163232: loss: 0.1903936214:
35: 166432: loss: 0.1903206135:
Dev-Acc: 35: Accuracy: 0.9381945729: precision: 0.4602921041: recall: 0.3429688828: f1: 0.3930624574
Train-Acc: 35: Accuracy: 0.9307080507: precision: 0.7455532926: recall: 0.3609887581: f1: 0.4864457831
36: 3232: loss: 0.1885459380:
36: 6432: loss: 0.1852221731:
36: 9632: loss: 0.1913618462:
36: 12832: loss: 0.1903711877:
36: 16032: loss: 0.1894134295:
36: 19232: loss: 0.1899695693:
36: 22432: loss: 0.1883282237:
36: 25632: loss: 0.1892012044:
36: 28832: loss: 0.1892901160:
36: 32032: loss: 0.1888755628:
36: 35232: loss: 0.1883409757:
36: 38432: loss: 0.1889077657:
36: 41632: loss: 0.1896921894:
36: 44832: loss: 0.1883145324:
36: 48032: loss: 0.1883323656:
36: 51232: loss: 0.1880625398:
36: 54432: loss: 0.1885124269:
36: 57632: loss: 0.1880739796:
36: 60832: loss: 0.1879392662:
36: 64032: loss: 0.1874809884:
36: 67232: loss: 0.1873338918:
36: 70432: loss: 0.1875147655:
36: 73632: loss: 0.1875337750:
36: 76832: loss: 0.1876745254:
36: 80032: loss: 0.1881807400:
36: 83232: loss: 0.1885130482:
36: 86432: loss: 0.1886096782:
36: 89632: loss: 0.1879783371:
36: 92832: loss: 0.1881532192:
36: 96032: loss: 0.1884039594:
36: 99232: loss: 0.1883593743:
36: 102432: loss: 0.1884285001:
36: 105632: loss: 0.1882423678:
36: 108832: loss: 0.1882636503:
36: 112032: loss: 0.1881301997:
36: 115232: loss: 0.1884022152:
36: 118432: loss: 0.1886194671:
36: 121632: loss: 0.1887566514:
36: 124832: loss: 0.1890035053:
36: 128032: loss: 0.1887373610:
36: 131232: loss: 0.1887439546:
36: 134432: loss: 0.1886209908:
36: 137632: loss: 0.1885476849:
36: 140832: loss: 0.1883054743:
36: 144032: loss: 0.1884502223:
36: 147232: loss: 0.1883566434:
36: 150432: loss: 0.1886983231:
36: 153632: loss: 0.1886946740:
36: 156832: loss: 0.1887827907:
36: 160032: loss: 0.1885324243:
36: 163232: loss: 0.1887676372:
36: 166432: loss: 0.1886663854:
Dev-Acc: 36: Accuracy: 0.9378274083: precision: 0.4566734189: recall: 0.3450093522: f1: 0.3930647036
Train-Acc: 36: Accuracy: 0.9309112430: precision: 0.7440181794: recall: 0.3659194004: f1: 0.4905693637
37: 3232: loss: 0.1799970238:
37: 6432: loss: 0.1740345071:
37: 9632: loss: 0.1728229010:
37: 12832: loss: 0.1770110927:
37: 16032: loss: 0.1800744002:
37: 19232: loss: 0.1820389191:
37: 22432: loss: 0.1840195764:
37: 25632: loss: 0.1857304880:
37: 28832: loss: 0.1870167297:
37: 32032: loss: 0.1878723334:
37: 35232: loss: 0.1886503048:
37: 38432: loss: 0.1881231453:
37: 41632: loss: 0.1889427469:
37: 44832: loss: 0.1887301355:
37: 48032: loss: 0.1887369977:
37: 51232: loss: 0.1886413076:
37: 54432: loss: 0.1883098186:
37: 57632: loss: 0.1874185026:
37: 60832: loss: 0.1873357993:
37: 64032: loss: 0.1866323256:
37: 67232: loss: 0.1864029371:
37: 70432: loss: 0.1867696495:
37: 73632: loss: 0.1864863640:
37: 76832: loss: 0.1868885801:
37: 80032: loss: 0.1874418755:
37: 83232: loss: 0.1870399759:
37: 86432: loss: 0.1872995154:
37: 89632: loss: 0.1875055846:
37: 92832: loss: 0.1871832628:
37: 96032: loss: 0.1867235009:
37: 99232: loss: 0.1866594711:
37: 102432: loss: 0.1867261653:
37: 105632: loss: 0.1865038179:
37: 108832: loss: 0.1865426543:
37: 112032: loss: 0.1864029489:
37: 115232: loss: 0.1865100111:
37: 118432: loss: 0.1865359114:
37: 121632: loss: 0.1868428333:
37: 124832: loss: 0.1870821028:
37: 128032: loss: 0.1874925896:
37: 131232: loss: 0.1871487096:
37: 134432: loss: 0.1871437043:
37: 137632: loss: 0.1872647196:
37: 140832: loss: 0.1872346365:
37: 144032: loss: 0.1872320072:
37: 147232: loss: 0.1871853888:
37: 150432: loss: 0.1871290345:
37: 153632: loss: 0.1873203949:
37: 156832: loss: 0.1872525472:
37: 160032: loss: 0.1873035816:
37: 163232: loss: 0.1877616413:
37: 166432: loss: 0.1877047125:
Dev-Acc: 37: Accuracy: 0.9377381206: precision: 0.4562222222: recall: 0.3490902908: f1: 0.3955302957
Train-Acc: 37: Accuracy: 0.9311443567: precision: 0.7439185616: recall: 0.3699296562: f1: 0.4941383095
38: 3232: loss: 0.1984956537:
38: 6432: loss: 0.1793532567:
38: 9632: loss: 0.1881112765:
38: 12832: loss: 0.1889601940:
38: 16032: loss: 0.1867572970:
38: 19232: loss: 0.1854007073:
38: 22432: loss: 0.1865674869:
38: 25632: loss: 0.1883728905:
38: 28832: loss: 0.1876296296:
38: 32032: loss: 0.1875064931:
38: 35232: loss: 0.1868910767:
38: 38432: loss: 0.1861225930:
38: 41632: loss: 0.1854629462:
38: 44832: loss: 0.1852580300:
38: 48032: loss: 0.1857896780:
38: 51232: loss: 0.1860647322:
38: 54432: loss: 0.1867158061:
38: 57632: loss: 0.1868924136:
38: 60832: loss: 0.1871540127:
38: 64032: loss: 0.1869479559:
38: 67232: loss: 0.1865357078:
38: 70432: loss: 0.1864179005:
38: 73632: loss: 0.1868898242:
38: 76832: loss: 0.1871623473:
38: 80032: loss: 0.1869183438:
38: 83232: loss: 0.1871290243:
38: 86432: loss: 0.1870887453:
38: 89632: loss: 0.1870794758:
38: 92832: loss: 0.1873960960:
38: 96032: loss: 0.1872600974:
38: 99232: loss: 0.1868323458:
38: 102432: loss: 0.1869332458:
38: 105632: loss: 0.1870523489:
38: 108832: loss: 0.1868946941:
38: 112032: loss: 0.1869441729:
38: 115232: loss: 0.1868928702:
38: 118432: loss: 0.1872774983:
38: 121632: loss: 0.1870744260:
38: 124832: loss: 0.1871313838:
38: 128032: loss: 0.1874372657:
38: 131232: loss: 0.1874974549:
38: 134432: loss: 0.1872854874:
38: 137632: loss: 0.1875817915:
38: 140832: loss: 0.1871277179:
38: 144032: loss: 0.1870628816:
38: 147232: loss: 0.1867974315:
38: 150432: loss: 0.1867080199:
38: 153632: loss: 0.1864677914:
38: 156832: loss: 0.1863317482:
38: 160032: loss: 0.1862031426:
38: 163232: loss: 0.1862376877:
38: 166432: loss: 0.1862623665:
Dev-Acc: 38: Accuracy: 0.9376984239: precision: 0.4561480829: recall: 0.3519809556: f1: 0.3973509934
Train-Acc: 38: Accuracy: 0.9315686822: precision: 0.7453999739: recall: 0.3755177174: f1: 0.4994316691
39: 3232: loss: 0.1805714636:
39: 6432: loss: 0.1844602858:
39: 9632: loss: 0.1847162467:
39: 12832: loss: 0.1861176068:
39: 16032: loss: 0.1883066732:
39: 19232: loss: 0.1860299285:
39: 22432: loss: 0.1860046460:
39: 25632: loss: 0.1859668554:
39: 28832: loss: 0.1844087849:
39: 32032: loss: 0.1851785953:
39: 35232: loss: 0.1859438078:
39: 38432: loss: 0.1857397133:
39: 41632: loss: 0.1860622106:
39: 44832: loss: 0.1872549674:
39: 48032: loss: 0.1871984914:
39: 51232: loss: 0.1866568125:
39: 54432: loss: 0.1855664548:
39: 57632: loss: 0.1853016486:
39: 60832: loss: 0.1845556239:
39: 64032: loss: 0.1849941536:
39: 67232: loss: 0.1854044605:
39: 70432: loss: 0.1854558894:
39: 73632: loss: 0.1860149833:
39: 76832: loss: 0.1859033557:
39: 80032: loss: 0.1855543267:
39: 83232: loss: 0.1856080818:
39: 86432: loss: 0.1851207012:
39: 89632: loss: 0.1854907177:
39: 92832: loss: 0.1857907587:
39: 96032: loss: 0.1862374232:
39: 99232: loss: 0.1867457188:
39: 102432: loss: 0.1869130331:
39: 105632: loss: 0.1868004435:
39: 108832: loss: 0.1866267056:
39: 112032: loss: 0.1865183036:
39: 115232: loss: 0.1862221839:
39: 118432: loss: 0.1861716729:
39: 121632: loss: 0.1859751142:
39: 124832: loss: 0.1860489500:
39: 128032: loss: 0.1860176420:
39: 131232: loss: 0.1863807282:
39: 134432: loss: 0.1863375800:
39: 137632: loss: 0.1860687146:
39: 140832: loss: 0.1860477019:
39: 144032: loss: 0.1859998022:
39: 147232: loss: 0.1855616671:
39: 150432: loss: 0.1857027145:
39: 153632: loss: 0.1860958822:
39: 156832: loss: 0.1860507855:
39: 160032: loss: 0.1859775747:
39: 163232: loss: 0.1858345026:
39: 166432: loss: 0.1858409737:
Dev-Acc: 39: Accuracy: 0.9373114705: precision: 0.4525309581: recall: 0.3541914640: f1: 0.3973674170
Train-Acc: 39: Accuracy: 0.9317420125: precision: 0.7447687936: recall: 0.3790677799: f1: 0.5024179846
40: 3232: loss: 0.1813264859:
40: 6432: loss: 0.1838634053:
40: 9632: loss: 0.1767273964:
40: 12832: loss: 0.1817900271:
40: 16032: loss: 0.1845660358:
40: 19232: loss: 0.1852186676:
40: 22432: loss: 0.1848772885:
40: 25632: loss: 0.1866276479:
40: 28832: loss: 0.1868687112:
40: 32032: loss: 0.1870426357:
40: 35232: loss: 0.1869226759:
40: 38432: loss: 0.1856566423:
40: 41632: loss: 0.1849422019:
40: 44832: loss: 0.1853216996:
40: 48032: loss: 0.1855055583:
40: 51232: loss: 0.1862525371:
40: 54432: loss: 0.1859304607:
40: 57632: loss: 0.1858547986:
40: 60832: loss: 0.1858253955:
40: 64032: loss: 0.1855299608:
40: 67232: loss: 0.1846247847:
40: 70432: loss: 0.1846200301:
40: 73632: loss: 0.1846637085:
40: 76832: loss: 0.1846364468:
40: 80032: loss: 0.1849321195:
40: 83232: loss: 0.1849201193:
40: 86432: loss: 0.1848551499:
40: 89632: loss: 0.1846409185:
40: 92832: loss: 0.1846754058:
40: 96032: loss: 0.1843127863:
40: 99232: loss: 0.1848351982:
40: 102432: loss: 0.1851820841:
40: 105632: loss: 0.1851227991:
40: 108832: loss: 0.1850851866:
40: 112032: loss: 0.1850570222:
40: 115232: loss: 0.1849900777:
40: 118432: loss: 0.1850584558:
40: 121632: loss: 0.1849096834:
40: 124832: loss: 0.1847738816:
40: 128032: loss: 0.1848883446:
40: 131232: loss: 0.1849602511:
40: 134432: loss: 0.1848313026:
40: 137632: loss: 0.1845521951:
40: 140832: loss: 0.1844236288:
40: 144032: loss: 0.1844855518:
40: 147232: loss: 0.1846065445:
40: 150432: loss: 0.1841768718:
40: 153632: loss: 0.1841861681:
40: 156832: loss: 0.1841528518:
40: 160032: loss: 0.1844099920:
40: 163232: loss: 0.1843637343:
40: 166432: loss: 0.1845215522:
Dev-Acc: 40: Accuracy: 0.9372717738: precision: 0.4526112186: recall: 0.3581023635: f1: 0.3998481109
Train-Acc: 40: Accuracy: 0.9320706725: precision: 0.7449356606: recall: 0.3843928736: f1: 0.5071118820
41: 3232: loss: 0.1880688107:
41: 6432: loss: 0.1884067227:
41: 9632: loss: 0.1845524448:
41: 12832: loss: 0.1859989371:
41: 16032: loss: 0.1855046083:
41: 19232: loss: 0.1873632135:
41: 22432: loss: 0.1863117222:
41: 25632: loss: 0.1861841558:
41: 28832: loss: 0.1873553250:
41: 32032: loss: 0.1861966399:
41: 35232: loss: 0.1863294618:
41: 38432: loss: 0.1849284742:
41: 41632: loss: 0.1851526322:
41: 44832: loss: 0.1855201490:
41: 48032: loss: 0.1860074667:
41: 51232: loss: 0.1862238309:
41: 54432: loss: 0.1863303091:
41: 57632: loss: 0.1867074762:
41: 60832: loss: 0.1873909282:
41: 64032: loss: 0.1872737070:
41: 67232: loss: 0.1873968914:
41: 70432: loss: 0.1870080580:
41: 73632: loss: 0.1870343965:
41: 76832: loss: 0.1867384004:
41: 80032: loss: 0.1862930789:
41: 83232: loss: 0.1858744545:
41: 86432: loss: 0.1851321016:
41: 89632: loss: 0.1847781324:
41: 92832: loss: 0.1850448448:
41: 96032: loss: 0.1846490758:
41: 99232: loss: 0.1848366082:
41: 102432: loss: 0.1848056342:
41: 105632: loss: 0.1847309067:
41: 108832: loss: 0.1849229766:
41: 112032: loss: 0.1849658744:
41: 115232: loss: 0.1849341941:
41: 118432: loss: 0.1847104601:
41: 121632: loss: 0.1848920851:
41: 124832: loss: 0.1847291957:
41: 128032: loss: 0.1849187452:
41: 131232: loss: 0.1846950288:
41: 134432: loss: 0.1844692429:
41: 137632: loss: 0.1849086092:
41: 140832: loss: 0.1850469754:
41: 144032: loss: 0.1850168454:
41: 147232: loss: 0.1849884969:
41: 150432: loss: 0.1843105621:
41: 153632: loss: 0.1843140304:
41: 156832: loss: 0.1838958972:
41: 160032: loss: 0.1843963647:
41: 163232: loss: 0.1841111975:
41: 166432: loss: 0.1840970117:
Dev-Acc: 41: Accuracy: 0.9369939566: precision: 0.4501594049: recall: 0.3601428329: f1: 0.4001511430
Train-Acc: 41: Accuracy: 0.9323396683: precision: 0.7450856855: recall: 0.3887318388: f1: 0.5109085411
42: 3232: loss: 0.1741480145:
42: 6432: loss: 0.1792091322:
42: 9632: loss: 0.1845961954:
42: 12832: loss: 0.1812368543:
42: 16032: loss: 0.1804792439:
42: 19232: loss: 0.1796640485:
42: 22432: loss: 0.1787273625:
42: 25632: loss: 0.1803563958:
42: 28832: loss: 0.1804839000:
42: 32032: loss: 0.1817307739:
42: 35232: loss: 0.1824718101:
42: 38432: loss: 0.1812317867:
42: 41632: loss: 0.1808627555:
42: 44832: loss: 0.1811073103:
42: 48032: loss: 0.1822292714:
42: 51232: loss: 0.1825616437:
42: 54432: loss: 0.1828365398:
42: 57632: loss: 0.1830550667:
42: 60832: loss: 0.1832981876:
42: 64032: loss: 0.1827690585:
42: 67232: loss: 0.1826974833:
42: 70432: loss: 0.1830640055:
42: 73632: loss: 0.1831355462:
42: 76832: loss: 0.1828814902:
42: 80032: loss: 0.1827340323:
42: 83232: loss: 0.1829683009:
42: 86432: loss: 0.1825709331:
42: 89632: loss: 0.1821333752:
42: 92832: loss: 0.1823482072:
42: 96032: loss: 0.1820802535:
42: 99232: loss: 0.1818767225:
42: 102432: loss: 0.1819248176:
42: 105632: loss: 0.1822898430:
42: 108832: loss: 0.1823908825:
42: 112032: loss: 0.1821677338:
42: 115232: loss: 0.1823673765:
42: 118432: loss: 0.1824114265:
42: 121632: loss: 0.1818883536:
42: 124832: loss: 0.1823635518:
42: 128032: loss: 0.1825459958:
42: 131232: loss: 0.1828475061:
42: 134432: loss: 0.1831961408:
42: 137632: loss: 0.1828147648:
42: 140832: loss: 0.1826210276:
42: 144032: loss: 0.1826050228:
42: 147232: loss: 0.1825382009:
42: 150432: loss: 0.1828879221:
42: 153632: loss: 0.1829801065:
42: 156832: loss: 0.1829230425:
42: 160032: loss: 0.1827646733:
42: 163232: loss: 0.1828225868:
42: 166432: loss: 0.1830536431:
Dev-Acc: 42: Accuracy: 0.9365871549: precision: 0.4466527197: recall: 0.3630334977: f1: 0.4005252791
Train-Acc: 42: Accuracy: 0.9326264858: precision: 0.7453888335: recall: 0.3932022878: f1: 0.5148267700
43: 3232: loss: 0.1842459111:
43: 6432: loss: 0.1804895654:
43: 9632: loss: 0.1844327659:
43: 12832: loss: 0.1843989297:
43: 16032: loss: 0.1849525066:
43: 19232: loss: 0.1858468692:
43: 22432: loss: 0.1863940847:
43: 25632: loss: 0.1868702120:
43: 28832: loss: 0.1855782464:
43: 32032: loss: 0.1858462031:
43: 35232: loss: 0.1861692973:
43: 38432: loss: 0.1859916849:
43: 41632: loss: 0.1859752956:
43: 44832: loss: 0.1855243385:
43: 48032: loss: 0.1848048161:
43: 51232: loss: 0.1843935434:
43: 54432: loss: 0.1841128485:
43: 57632: loss: 0.1837716169:
43: 60832: loss: 0.1839157611:
43: 64032: loss: 0.1834054436:
43: 67232: loss: 0.1833535448:
43: 70432: loss: 0.1835437047:
43: 73632: loss: 0.1830649185:
43: 76832: loss: 0.1829021815:
43: 80032: loss: 0.1826076744:
43: 83232: loss: 0.1828684587:
43: 86432: loss: 0.1828832247:
43: 89632: loss: 0.1834142242:
43: 92832: loss: 0.1831864801:
43: 96032: loss: 0.1830537816:
43: 99232: loss: 0.1832733116:
43: 102432: loss: 0.1828443346:
43: 105632: loss: 0.1831250538:
43: 108832: loss: 0.1834690510:
43: 112032: loss: 0.1832859228:
43: 115232: loss: 0.1833267598:
43: 118432: loss: 0.1831152420:
43: 121632: loss: 0.1831976034:
43: 124832: loss: 0.1830251397:
43: 128032: loss: 0.1830835281:
43: 131232: loss: 0.1831321118:
43: 134432: loss: 0.1828891057:
43: 137632: loss: 0.1830177916:
43: 140832: loss: 0.1828946106:
43: 144032: loss: 0.1828293227:
43: 147232: loss: 0.1828245597:
43: 150432: loss: 0.1827750897:
43: 153632: loss: 0.1825562017:
43: 156832: loss: 0.1825291091:
43: 160032: loss: 0.1825327986:
43: 163232: loss: 0.1824281695:
43: 166432: loss: 0.1822227745:
Dev-Acc: 43: Accuracy: 0.9365177155: precision: 0.4463804190: recall: 0.3659241626: f1: 0.4021678191
Train-Acc: 43: Accuracy: 0.9329253435: precision: 0.7460513327: recall: 0.3974755111: f1: 0.5186360712
44: 3232: loss: 0.1810753133:
44: 6432: loss: 0.1918400754:
44: 9632: loss: 0.1891104998:
44: 12832: loss: 0.1903190912:
44: 16032: loss: 0.1854390428:
44: 19232: loss: 0.1839811437:
44: 22432: loss: 0.1829847093:
44: 25632: loss: 0.1833757180:
44: 28832: loss: 0.1827776800:
44: 32032: loss: 0.1830608785:
44: 35232: loss: 0.1832546667:
44: 38432: loss: 0.1813940315:
44: 41632: loss: 0.1819843739:
44: 44832: loss: 0.1823735121:
44: 48032: loss: 0.1820997827:
44: 51232: loss: 0.1824389378:
44: 54432: loss: 0.1824927685:
44: 57632: loss: 0.1842843576:
44: 60832: loss: 0.1839213830:
44: 64032: loss: 0.1836069011:
44: 67232: loss: 0.1838095395:
44: 70432: loss: 0.1839073983:
44: 73632: loss: 0.1838736999:
44: 76832: loss: 0.1833762006:
44: 80032: loss: 0.1837752002:
44: 83232: loss: 0.1837889213:
44: 86432: loss: 0.1834277594:
44: 89632: loss: 0.1834683609:
44: 92832: loss: 0.1834369804:
44: 96032: loss: 0.1836763037:
44: 99232: loss: 0.1835471681:
44: 102432: loss: 0.1832892097:
44: 105632: loss: 0.1826157755:
44: 108832: loss: 0.1823179239:
44: 112032: loss: 0.1822897658:
44: 115232: loss: 0.1817419101:
44: 118432: loss: 0.1820076382:
44: 121632: loss: 0.1819632933:
44: 124832: loss: 0.1824291157:
44: 128032: loss: 0.1825523914:
44: 131232: loss: 0.1821401199:
44: 134432: loss: 0.1820497080:
44: 137632: loss: 0.1820825808:
44: 140832: loss: 0.1816739642:
44: 144032: loss: 0.1817709496:
44: 147232: loss: 0.1817095524:
44: 150432: loss: 0.1815870815:
44: 153632: loss: 0.1814478318:
44: 156832: loss: 0.1813941333:
44: 160032: loss: 0.1813230566:
44: 163232: loss: 0.1812281055:
44: 166432: loss: 0.1814801947:
Dev-Acc: 44: Accuracy: 0.9363887310: precision: 0.4453833471: recall: 0.3674545145: f1: 0.4026833131
Train-Acc: 44: Accuracy: 0.9331524372: precision: 0.7460278660: recall: 0.4012885412: f1: 0.5218655153
45: 3232: loss: 0.1796506428:
45: 6432: loss: 0.1857520502:
45: 9632: loss: 0.1818395175:
45: 12832: loss: 0.1819758101:
45: 16032: loss: 0.1815244674:
45: 19232: loss: 0.1819236618:
45: 22432: loss: 0.1814646756:
45: 25632: loss: 0.1804509735:
45: 28832: loss: 0.1815000680:
45: 32032: loss: 0.1815958328:
45: 35232: loss: 0.1820563947:
45: 38432: loss: 0.1808821668:
45: 41632: loss: 0.1819900725:
45: 44832: loss: 0.1819542208:
45: 48032: loss: 0.1817749167:
45: 51232: loss: 0.1819709577:
45: 54432: loss: 0.1820446042:
45: 57632: loss: 0.1816234764:
45: 60832: loss: 0.1814316598:
45: 64032: loss: 0.1811629379:
45: 67232: loss: 0.1806962032:
45: 70432: loss: 0.1808665790:
45: 73632: loss: 0.1800404241:
45: 76832: loss: 0.1806194047:
45: 80032: loss: 0.1804629918:
45: 83232: loss: 0.1800322834:
45: 86432: loss: 0.1803269054:
45: 89632: loss: 0.1801324310:
45: 92832: loss: 0.1798363511:
45: 96032: loss: 0.1800853412:
45: 99232: loss: 0.1801031117:
45: 102432: loss: 0.1797924130:
45: 105632: loss: 0.1800738056:
45: 108832: loss: 0.1802956818:
45: 112032: loss: 0.1805767364:
45: 115232: loss: 0.1803100892:
45: 118432: loss: 0.1802177232:
45: 121632: loss: 0.1802095205:
45: 124832: loss: 0.1798025843:
45: 128032: loss: 0.1799601138:
45: 131232: loss: 0.1800076857:
45: 134432: loss: 0.1797180391:
45: 137632: loss: 0.1797815621:
45: 140832: loss: 0.1799482778:
45: 144032: loss: 0.1799983747:
45: 147232: loss: 0.1799613517:
45: 150432: loss: 0.1799428859:
45: 153632: loss: 0.1802474671:
45: 156832: loss: 0.1802323051:
45: 160032: loss: 0.1803545385:
45: 163232: loss: 0.1804633833:
45: 166432: loss: 0.1803064574:
Dev-Acc: 45: Accuracy: 0.9364879131: precision: 0.4468520033: recall: 0.3717054923: f1: 0.4058293883
Train-Acc: 45: Accuracy: 0.9333377481: precision: 0.7458489880: recall: 0.4045756361: f1: 0.5245929588
46: 3232: loss: 0.1867470399:
46: 6432: loss: 0.1939647881:
46: 9632: loss: 0.1962679681:
46: 12832: loss: 0.1927360991:
46: 16032: loss: 0.1896978111:
46: 19232: loss: 0.1851819426:
46: 22432: loss: 0.1857152307:
46: 25632: loss: 0.1851078130:
46: 28832: loss: 0.1841414373:
46: 32032: loss: 0.1831686484:
46: 35232: loss: 0.1829918238:
46: 38432: loss: 0.1834255519:
46: 41632: loss: 0.1839989767:
46: 44832: loss: 0.1840544500:
46: 48032: loss: 0.1834586854:
46: 51232: loss: 0.1830360509:
46: 54432: loss: 0.1837635372:
46: 57632: loss: 0.1831204773:
46: 60832: loss: 0.1829662343:
46: 64032: loss: 0.1822371898:
46: 67232: loss: 0.1817820747:
46: 70432: loss: 0.1809411418:
46: 73632: loss: 0.1805427570:
46: 76832: loss: 0.1806942221:
46: 80032: loss: 0.1803636073:
46: 83232: loss: 0.1809241081:
46: 86432: loss: 0.1807055148:
46: 89632: loss: 0.1809643650:
46: 92832: loss: 0.1809808014:
46: 96032: loss: 0.1805999001:
46: 99232: loss: 0.1801766061:
46: 102432: loss: 0.1802843153:
46: 105632: loss: 0.1802260838:
46: 108832: loss: 0.1801769229:
46: 112032: loss: 0.1803080156:
46: 115232: loss: 0.1807980290:
46: 118432: loss: 0.1809565400:
46: 121632: loss: 0.1809496190:
46: 124832: loss: 0.1809435069:
46: 128032: loss: 0.1812022478:
46: 131232: loss: 0.1814633577:
46: 134432: loss: 0.1813065340:
46: 137632: loss: 0.1810670659:
46: 140832: loss: 0.1810590522:
46: 144032: loss: 0.1809054336:
46: 147232: loss: 0.1805571607:
46: 150432: loss: 0.1804560073:
46: 153632: loss: 0.1803561473:
46: 156832: loss: 0.1804770782:
46: 160032: loss: 0.1803470891:
46: 163232: loss: 0.1800476581:
46: 166432: loss: 0.1798849686:
Dev-Acc: 46: Accuracy: 0.9361902475: precision: 0.4442190669: recall: 0.3723856487: f1: 0.4051429100
Train-Acc: 46: Accuracy: 0.9336365461: precision: 0.7469039317: recall: 0.4083886661: f1: 0.5280516831
47: 3232: loss: 0.1580554856:
47: 6432: loss: 0.1628666211:
47: 9632: loss: 0.1692897551:
47: 12832: loss: 0.1730251330:
47: 16032: loss: 0.1728436216:
47: 19232: loss: 0.1731400290:
47: 22432: loss: 0.1745446455:
47: 25632: loss: 0.1776413040:
47: 28832: loss: 0.1778278550:
47: 32032: loss: 0.1766015476:
47: 35232: loss: 0.1776707346:
47: 38432: loss: 0.1774205729:
47: 41632: loss: 0.1768123773:
47: 44832: loss: 0.1773255573:
47: 48032: loss: 0.1783481046:
47: 51232: loss: 0.1779950558:
47: 54432: loss: 0.1778384490:
47: 57632: loss: 0.1782492865:
47: 60832: loss: 0.1784693185:
47: 64032: loss: 0.1784990669:
47: 67232: loss: 0.1786046241:
47: 70432: loss: 0.1793070700:
47: 73632: loss: 0.1791994571:
47: 76832: loss: 0.1792805095:
47: 80032: loss: 0.1796043381:
47: 83232: loss: 0.1796010871:
47: 86432: loss: 0.1794219696:
47: 89632: loss: 0.1794941280:
47: 92832: loss: 0.1794837071:
47: 96032: loss: 0.1790343218:
47: 99232: loss: 0.1796000757:
47: 102432: loss: 0.1799539308:
47: 105632: loss: 0.1795040855:
47: 108832: loss: 0.1797730426:
47: 112032: loss: 0.1800257887:
47: 115232: loss: 0.1798799424:
47: 118432: loss: 0.1801180926:
47: 121632: loss: 0.1799905202:
47: 124832: loss: 0.1794600901:
47: 128032: loss: 0.1793683766:
47: 131232: loss: 0.1794686010:
47: 134432: loss: 0.1792562699:
47: 137632: loss: 0.1793281432:
47: 140832: loss: 0.1793511036:
47: 144032: loss: 0.1794650564:
47: 147232: loss: 0.1794180677:
47: 150432: loss: 0.1792718106:
47: 153632: loss: 0.1794508376:
47: 156832: loss: 0.1792626772:
47: 160032: loss: 0.1791763296:
47: 163232: loss: 0.1792949370:
47: 166432: loss: 0.1792293449:
Dev-Acc: 47: Accuracy: 0.9361605048: precision: 0.4442652691: recall: 0.3747661962: f1: 0.4065670540
Train-Acc: 47: Accuracy: 0.9338636398: precision: 0.7475809342: recall: 0.4114127934: f1: 0.5307437876
48: 3232: loss: 0.1688232759:
48: 6432: loss: 0.1690771280:
48: 9632: loss: 0.1749837816:
48: 12832: loss: 0.1729990229:
48: 16032: loss: 0.1736384020:
48: 19232: loss: 0.1746132472:
48: 22432: loss: 0.1754145222:
48: 25632: loss: 0.1759385305:
48: 28832: loss: 0.1766634938:
48: 32032: loss: 0.1770047933:
48: 35232: loss: 0.1772320959:
48: 38432: loss: 0.1767629298:
48: 41632: loss: 0.1770640675:
48: 44832: loss: 0.1774969250:
48: 48032: loss: 0.1778165617:
48: 51232: loss: 0.1771023334:
48: 54432: loss: 0.1765206634:
48: 57632: loss: 0.1776755229:
48: 60832: loss: 0.1779104212:
48: 64032: loss: 0.1784253155:
48: 67232: loss: 0.1778243728:
48: 70432: loss: 0.1779657599:
48: 73632: loss: 0.1780341037:
48: 76832: loss: 0.1786814102:
48: 80032: loss: 0.1788684534:
48: 83232: loss: 0.1784955741:
48: 86432: loss: 0.1791074216:
48: 89632: loss: 0.1791375991:
48: 92832: loss: 0.1790602441:
48: 96032: loss: 0.1789657828:
48: 99232: loss: 0.1790645847:
48: 102432: loss: 0.1788773381:
48: 105632: loss: 0.1787997025:
48: 108832: loss: 0.1786360779:
48: 112032: loss: 0.1789766716:
48: 115232: loss: 0.1789790360:
48: 118432: loss: 0.1789051740:
48: 121632: loss: 0.1786882904:
48: 124832: loss: 0.1785162025:
48: 128032: loss: 0.1787197423:
48: 131232: loss: 0.1786280090:
48: 134432: loss: 0.1784184262:
48: 137632: loss: 0.1784917526:
48: 140832: loss: 0.1784364215:
48: 144032: loss: 0.1785954678:
48: 147232: loss: 0.1787693154:
48: 150432: loss: 0.1784602167:
48: 153632: loss: 0.1784204554:
48: 156832: loss: 0.1782017995:
48: 160032: loss: 0.1781302288:
48: 163232: loss: 0.1779900193:
48: 166432: loss: 0.1782490598:
Dev-Acc: 48: Accuracy: 0.9361703992: precision: 0.4445114596: recall: 0.3759564700: f1: 0.4073698756
Train-Acc: 48: Accuracy: 0.9340489507: precision: 0.7479221088: recall: 0.4141082112: f1: 0.5330681674
49: 3232: loss: 0.1934509307:
49: 6432: loss: 0.1857606040:
49: 9632: loss: 0.1842997811:
49: 12832: loss: 0.1818688018:
49: 16032: loss: 0.1833504631:
49: 19232: loss: 0.1817567797:
49: 22432: loss: 0.1834659308:
49: 25632: loss: 0.1819841878:
49: 28832: loss: 0.1813803037:
49: 32032: loss: 0.1793258184:
49: 35232: loss: 0.1799681790:
49: 38432: loss: 0.1799649443:
49: 41632: loss: 0.1798388732:
49: 44832: loss: 0.1802531499:
49: 48032: loss: 0.1790438984:
49: 51232: loss: 0.1786615100:
49: 54432: loss: 0.1780869110:
49: 57632: loss: 0.1779665388:
49: 60832: loss: 0.1771735415:
49: 64032: loss: 0.1771612652:
49: 67232: loss: 0.1766354733:
49: 70432: loss: 0.1763525423:
49: 73632: loss: 0.1761565794:
49: 76832: loss: 0.1761711824:
49: 80032: loss: 0.1756654032:
49: 83232: loss: 0.1757398731:
49: 86432: loss: 0.1768794947:
49: 89632: loss: 0.1767710205:
49: 92832: loss: 0.1771771117:
49: 96032: loss: 0.1769098318:
49: 99232: loss: 0.1768969362:
49: 102432: loss: 0.1763253180:
49: 105632: loss: 0.1765539018:
49: 108832: loss: 0.1769899508:
49: 112032: loss: 0.1773930510:
49: 115232: loss: 0.1771861573:
49: 118432: loss: 0.1771360150:
49: 121632: loss: 0.1774943372:
49: 124832: loss: 0.1771687889:
49: 128032: loss: 0.1773454697:
49: 131232: loss: 0.1769685612:
49: 134432: loss: 0.1774040161:
49: 137632: loss: 0.1771699751:
49: 140832: loss: 0.1772614366:
49: 144032: loss: 0.1770601660:
49: 147232: loss: 0.1769989974:
49: 150432: loss: 0.1769746969:
49: 153632: loss: 0.1771751731:
49: 156832: loss: 0.1771598499:
49: 160032: loss: 0.1769856724:
49: 163232: loss: 0.1773840909:
49: 166432: loss: 0.1772950812:
Dev-Acc: 49: Accuracy: 0.9360612631: precision: 0.4438011579: recall: 0.3779969393: f1: 0.4082644628
Train-Acc: 49: Accuracy: 0.9342760444: precision: 0.7481743227: recall: 0.4175925317: f1: 0.5360111388
50: 3232: loss: 0.1834036663:
50: 6432: loss: 0.1806356532:
50: 9632: loss: 0.1815027344:
50: 12832: loss: 0.1766417136:
50: 16032: loss: 0.1774590801:
50: 19232: loss: 0.1777722398:
50: 22432: loss: 0.1755399267:
50: 25632: loss: 0.1750766397:
50: 28832: loss: 0.1752580871:
50: 32032: loss: 0.1749259005:
50: 35232: loss: 0.1744697404:
50: 38432: loss: 0.1745701362:
50: 41632: loss: 0.1743197043:
50: 44832: loss: 0.1737215857:
50: 48032: loss: 0.1744308311:
50: 51232: loss: 0.1742675493:
50: 54432: loss: 0.1748675005:
50: 57632: loss: 0.1748192710:
50: 60832: loss: 0.1746366772:
50: 64032: loss: 0.1749635112:
50: 67232: loss: 0.1746578463:
50: 70432: loss: 0.1743997071:
50: 73632: loss: 0.1748180897:
50: 76832: loss: 0.1755616470:
50: 80032: loss: 0.1759364184:
50: 83232: loss: 0.1759355647:
50: 86432: loss: 0.1757734012:
50: 89632: loss: 0.1753070522:
50: 92832: loss: 0.1748266012:
50: 96032: loss: 0.1748335976:
50: 99232: loss: 0.1748950211:
50: 102432: loss: 0.1746410416:
50: 105632: loss: 0.1752837719:
50: 108832: loss: 0.1760501649:
50: 112032: loss: 0.1762147092:
50: 115232: loss: 0.1762137796:
50: 118432: loss: 0.1762960599:
50: 121632: loss: 0.1764498361:
50: 124832: loss: 0.1765799139:
50: 128032: loss: 0.1762910378:
50: 131232: loss: 0.1762131661:
50: 134432: loss: 0.1767285201:
50: 137632: loss: 0.1766709151:
50: 140832: loss: 0.1765774129:
50: 144032: loss: 0.1765200883:
50: 147232: loss: 0.1768109885:
50: 150432: loss: 0.1766231521:
50: 153632: loss: 0.1767871791:
50: 156832: loss: 0.1769586716:
50: 160032: loss: 0.1769351108:
50: 163232: loss: 0.1769850447:
50: 166432: loss: 0.1770370583:
Dev-Acc: 50: Accuracy: 0.9359918237: precision: 0.4434748116: recall: 0.3802074477: f1: 0.4094113339
Train-Acc: 50: Accuracy: 0.9344075322: precision: 0.7481255858: recall: 0.4198277562: f1: 0.5378363583
