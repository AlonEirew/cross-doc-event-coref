1: 3232: loss: 0.7096930283:
1: 6432: loss: 0.7078628597:
1: 9632: loss: 0.7069279067:
1: 12832: loss: 0.7062362909:
1: 16032: loss: 0.7052760290:
1: 19232: loss: 0.7045255835:
1: 22432: loss: 0.7036874715:
1: 25632: loss: 0.7027131055:
1: 28832: loss: 0.7017498285:
1: 32032: loss: 0.7008981225:
1: 35232: loss: 0.7002181942:
1: 38432: loss: 0.6994278524:
1: 41632: loss: 0.6984843575:
1: 44832: loss: 0.6976051316:
1: 48032: loss: 0.6967114698:
1: 51232: loss: 0.6958339429:
1: 54432: loss: 0.6949826915:
1: 57632: loss: 0.6941746781:
1: 60832: loss: 0.6933586607:
1: 64032: loss: 0.6924716136:
1: 67232: loss: 0.6916188921:
1: 70432: loss: 0.6907760879:
1: 73632: loss: 0.6899620375:
1: 76832: loss: 0.6891404515:
1: 80032: loss: 0.6882947471:
1: 83232: loss: 0.6875416143:
1: 86432: loss: 0.6866985631:
1: 89632: loss: 0.6858446520:
1: 92832: loss: 0.6850062666:
1: 96032: loss: 0.6841868732:
1: 99232: loss: 0.6833186960:
1: 102432: loss: 0.6824609211:
1: 105632: loss: 0.6815928219:
1: 108832: loss: 0.6807794347:
1: 112032: loss: 0.6799604088:
1: 115232: loss: 0.6791194355:
1: 118432: loss: 0.6783471288:
1: 121632: loss: 0.6775537842:
1: 124832: loss: 0.6767594661:
1: 128032: loss: 0.6759561829:
1: 131232: loss: 0.6751875513:
1: 134432: loss: 0.6743764614:
Dev-Acc: 1: Accuracy: 0.9261489511: precision: 0.0732240437: recall: 0.0227852406: f1: 0.0347555440
Train-Acc: 1: Accuracy: 0.8804958463: precision: 0.2794625720: recall: 0.0478601012: f1: 0.0817242928
2: 3232: loss: 0.6390621430:
2: 6432: loss: 0.6386736158:
2: 9632: loss: 0.6374958318:
2: 12832: loss: 0.6369918601:
2: 16032: loss: 0.6363665794:
2: 19232: loss: 0.6355602546:
2: 22432: loss: 0.6347459753:
2: 25632: loss: 0.6339892333:
2: 28832: loss: 0.6331794568:
2: 32032: loss: 0.6323603097:
2: 35232: loss: 0.6317440234:
2: 38432: loss: 0.6309006256:
2: 41632: loss: 0.6300367703:
2: 44832: loss: 0.6291673681:
2: 48032: loss: 0.6284449434:
2: 51232: loss: 0.6275349003:
2: 54432: loss: 0.6269300078:
2: 57632: loss: 0.6262906156:
2: 60832: loss: 0.6254587282:
2: 64032: loss: 0.6245692623:
2: 67232: loss: 0.6238249668:
2: 70432: loss: 0.6230462943:
2: 73632: loss: 0.6221465456:
2: 76832: loss: 0.6213943606:
2: 80032: loss: 0.6207044325:
2: 83232: loss: 0.6200044564:
2: 86432: loss: 0.6192618045:
2: 89632: loss: 0.6185784915:
2: 92832: loss: 0.6178772105:
2: 96032: loss: 0.6171444389:
2: 99232: loss: 0.6164326098:
2: 102432: loss: 0.6156602170:
2: 105632: loss: 0.6148855207:
2: 108832: loss: 0.6141852817:
2: 112032: loss: 0.6134690967:
2: 115232: loss: 0.6127552092:
2: 118432: loss: 0.6120462090:
2: 121632: loss: 0.6113757065:
2: 124832: loss: 0.6105948366:
2: 128032: loss: 0.6098899366:
2: 131232: loss: 0.6091698900:
2: 134432: loss: 0.6084814271:
Dev-Acc: 2: Accuracy: 0.9416574240: precision: 1.0000000000: recall: 0.0001700391: f1: 0.0003400204
Train-Acc: 2: Accuracy: 0.8889254332: precision: 1.0000000000: recall: 0.0003287095: f1: 0.0006572029
3: 3232: loss: 0.5769263446:
3: 6432: loss: 0.5763994446:
3: 9632: loss: 0.5768054658:
3: 12832: loss: 0.5758752549:
3: 16032: loss: 0.5749626234:
3: 19232: loss: 0.5743066004:
3: 22432: loss: 0.5729382902:
3: 25632: loss: 0.5721619212:
3: 28832: loss: 0.5714162088:
3: 32032: loss: 0.5706531818:
3: 35232: loss: 0.5700815767:
3: 38432: loss: 0.5691067080:
3: 41632: loss: 0.5681047982:
3: 44832: loss: 0.5671895605:
3: 48032: loss: 0.5667579549:
3: 51232: loss: 0.5663800164:
3: 54432: loss: 0.5656145104:
3: 57632: loss: 0.5650436662:
3: 60832: loss: 0.5642246154:
3: 64032: loss: 0.5636060400:
3: 67232: loss: 0.5629878085:
3: 70432: loss: 0.5624485094:
3: 73632: loss: 0.5617158353:
3: 76832: loss: 0.5608602242:
3: 80032: loss: 0.5600802123:
3: 83232: loss: 0.5593794117:
3: 86432: loss: 0.5586478697:
3: 89632: loss: 0.5579599473:
3: 92832: loss: 0.5572659789:
3: 96032: loss: 0.5563205374:
3: 99232: loss: 0.5556812028:
3: 102432: loss: 0.5551092768:
3: 105632: loss: 0.5544517773:
3: 108832: loss: 0.5538487857:
3: 112032: loss: 0.5531461269:
3: 115232: loss: 0.5524846982:
3: 118432: loss: 0.5518039084:
3: 121632: loss: 0.5511079196:
3: 124832: loss: 0.5504718941:
3: 128032: loss: 0.5500130976:
3: 131232: loss: 0.5494708357:
3: 134432: loss: 0.5489077067:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.5212054276:
4: 6432: loss: 0.5163943148:
4: 9632: loss: 0.5170160579:
4: 12832: loss: 0.5176221374:
4: 16032: loss: 0.5160315592:
4: 19232: loss: 0.5150181082:
4: 22432: loss: 0.5141108670:
4: 25632: loss: 0.5143524878:
4: 28832: loss: 0.5142224475:
4: 32032: loss: 0.5138601762:
4: 35232: loss: 0.5128337133:
4: 38432: loss: 0.5122690112:
4: 41632: loss: 0.5117764894:
4: 44832: loss: 0.5112927606:
4: 48032: loss: 0.5108120409:
4: 51232: loss: 0.5101411369:
4: 54432: loss: 0.5096297689:
4: 57632: loss: 0.5088078601:
4: 60832: loss: 0.5085088208:
4: 64032: loss: 0.5078648579:
4: 67232: loss: 0.5074501137:
4: 70432: loss: 0.5069983401:
4: 73632: loss: 0.5063965923:
4: 76832: loss: 0.5059845165:
4: 80032: loss: 0.5056697088:
4: 83232: loss: 0.5051050111:
4: 86432: loss: 0.5045581393:
4: 89632: loss: 0.5040665390:
4: 92832: loss: 0.5033170463:
4: 96032: loss: 0.5028334227:
4: 99232: loss: 0.5022211682:
4: 102432: loss: 0.5014365906:
4: 105632: loss: 0.5007396439:
4: 108832: loss: 0.5003201196:
4: 112032: loss: 0.4994318633:
4: 115232: loss: 0.4988949712:
4: 118432: loss: 0.4982731933:
4: 121632: loss: 0.4975784065:
4: 124832: loss: 0.4969155378:
4: 128032: loss: 0.4963803221:
4: 131232: loss: 0.4956737832:
4: 134432: loss: 0.4953624544:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4789079276:
5: 6432: loss: 0.4747523233:
5: 9632: loss: 0.4725700838:
5: 12832: loss: 0.4716378047:
5: 16032: loss: 0.4697150254:
5: 19232: loss: 0.4687345765:
5: 22432: loss: 0.4681843971:
5: 25632: loss: 0.4687271750:
5: 28832: loss: 0.4674717393:
5: 32032: loss: 0.4675737520:
5: 35232: loss: 0.4659673002:
5: 38432: loss: 0.4649952428:
5: 41632: loss: 0.4641635543:
5: 44832: loss: 0.4632999139:
5: 48032: loss: 0.4627981151:
5: 51232: loss: 0.4620683211:
5: 54432: loss: 0.4617070440:
5: 57632: loss: 0.4609316978:
5: 60832: loss: 0.4601801060:
5: 64032: loss: 0.4599679393:
5: 67232: loss: 0.4593015381:
5: 70432: loss: 0.4588752664:
5: 73632: loss: 0.4581597254:
5: 76832: loss: 0.4572134343:
5: 80032: loss: 0.4570101961:
5: 83232: loss: 0.4563842744:
5: 86432: loss: 0.4558783766:
5: 89632: loss: 0.4554996382:
5: 92832: loss: 0.4550045245:
5: 96032: loss: 0.4546661279:
5: 99232: loss: 0.4539516878:
5: 102432: loss: 0.4535772150:
5: 105632: loss: 0.4527251727:
5: 108832: loss: 0.4523581437:
5: 112032: loss: 0.4516052954:
5: 115232: loss: 0.4510900293:
5: 118432: loss: 0.4507870288:
5: 121632: loss: 0.4502923135:
5: 124832: loss: 0.4498918969:
5: 128032: loss: 0.4497543789:
5: 131232: loss: 0.4494351149:
5: 134432: loss: 0.4489215843:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.4379089352:
6: 6432: loss: 0.4299750403:
6: 9632: loss: 0.4250047163:
6: 12832: loss: 0.4244258198:
6: 16032: loss: 0.4262945448:
6: 19232: loss: 0.4253626161:
6: 22432: loss: 0.4243018157:
6: 25632: loss: 0.4240138066:
6: 28832: loss: 0.4232830271:
6: 32032: loss: 0.4225387489:
6: 35232: loss: 0.4221151354:
6: 38432: loss: 0.4213061915:
6: 41632: loss: 0.4212479456:
6: 44832: loss: 0.4209983254:
6: 48032: loss: 0.4199085501:
6: 51232: loss: 0.4198797299:
6: 54432: loss: 0.4196151129:
6: 57632: loss: 0.4194735270:
6: 60832: loss: 0.4186528075:
6: 64032: loss: 0.4176808494:
6: 67232: loss: 0.4175199366:
6: 70432: loss: 0.4174469362:
6: 73632: loss: 0.4175114677:
6: 76832: loss: 0.4171940715:
6: 80032: loss: 0.4171516970:
6: 83232: loss: 0.4168632997:
6: 86432: loss: 0.4167273199:
6: 89632: loss: 0.4163060060:
6: 92832: loss: 0.4158663472:
6: 96032: loss: 0.4153899037:
6: 99232: loss: 0.4149590774:
6: 102432: loss: 0.4144089562:
6: 105632: loss: 0.4139634939:
6: 108832: loss: 0.4135060168:
6: 112032: loss: 0.4131370403:
6: 115232: loss: 0.4127680471:
6: 118432: loss: 0.4123406730:
6: 121632: loss: 0.4120570953:
6: 124832: loss: 0.4115252915:
6: 128032: loss: 0.4112925955:
6: 131232: loss: 0.4109492850:
6: 134432: loss: 0.4105521289:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.4000214601:
7: 6432: loss: 0.3969339809:
7: 9632: loss: 0.3941231573:
7: 12832: loss: 0.3925554966:
7: 16032: loss: 0.3907344448:
7: 19232: loss: 0.3910211843:
7: 22432: loss: 0.3904143899:
7: 25632: loss: 0.3903668718:
7: 28832: loss: 0.3901731663:
7: 32032: loss: 0.3907122966:
7: 35232: loss: 0.3900517260:
7: 38432: loss: 0.3901455287:
7: 41632: loss: 0.3897592869:
7: 44832: loss: 0.3889447200:
7: 48032: loss: 0.3888920538:
7: 51232: loss: 0.3880680078:
7: 54432: loss: 0.3880470272:
7: 57632: loss: 0.3878377761:
7: 60832: loss: 0.3873768365:
7: 64032: loss: 0.3870378271:
7: 67232: loss: 0.3874610174:
7: 70432: loss: 0.3876285812:
7: 73632: loss: 0.3876518394:
7: 76832: loss: 0.3870325019:
7: 80032: loss: 0.3865828741:
7: 83232: loss: 0.3858018400:
7: 86432: loss: 0.3854353025:
7: 89632: loss: 0.3852551357:
7: 92832: loss: 0.3850831052:
7: 96032: loss: 0.3840896467:
7: 99232: loss: 0.3837358306:
7: 102432: loss: 0.3837302961:
7: 105632: loss: 0.3833461302:
7: 108832: loss: 0.3832945965:
7: 112032: loss: 0.3826839591:
7: 115232: loss: 0.3822147879:
7: 118432: loss: 0.3820430776:
7: 121632: loss: 0.3817910132:
7: 124832: loss: 0.3814651541:
7: 128032: loss: 0.3809160822:
7: 131232: loss: 0.3806107309:
7: 134432: loss: 0.3804303625:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3635316381:
8: 6432: loss: 0.3721704490:
8: 9632: loss: 0.3686372123:
8: 12832: loss: 0.3690531770:
8: 16032: loss: 0.3682251856:
8: 19232: loss: 0.3661326795:
8: 22432: loss: 0.3668606788:
8: 25632: loss: 0.3661362049:
8: 28832: loss: 0.3663751284:
8: 32032: loss: 0.3653124530:
8: 35232: loss: 0.3648682830:
8: 38432: loss: 0.3643624539:
8: 41632: loss: 0.3640638093:
8: 44832: loss: 0.3637317033:
8: 48032: loss: 0.3633730156:
8: 51232: loss: 0.3633258418:
8: 54432: loss: 0.3633609921:
8: 57632: loss: 0.3632124894:
8: 60832: loss: 0.3629474613:
8: 64032: loss: 0.3627165798:
8: 67232: loss: 0.3625664968:
8: 70432: loss: 0.3626300175:
8: 73632: loss: 0.3622170203:
8: 76832: loss: 0.3619222472:
8: 80032: loss: 0.3616902035:
8: 83232: loss: 0.3608571176:
8: 86432: loss: 0.3609141146:
8: 89632: loss: 0.3604640868:
8: 92832: loss: 0.3602330655:
8: 96032: loss: 0.3598387010:
8: 99232: loss: 0.3594937616:
8: 102432: loss: 0.3593259530:
8: 105632: loss: 0.3588655790:
8: 108832: loss: 0.3586150675:
8: 112032: loss: 0.3584895948:
8: 115232: loss: 0.3581625658:
8: 118432: loss: 0.3577619036:
8: 121632: loss: 0.3571548155:
8: 124832: loss: 0.3567818406:
8: 128032: loss: 0.3564353096:
8: 131232: loss: 0.3563265381:
8: 134432: loss: 0.3563817778:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3446514875:
9: 6432: loss: 0.3463664839:
9: 9632: loss: 0.3448845216:
9: 12832: loss: 0.3425830401:
9: 16032: loss: 0.3423963949:
9: 19232: loss: 0.3424303767:
9: 22432: loss: 0.3436105946:
9: 25632: loss: 0.3443954876:
9: 28832: loss: 0.3426983670:
9: 32032: loss: 0.3419429564:
9: 35232: loss: 0.3421149364:
9: 38432: loss: 0.3432015869:
9: 41632: loss: 0.3428589453:
9: 44832: loss: 0.3431214524:
9: 48032: loss: 0.3434921286:
9: 51232: loss: 0.3444705915:
9: 54432: loss: 0.3446208040:
9: 57632: loss: 0.3447941989:
9: 60832: loss: 0.3447374951:
9: 64032: loss: 0.3447431000:
9: 67232: loss: 0.3440817244:
9: 70432: loss: 0.3437676569:
9: 73632: loss: 0.3443307015:
9: 76832: loss: 0.3436665792:
9: 80032: loss: 0.3430435898:
9: 83232: loss: 0.3425977217:
9: 86432: loss: 0.3415934742:
9: 89632: loss: 0.3413555326:
9: 92832: loss: 0.3412506811:
9: 96032: loss: 0.3409489444:
9: 99232: loss: 0.3409665871:
9: 102432: loss: 0.3405070838:
9: 105632: loss: 0.3402864354:
9: 108832: loss: 0.3401750786:
9: 112032: loss: 0.3400843566:
9: 115232: loss: 0.3400763123:
9: 118432: loss: 0.3401103324:
9: 121632: loss: 0.3399286645:
9: 124832: loss: 0.3396242748:
9: 128032: loss: 0.3394087371:
9: 131232: loss: 0.3390357059:
9: 134432: loss: 0.3387963303:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.3394562635:
10: 6432: loss: 0.3399997818:
10: 9632: loss: 0.3360729469:
10: 12832: loss: 0.3314115148:
10: 16032: loss: 0.3301592436:
10: 19232: loss: 0.3291013955:
10: 22432: loss: 0.3265528291:
10: 25632: loss: 0.3244947599:
10: 28832: loss: 0.3259730046:
10: 32032: loss: 0.3252279285:
10: 35232: loss: 0.3243219388:
10: 38432: loss: 0.3241330864:
10: 41632: loss: 0.3236223748:
10: 44832: loss: 0.3239123917:
10: 48032: loss: 0.3243119031:
10: 51232: loss: 0.3252536669:
10: 54432: loss: 0.3250050327:
10: 57632: loss: 0.3250917678:
10: 60832: loss: 0.3256004834:
10: 64032: loss: 0.3254467757:
10: 67232: loss: 0.3257777824:
10: 70432: loss: 0.3253397685:
10: 73632: loss: 0.3254355478:
10: 76832: loss: 0.3250670910:
10: 80032: loss: 0.3246772003:
10: 83232: loss: 0.3242648009:
10: 86432: loss: 0.3242775161:
10: 89632: loss: 0.3239657160:
10: 92832: loss: 0.3238495249:
10: 96032: loss: 0.3231193563:
10: 99232: loss: 0.3230032154:
10: 102432: loss: 0.3228599014:
10: 105632: loss: 0.3228847727:
10: 108832: loss: 0.3227614919:
10: 112032: loss: 0.3228382086:
10: 115232: loss: 0.3232647599:
10: 118432: loss: 0.3233184019:
10: 121632: loss: 0.3231325880:
10: 124832: loss: 0.3234550374:
10: 128032: loss: 0.3233074215:
10: 131232: loss: 0.3237152214:
10: 134432: loss: 0.3239867982:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.3197711286:
11: 6432: loss: 0.3185363071:
11: 9632: loss: 0.3147252452:
11: 12832: loss: 0.3170658660:
11: 16032: loss: 0.3173668551:
11: 19232: loss: 0.3155893014:
11: 22432: loss: 0.3169975292:
11: 25632: loss: 0.3163351540:
11: 28832: loss: 0.3156608581:
11: 32032: loss: 0.3146349393:
11: 35232: loss: 0.3162392548:
11: 38432: loss: 0.3154094576:
11: 41632: loss: 0.3147674382:
11: 44832: loss: 0.3159726032:
11: 48032: loss: 0.3159624193:
11: 51232: loss: 0.3152028901:
11: 54432: loss: 0.3154986740:
11: 57632: loss: 0.3152483749:
11: 60832: loss: 0.3155083912:
11: 64032: loss: 0.3156052438:
11: 67232: loss: 0.3157649949:
11: 70432: loss: 0.3154832324:
11: 73632: loss: 0.3151087906:
11: 76832: loss: 0.3147290075:
11: 80032: loss: 0.3146938590:
11: 83232: loss: 0.3152406526:
11: 86432: loss: 0.3150193915:
11: 89632: loss: 0.3150322930:
11: 92832: loss: 0.3147090249:
11: 96032: loss: 0.3148748702:
11: 99232: loss: 0.3146578994:
11: 102432: loss: 0.3142307340:
11: 105632: loss: 0.3138375056:
11: 108832: loss: 0.3135416663:
11: 112032: loss: 0.3130873778:
11: 115232: loss: 0.3133993199:
11: 118432: loss: 0.3132242718:
11: 121632: loss: 0.3131528753:
11: 124832: loss: 0.3126349611:
11: 128032: loss: 0.3123169255:
11: 131232: loss: 0.3122462041:
11: 134432: loss: 0.3119522080:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.3090593450:
12: 6432: loss: 0.3117762492:
12: 9632: loss: 0.3129316822:
12: 12832: loss: 0.3118967273:
12: 16032: loss: 0.3116264709:
12: 19232: loss: 0.3075871713:
12: 22432: loss: 0.3059215939:
12: 25632: loss: 0.3060833276:
12: 28832: loss: 0.3059539562:
12: 32032: loss: 0.3058484950:
12: 35232: loss: 0.3045376159:
12: 38432: loss: 0.3044667419:
12: 41632: loss: 0.3046843248:
12: 44832: loss: 0.3045464278:
12: 48032: loss: 0.3043007734:
12: 51232: loss: 0.3040927323:
12: 54432: loss: 0.3040913352:
12: 57632: loss: 0.3039123106:
12: 60832: loss: 0.3042712354:
12: 64032: loss: 0.3032885311:
12: 67232: loss: 0.3031355759:
12: 70432: loss: 0.3035367029:
12: 73632: loss: 0.3032627992:
12: 76832: loss: 0.3030260557:
12: 80032: loss: 0.3024228272:
12: 83232: loss: 0.3024830556:
12: 86432: loss: 0.3021391603:
12: 89632: loss: 0.3019898961:
12: 92832: loss: 0.3022800647:
12: 96032: loss: 0.3019424380:
12: 99232: loss: 0.3015778456:
12: 102432: loss: 0.3013582485:
12: 105632: loss: 0.3015459116:
12: 108832: loss: 0.3015859058:
12: 112032: loss: 0.3017450623:
12: 115232: loss: 0.3020257605:
12: 118432: loss: 0.3019360333:
12: 121632: loss: 0.3019117535:
12: 124832: loss: 0.3017799509:
12: 128032: loss: 0.3015916278:
12: 131232: loss: 0.3017964313:
12: 134432: loss: 0.3016380232:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2940458944:
13: 6432: loss: 0.2944374941:
13: 9632: loss: 0.2962472975:
13: 12832: loss: 0.2981727892:
13: 16032: loss: 0.2994268970:
13: 19232: loss: 0.2994323084:
13: 22432: loss: 0.2976682745:
13: 25632: loss: 0.2962311165:
13: 28832: loss: 0.2971885605:
13: 32032: loss: 0.2967963166:
13: 35232: loss: 0.2939538192:
13: 38432: loss: 0.2935500767:
13: 41632: loss: 0.2938456719:
13: 44832: loss: 0.2938659031:
13: 48032: loss: 0.2941120696:
13: 51232: loss: 0.2932447847:
13: 54432: loss: 0.2941071936:
13: 57632: loss: 0.2939028139:
13: 60832: loss: 0.2937998471:
13: 64032: loss: 0.2935918969:
13: 67232: loss: 0.2934006943:
13: 70432: loss: 0.2933111152:
13: 73632: loss: 0.2939278967:
13: 76832: loss: 0.2937161608:
13: 80032: loss: 0.2939760350:
13: 83232: loss: 0.2940849809:
13: 86432: loss: 0.2937459768:
13: 89632: loss: 0.2931376475:
13: 92832: loss: 0.2937867560:
13: 96032: loss: 0.2940551653:
13: 99232: loss: 0.2939935109:
13: 102432: loss: 0.2938960699:
13: 105632: loss: 0.2942359436:
13: 108832: loss: 0.2935981477:
13: 112032: loss: 0.2933139135:
13: 115232: loss: 0.2933957811:
13: 118432: loss: 0.2928786790:
13: 121632: loss: 0.2928263182:
13: 124832: loss: 0.2926520540:
13: 128032: loss: 0.2928264652:
13: 131232: loss: 0.2929148460:
13: 134432: loss: 0.2926802560:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.8892102838: precision: 1.0000000000: recall: 0.0028926435: f1: 0.0057686005
14: 3232: loss: 0.2931587488:
14: 6432: loss: 0.3013044717:
14: 9632: loss: 0.2966512394:
14: 12832: loss: 0.2918143024:
14: 16032: loss: 0.2901179874:
14: 19232: loss: 0.2925304252:
14: 22432: loss: 0.2920866781:
14: 25632: loss: 0.2904382074:
14: 28832: loss: 0.2897275966:
14: 32032: loss: 0.2899487618:
14: 35232: loss: 0.2880703455:
14: 38432: loss: 0.2889528954:
14: 41632: loss: 0.2887610392:
14: 44832: loss: 0.2884577157:
14: 48032: loss: 0.2885022954:
14: 51232: loss: 0.2874488822:
14: 54432: loss: 0.2871216536:
14: 57632: loss: 0.2865723201:
14: 60832: loss: 0.2866915493:
14: 64032: loss: 0.2861343727:
14: 67232: loss: 0.2861997291:
14: 70432: loss: 0.2864548840:
14: 73632: loss: 0.2859364946:
14: 76832: loss: 0.2859214498:
14: 80032: loss: 0.2858361479:
14: 83232: loss: 0.2858671695:
14: 86432: loss: 0.2862929772:
14: 89632: loss: 0.2858105834:
14: 92832: loss: 0.2856861654:
14: 96032: loss: 0.2855418821:
14: 99232: loss: 0.2860129690:
14: 102432: loss: 0.2861284978:
14: 105632: loss: 0.2858085112:
14: 108832: loss: 0.2859937642:
14: 112032: loss: 0.2857164456:
14: 115232: loss: 0.2855413401:
14: 118432: loss: 0.2855203569:
14: 121632: loss: 0.2856445290:
14: 124832: loss: 0.2858305603:
14: 128032: loss: 0.2855585509:
14: 131232: loss: 0.2852609571:
14: 134432: loss: 0.2849642593:
Dev-Acc: 14: Accuracy: 0.9416574240: precision: 0.5200000000: recall: 0.0022105084: f1: 0.0044023027
Train-Acc: 14: Accuracy: 0.8905689716: precision: 0.8506097561: recall: 0.0183419893: f1: 0.0359096467
15: 3232: loss: 0.2849934416:
15: 6432: loss: 0.2823551174:
15: 9632: loss: 0.2860842915:
15: 12832: loss: 0.2865023580:
15: 16032: loss: 0.2860474414:
15: 19232: loss: 0.2872862669:
15: 22432: loss: 0.2858781622:
15: 25632: loss: 0.2890552754:
15: 28832: loss: 0.2893908783:
15: 32032: loss: 0.2899844620:
15: 35232: loss: 0.2883348008:
15: 38432: loss: 0.2861091997:
15: 41632: loss: 0.2851549603:
15: 44832: loss: 0.2841388916:
15: 48032: loss: 0.2839134338:
15: 51232: loss: 0.2824402548:
15: 54432: loss: 0.2825120370:
15: 57632: loss: 0.2814658238:
15: 60832: loss: 0.2818704128:
15: 64032: loss: 0.2811021539:
15: 67232: loss: 0.2813631373:
15: 70432: loss: 0.2810609389:
15: 73632: loss: 0.2815056322:
15: 76832: loss: 0.2808563939:
15: 80032: loss: 0.2814200103:
15: 83232: loss: 0.2813687417:
15: 86432: loss: 0.2804731546:
15: 89632: loss: 0.2801408053:
15: 92832: loss: 0.2804203710:
15: 96032: loss: 0.2795918374:
15: 99232: loss: 0.2791073552:
15: 102432: loss: 0.2791191668:
15: 105632: loss: 0.2787149933:
15: 108832: loss: 0.2786482801:
15: 112032: loss: 0.2784668534:
15: 115232: loss: 0.2784961875:
15: 118432: loss: 0.2781478246:
15: 121632: loss: 0.2779777257:
15: 124832: loss: 0.2777926651:
15: 128032: loss: 0.2777462753:
15: 131232: loss: 0.2778649720:
15: 134432: loss: 0.2779649120:
Dev-Acc: 15: Accuracy: 0.9421138167: precision: 0.7117117117: recall: 0.0134330896: f1: 0.0263684913
Train-Acc: 15: Accuracy: 0.8918983936: precision: 0.8159509202: recall: 0.0349746894: f1: 0.0670743239
16: 3232: loss: 0.2659997483:
16: 6432: loss: 0.2692243816:
16: 9632: loss: 0.2725569406:
16: 12832: loss: 0.2709818121:
16: 16032: loss: 0.2709203175:
16: 19232: loss: 0.2745033663:
16: 22432: loss: 0.2763733033:
16: 25632: loss: 0.2754143880:
16: 28832: loss: 0.2745138295:
16: 32032: loss: 0.2743047053:
16: 35232: loss: 0.2749840269:
16: 38432: loss: 0.2754760648:
16: 41632: loss: 0.2748642412:
16: 44832: loss: 0.2747195542:
16: 48032: loss: 0.2748001718:
16: 51232: loss: 0.2736752365:
16: 54432: loss: 0.2735078702:
16: 57632: loss: 0.2742115841:
16: 60832: loss: 0.2740065027:
16: 64032: loss: 0.2738513172:
16: 67232: loss: 0.2735964638:
16: 70432: loss: 0.2738059277:
16: 73632: loss: 0.2739727257:
16: 76832: loss: 0.2739853641:
16: 80032: loss: 0.2740420804:
16: 83232: loss: 0.2739527327:
16: 86432: loss: 0.2740064886:
16: 89632: loss: 0.2746333592:
16: 92832: loss: 0.2741999109:
16: 96032: loss: 0.2740579997:
16: 99232: loss: 0.2741711617:
16: 102432: loss: 0.2744388849:
16: 105632: loss: 0.2740549229:
16: 108832: loss: 0.2738084236:
16: 112032: loss: 0.2735116474:
16: 115232: loss: 0.2734090367:
16: 118432: loss: 0.2729841040:
16: 121632: loss: 0.2726251624:
16: 124832: loss: 0.2726843372:
16: 128032: loss: 0.2726626426:
16: 131232: loss: 0.2722078167:
16: 134432: loss: 0.2717531922:
Dev-Acc: 16: Accuracy: 0.9424015880: precision: 0.6187500000: recall: 0.0336677436: f1: 0.0638606676
Train-Acc: 16: Accuracy: 0.8936807513: precision: 0.8588621444: recall: 0.0516073894: f1: 0.0973643411
17: 3232: loss: 0.2568382264:
17: 6432: loss: 0.2601134314:
17: 9632: loss: 0.2650369607:
17: 12832: loss: 0.2613631846:
17: 16032: loss: 0.2597989820:
17: 19232: loss: 0.2641706257:
17: 22432: loss: 0.2635395826:
17: 25632: loss: 0.2656649681:
17: 28832: loss: 0.2657640681:
17: 32032: loss: 0.2665345109:
17: 35232: loss: 0.2658400421:
17: 38432: loss: 0.2666700153:
17: 41632: loss: 0.2669614900:
17: 44832: loss: 0.2665046925:
17: 48032: loss: 0.2671524108:
17: 51232: loss: 0.2673273488:
17: 54432: loss: 0.2667104143:
17: 57632: loss: 0.2665900488:
17: 60832: loss: 0.2669117412:
17: 64032: loss: 0.2667977250:
17: 67232: loss: 0.2666436940:
17: 70432: loss: 0.2664726931:
17: 73632: loss: 0.2668825499:
17: 76832: loss: 0.2667902413:
17: 80032: loss: 0.2668436770:
17: 83232: loss: 0.2668508018:
17: 86432: loss: 0.2669942862:
17: 89632: loss: 0.2664289524:
17: 92832: loss: 0.2669543020:
17: 96032: loss: 0.2669950967:
17: 99232: loss: 0.2662580085:
17: 102432: loss: 0.2661293974:
17: 105632: loss: 0.2659308820:
17: 108832: loss: 0.2658425251:
17: 112032: loss: 0.2655062909:
17: 115232: loss: 0.2653073556:
17: 118432: loss: 0.2649293618:
17: 121632: loss: 0.2652094362:
17: 124832: loss: 0.2650618154:
17: 128032: loss: 0.2650494025:
17: 131232: loss: 0.2649183056:
17: 134432: loss: 0.2651026496:
Dev-Acc: 17: Accuracy: 0.9420542717: precision: 0.5448577681: recall: 0.0423397381: f1: 0.0785736825
Train-Acc: 17: Accuracy: 0.8958356380: precision: 0.8666152660: recall: 0.0738938926: f1: 0.1361763993
18: 3232: loss: 0.2565662131:
18: 6432: loss: 0.2515549009:
18: 9632: loss: 0.2545225963:
18: 12832: loss: 0.2583683324:
18: 16032: loss: 0.2612956274:
18: 19232: loss: 0.2623720191:
18: 22432: loss: 0.2607745142:
18: 25632: loss: 0.2609411064:
18: 28832: loss: 0.2614242855:
18: 32032: loss: 0.2620764438:
18: 35232: loss: 0.2620369292:
18: 38432: loss: 0.2603923055:
18: 41632: loss: 0.2597806123:
18: 44832: loss: 0.2587941802:
18: 48032: loss: 0.2599050764:
18: 51232: loss: 0.2598370958:
18: 54432: loss: 0.2596453650:
18: 57632: loss: 0.2593715138:
18: 60832: loss: 0.2601926442:
18: 64032: loss: 0.2601153597:
18: 67232: loss: 0.2607241305:
18: 70432: loss: 0.2608635024:
18: 73632: loss: 0.2611387429:
18: 76832: loss: 0.2606175040:
18: 80032: loss: 0.2599480894:
18: 83232: loss: 0.2601467237:
18: 86432: loss: 0.2600071592:
18: 89632: loss: 0.2599482693:
18: 92832: loss: 0.2604419932:
18: 96032: loss: 0.2607401621:
18: 99232: loss: 0.2610572820:
18: 102432: loss: 0.2608337431:
18: 105632: loss: 0.2609475958:
18: 108832: loss: 0.2607168368:
18: 112032: loss: 0.2604109291:
18: 115232: loss: 0.2605824056:
18: 118432: loss: 0.2605115802:
18: 121632: loss: 0.2604281623:
18: 124832: loss: 0.2604054366:
18: 128032: loss: 0.2600909608:
18: 131232: loss: 0.2601250929:
18: 134432: loss: 0.2600210765:
Dev-Acc: 18: Accuracy: 0.9425107241: precision: 0.5568627451: recall: 0.0724366604: f1: 0.1281974120
Train-Acc: 18: Accuracy: 0.8985748887: precision: 0.8622950820: recall: 0.1037407140: f1: 0.1852003990
19: 3232: loss: 0.2717392243:
19: 6432: loss: 0.2686866765:
19: 9632: loss: 0.2688135324:
19: 12832: loss: 0.2669736604:
19: 16032: loss: 0.2682579708:
19: 19232: loss: 0.2662059549:
19: 22432: loss: 0.2651909486:
19: 25632: loss: 0.2647781434:
19: 28832: loss: 0.2624707319:
19: 32032: loss: 0.2614617598:
19: 35232: loss: 0.2606742983:
19: 38432: loss: 0.2599836866:
19: 41632: loss: 0.2601447773:
19: 44832: loss: 0.2596149001:
19: 48032: loss: 0.2589088539:
19: 51232: loss: 0.2588771874:
19: 54432: loss: 0.2580636328:
19: 57632: loss: 0.2573553115:
19: 60832: loss: 0.2576534387:
19: 64032: loss: 0.2577324338:
19: 67232: loss: 0.2574987026:
19: 70432: loss: 0.2576669485:
19: 73632: loss: 0.2575439181:
19: 76832: loss: 0.2572673947:
19: 80032: loss: 0.2577366318:
19: 83232: loss: 0.2578969251:
19: 86432: loss: 0.2578133006:
19: 89632: loss: 0.2575834419:
19: 92832: loss: 0.2570243283:
19: 96032: loss: 0.2567028819:
19: 99232: loss: 0.2564667718:
19: 102432: loss: 0.2562377344:
19: 105632: loss: 0.2558852806:
19: 108832: loss: 0.2555771673:
19: 112032: loss: 0.2558702418:
19: 115232: loss: 0.2555510431:
19: 118432: loss: 0.2556816210:
19: 121632: loss: 0.2557377643:
19: 124832: loss: 0.2555182472:
19: 128032: loss: 0.2553945995:
19: 131232: loss: 0.2559984372:
19: 134432: loss: 0.2557018113:
Dev-Acc: 19: Accuracy: 0.9436616898: precision: 0.5834018077: recall: 0.1207277674: f1: 0.2000563539
Train-Acc: 19: Accuracy: 0.9019861221: precision: 0.8712215321: recall: 0.1383209519: f1: 0.2387382276
20: 3232: loss: 0.2550795195:
20: 6432: loss: 0.2537631021:
20: 9632: loss: 0.2565130097:
20: 12832: loss: 0.2510244006:
20: 16032: loss: 0.2507806347:
20: 19232: loss: 0.2492968253:
20: 22432: loss: 0.2514435294:
20: 25632: loss: 0.2498674174:
20: 28832: loss: 0.2505403849:
20: 32032: loss: 0.2495483844:
20: 35232: loss: 0.2487237903:
20: 38432: loss: 0.2481542194:
20: 41632: loss: 0.2487829336:
20: 44832: loss: 0.2498610257:
20: 48032: loss: 0.2499170974:
20: 51232: loss: 0.2489790758:
20: 54432: loss: 0.2496745058:
20: 57632: loss: 0.2500993389:
20: 60832: loss: 0.2501206650:
20: 64032: loss: 0.2495785838:
20: 67232: loss: 0.2508063623:
20: 70432: loss: 0.2509380258:
20: 73632: loss: 0.2514145118:
20: 76832: loss: 0.2511528394:
20: 80032: loss: 0.2509196768:
20: 83232: loss: 0.2503980431:
20: 86432: loss: 0.2502947732:
20: 89632: loss: 0.2508494510:
20: 92832: loss: 0.2513783905:
20: 96032: loss: 0.2512496282:
20: 99232: loss: 0.2514291500:
20: 102432: loss: 0.2511946984:
20: 105632: loss: 0.2513778056:
20: 108832: loss: 0.2517185495:
20: 112032: loss: 0.2519345537:
20: 115232: loss: 0.2521342129:
20: 118432: loss: 0.2522341828:
20: 121632: loss: 0.2519845968:
20: 124832: loss: 0.2514402399:
20: 128032: loss: 0.2511622693:
20: 131232: loss: 0.2514968656:
20: 134432: loss: 0.2511867649:
Dev-Acc: 20: Accuracy: 0.9437609315: precision: 0.5760171306: recall: 0.1372215610: f1: 0.2216424059
Train-Acc: 20: Accuracy: 0.9052367210: precision: 0.8644951140: recall: 0.1744789955: f1: 0.2903561074
