1: 6464: loss: 0.6992479998:
1: 12864: loss: 0.6897893697:
1: 19264: loss: 0.6816867956:
1: 25664: loss: 0.6736903027:
1: 32064: loss: 0.6656912829:
1: 38464: loss: 0.6580874723:
1: 44864: loss: 0.6507835165:
1: 51264: loss: 0.6433237632:
1: 57664: loss: 0.6360984404:
1: 64064: loss: 0.6287885421:
1: 70464: loss: 0.6211664743:
1: 76864: loss: 0.6139945343:
1: 83264: loss: 0.6066380838:
1: 89664: loss: 0.5994177170:
1: 96064: loss: 0.5925303617:
1: 102464: loss: 0.5863318712:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 6464: loss: 0.4595565224:
2: 12864: loss: 0.4575676222:
2: 19264: loss: 0.4521987295:
2: 25664: loss: 0.4476062474:
2: 32064: loss: 0.4428058144:
2: 38464: loss: 0.4389357873:
2: 44864: loss: 0.4353699278:
2: 51264: loss: 0.4316659458:
2: 57664: loss: 0.4277572957:
2: 64064: loss: 0.4240254431:
2: 70464: loss: 0.4204391749:
2: 76864: loss: 0.4170978941:
2: 83264: loss: 0.4132583478:
2: 89664: loss: 0.4104759972:
2: 96064: loss: 0.4073000661:
2: 102464: loss: 0.4035418714:
Dev-Acc: 2: Accuracy: 0.9420344234: precision: 0.7671232877: recall: 0.0095221901: f1: 0.0188108834
Train-Acc: 2: Accuracy: 0.8606647253: precision: 0.8930817610: recall: 0.0280060483: f1: 0.0543090260
3: 6464: loss: 0.3444680052:
3: 12864: loss: 0.3406751239:
3: 19264: loss: 0.3403544309:
3: 25664: loss: 0.3387819004:
3: 32064: loss: 0.3373554267:
3: 38464: loss: 0.3350624332:
3: 44864: loss: 0.3320083708:
3: 51264: loss: 0.3308921680:
3: 57664: loss: 0.3294923356:
3: 64064: loss: 0.3279460856:
3: 70464: loss: 0.3261133186:
3: 76864: loss: 0.3243798871:
3: 83264: loss: 0.3226083330:
3: 89664: loss: 0.3214070053:
3: 96064: loss: 0.3191331235:
3: 102464: loss: 0.3174128500:
Dev-Acc: 3: Accuracy: 0.9361009598: precision: 0.4330859469: recall: 0.3076007482: f1: 0.3597136608
Train-Acc: 3: Accuracy: 0.8911502361: precision: 0.8783699060: recall: 0.2763131944: f1: 0.4203840768
4: 6464: loss: 0.2841731432:
4: 12864: loss: 0.2887411521:
4: 19264: loss: 0.2835556518:
4: 25664: loss: 0.2835106779:
4: 32064: loss: 0.2843680116:
4: 38464: loss: 0.2841208733:
4: 44864: loss: 0.2820907375:
4: 51264: loss: 0.2808951108:
4: 57664: loss: 0.2795063948:
4: 64064: loss: 0.2784881285:
4: 70464: loss: 0.2767262985:
4: 76864: loss: 0.2759446575:
4: 83264: loss: 0.2755890032:
4: 89664: loss: 0.2746174405:
4: 96064: loss: 0.2735523326:
4: 102464: loss: 0.2730907769:
Dev-Acc: 4: Accuracy: 0.9148178101: precision: 0.3386634845: recall: 0.4825709913: f1: 0.3980085548
Train-Acc: 4: Accuracy: 0.9036693573: precision: 0.7833447724: recall: 0.4502005128: f1: 0.5717864151
5: 6464: loss: 0.2585883866:
5: 12864: loss: 0.2577083190:
5: 19264: loss: 0.2588922150:
5: 25664: loss: 0.2580499925:
5: 32064: loss: 0.2567791273:
5: 38464: loss: 0.2553367913:
5: 44864: loss: 0.2533953275:
5: 51264: loss: 0.2516775313:
5: 57664: loss: 0.2518989414:
5: 64064: loss: 0.2506191557:
5: 70464: loss: 0.2510936778:
5: 76864: loss: 0.2501391409:
5: 83264: loss: 0.2489077607:
5: 89664: loss: 0.2483523386:
5: 96064: loss: 0.2470893645:
5: 102464: loss: 0.2458154792:
Dev-Acc: 5: Accuracy: 0.9077234268: precision: 0.3225002596: recall: 0.5281414725: f1: 0.4004641568
Train-Acc: 5: Accuracy: 0.9106097817: precision: 0.7906074528: recall: 0.5091052528: f1: 0.6193713509
6: 6464: loss: 0.2386762570:
6: 12864: loss: 0.2381678532:
6: 19264: loss: 0.2367935440:
6: 25664: loss: 0.2351070304:
6: 32064: loss: 0.2353569440:
6: 38464: loss: 0.2343044741:
6: 44864: loss: 0.2334609430:
6: 51264: loss: 0.2326248787:
6: 57664: loss: 0.2309656252:
6: 64064: loss: 0.2302183827:
6: 70464: loss: 0.2304942576:
6: 76864: loss: 0.2299445901:
6: 83264: loss: 0.2292481985:
6: 89664: loss: 0.2289799066:
6: 96064: loss: 0.2286013993:
6: 102464: loss: 0.2284115343:
Dev-Acc: 6: Accuracy: 0.9015220404: precision: 0.3081229835: recall: 0.5521169869: f1: 0.3955173884
Train-Acc: 6: Accuracy: 0.9144885540: precision: 0.7956041828: recall: 0.5402011702: f1: 0.6434864325
7: 6464: loss: 0.2178523575:
7: 12864: loss: 0.2201912377:
7: 19264: loss: 0.2192889513:
7: 25664: loss: 0.2204202709:
7: 32064: loss: 0.2207667047:
7: 38464: loss: 0.2214959876:
7: 44864: loss: 0.2211493022:
7: 51264: loss: 0.2208261184:
7: 57664: loss: 0.2204441030:
7: 64064: loss: 0.2200539295:
7: 70464: loss: 0.2198573164:
7: 76864: loss: 0.2188603471:
7: 83264: loss: 0.2185441304:
7: 89664: loss: 0.2180822380:
7: 96064: loss: 0.2167498331:
7: 102464: loss: 0.2166414208:
Dev-Acc: 7: Accuracy: 0.9002519846: precision: 0.3055917987: recall: 0.5575582384: f1: 0.3947986274
Train-Acc: 7: Accuracy: 0.9175220728: precision: 0.8074017405: recall: 0.5550588390: f1: 0.6578619293
8: 6464: loss: 0.2130188560:
8: 12864: loss: 0.2114519271:
8: 19264: loss: 0.2076841370:
8: 25664: loss: 0.2075352404:
8: 32064: loss: 0.2080186637:
8: 38464: loss: 0.2054838005:
8: 44864: loss: 0.2079671704:
8: 51264: loss: 0.2071380662:
8: 57664: loss: 0.2082171080:
8: 64064: loss: 0.2080334978:
8: 70464: loss: 0.2078049356:
8: 76864: loss: 0.2078251101:
8: 83264: loss: 0.2078773979:
8: 89664: loss: 0.2074222433:
8: 96064: loss: 0.2067142768:
8: 102464: loss: 0.2066635513:
Dev-Acc: 8: Accuracy: 0.8988628983: precision: 0.3015463918: recall: 0.5570481211: f1: 0.3912809794
Train-Acc: 8: Accuracy: 0.9220395088: precision: 0.8246570194: recall: 0.5769508908: f1: 0.6789154063
9: 6464: loss: 0.1946713456:
9: 12864: loss: 0.1923047969:
9: 19264: loss: 0.1927349365:
9: 25664: loss: 0.1952162875:
9: 32064: loss: 0.1960332912:
9: 38464: loss: 0.1965045458:
9: 44864: loss: 0.1974751691:
9: 51264: loss: 0.1967071264:
9: 57664: loss: 0.1973952013:
9: 64064: loss: 0.1970580989:
9: 70464: loss: 0.1975096941:
9: 76864: loss: 0.1975757203:
9: 83264: loss: 0.1982413503:
9: 89664: loss: 0.1982226784:
9: 96064: loss: 0.1982877709:
9: 102464: loss: 0.1987634607:
Dev-Acc: 9: Accuracy: 0.8957275152: precision: 0.2918315941: recall: 0.5516068696: f1: 0.3817144202
Train-Acc: 9: Accuracy: 0.9257116914: precision: 0.8336532310: recall: 0.5996318454: f1: 0.6975374732
10: 6464: loss: 0.1939725928:
10: 12864: loss: 0.1930031698:
10: 19264: loss: 0.1963234553:
10: 25664: loss: 0.1981112838:
10: 32064: loss: 0.1983959401:
10: 38464: loss: 0.1961663594:
10: 44864: loss: 0.1962765252:
10: 51264: loss: 0.1957496847:
10: 57664: loss: 0.1947492445:
10: 64064: loss: 0.1946375926:
10: 70464: loss: 0.1938122746:
10: 76864: loss: 0.1937506463:
10: 83264: loss: 0.1945904249:
10: 89664: loss: 0.1941014513:
10: 96064: loss: 0.1930094144:
10: 102464: loss: 0.1928206303:
Dev-Acc: 10: Accuracy: 0.8940902948: precision: 0.2841963080: recall: 0.5366434280: f1: 0.3716001413
Train-Acc: 10: Accuracy: 0.9285010099: precision: 0.8470674219: recall: 0.6095588719: f1: 0.7089498031
11: 6464: loss: 0.1855999646:
11: 12864: loss: 0.1928811022:
11: 19264: loss: 0.1895264738:
11: 25664: loss: 0.1917379164:
11: 32064: loss: 0.1907815643:
11: 38464: loss: 0.1913503431:
11: 44864: loss: 0.1900416343:
11: 51264: loss: 0.1884444251:
11: 57664: loss: 0.1894796893:
11: 64064: loss: 0.1891129754:
11: 70464: loss: 0.1891909721:
11: 76864: loss: 0.1882539905:
11: 83264: loss: 0.1880965419:
11: 89664: loss: 0.1884868764:
11: 96064: loss: 0.1879933236:
11: 102464: loss: 0.1878799262:
Dev-Acc: 11: Accuracy: 0.8938224316: precision: 0.2765622103: recall: 0.5072266621: f1: 0.3579528409
Train-Acc: 11: Accuracy: 0.9304450750: precision: 0.8651632825: recall: 0.6078495825: f1: 0.7140319716
12: 6464: loss: 0.1906626596:
12: 12864: loss: 0.1914536167:
12: 19264: loss: 0.1881676236:
12: 25664: loss: 0.1903589240:
12: 32064: loss: 0.1883643282:
12: 38464: loss: 0.1868801174:
12: 44864: loss: 0.1847097082:
12: 51264: loss: 0.1844409362:
12: 57664: loss: 0.1843322217:
12: 64064: loss: 0.1839583387:
12: 70464: loss: 0.1829228573:
12: 76864: loss: 0.1826284491:
12: 83264: loss: 0.1822506090:
12: 89664: loss: 0.1822232258:
12: 96064: loss: 0.1826704992:
12: 102464: loss: 0.1825545155:
Dev-Acc: 12: Accuracy: 0.8864601254: precision: 0.2622670542: recall: 0.5216799864: f1: 0.3490528471
Train-Acc: 12: Accuracy: 0.9332250357: precision: 0.8610393083: recall: 0.6350667280: f1: 0.7309875142
13: 6464: loss: 0.1850131170:
13: 12864: loss: 0.1908365554:
13: 19264: loss: 0.1872092164:
13: 25664: loss: 0.1847816700:
13: 32064: loss: 0.1838909590:
13: 38464: loss: 0.1819904018:
13: 44864: loss: 0.1805162079:
13: 51264: loss: 0.1790210892:
13: 57664: loss: 0.1771937960:
13: 64064: loss: 0.1772502105:
13: 70464: loss: 0.1775867127:
13: 76864: loss: 0.1778232473:
13: 83264: loss: 0.1780325781:
13: 89664: loss: 0.1784045962:
13: 96064: loss: 0.1783027353:
13: 102464: loss: 0.1784344468:
Dev-Acc: 13: Accuracy: 0.8795443773: precision: 0.2484526967: recall: 0.5255908859: f1: 0.3374085798
Train-Acc: 13: Accuracy: 0.9353475571: precision: 0.8634022868: recall: 0.6503188482: f1: 0.7418629069
14: 6464: loss: 0.1758801235:
14: 12864: loss: 0.1752279681:
14: 19264: loss: 0.1726301733:
14: 25664: loss: 0.1754828122:
14: 32064: loss: 0.1769110785:
14: 38464: loss: 0.1740303945:
14: 44864: loss: 0.1732276560:
14: 51264: loss: 0.1739453481:
14: 57664: loss: 0.1736148764:
14: 64064: loss: 0.1755268017:
14: 70464: loss: 0.1751339124:
14: 76864: loss: 0.1745925802:
14: 83264: loss: 0.1749245649:
14: 89664: loss: 0.1748632911:
14: 96064: loss: 0.1748195351:
14: 102464: loss: 0.1744382518:
Dev-Acc: 14: Accuracy: 0.8792070150: precision: 0.2431218875: recall: 0.5063764666: f1: 0.3285162714
Train-Acc: 14: Accuracy: 0.9360613227: precision: 0.8760404547: recall: 0.6434816909: f1: 0.7419648272
15: 6464: loss: 0.1710733355:
15: 12864: loss: 0.1735422244:
15: 19264: loss: 0.1702386436:
15: 25664: loss: 0.1696864808:
15: 32064: loss: 0.1715604341:
15: 38464: loss: 0.1722776998:
15: 44864: loss: 0.1711441749:
15: 51264: loss: 0.1706519721:
15: 57664: loss: 0.1704477384:
15: 64064: loss: 0.1701008044:
15: 70464: loss: 0.1699465945:
15: 76864: loss: 0.1699682980:
15: 83264: loss: 0.1705911084:
15: 89664: loss: 0.1705536644:
15: 96064: loss: 0.1704432680:
15: 102464: loss: 0.1706674497:
Dev-Acc: 15: Accuracy: 0.8734918237: precision: 0.2338628439: recall: 0.5131780309: f1: 0.3213030981
Train-Acc: 15: Accuracy: 0.9377706051: precision: 0.8757769413: recall: 0.6576819407: f1: 0.7512202448
16: 6464: loss: 0.1657070932:
16: 12864: loss: 0.1692363464:
16: 19264: loss: 0.1673794033:
16: 25664: loss: 0.1668257685:
16: 32064: loss: 0.1669114609:
16: 38464: loss: 0.1662274875:
16: 44864: loss: 0.1665246283:
16: 51264: loss: 0.1667601059:
16: 57664: loss: 0.1667016776:
16: 64064: loss: 0.1670110544:
16: 70464: loss: 0.1672196881:
16: 76864: loss: 0.1666854997:
16: 83264: loss: 0.1670874270:
16: 89664: loss: 0.1670077113:
16: 96064: loss: 0.1668926981:
16: 102464: loss: 0.1671703290:
Dev-Acc: 16: Accuracy: 0.8636687994: precision: 0.2214108472: recall: 0.5310321374: f1: 0.3125187631
Train-Acc: 16: Accuracy: 0.9401373267: precision: 0.8693471537: recall: 0.6837157320: f1: 0.7654375506
17: 6464: loss: 0.1580707194:
17: 12864: loss: 0.1610729965:
17: 19264: loss: 0.1611650540:
17: 25664: loss: 0.1623396895:
17: 32064: loss: 0.1629855878:
17: 38464: loss: 0.1628436539:
17: 44864: loss: 0.1632266074:
17: 51264: loss: 0.1637052969:
17: 57664: loss: 0.1647075866:
17: 64064: loss: 0.1639351623:
17: 70464: loss: 0.1638302862:
17: 76864: loss: 0.1645582223:
17: 83264: loss: 0.1647004185:
17: 89664: loss: 0.1647029822:
17: 96064: loss: 0.1646063819:
17: 102464: loss: 0.1645893198:
Dev-Acc: 17: Accuracy: 0.8616446853: precision: 0.2180966366: recall: 0.5303519810: f1: 0.3090873055
Train-Acc: 17: Accuracy: 0.9410201311: precision: 0.8722180545: recall: 0.6879232135: f1: 0.7691855337
18: 6464: loss: 0.1670171300:
18: 12864: loss: 0.1621282992:
18: 19264: loss: 0.1633594975:
18: 25664: loss: 0.1613472468:
18: 32064: loss: 0.1617925029:
18: 38464: loss: 0.1609908129:
18: 44864: loss: 0.1609960031:
18: 51264: loss: 0.1610658325:
18: 57664: loss: 0.1608964105:
18: 64064: loss: 0.1614285707:
18: 70464: loss: 0.1601711195:
18: 76864: loss: 0.1610858311:
18: 83264: loss: 0.1613536407:
18: 89664: loss: 0.1618868897:
18: 96064: loss: 0.1624328969:
18: 102464: loss: 0.1621457435:
Dev-Acc: 18: Accuracy: 0.8593724966: precision: 0.2130398671: recall: 0.5233803775: f1: 0.3028186335
Train-Acc: 18: Accuracy: 0.9415648580: precision: 0.8748227838: recall: 0.6896325028: f1: 0.7712668186
19: 6464: loss: 0.1705444698:
19: 12864: loss: 0.1660564727:
19: 19264: loss: 0.1628607268:
19: 25664: loss: 0.1622280461:
19: 32064: loss: 0.1622324316:
19: 38464: loss: 0.1601917026:
19: 44864: loss: 0.1606900650:
19: 51264: loss: 0.1596202139:
19: 57664: loss: 0.1589741817:
19: 64064: loss: 0.1590164995:
19: 70464: loss: 0.1591756146:
19: 76864: loss: 0.1593996580:
19: 83264: loss: 0.1600076318:
19: 89664: loss: 0.1599692971:
19: 96064: loss: 0.1598833216:
19: 102464: loss: 0.1591850820:
Dev-Acc: 19: Accuracy: 0.8579139709: precision: 0.2105768571: recall: 0.5220200646: f1: 0.3000977517
Train-Acc: 19: Accuracy: 0.9423349500: precision: 0.8791273092: recall: 0.6914075340: f1: 0.7740487230
20: 6464: loss: 0.1590102841:
20: 12864: loss: 0.1572632761:
20: 19264: loss: 0.1596786416:
20: 25664: loss: 0.1594321830:
20: 32064: loss: 0.1595665111:
20: 38464: loss: 0.1581101769:
20: 44864: loss: 0.1584556711:
20: 51264: loss: 0.1577647510:
20: 57664: loss: 0.1574418465:
20: 64064: loss: 0.1574916867:
20: 70464: loss: 0.1576285395:
20: 76864: loss: 0.1579708905:
20: 83264: loss: 0.1574617199:
20: 89664: loss: 0.1570638193:
20: 96064: loss: 0.1572558534:
20: 102464: loss: 0.1569893292:
Dev-Acc: 20: Accuracy: 0.8542724848: precision: 0.2067013056: recall: 0.5276313552: f1: 0.2970372852
Train-Acc: 20: Accuracy: 0.9430956841: precision: 0.8786198908: recall: 0.6981132075: f1: 0.7780342162
