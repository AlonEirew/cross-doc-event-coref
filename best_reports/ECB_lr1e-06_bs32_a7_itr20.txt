1: 3232: loss: 0.7004256171:
1: 6432: loss: 0.6920531976:
1: 9632: loss: 0.6846652196:
1: 12832: loss: 0.6771725689:
1: 16032: loss: 0.6697816740:
1: 19232: loss: 0.6625179099:
1: 22432: loss: 0.6552828531:
1: 25632: loss: 0.6483917418:
1: 28832: loss: 0.6412638638:
1: 32032: loss: 0.6341981762:
1: 35232: loss: 0.6274685383:
1: 38432: loss: 0.6205869593:
1: 41632: loss: 0.6140555736:
1: 44832: loss: 0.6069765672:
1: 48032: loss: 0.6010257530:
1: 51232: loss: 0.5948206917:
1: 54432: loss: 0.5884756097:
1: 57632: loss: 0.5825986802:
1: 60832: loss: 0.5764510402:
1: 64032: loss: 0.5705439611:
1: 67232: loss: 0.5645983155:
1: 70432: loss: 0.5592401528:
1: 73632: loss: 0.5535356157:
1: 76832: loss: 0.5482142872:
1: 80032: loss: 0.5427905663:
1: 83232: loss: 0.5377258312:
1: 86432: loss: 0.5329008914:
1: 89632: loss: 0.5280188583:
1: 92832: loss: 0.5228567228:
1: 96032: loss: 0.5182619469:
1: 99232: loss: 0.5136135897:
1: 102432: loss: 0.5090486225:
1: 105632: loss: 0.5048880489:
1: 108832: loss: 0.5004999662:
1: 112032: loss: 0.4961723343:
1: 115232: loss: 0.4918879235:
1: 118432: loss: 0.4884636483:
1: 121632: loss: 0.4846216215:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 3232: loss: 0.3436928853:
2: 6432: loss: 0.3410833801:
2: 9632: loss: 0.3426269951:
2: 12832: loss: 0.3398832041:
2: 16032: loss: 0.3386343157:
2: 19232: loss: 0.3340740592:
2: 22432: loss: 0.3322435604:
2: 25632: loss: 0.3320444423:
2: 28832: loss: 0.3311428698:
2: 32032: loss: 0.3291064269:
2: 35232: loss: 0.3263234216:
2: 38432: loss: 0.3248002184:
2: 41632: loss: 0.3232513767:
2: 44832: loss: 0.3206294047:
2: 48032: loss: 0.3199315736:
2: 51232: loss: 0.3186832049:
2: 54432: loss: 0.3173458946:
2: 57632: loss: 0.3156183782:
2: 60832: loss: 0.3147197842:
2: 64032: loss: 0.3136201795:
2: 67232: loss: 0.3119066651:
2: 70432: loss: 0.3115487348:
2: 73632: loss: 0.3109726140:
2: 76832: loss: 0.3103836324:
2: 80032: loss: 0.3093024147:
2: 83232: loss: 0.3084590784:
2: 86432: loss: 0.3078038852:
2: 89632: loss: 0.3066014366:
2: 92832: loss: 0.3052181401:
2: 96032: loss: 0.3038719859:
2: 99232: loss: 0.3026795122:
2: 102432: loss: 0.3017727935:
2: 105632: loss: 0.3012227343:
2: 108832: loss: 0.3000386134:
2: 112032: loss: 0.2994833088:
2: 115232: loss: 0.2983643666:
2: 118432: loss: 0.2979811759:
2: 121632: loss: 0.2970018185:
Dev-Acc: 2: Accuracy: 0.9400698543: precision: 0.4743465634: recall: 0.2499574902: f1: 0.3273942094
Train-Acc: 2: Accuracy: 0.8997600675: precision: 0.8565680473: recall: 0.2379199264: f1: 0.3724017288
3: 3232: loss: 0.2656881735:
3: 6432: loss: 0.2636084819:
3: 9632: loss: 0.2616500591:
3: 12832: loss: 0.2601813580:
3: 16032: loss: 0.2635940104:
3: 19232: loss: 0.2618835239:
3: 22432: loss: 0.2609481054:
3: 25632: loss: 0.2604983671:
3: 28832: loss: 0.2619391308:
3: 32032: loss: 0.2608182983:
3: 35232: loss: 0.2588540771:
3: 38432: loss: 0.2573071555:
3: 41632: loss: 0.2566466053:
3: 44832: loss: 0.2564380047:
3: 48032: loss: 0.2559860218:
3: 51232: loss: 0.2552412985:
3: 54432: loss: 0.2549118618:
3: 57632: loss: 0.2548923158:
3: 60832: loss: 0.2530916973:
3: 64032: loss: 0.2531772031:
3: 67232: loss: 0.2524971085:
3: 70432: loss: 0.2519436250:
3: 73632: loss: 0.2512805897:
3: 76832: loss: 0.2502800547:
3: 80032: loss: 0.2505164777:
3: 83232: loss: 0.2500689109:
3: 86432: loss: 0.2495150061:
3: 89632: loss: 0.2492813478:
3: 92832: loss: 0.2485317141:
3: 96032: loss: 0.2478659040:
3: 99232: loss: 0.2472014997:
3: 102432: loss: 0.2472692114:
3: 105632: loss: 0.2466894219:
3: 108832: loss: 0.2464763867:
3: 112032: loss: 0.2458732053:
3: 115232: loss: 0.2452320779:
3: 118432: loss: 0.2450224490:
3: 121632: loss: 0.2446093444:
Dev-Acc: 3: Accuracy: 0.9191637635: precision: 0.3571968742: recall: 0.4818908349: f1: 0.4102786826
Train-Acc: 3: Accuracy: 0.9109937549: precision: 0.7377850163: recall: 0.4467161922: f1: 0.5564882683
4: 3232: loss: 0.2291780647:
4: 6432: loss: 0.2251692143:
4: 9632: loss: 0.2295826273:
4: 12832: loss: 0.2292828397:
4: 16032: loss: 0.2298699645:
4: 19232: loss: 0.2282798867:
4: 22432: loss: 0.2281781960:
4: 25632: loss: 0.2287122992:
4: 28832: loss: 0.2302483737:
4: 32032: loss: 0.2292556512:
4: 35232: loss: 0.2299151779:
4: 38432: loss: 0.2303303639:
4: 41632: loss: 0.2287812525:
4: 44832: loss: 0.2282748302:
4: 48032: loss: 0.2268150672:
4: 51232: loss: 0.2268217441:
4: 54432: loss: 0.2265448515:
4: 57632: loss: 0.2258800441:
4: 60832: loss: 0.2255162534:
4: 64032: loss: 0.2256712467:
4: 67232: loss: 0.2252910852:
4: 70432: loss: 0.2254892482:
4: 73632: loss: 0.2247098791:
4: 76832: loss: 0.2245571432:
4: 80032: loss: 0.2238965622:
4: 83232: loss: 0.2236247738:
4: 86432: loss: 0.2234011421:
4: 89632: loss: 0.2229546481:
4: 92832: loss: 0.2221832010:
4: 96032: loss: 0.2220892684:
4: 99232: loss: 0.2222650517:
4: 102432: loss: 0.2216752630:
4: 105632: loss: 0.2210065232:
4: 108832: loss: 0.2207009180:
4: 112032: loss: 0.2202756064:
4: 115232: loss: 0.2197607582:
4: 118432: loss: 0.2198905791:
4: 121632: loss: 0.2198021484:
Dev-Acc: 4: Accuracy: 0.9220114350: precision: 0.3746992529: recall: 0.5031457235: f1: 0.4295253302
Train-Acc: 4: Accuracy: 0.9182253480: precision: 0.7799659357: recall: 0.4816908816: f1: 0.5955700061
5: 3232: loss: 0.2120199863:
5: 6432: loss: 0.2058471654:
5: 9632: loss: 0.2050230596:
5: 12832: loss: 0.2075664055:
5: 16032: loss: 0.2067687333:
5: 19232: loss: 0.2085711484:
5: 22432: loss: 0.2093055256:
5: 25632: loss: 0.2090134678:
5: 28832: loss: 0.2087112780:
5: 32032: loss: 0.2072072005:
5: 35232: loss: 0.2075788722:
5: 38432: loss: 0.2093215614:
5: 41632: loss: 0.2079542692:
5: 44832: loss: 0.2068136334:
5: 48032: loss: 0.2063306903:
5: 51232: loss: 0.2074350435:
5: 54432: loss: 0.2074923355:
5: 57632: loss: 0.2071524670:
5: 60832: loss: 0.2077834037:
5: 64032: loss: 0.2080230458:
5: 67232: loss: 0.2076599819:
5: 70432: loss: 0.2078984337:
5: 73632: loss: 0.2078287904:
5: 76832: loss: 0.2075995371:
5: 80032: loss: 0.2070872049:
5: 83232: loss: 0.2069702520:
5: 86432: loss: 0.2067744387:
5: 89632: loss: 0.2066832681:
5: 92832: loss: 0.2066281205:
5: 96032: loss: 0.2059082983:
5: 99232: loss: 0.2057484149:
5: 102432: loss: 0.2058380177:
5: 105632: loss: 0.2057605714:
5: 108832: loss: 0.2055813964:
5: 112032: loss: 0.2059627391:
5: 115232: loss: 0.2059537788:
5: 118432: loss: 0.2056556999:
5: 121632: loss: 0.2051400206:
Dev-Acc: 5: Accuracy: 0.9240058064: precision: 0.3807031669: recall: 0.4824009522: f1: 0.4255606390
Train-Acc: 5: Accuracy: 0.9216685891: precision: 0.8096848075: recall: 0.4880678456: f1: 0.6090237900
6: 3232: loss: 0.2075632510:
6: 6432: loss: 0.2134625831:
6: 9632: loss: 0.2088615265:
6: 12832: loss: 0.2044844507:
6: 16032: loss: 0.2017206063:
6: 19232: loss: 0.2012683957:
6: 22432: loss: 0.2021537377:
6: 25632: loss: 0.2009799891:
6: 28832: loss: 0.2003572590:
6: 32032: loss: 0.2000529960:
6: 35232: loss: 0.1999368362:
6: 38432: loss: 0.1994836147:
6: 41632: loss: 0.1991507833:
6: 44832: loss: 0.1981446805:
6: 48032: loss: 0.1981955332:
6: 51232: loss: 0.1993186268:
6: 54432: loss: 0.1990042688:
6: 57632: loss: 0.1986668671:
6: 60832: loss: 0.1980002201:
6: 64032: loss: 0.1979798717:
6: 67232: loss: 0.1978396068:
6: 70432: loss: 0.1972635456:
6: 73632: loss: 0.1969203214:
6: 76832: loss: 0.1964953632:
6: 80032: loss: 0.1968429913:
6: 83232: loss: 0.1966183300:
6: 86432: loss: 0.1961754921:
6: 89632: loss: 0.1959794004:
6: 92832: loss: 0.1956684995:
6: 96032: loss: 0.1954932076:
6: 99232: loss: 0.1959767987:
6: 102432: loss: 0.1959667499:
6: 105632: loss: 0.1957052602:
6: 108832: loss: 0.1955329774:
6: 112032: loss: 0.1954080163:
6: 115232: loss: 0.1954717762:
6: 118432: loss: 0.1952457991:
6: 121632: loss: 0.1948418061:
Dev-Acc: 6: Accuracy: 0.9207314253: precision: 0.3630101378: recall: 0.4749192314: f1: 0.4114917127
Train-Acc: 6: Accuracy: 0.9247831106: precision: 0.8155208333: recall: 0.5146933140: f1: 0.6310910483
7: 3232: loss: 0.1913511010:
7: 6432: loss: 0.1937867763:
7: 9632: loss: 0.1969019013:
7: 12832: loss: 0.1940311232:
7: 16032: loss: 0.1926792213:
7: 19232: loss: 0.1911804194:
7: 22432: loss: 0.1927499603:
7: 25632: loss: 0.1916259362:
7: 28832: loss: 0.1918900548:
7: 32032: loss: 0.1925886358:
7: 35232: loss: 0.1905144806:
7: 38432: loss: 0.1900108480:
7: 41632: loss: 0.1904514885:
7: 44832: loss: 0.1903675421:
7: 48032: loss: 0.1907196987:
7: 51232: loss: 0.1905184344:
7: 54432: loss: 0.1905603552:
7: 57632: loss: 0.1907248754:
7: 60832: loss: 0.1907930411:
7: 64032: loss: 0.1900411702:
7: 67232: loss: 0.1898972835:
7: 70432: loss: 0.1895445673:
7: 73632: loss: 0.1892019349:
7: 76832: loss: 0.1895133916:
7: 80032: loss: 0.1890351222:
7: 83232: loss: 0.1897204592:
7: 86432: loss: 0.1894035800:
7: 89632: loss: 0.1894703990:
7: 92832: loss: 0.1893071065:
7: 96032: loss: 0.1889599859:
7: 99232: loss: 0.1890432721:
7: 102432: loss: 0.1887525514:
7: 105632: loss: 0.1883473639:
7: 108832: loss: 0.1878884790:
7: 112032: loss: 0.1873608744:
7: 115232: loss: 0.1873851600:
7: 118432: loss: 0.1872998397:
7: 121632: loss: 0.1871508517:
Dev-Acc: 7: Accuracy: 0.9198186398: precision: 0.3557566221: recall: 0.4613161027: f1: 0.4017176279
Train-Acc: 7: Accuracy: 0.9278483391: precision: 0.8267452495: recall: 0.5348760765: f1: 0.6495289797
8: 3232: loss: 0.1816518159:
8: 6432: loss: 0.1794783114:
8: 9632: loss: 0.1833767310:
8: 12832: loss: 0.1825080119:
8: 16032: loss: 0.1860729882:
8: 19232: loss: 0.1856699511:
8: 22432: loss: 0.1845120735:
8: 25632: loss: 0.1823357564:
8: 28832: loss: 0.1825241604:
8: 32032: loss: 0.1820958555:
8: 35232: loss: 0.1817995928:
8: 38432: loss: 0.1815314013:
8: 41632: loss: 0.1808120519:
8: 44832: loss: 0.1810285519:
8: 48032: loss: 0.1817870680:
8: 51232: loss: 0.1814148571:
8: 54432: loss: 0.1826561935:
8: 57632: loss: 0.1816281364:
8: 60832: loss: 0.1821838238:
8: 64032: loss: 0.1818013371:
8: 67232: loss: 0.1813365789:
8: 70432: loss: 0.1813893176:
8: 73632: loss: 0.1812940437:
8: 76832: loss: 0.1807873640:
8: 80032: loss: 0.1805116742:
8: 83232: loss: 0.1803145861:
8: 86432: loss: 0.1804444632:
8: 89632: loss: 0.1798795882:
8: 92832: loss: 0.1801799881:
8: 96032: loss: 0.1803280010:
8: 99232: loss: 0.1801695429:
8: 102432: loss: 0.1805441165:
8: 105632: loss: 0.1806999164:
8: 108832: loss: 0.1807096642:
8: 112032: loss: 0.1805209897:
8: 115232: loss: 0.1808837196:
8: 118432: loss: 0.1809459643:
8: 121632: loss: 0.1809704222:
Dev-Acc: 8: Accuracy: 0.9129623771: precision: 0.3283458022: recall: 0.4701581364: f1: 0.3866592085
Train-Acc: 8: Accuracy: 0.9314969778: precision: 0.8298627771: recall: 0.5685359279: f1: 0.6747815231
9: 3232: loss: 0.1740017420:
9: 6432: loss: 0.1686993015:
9: 9632: loss: 0.1724461665:
9: 12832: loss: 0.1757111645:
9: 16032: loss: 0.1750317971:
9: 19232: loss: 0.1759478705:
9: 22432: loss: 0.1776926454:
9: 25632: loss: 0.1770765896:
9: 28832: loss: 0.1782292776:
9: 32032: loss: 0.1771258539:
9: 35232: loss: 0.1767654900:
9: 38432: loss: 0.1772947775:
9: 41632: loss: 0.1777622838:
9: 44832: loss: 0.1771563064:
9: 48032: loss: 0.1759680034:
9: 51232: loss: 0.1758426380:
9: 54432: loss: 0.1763241185:
9: 57632: loss: 0.1769695158:
9: 60832: loss: 0.1771326492:
9: 64032: loss: 0.1781580261:
9: 67232: loss: 0.1777755598:
9: 70432: loss: 0.1773679103:
9: 73632: loss: 0.1771735034:
9: 76832: loss: 0.1772148166:
9: 80032: loss: 0.1773016046:
9: 83232: loss: 0.1768265919:
9: 86432: loss: 0.1770637607:
9: 89632: loss: 0.1765831296:
9: 92832: loss: 0.1769318947:
9: 96032: loss: 0.1770075294:
9: 99232: loss: 0.1763308064:
9: 102432: loss: 0.1760832380:
9: 105632: loss: 0.1757929070:
9: 108832: loss: 0.1754105708:
9: 112032: loss: 0.1752433933:
9: 115232: loss: 0.1754063924:
9: 118432: loss: 0.1753876633:
9: 121632: loss: 0.1752394900:
Dev-Acc: 9: Accuracy: 0.9079516530: precision: 0.3099395567: recall: 0.4708382928: f1: 0.3738103274
Train-Acc: 9: Accuracy: 0.9340444803: precision: 0.8332869468: recall: 0.5904937216: f1: 0.6911889188
10: 3232: loss: 0.1730696426:
10: 6432: loss: 0.1705408516:
10: 9632: loss: 0.1699096030:
10: 12832: loss: 0.1732841306:
10: 16032: loss: 0.1730347950:
10: 19232: loss: 0.1727434066:
10: 22432: loss: 0.1727799844:
10: 25632: loss: 0.1730875921:
10: 28832: loss: 0.1721085409:
10: 32032: loss: 0.1710578211:
10: 35232: loss: 0.1709150602:
10: 38432: loss: 0.1710097020:
10: 41632: loss: 0.1716718236:
10: 44832: loss: 0.1719663970:
10: 48032: loss: 0.1711851109:
10: 51232: loss: 0.1714274637:
10: 54432: loss: 0.1721315222:
10: 57632: loss: 0.1716253678:
10: 60832: loss: 0.1723732478:
10: 64032: loss: 0.1726253134:
10: 67232: loss: 0.1729847581:
10: 70432: loss: 0.1728117063:
10: 73632: loss: 0.1725956882:
10: 76832: loss: 0.1726031035:
10: 80032: loss: 0.1724281244:
10: 83232: loss: 0.1720327324:
10: 86432: loss: 0.1718064433:
10: 89632: loss: 0.1718801507:
10: 92832: loss: 0.1713854604:
10: 96032: loss: 0.1712427528:
10: 99232: loss: 0.1711058147:
10: 102432: loss: 0.1709023031:
10: 105632: loss: 0.1708442299:
10: 108832: loss: 0.1707425287:
10: 112032: loss: 0.1706605825:
10: 115232: loss: 0.1705597304:
10: 118432: loss: 0.1706571559:
10: 121632: loss: 0.1706854515:
Dev-Acc: 10: Accuracy: 0.9051436782: precision: 0.3004231312: recall: 0.4708382928: f1: 0.3668035501
Train-Acc: 10: Accuracy: 0.9360989332: precision: 0.8431644051: recall: 0.6004864900: f1: 0.7014283520
11: 3232: loss: 0.1838975142:
11: 6432: loss: 0.1729080945:
11: 9632: loss: 0.1767751751:
11: 12832: loss: 0.1733270191:
11: 16032: loss: 0.1721459943:
11: 19232: loss: 0.1710195492:
11: 22432: loss: 0.1684928417:
11: 25632: loss: 0.1677912709:
11: 28832: loss: 0.1681578721:
11: 32032: loss: 0.1668144378:
11: 35232: loss: 0.1662040946:
11: 38432: loss: 0.1670004941:
11: 41632: loss: 0.1663891067:
11: 44832: loss: 0.1662286141:
11: 48032: loss: 0.1672123350:
11: 51232: loss: 0.1677867777:
11: 54432: loss: 0.1681363490:
11: 57632: loss: 0.1681286627:
11: 60832: loss: 0.1673098131:
11: 64032: loss: 0.1671186426:
11: 67232: loss: 0.1678058666:
11: 70432: loss: 0.1681040884:
11: 73632: loss: 0.1678737260:
11: 76832: loss: 0.1671665829:
11: 80032: loss: 0.1670888701:
11: 83232: loss: 0.1671683670:
11: 86432: loss: 0.1668289424:
11: 89632: loss: 0.1666217728:
11: 92832: loss: 0.1665679428:
11: 96032: loss: 0.1665380211:
11: 99232: loss: 0.1663983980:
11: 102432: loss: 0.1661272408:
11: 105632: loss: 0.1658696279:
11: 108832: loss: 0.1658483273:
11: 112032: loss: 0.1659345268:
11: 115232: loss: 0.1660004443:
11: 118432: loss: 0.1664394744:
11: 121632: loss: 0.1662602986:
Dev-Acc: 11: Accuracy: 0.9008076787: precision: 0.2872208437: recall: 0.4723686448: f1: 0.3572301164
Train-Acc: 11: Accuracy: 0.9376685023: precision: 0.8438232642: recall: 0.6152126750: f1: 0.7116079237
12: 3232: loss: 0.1616246452:
12: 6432: loss: 0.1654419966:
12: 9632: loss: 0.1615574105:
12: 12832: loss: 0.1604207105:
12: 16032: loss: 0.1623318391:
12: 19232: loss: 0.1620648944:
12: 22432: loss: 0.1629870376:
12: 25632: loss: 0.1639818975:
12: 28832: loss: 0.1632043438:
12: 32032: loss: 0.1633981528:
12: 35232: loss: 0.1625306469:
12: 38432: loss: 0.1631692741:
12: 41632: loss: 0.1638947294:
12: 44832: loss: 0.1637503577:
12: 48032: loss: 0.1630806176:
12: 51232: loss: 0.1626895290:
12: 54432: loss: 0.1625590386:
12: 57632: loss: 0.1624729593:
12: 60832: loss: 0.1614863631:
12: 64032: loss: 0.1614944825:
12: 67232: loss: 0.1623361330:
12: 70432: loss: 0.1630673659:
12: 73632: loss: 0.1630840645:
12: 76832: loss: 0.1631336759:
12: 80032: loss: 0.1631661850:
12: 83232: loss: 0.1633803102:
12: 86432: loss: 0.1631648154:
12: 89632: loss: 0.1632452291:
12: 92832: loss: 0.1633331056:
12: 96032: loss: 0.1634889751:
12: 99232: loss: 0.1630024728:
12: 102432: loss: 0.1628626219:
12: 105632: loss: 0.1626817508:
12: 108832: loss: 0.1627420324:
12: 112032: loss: 0.1629237215:
12: 115232: loss: 0.1632203833:
12: 118432: loss: 0.1627226307:
12: 121632: loss: 0.1626268829:
Dev-Acc: 12: Accuracy: 0.8979798555: precision: 0.2797958571: recall: 0.4754293488: f1: 0.3522741590
Train-Acc: 12: Accuracy: 0.9389751554: precision: 0.8456620194: recall: 0.6260600881: f1: 0.7194771834
13: 3232: loss: 0.1548532735:
13: 6432: loss: 0.1589063780:
13: 9632: loss: 0.1535667100:
13: 12832: loss: 0.1579180299:
13: 16032: loss: 0.1552091407:
13: 19232: loss: 0.1584807423:
13: 22432: loss: 0.1572495256:
13: 25632: loss: 0.1611864703:
13: 28832: loss: 0.1615757849:
13: 32032: loss: 0.1603694849:
13: 35232: loss: 0.1596176055:
13: 38432: loss: 0.1585529556:
13: 41632: loss: 0.1588668508:
13: 44832: loss: 0.1582706437:
13: 48032: loss: 0.1574350216:
13: 51232: loss: 0.1573092649:
13: 54432: loss: 0.1576440137:
13: 57632: loss: 0.1577148392:
13: 60832: loss: 0.1578436795:
13: 64032: loss: 0.1580807812:
13: 67232: loss: 0.1578534288:
13: 70432: loss: 0.1572928084:
13: 73632: loss: 0.1568979450:
13: 76832: loss: 0.1573268028:
13: 80032: loss: 0.1580283053:
13: 83232: loss: 0.1580063642:
13: 86432: loss: 0.1583820191:
13: 89632: loss: 0.1584539703:
13: 92832: loss: 0.1588662101:
13: 96032: loss: 0.1586734044:
13: 99232: loss: 0.1587511684:
13: 102432: loss: 0.1588621441:
13: 105632: loss: 0.1592717952:
13: 108832: loss: 0.1590125811:
13: 112032: loss: 0.1590044675:
13: 115232: loss: 0.1589368838:
13: 118432: loss: 0.1588484593:
13: 121632: loss: 0.1590103730:
Dev-Acc: 13: Accuracy: 0.8948642612: precision: 0.2727710843: recall: 0.4812106785: f1: 0.3481791339
Train-Acc: 13: Accuracy: 0.9403721690: precision: 0.8502862175: recall: 0.6347380185: f1: 0.7268689302
14: 3232: loss: 0.1517689292:
14: 6432: loss: 0.1488150979:
14: 9632: loss: 0.1514952903:
14: 12832: loss: 0.1557018096:
14: 16032: loss: 0.1550345132:
14: 19232: loss: 0.1578748046:
14: 22432: loss: 0.1579356320:
14: 25632: loss: 0.1594706656:
14: 28832: loss: 0.1572661598:
14: 32032: loss: 0.1573585772:
14: 35232: loss: 0.1564705539:
14: 38432: loss: 0.1566928465:
14: 41632: loss: 0.1562104268:
14: 44832: loss: 0.1568225410:
14: 48032: loss: 0.1575776711:
14: 51232: loss: 0.1574521059:
14: 54432: loss: 0.1573978898:
14: 57632: loss: 0.1570349493:
14: 60832: loss: 0.1569836558:
14: 64032: loss: 0.1568670971:
14: 67232: loss: 0.1563345643:
14: 70432: loss: 0.1568098600:
14: 73632: loss: 0.1576543738:
14: 76832: loss: 0.1574496288:
14: 80032: loss: 0.1575268662:
14: 83232: loss: 0.1570880611:
14: 86432: loss: 0.1570941425:
14: 89632: loss: 0.1572941727:
14: 92832: loss: 0.1570065911:
14: 96032: loss: 0.1567630827:
14: 99232: loss: 0.1566796729:
14: 102432: loss: 0.1565987280:
14: 105632: loss: 0.1569948864:
14: 108832: loss: 0.1566836741:
14: 112032: loss: 0.1566711685:
14: 115232: loss: 0.1565438691:
14: 118432: loss: 0.1564185335:
14: 121632: loss: 0.1561632234:
Dev-Acc: 14: Accuracy: 0.8951321244: precision: 0.2714954182: recall: 0.4735589186: f1: 0.3451267117
Train-Acc: 14: Accuracy: 0.9409145117: precision: 0.8579205712: recall: 0.6319768589: f1: 0.7278164749
15: 3232: loss: 0.1703132594:
15: 6432: loss: 0.1606655226:
15: 9632: loss: 0.1609974423:
15: 12832: loss: 0.1586403676:
15: 16032: loss: 0.1589018972:
15: 19232: loss: 0.1565280849:
15: 22432: loss: 0.1565005288:
15: 25632: loss: 0.1583719077:
15: 28832: loss: 0.1579921266:
15: 32032: loss: 0.1570004790:
15: 35232: loss: 0.1570083319:
15: 38432: loss: 0.1566742744:
15: 41632: loss: 0.1559269913:
15: 44832: loss: 0.1563289658:
15: 48032: loss: 0.1556912432:
15: 51232: loss: 0.1550892448:
15: 54432: loss: 0.1552133464:
15: 57632: loss: 0.1553241674:
15: 60832: loss: 0.1549762505:
15: 64032: loss: 0.1542366524:
15: 67232: loss: 0.1538405457:
15: 70432: loss: 0.1535932775:
15: 73632: loss: 0.1539667629:
15: 76832: loss: 0.1538015184:
15: 80032: loss: 0.1541299466:
15: 83232: loss: 0.1532817344:
15: 86432: loss: 0.1530144947:
15: 89632: loss: 0.1533316788:
15: 92832: loss: 0.1535310873:
15: 96032: loss: 0.1534066903:
15: 99232: loss: 0.1530071370:
15: 102432: loss: 0.1531944608:
15: 105632: loss: 0.1530737628:
15: 108832: loss: 0.1530661974:
15: 112032: loss: 0.1530247079:
15: 115232: loss: 0.1531266084:
15: 118432: loss: 0.1528193940:
15: 121632: loss: 0.1528794181:
Dev-Acc: 15: Accuracy: 0.8885040879: precision: 0.2582159624: recall: 0.4863118517: f1: 0.3373238191
Train-Acc: 15: Accuracy: 0.9423279762: precision: 0.8462513735: recall: 0.6582078759: f1: 0.7404777753
16: 3232: loss: 0.1695212602:
16: 6432: loss: 0.1660316013:
16: 9632: loss: 0.1603728232:
16: 12832: loss: 0.1568598709:
16: 16032: loss: 0.1578739557:
16: 19232: loss: 0.1565085274:
16: 22432: loss: 0.1536728847:
16: 25632: loss: 0.1525321021:
16: 28832: loss: 0.1523334647:
16: 32032: loss: 0.1514808955:
16: 35232: loss: 0.1512426727:
16: 38432: loss: 0.1508911276:
16: 41632: loss: 0.1513418642:
16: 44832: loss: 0.1508678387:
16: 48032: loss: 0.1512000586:
16: 51232: loss: 0.1512060379:
16: 54432: loss: 0.1506197925:
16: 57632: loss: 0.1501030610:
16: 60832: loss: 0.1503662736:
16: 64032: loss: 0.1506696940:
16: 67232: loss: 0.1507166554:
16: 70432: loss: 0.1502925548:
16: 73632: loss: 0.1506551309:
16: 76832: loss: 0.1508675048:
16: 80032: loss: 0.1506123998:
16: 83232: loss: 0.1508072074:
16: 86432: loss: 0.1506096783:
16: 89632: loss: 0.1506060860:
16: 92832: loss: 0.1504070141:
16: 96032: loss: 0.1507741246:
16: 99232: loss: 0.1509179000:
16: 102432: loss: 0.1510567396:
16: 105632: loss: 0.1509870964:
16: 108832: loss: 0.1509285747:
16: 112032: loss: 0.1508921982:
16: 115232: loss: 0.1506166871:
16: 118432: loss: 0.1503613443:
16: 121632: loss: 0.1502081787:
Dev-Acc: 16: Accuracy: 0.8872539401: precision: 0.2543026174: recall: 0.4824009522: f1: 0.3330398544
Train-Acc: 16: Accuracy: 0.9434620142: precision: 0.8559343758: recall: 0.6585365854: f1: 0.7443709594
17: 3232: loss: 0.1504272668:
17: 6432: loss: 0.1499572731:
17: 9632: loss: 0.1477273397:
17: 12832: loss: 0.1525999362:
17: 16032: loss: 0.1507209531:
17: 19232: loss: 0.1533689156:
17: 22432: loss: 0.1545797307:
17: 25632: loss: 0.1542157929:
17: 28832: loss: 0.1534271734:
17: 32032: loss: 0.1519485272:
17: 35232: loss: 0.1527927917:
17: 38432: loss: 0.1527180571:
17: 41632: loss: 0.1509663858:
17: 44832: loss: 0.1508772139:
17: 48032: loss: 0.1503204259:
17: 51232: loss: 0.1503726752:
17: 54432: loss: 0.1510887836:
17: 57632: loss: 0.1514888891:
17: 60832: loss: 0.1512067553:
17: 64032: loss: 0.1508236163:
17: 67232: loss: 0.1510376099:
17: 70432: loss: 0.1507839747:
17: 73632: loss: 0.1496158411:
17: 76832: loss: 0.1499976195:
17: 80032: loss: 0.1499310752:
17: 83232: loss: 0.1507102916:
17: 86432: loss: 0.1497738515:
17: 89632: loss: 0.1498118418:
17: 92832: loss: 0.1496789935:
17: 96032: loss: 0.1495292183:
17: 99232: loss: 0.1494356336:
17: 102432: loss: 0.1489436706:
17: 105632: loss: 0.1484256355:
17: 108832: loss: 0.1484582310:
17: 112032: loss: 0.1486328257:
17: 115232: loss: 0.1485837297:
17: 118432: loss: 0.1482278793:
17: 121632: loss: 0.1479901417:
Dev-Acc: 17: Accuracy: 0.8850511909: precision: 0.2494729445: recall: 0.4829110695: f1: 0.3289892847
Train-Acc: 17: Accuracy: 0.9447357655: precision: 0.8587250592: recall: 0.6677404510: f1: 0.7512851807
18: 3232: loss: 0.1548227895:
18: 6432: loss: 0.1493386968:
18: 9632: loss: 0.1435098103:
18: 12832: loss: 0.1457697196:
18: 16032: loss: 0.1452087134:
18: 19232: loss: 0.1465413162:
18: 22432: loss: 0.1463167556:
18: 25632: loss: 0.1467017518:
18: 28832: loss: 0.1464423721:
18: 32032: loss: 0.1466241005:
18: 35232: loss: 0.1478580064:
18: 38432: loss: 0.1479862865:
18: 41632: loss: 0.1477130534:
18: 44832: loss: 0.1473138283:
18: 48032: loss: 0.1471321781:
18: 51232: loss: 0.1469567225:
18: 54432: loss: 0.1468830911:
18: 57632: loss: 0.1473027400:
18: 60832: loss: 0.1468293423:
18: 64032: loss: 0.1465941253:
18: 67232: loss: 0.1470648132:
18: 70432: loss: 0.1465761868:
18: 73632: loss: 0.1464946942:
18: 76832: loss: 0.1467547232:
18: 80032: loss: 0.1465365535:
18: 83232: loss: 0.1460299716:
18: 86432: loss: 0.1457435975:
18: 89632: loss: 0.1459371679:
18: 92832: loss: 0.1455788522:
18: 96032: loss: 0.1452135929:
18: 99232: loss: 0.1454104908:
18: 102432: loss: 0.1453410018:
18: 105632: loss: 0.1454940375:
18: 108832: loss: 0.1453235332:
18: 112032: loss: 0.1457016401:
18: 115232: loss: 0.1457207103:
18: 118432: loss: 0.1459447124:
18: 121632: loss: 0.1455358282:
Dev-Acc: 18: Accuracy: 0.8792367578: precision: 0.2405543640: recall: 0.4958340418: f1: 0.3239460090
Train-Acc: 18: Accuracy: 0.9459355474: precision: 0.8549925975: recall: 0.6833870225: f1: 0.7596185465
19: 3232: loss: 0.1457240916:
19: 6432: loss: 0.1419142427:
19: 9632: loss: 0.1411550368:
19: 12832: loss: 0.1424862167:
19: 16032: loss: 0.1434643905:
19: 19232: loss: 0.1432693524:
19: 22432: loss: 0.1428901544:
19: 25632: loss: 0.1428841982:
19: 28832: loss: 0.1439157437:
19: 32032: loss: 0.1446851750:
19: 35232: loss: 0.1445810564:
19: 38432: loss: 0.1435289836:
19: 41632: loss: 0.1434924223:
19: 44832: loss: 0.1431030457:
19: 48032: loss: 0.1428971908:
19: 51232: loss: 0.1428413779:
19: 54432: loss: 0.1426436114:
19: 57632: loss: 0.1435150015:
19: 60832: loss: 0.1430306165:
19: 64032: loss: 0.1439523906:
19: 67232: loss: 0.1436495327:
19: 70432: loss: 0.1435982148:
19: 73632: loss: 0.1441903901:
19: 76832: loss: 0.1438658791:
19: 80032: loss: 0.1436355729:
19: 83232: loss: 0.1439471144:
19: 86432: loss: 0.1438777840:
19: 89632: loss: 0.1439001105:
19: 92832: loss: 0.1435597322:
19: 96032: loss: 0.1436582798:
19: 99232: loss: 0.1437844701:
19: 102432: loss: 0.1439109429:
19: 105632: loss: 0.1435763938:
19: 108832: loss: 0.1434142768:
19: 112032: loss: 0.1433421784:
19: 115232: loss: 0.1436420598:
19: 118432: loss: 0.1432234746:
19: 121632: loss: 0.1430943228:
Dev-Acc: 19: Accuracy: 0.8798320889: precision: 0.2413649950: recall: 0.4943036898: f1: 0.3243514644
Train-Acc: 19: Accuracy: 0.9465107918: precision: 0.8582839262: recall: 0.6852277957: f1: 0.7620544690
20: 3232: loss: 0.1526737705:
20: 6432: loss: 0.1465025355:
20: 9632: loss: 0.1435549073:
20: 12832: loss: 0.1433809271:
20: 16032: loss: 0.1413315891:
20: 19232: loss: 0.1419730587:
20: 22432: loss: 0.1419387414:
20: 25632: loss: 0.1418463053:
20: 28832: loss: 0.1429508623:
20: 32032: loss: 0.1426959569:
20: 35232: loss: 0.1433113280:
20: 38432: loss: 0.1427945372:
20: 41632: loss: 0.1418650580:
20: 44832: loss: 0.1410380167:
20: 48032: loss: 0.1411192188:
20: 51232: loss: 0.1409325566:
20: 54432: loss: 0.1415984116:
20: 57632: loss: 0.1421450364:
20: 60832: loss: 0.1424303021:
20: 64032: loss: 0.1421477173:
20: 67232: loss: 0.1427156259:
20: 70432: loss: 0.1425089809:
20: 73632: loss: 0.1428679944:
20: 76832: loss: 0.1430618682:
20: 80032: loss: 0.1426726275:
20: 83232: loss: 0.1420389905:
20: 86432: loss: 0.1421913275:
20: 89632: loss: 0.1425295340:
20: 92832: loss: 0.1424596975:
20: 96032: loss: 0.1422340926:
20: 99232: loss: 0.1423020153:
20: 102432: loss: 0.1420690359:
20: 105632: loss: 0.1421771745:
20: 108832: loss: 0.1419721859:
20: 112032: loss: 0.1418656615:
20: 115232: loss: 0.1413029029:
20: 118432: loss: 0.1405268504:
20: 121632: loss: 0.1407788676:
Dev-Acc: 20: Accuracy: 0.8829079866: precision: 0.2431447414: recall: 0.4764495834: f1: 0.3219764436
Train-Acc: 20: Accuracy: 0.9465765357: precision: 0.8714602525: recall: 0.6716849648: f1: 0.7586411732
