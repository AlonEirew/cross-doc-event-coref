1: 3232: loss: 0.6916957784:
1: 6432: loss: 0.6910968381:
1: 9632: loss: 0.6908201647:
1: 12832: loss: 0.6904682307:
1: 16032: loss: 0.6902152098:
1: 19232: loss: 0.6898973339:
1: 22432: loss: 0.6895704484:
1: 25632: loss: 0.6891965654:
1: 28832: loss: 0.6888950582:
1: 32032: loss: 0.6885211326:
1: 35232: loss: 0.6881776743:
1: 38432: loss: 0.6878577446:
1: 41632: loss: 0.6875349581:
1: 44832: loss: 0.6871882876:
1: 48032: loss: 0.6867997847:
1: 51232: loss: 0.6864437129:
1: 54432: loss: 0.6861290691:
1: 57632: loss: 0.6857919769:
1: 60832: loss: 0.6854615347:
1: 64032: loss: 0.6851700924:
1: 67232: loss: 0.6848316569:
1: 70432: loss: 0.6844754558:
1: 73632: loss: 0.6841635493:
1: 76832: loss: 0.6838463279:
1: 80032: loss: 0.6834922894:
1: 83232: loss: 0.6831436557:
1: 86432: loss: 0.6827938243:
1: 89632: loss: 0.6824590002:
1: 92832: loss: 0.6821532958:
1: 96032: loss: 0.6818162387:
1: 99232: loss: 0.6814686113:
1: 102432: loss: 0.6811243025:
1: 105632: loss: 0.6807646786:
1: 108832: loss: 0.6803881309:
1: 112032: loss: 0.6800398534:
1: 115232: loss: 0.6797165128:
1: 118432: loss: 0.6793680150:
1: 121632: loss: 0.6790462111:
1: 124832: loss: 0.6787064027:
1: 128032: loss: 0.6783707405:
1: 131232: loss: 0.6780591648:
1: 134432: loss: 0.6777287042:
1: 137632: loss: 0.6774298710:
1: 140832: loss: 0.6770887538:
1: 144032: loss: 0.6767684420:
1: 147232: loss: 0.6764451667:
1: 150432: loss: 0.6761229494:
1: 153632: loss: 0.6757642715:
1: 156832: loss: 0.6754745597:
1: 160032: loss: 0.6751315547:
1: 163232: loss: 0.6748157783:
1: 166432: loss: 0.6744669968:
1: 169632: loss: 0.6741197675:
1: 172832: loss: 0.6737910053:
1: 176032: loss: 0.6734476507:
1: 179232: loss: 0.6730852045:
1: 182432: loss: 0.6727298032:
1: 185632: loss: 0.6724145331:
1: 188832: loss: 0.6720735795:
1: 192032: loss: 0.6717391672:
1: 195232: loss: 0.6714281560:
1: 198432: loss: 0.6711024235:
1: 201632: loss: 0.6707629021:
1: 204832: loss: 0.6704369311:
1: 208032: loss: 0.6701251616:
1: 211232: loss: 0.6698106464:
1: 214432: loss: 0.6694896224:
1: 217632: loss: 0.6691584115:
1: 220832: loss: 0.6688340432:
1: 224032: loss: 0.6685002527:
1: 227232: loss: 0.6681637841:
1: 230432: loss: 0.6678389034:
1: 233632: loss: 0.6674959539:
1: 236832: loss: 0.6671507045:
1: 240032: loss: 0.6668263120:
1: 243232: loss: 0.6665164434:
1: 246432: loss: 0.6661634371:
1: 249632: loss: 0.6658451582:
1: 252832: loss: 0.6655174913:
1: 256032: loss: 0.6651991541:
1: 259232: loss: 0.6648613885:
1: 262432: loss: 0.6644913825:
1: 265632: loss: 0.6641536796:
1: 268832: loss: 0.6638347294:
1: 272032: loss: 0.6634661215:
1: 275232: loss: 0.6631360844:
1: 278432: loss: 0.6628049329:
1: 281632: loss: 0.6624754234:
1: 284832: loss: 0.6621163565:
1: 288032: loss: 0.6617753580:
1: 291232: loss: 0.6614390070:
1: 294432: loss: 0.6610849898:
1: 297632: loss: 0.6607576885:
1: 300832: loss: 0.6604330300:
1: 304032: loss: 0.6600866706:
1: 307232: loss: 0.6597698799:
1: 310432: loss: 0.6594233466:
1: 313632: loss: 0.6590654704:
1: 316832: loss: 0.6587135181:
1: 320032: loss: 0.6583855213:
Dev-Acc: 1: Accuracy: 0.6609878540: precision: 0.6300398590: recall: 0.6849868745: f1: 0.6563654149
Train-Acc: 1: Accuracy: 0.8444989324: precision: 0.9505577090: recall: 0.7088388556: f1: 0.8120930861
2: 3232: loss: 0.6227327192:
2: 6432: loss: 0.6217809629:
2: 9632: loss: 0.6214425244:
2: 12832: loss: 0.6209147291:
2: 16032: loss: 0.6205573096:
2: 19232: loss: 0.6203504032:
2: 22432: loss: 0.6201541167:
2: 25632: loss: 0.6201980571:
2: 28832: loss: 0.6200311829:
2: 32032: loss: 0.6201932825:
2: 35232: loss: 0.6198372792:
2: 38432: loss: 0.6191320850:
2: 41632: loss: 0.6186478555:
2: 44832: loss: 0.6183156062:
2: 48032: loss: 0.6179716614:
2: 51232: loss: 0.6174405159:
2: 54432: loss: 0.6170122876:
2: 57632: loss: 0.6166832713:
2: 60832: loss: 0.6162825277:
2: 64032: loss: 0.6159820940:
2: 67232: loss: 0.6155801860:
2: 70432: loss: 0.6149843183:
2: 73632: loss: 0.6145866388:
2: 76832: loss: 0.6141722253:
2: 80032: loss: 0.6138600823:
2: 83232: loss: 0.6134085526:
2: 86432: loss: 0.6130566083:
2: 89632: loss: 0.6125853179:
2: 92832: loss: 0.6121728588:
2: 96032: loss: 0.6117322243:
2: 99232: loss: 0.6114245561:
2: 102432: loss: 0.6110385732:
2: 105632: loss: 0.6105409236:
2: 108832: loss: 0.6100864436:
2: 112032: loss: 0.6096711225:
2: 115232: loss: 0.6092476987:
2: 118432: loss: 0.6088004841:
2: 121632: loss: 0.6083841836:
2: 124832: loss: 0.6078626316:
2: 128032: loss: 0.6074769978:
2: 131232: loss: 0.6071123711:
2: 134432: loss: 0.6067111055:
2: 137632: loss: 0.6062422935:
2: 140832: loss: 0.6058714241:
2: 144032: loss: 0.6054786030:
2: 147232: loss: 0.6051054881:
2: 150432: loss: 0.6046573333:
2: 153632: loss: 0.6042757062:
2: 156832: loss: 0.6038695062:
2: 160032: loss: 0.6034885952:
2: 163232: loss: 0.6031101375:
2: 166432: loss: 0.6026838333:
2: 169632: loss: 0.6022529777:
2: 172832: loss: 0.6018532932:
2: 176032: loss: 0.6014471907:
2: 179232: loss: 0.6010750926:
2: 182432: loss: 0.6006342393:
2: 185632: loss: 0.6002608502:
2: 188832: loss: 0.5999319681:
2: 192032: loss: 0.5994960168:
2: 195232: loss: 0.5990951369:
2: 198432: loss: 0.5987168021:
2: 201632: loss: 0.5983350850:
2: 204832: loss: 0.5979623766:
2: 208032: loss: 0.5975110836:
2: 211232: loss: 0.5970987544:
2: 214432: loss: 0.5967094176:
2: 217632: loss: 0.5963313281:
2: 220832: loss: 0.5958921055:
2: 224032: loss: 0.5954210336:
2: 227232: loss: 0.5950067748:
2: 230432: loss: 0.5946030120:
2: 233632: loss: 0.5941341805:
2: 236832: loss: 0.5937766852:
2: 240032: loss: 0.5933528950:
2: 243232: loss: 0.5929287938:
2: 246432: loss: 0.5925480835:
2: 249632: loss: 0.5921536617:
2: 252832: loss: 0.5917301055:
2: 256032: loss: 0.5912903726:
2: 259232: loss: 0.5908520653:
2: 262432: loss: 0.5903883900:
2: 265632: loss: 0.5900237422:
2: 268832: loss: 0.5895978962:
2: 272032: loss: 0.5891582783:
2: 275232: loss: 0.5887397663:
2: 278432: loss: 0.5882267378:
2: 281632: loss: 0.5878312004:
2: 284832: loss: 0.5874093629:
2: 288032: loss: 0.5870339980:
2: 291232: loss: 0.5866079423:
2: 294432: loss: 0.5861612314:
2: 297632: loss: 0.5857393722:
2: 300832: loss: 0.5852902045:
2: 304032: loss: 0.5848963665:
2: 307232: loss: 0.5844256337:
2: 310432: loss: 0.5840405595:
2: 313632: loss: 0.5836012333:
2: 316832: loss: 0.5831705912:
2: 320032: loss: 0.5826904305:
Dev-Acc: 2: Accuracy: 0.6500374079: precision: 0.5989705789: recall: 0.7855327305: f1: 0.6796820075
Train-Acc: 2: Accuracy: 0.8848112226: precision: 0.9431777193: recall: 0.8055386229: f1: 0.8689414776
3: 3232: loss: 0.5368371695:
3: 6432: loss: 0.5345206828:
3: 9632: loss: 0.5346915073:
3: 12832: loss: 0.5360320865:
3: 16032: loss: 0.5352985706:
3: 19232: loss: 0.5350400280:
3: 22432: loss: 0.5348370957:
3: 25632: loss: 0.5356096578:
3: 28832: loss: 0.5348525106:
3: 32032: loss: 0.5345638891:
3: 35232: loss: 0.5343388424:
3: 38432: loss: 0.5338750764:
3: 41632: loss: 0.5337192756:
3: 44832: loss: 0.5334148146:
3: 48032: loss: 0.5330701181:
3: 51232: loss: 0.5325800399:
3: 54432: loss: 0.5322292598:
3: 57632: loss: 0.5318957208:
3: 60832: loss: 0.5313303412:
3: 64032: loss: 0.5307610253:
3: 67232: loss: 0.5302554325:
3: 70432: loss: 0.5299276838:
3: 73632: loss: 0.5294951804:
3: 76832: loss: 0.5293118493:
3: 80032: loss: 0.5289956407:
3: 83232: loss: 0.5284371064:
3: 86432: loss: 0.5277592900:
3: 89632: loss: 0.5268621137:
3: 92832: loss: 0.5263757832:
3: 96032: loss: 0.5258830891:
3: 99232: loss: 0.5254401021:
3: 102432: loss: 0.5250261743:
3: 105632: loss: 0.5245179649:
3: 108832: loss: 0.5239105330:
3: 112032: loss: 0.5234797523:
3: 115232: loss: 0.5230057742:
3: 118432: loss: 0.5224952964:
3: 121632: loss: 0.5221806323:
3: 124832: loss: 0.5217113180:
3: 128032: loss: 0.5213169634:
3: 131232: loss: 0.5208158711:
3: 134432: loss: 0.5203590031:
3: 137632: loss: 0.5198449381:
3: 140832: loss: 0.5192506715:
3: 144032: loss: 0.5186024513:
3: 147232: loss: 0.5180930561:
3: 150432: loss: 0.5176123012:
3: 153632: loss: 0.5171770728:
3: 156832: loss: 0.5168255578:
3: 160032: loss: 0.5163375312:
3: 163232: loss: 0.5159217691:
3: 166432: loss: 0.5155387586:
3: 169632: loss: 0.5152309542:
3: 172832: loss: 0.5148271025:
3: 176032: loss: 0.5143223500:
3: 179232: loss: 0.5138427139:
3: 182432: loss: 0.5133856630:
3: 185632: loss: 0.5129489332:
3: 188832: loss: 0.5123393660:
3: 192032: loss: 0.5119606510:
3: 195232: loss: 0.5114574306:
3: 198432: loss: 0.5110791679:
3: 201632: loss: 0.5106879624:
3: 204832: loss: 0.5101446842:
3: 208032: loss: 0.5097009213:
3: 211232: loss: 0.5092711964:
3: 214432: loss: 0.5089317517:
3: 217632: loss: 0.5084883260:
3: 220832: loss: 0.5079456594:
3: 224032: loss: 0.5075183688:
3: 227232: loss: 0.5070728703:
3: 230432: loss: 0.5065067700:
3: 233632: loss: 0.5060935093:
3: 236832: loss: 0.5055598523:
3: 240032: loss: 0.5051405402:
3: 243232: loss: 0.5046581002:
3: 246432: loss: 0.5042359893:
3: 249632: loss: 0.5037839396:
3: 252832: loss: 0.5033416050:
3: 256032: loss: 0.5028738148:
3: 259232: loss: 0.5024934977:
3: 262432: loss: 0.5019999979:
3: 265632: loss: 0.5014477497:
3: 268832: loss: 0.5010160273:
3: 272032: loss: 0.5005693192:
3: 275232: loss: 0.5001298142:
3: 278432: loss: 0.4997093117:
3: 281632: loss: 0.4992099191:
3: 284832: loss: 0.4987045931:
3: 288032: loss: 0.4982869804:
3: 291232: loss: 0.4978892646:
3: 294432: loss: 0.4974093613:
3: 297632: loss: 0.4969519225:
3: 300832: loss: 0.4964831832:
3: 304032: loss: 0.4959416631:
3: 307232: loss: 0.4954656851:
3: 310432: loss: 0.4950535874:
3: 313632: loss: 0.4945605647:
3: 316832: loss: 0.4940424096:
3: 320032: loss: 0.4936487377:
Dev-Acc: 3: Accuracy: 0.6086579561: precision: 0.5569250283: recall: 0.8416184008: f1: 0.6702950254
Train-Acc: 3: Accuracy: 0.9080745578: precision: 0.9403276857: recall: 0.8607019959: f1: 0.8987546670
