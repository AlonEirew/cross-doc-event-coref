ssh://alon_nlp@MLU-01-TitanXP.iil.intel.com:22/home/alon_nlp/venv/bin/python -u /home/alon_nlp/cross-doc-coref/src/pairwize_model/train.py
2019-11-15 10:04:45.493170: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-15 10:04:45.523932: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999980000 Hz
2019-11-15 10:04:45.526575: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5b839c0 executing computations on platform Host. Devices:
2019-11-15 10:04:45.526595: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
INFO:src.utils.dataset_utils:Create Features:SPLIT.TRAIN
INFO:src.utils.json_utils:Loading mentions from-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/WEC_Train_Event_gold_mentions.json
INFO:src.utils.json_utils:Mentions file-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/WEC_Train_Event_gold_mentions.json, took:1.3892 sec to load
INFO:src.utils.dataset_utils:Create pos/neg examples
INFO:src.utils.dataset_utils:pos-160580
INFO:src.utils.dataset_utils:neg-170460
INFO:src.utils.dataset_utils:Create Features:SPLIT.VALIDATION
INFO:src.utils.json_utils:Loading mentions from-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/WEC_Dev_Event_gold_mentions.json
INFO:src.utils.json_utils:Mentions file-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/WEC_Dev_Event_gold_mentions.json, took:0.1668 sec to load
INFO:src.utils.dataset_utils:Create pos/neg examples
INFO:src.utils.dataset_utils:pos-25400
INFO:src.utils.dataset_utils:neg-26955
INFO:__main__:1: 3232: loss: 0.6903026456:
INFO:__main__:1: 6432: loss: 0.6896773949:
INFO:__main__:1: 9632: loss: 0.6891655433:
INFO:__main__:1: 12832: loss: 0.6887252569:
INFO:__main__:1: 16032: loss: 0.6883795519:
INFO:__main__:1: 19232: loss: 0.6881782871:
INFO:__main__:1: 22432: loss: 0.6878716089:
INFO:__main__:1: 25632: loss: 0.6876055717:
INFO:__main__:1: 28832: loss: 0.6872792707:
INFO:__main__:1: 32032: loss: 0.6869992765:
INFO:__main__:1: 35232: loss: 0.6867198707:
INFO:__main__:1: 38432: loss: 0.6864618353:
INFO:__main__:1: 41632: loss: 0.6862062271:
INFO:__main__:1: 44832: loss: 0.6859122850:
INFO:__main__:1: 48032: loss: 0.6856036400:
INFO:__main__:1: 51232: loss: 0.6852956386:
INFO:__main__:1: 54432: loss: 0.6850422779:
INFO:__main__:1: 57632: loss: 0.6847119155:
INFO:__main__:1: 60832: loss: 0.6844171437:
INFO:__main__:1: 64032: loss: 0.6841034373:
INFO:__main__:1: 67232: loss: 0.6838443541:
INFO:__main__:1: 70432: loss: 0.6835479728:
INFO:__main__:1: 73632: loss: 0.6832543441:
INFO:__main__:1: 76832: loss: 0.6829716473:
INFO:__main__:1: 80032: loss: 0.6827094314:
INFO:__main__:1: 83232: loss: 0.6824110206:
INFO:__main__:1: 86432: loss: 0.6821172826:
INFO:__main__:1: 89632: loss: 0.6818214139:
INFO:__main__:1: 92832: loss: 0.6815523060:
INFO:__main__:1: 96032: loss: 0.6812685656:
INFO:__main__:1: 99232: loss: 0.6809566218:
INFO:__main__:1: 102432: loss: 0.6806891650:
INFO:__main__:1: 105632: loss: 0.6804254415:
INFO:__main__:1: 108832: loss: 0.6801735634:
INFO:__main__:1: 112032: loss: 0.6799042202:
INFO:__main__:1: 115232: loss: 0.6796416757:
INFO:__main__:1: 118432: loss: 0.6793388202:
INFO:__main__:1: 121632: loss: 0.6790453603:
INFO:__main__:1: 124832: loss: 0.6787510372:
INFO:__main__:1: 128032: loss: 0.6784759302:
INFO:__main__:1: 131232: loss: 0.6782119790:
INFO:__main__:1: 134432: loss: 0.6779420817:
INFO:__main__:1: 137632: loss: 0.6776884085:
INFO:__main__:1: 140832: loss: 0.6773822362:
INFO:__main__:1: 144032: loss: 0.6771359776:
INFO:__main__:1: 147232: loss: 0.6768494199:
INFO:__main__:1: 150432: loss: 0.6765375381:
INFO:__main__:1: 153632: loss: 0.6762529307:
INFO:__main__:1: 156832: loss: 0.6759665798:
INFO:__main__:1: 160032: loss: 0.6756848013:
INFO:__main__:1: 163232: loss: 0.6753806488:
INFO:__main__:1: 166432: loss: 0.6751095070:
INFO:__main__:1: 169632: loss: 0.6748308803:
INFO:__main__:1: 172832: loss: 0.6745269484:
INFO:__main__:1: 176032: loss: 0.6742455612:
INFO:__main__:1: 179232: loss: 0.6739589501:
INFO:__main__:1: 182432: loss: 0.6737141512:
INFO:__main__:1: 185632: loss: 0.6734378272:
INFO:__main__:1: 188832: loss: 0.6731443095:
INFO:__main__:1: 192032: loss: 0.6728707237:
INFO:__main__:1: 195232: loss: 0.6725363219:
INFO:__main__:1: 198432: loss: 0.6722347203:
INFO:__main__:1: 201632: loss: 0.6719485556:
INFO:__main__:1: 204832: loss: 0.6716402598:
INFO:__main__:1: 208032: loss: 0.6713584214:
INFO:__main__:1: 211232: loss: 0.6710872615:
INFO:__main__:1: 214432: loss: 0.6708056031:
INFO:__main__:1: 217632: loss: 0.6705147038:
INFO:__main__:1: 220832: loss: 0.6702525582:
INFO:__main__:1: 224032: loss: 0.6699607468:
INFO:__main__:1: 227232: loss: 0.6696531826:
INFO:__main__:1: 230432: loss: 0.6693784645:
INFO:__main__:1: 233632: loss: 0.6690780263:
INFO:__main__:1: 236832: loss: 0.6687988704:
INFO:__main__:1: 240032: loss: 0.6685454746:
INFO:__main__:1: 243232: loss: 0.6682420483:
INFO:__main__:1: 246432: loss: 0.6679242820:
INFO:__main__:1: 249632: loss: 0.6676527084:
INFO:__main__:1: 252832: loss: 0.6673383465:
INFO:__main__:1: 256032: loss: 0.6670514805:
INFO:__main__:1: 259232: loss: 0.6667514654:
INFO:__main__:1: 262432: loss: 0.6664386170:
INFO:__main__:1: 265632: loss: 0.6661440787:
INFO:__main__:1: 268832: loss: 0.6658675286:
INFO:__main__:1: 272032: loss: 0.6655610973:
INFO:__main__:1: 275232: loss: 0.6652469423:
INFO:__main__:1: 278432: loss: 0.6649621692:
INFO:__main__:1: 281632: loss: 0.6646793429:
INFO:__main__:1: 284832: loss: 0.6643516863:
INFO:__main__:1: 288032: loss: 0.6640692898:
INFO:__main__:1: 291232: loss: 0.6637707208:
INFO:__main__:1: 294432: loss: 0.6634876375:
INFO:__main__:1: 297632: loss: 0.6631924742:
INFO:__main__:1: 300832: loss: 0.6628890583:
INFO:__main__:1: 304032: loss: 0.6625836981:
INFO:__main__:1: 307232: loss: 0.6622914973:
INFO:__main__:1: 310432: loss: 0.6620069964:
INFO:__main__:1: 313632: loss: 0.6617107074:
INFO:__main__:1: 316832: loss: 0.6614340850:
INFO:__main__:1: 320032: loss: 0.6611524920:
INFO:__main__:1: 323232: loss: 0.6608746736:
INFO:__main__:1: 326432: loss: 0.6605825303:
INFO:__main__:1: 329632: loss: 0.6602930988:
INFO:__main__:Dev-Acc: 1: Accuracy: 0.6456307769: precision: 0.6240983072: recall: 0.6778346457: f1: 0.6498575122
INFO:__main__:Train-Acc: 1: Accuracy: 0.8361164927: precision: 0.9437877725: recall: 0.7040851912: f1: 0.8065026964
INFO:__main__:2: 3232: loss: 0.6311258018:
INFO:__main__:2: 6432: loss: 0.6287106106:
INFO:__main__:2: 9632: loss: 0.6285949405:
INFO:__main__:2: 12832: loss: 0.6277621888:
INFO:__main__:2: 16032: loss: 0.6274661845:
INFO:__main__:2: 19232: loss: 0.6273669443:
INFO:__main__:2: 22432: loss: 0.6269042780:
INFO:__main__:2: 25632: loss: 0.6265044974:
INFO:__main__:2: 28832: loss: 0.6263575141:
INFO:__main__:2: 32032: loss: 0.6260947934:
INFO:__main__:2: 35232: loss: 0.6258914120:
INFO:__main__:2: 38432: loss: 0.6255358833:
INFO:__main__:2: 41632: loss: 0.6253142760:
INFO:__main__:2: 44832: loss: 0.6250189947:
INFO:__main__:2: 48032: loss: 0.6245874026:
INFO:__main__:2: 51232: loss: 0.6244330296:
INFO:__main__:2: 54432: loss: 0.6241658434:
INFO:__main__:2: 57632: loss: 0.6237732260:
INFO:__main__:2: 60832: loss: 0.6234021436:
INFO:__main__:2: 64032: loss: 0.6230287417:
INFO:__main__:2: 67232: loss: 0.6226327153:
INFO:__main__:2: 70432: loss: 0.6223152875:
INFO:__main__:2: 73632: loss: 0.6219283821:
INFO:__main__:2: 76832: loss: 0.6215449956:
INFO:__main__:2: 80032: loss: 0.6211791549:
INFO:__main__:2: 83232: loss: 0.6207775511:
INFO:__main__:2: 86432: loss: 0.6204521902:
INFO:__main__:2: 89632: loss: 0.6199828953:
INFO:__main__:2: 92832: loss: 0.6196889120:
INFO:__main__:2: 96032: loss: 0.6194441789:
INFO:__main__:2: 99232: loss: 0.6190446919:
INFO:__main__:2: 102432: loss: 0.6187048453:
INFO:__main__:2: 105632: loss: 0.6183391901:
INFO:__main__:2: 108832: loss: 0.6179506353:
INFO:__main__:2: 112032: loss: 0.6175655392:
INFO:__main__:2: 115232: loss: 0.6171537940:
INFO:__main__:2: 118432: loss: 0.6168506010:
INFO:__main__:2: 121632: loss: 0.6164994608:
INFO:__main__:2: 124832: loss: 0.6160996812:
INFO:__main__:2: 128032: loss: 0.6157412202:
INFO:__main__:2: 131232: loss: 0.6153392913:
INFO:__main__:2: 134432: loss: 0.6150022024:
INFO:__main__:2: 137632: loss: 0.6146215965:
INFO:__main__:2: 140832: loss: 0.6143261666:
INFO:__main__:2: 144032: loss: 0.6139448649:
INFO:__main__:2: 147232: loss: 0.6135241175:
INFO:__main__:2: 150432: loss: 0.6132633888:
INFO:__main__:2: 153632: loss: 0.6128926626:
INFO:__main__:2: 156832: loss: 0.6125801474:
INFO:__main__:2: 160032: loss: 0.6122287926:
INFO:__main__:2: 163232: loss: 0.6119077814:
INFO:__main__:2: 166432: loss: 0.6115079423:
INFO:__main__:2: 169632: loss: 0.6111001357:
INFO:__main__:2: 172832: loss: 0.6107546232:
INFO:__main__:2: 176032: loss: 0.6104263859:
INFO:__main__:2: 179232: loss: 0.6100994271:
INFO:__main__:2: 182432: loss: 0.6097971139:
INFO:__main__:2: 185632: loss: 0.6094580705:
INFO:__main__:2: 188832: loss: 0.6090782334:
INFO:__main__:2: 192032: loss: 0.6087259535:
INFO:__main__:2: 195232: loss: 0.6084439012:
INFO:__main__:2: 198432: loss: 0.6080689394:
INFO:__main__:2: 201632: loss: 0.6077982794:
INFO:__main__:2: 204832: loss: 0.6074479601:
INFO:__main__:2: 208032: loss: 0.6071249261:
INFO:__main__:2: 211232: loss: 0.6067902089:
INFO:__main__:2: 214432: loss: 0.6064380414:
INFO:__main__:2: 217632: loss: 0.6060567266:
INFO:__main__:2: 220832: loss: 0.6057031287:
INFO:__main__:2: 224032: loss: 0.6053301890:
INFO:__main__:2: 227232: loss: 0.6050220369:
INFO:__main__:2: 230432: loss: 0.6046806112:
INFO:__main__:2: 233632: loss: 0.6042842755:
INFO:__main__:2: 236832: loss: 0.6039345904:
INFO:__main__:2: 240032: loss: 0.6035659150:
INFO:__main__:2: 243232: loss: 0.6031631037:
INFO:__main__:2: 246432: loss: 0.6027851937:
INFO:__main__:2: 249632: loss: 0.6024861046:
INFO:__main__:2: 252832: loss: 0.6020695689:
INFO:__main__:2: 256032: loss: 0.6016291558:
INFO:__main__:2: 259232: loss: 0.6012266910:
INFO:__main__:2: 262432: loss: 0.6008590551:
INFO:__main__:2: 265632: loss: 0.6004754110:
INFO:__main__:2: 268832: loss: 0.6001338228:
INFO:__main__:2: 272032: loss: 0.5997971371:
INFO:__main__:2: 275232: loss: 0.5993825621:
INFO:__main__:2: 278432: loss: 0.5989975854:
INFO:__main__:2: 281632: loss: 0.5986512928:
INFO:__main__:2: 284832: loss: 0.5982656693:
INFO:__main__:2: 288032: loss: 0.5979184058:
INFO:__main__:2: 291232: loss: 0.5975817928:
INFO:__main__:2: 294432: loss: 0.5972378336:
INFO:__main__:2: 297632: loss: 0.5968920643:
INFO:__main__:2: 300832: loss: 0.5965294969:
INFO:__main__:2: 304032: loss: 0.5962030560:
INFO:__main__:2: 307232: loss: 0.5958230230:
INFO:__main__:2: 310432: loss: 0.5954742104:
INFO:__main__:2: 313632: loss: 0.5950736780:
INFO:__main__:2: 316832: loss: 0.5946781709:
INFO:__main__:2: 320032: loss: 0.5943279456:
INFO:__main__:2: 323232: loss: 0.5939518702:
INFO:__main__:2: 326432: loss: 0.5936117711:
INFO:__main__:2: 329632: loss: 0.5932460597:
INFO:__main__:Dev-Acc: 2: Accuracy: 0.6194823980: precision: 0.5809014650: recall: 0.7742913386: f1: 0.6637977589
INFO:__main__:Train-Acc: 2: Accuracy: 0.8857660890: precision: 0.9570786048: recall: 0.8003985552: f1: 0.8717545240
INFO:__main__:3: 3232: loss: 0.5598790270:
INFO:__main__:3: 6432: loss: 0.5572275451:
INFO:__main__:3: 9632: loss: 0.5552585931:
INFO:__main__:3: 12832: loss: 0.5542354763:
INFO:__main__:3: 16032: loss: 0.5535505866:
INFO:__main__:3: 19232: loss: 0.5530841909:
INFO:__main__:3: 22432: loss: 0.5528249974:
INFO:__main__:3: 25632: loss: 0.5521080719:
INFO:__main__:3: 28832: loss: 0.5521285279:
INFO:__main__:3: 32032: loss: 0.5522594303:
INFO:__main__:3: 35232: loss: 0.5518896989:
INFO:__main__:3: 38432: loss: 0.5512084111:
INFO:__main__:3: 41632: loss: 0.5508326440:
INFO:__main__:3: 44832: loss: 0.5504125887:
INFO:__main__:3: 48032: loss: 0.5495869366:
INFO:__main__:3: 51232: loss: 0.5491324504:
INFO:__main__:3: 54432: loss: 0.5490459833:
INFO:__main__:3: 57632: loss: 0.5486753468:
INFO:__main__:3: 60832: loss: 0.5479996658:
INFO:__main__:3: 64032: loss: 0.5475885761:
INFO:__main__:3: 67232: loss: 0.5471936263:
INFO:__main__:3: 70432: loss: 0.5467769700:
INFO:__main__:3: 73632: loss: 0.5463514833:
INFO:__main__:3: 76832: loss: 0.5459255674:
INFO:__main__:3: 80032: loss: 0.5453653763:
INFO:__main__:3: 83232: loss: 0.5446341811:
INFO:__main__:3: 86432: loss: 0.5442253497:
INFO:__main__:3: 89632: loss: 0.5438692387:
INFO:__main__:3: 92832: loss: 0.5433914774:
INFO:__main__:3: 96032: loss: 0.5429630219:
INFO:__main__:3: 99232: loss: 0.5424923192:
INFO:__main__:3: 102432: loss: 0.5421558352:
INFO:__main__:3: 105632: loss: 0.5415935115:
INFO:__main__:3: 108832: loss: 0.5412551158:
INFO:__main__:3: 112032: loss: 0.5408641709:
INFO:__main__:3: 115232: loss: 0.5404746529:
INFO:__main__:3: 118432: loss: 0.5401931446:
INFO:__main__:3: 121632: loss: 0.5396886066:
INFO:__main__:3: 124832: loss: 0.5391954877:
INFO:__main__:3: 128032: loss: 0.5387650536:
INFO:__main__:3: 131232: loss: 0.5384099554:
INFO:__main__:3: 134432: loss: 0.5380667635:
INFO:__main__:3: 137632: loss: 0.5377154825:
INFO:__main__:3: 140832: loss: 0.5372955764:
INFO:__main__:3: 144032: loss: 0.5369453908:
INFO:__main__:3: 147232: loss: 0.5364623107:
INFO:__main__:3: 150432: loss: 0.5359982265:
INFO:__main__:3: 153632: loss: 0.5356635465:
INFO:__main__:3: 156832: loss: 0.5352569276:
INFO:__main__:3: 160032: loss: 0.5347360302:
INFO:__main__:3: 163232: loss: 0.5343538374:
INFO:__main__:3: 166432: loss: 0.5339780457:
INFO:__main__:3: 169632: loss: 0.5335327125:
INFO:__main__:3: 172832: loss: 0.5331758771:
INFO:__main__:3: 176032: loss: 0.5328966797:
INFO:__main__:3: 179232: loss: 0.5324784667:
INFO:__main__:3: 182432: loss: 0.5320685287:
INFO:__main__:3: 185632: loss: 0.5316874218:
INFO:__main__:3: 188832: loss: 0.5312717436:
INFO:__main__:3: 192032: loss: 0.5308617969:
INFO:__main__:3: 195232: loss: 0.5304166911:
INFO:__main__:3: 198432: loss: 0.5299869272:
INFO:__main__:3: 201632: loss: 0.5295462717:
INFO:__main__:3: 204832: loss: 0.5291823022:
INFO:__main__:3: 208032: loss: 0.5287596797:
INFO:__main__:3: 211232: loss: 0.5283257342:
INFO:__main__:3: 214432: loss: 0.5279591694:
INFO:__main__:3: 217632: loss: 0.5275401963:
INFO:__main__:3: 220832: loss: 0.5271909459:
INFO:__main__:3: 224032: loss: 0.5268438931:
INFO:__main__:3: 227232: loss: 0.5263823959:
INFO:__main__:3: 230432: loss: 0.5260256616:
INFO:__main__:3: 233632: loss: 0.5255851844:
INFO:__main__:3: 236832: loss: 0.5252274551:
INFO:__main__:3: 240032: loss: 0.5248581633:
INFO:__main__:3: 243232: loss: 0.5244259607:
INFO:__main__:3: 246432: loss: 0.5239949002:
INFO:__main__:3: 249632: loss: 0.5236665188:
INFO:__main__:3: 252832: loss: 0.5232243581:
INFO:__main__:3: 256032: loss: 0.5228325349:
INFO:__main__:3: 259232: loss: 0.5223835825:
INFO:__main__:3: 262432: loss: 0.5218866612:
INFO:__main__:3: 265632: loss: 0.5214771144:
INFO:__main__:3: 268832: loss: 0.5210649346:
INFO:__main__:3: 272032: loss: 0.5207435832:
INFO:__main__:3: 275232: loss: 0.5203343464:
INFO:__main__:3: 278432: loss: 0.5199253918:
INFO:__main__:3: 281632: loss: 0.5195511705:
INFO:__main__:3: 284832: loss: 0.5190668845:
INFO:__main__:3: 288032: loss: 0.5186395330:
INFO:__main__:3: 291232: loss: 0.5182612351:
INFO:__main__:3: 294432: loss: 0.5177798929:
INFO:__main__:3: 297632: loss: 0.5174709035:
INFO:__main__:3: 300832: loss: 0.5170855101:
INFO:__main__:3: 304032: loss: 0.5166842139:
INFO:__main__:3: 307232: loss: 0.5162277898:
INFO:__main__:3: 310432: loss: 0.5158390701:
INFO:__main__:3: 313632: loss: 0.5153909146:
INFO:__main__:3: 316832: loss: 0.5150001278:
INFO:__main__:3: 320032: loss: 0.5146058150:
INFO:__main__:3: 323232: loss: 0.5142584502:
INFO:__main__:3: 326432: loss: 0.5137979101:
INFO:__main__:3: 329632: loss: 0.5133475442:
INFO:__main__:Dev-Acc: 3: Accuracy: 0.5950530171: precision: 0.5548014930: recall: 0.8368110236: f1: 0.6672317182
INFO:__main__:Train-Acc: 3: Accuracy: 0.9127718806: precision: 0.9559756266: recall: 0.8597708307: f1: 0.9053245902

Process finished with exit code 0
