ssh://alon_nlp@MLU-01-TitanXP.iil.intel.com:22/home/alon_nlp/venv/bin/python -u /home/alon_nlp/cross-doc-coref/src/pairwize_model/train.py
2019-11-15 09:53:50.460375: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-15 09:53:50.488009: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999980000 Hz
2019-11-15 09:53:50.490926: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x54d59c0 executing computations on platform Host. Devices:
2019-11-15 09:53:50.490961: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
INFO:src.utils.dataset_utils:Create Features:SPLIT.TRAIN
INFO:src.utils.json_utils:Loading mentions from-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/Min_WEC_Train_Event_gold_mentions.json
INFO:src.utils.json_utils:Mentions file-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/Min_WEC_Train_Event_gold_mentions.json, took:1.5051 sec to load
INFO:src.utils.dataset_utils:Create pos/neg examples
INFO:src.utils.dataset_utils:pos-160580
INFO:src.utils.dataset_utils:neg-170460
INFO:src.utils.dataset_utils:Create Features:SPLIT.VALIDATION
INFO:src.utils.json_utils:Loading mentions from-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/Min_WEC_Dev_Event_gold_mentions.json
INFO:src.utils.json_utils:Mentions file-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/Min_WEC_Dev_Event_gold_mentions.json, took:0.2053 sec to load
INFO:src.utils.dataset_utils:Create pos/neg examples
INFO:src.utils.dataset_utils:pos-25400
INFO:src.utils.dataset_utils:neg-26955
INFO:__main__:1: 3232: loss: 0.6921867400:
INFO:__main__:1: 6432: loss: 0.6913879567:
INFO:__main__:1: 9632: loss: 0.6910248250:
INFO:__main__:1: 12832: loss: 0.6905069900:
INFO:__main__:1: 16032: loss: 0.6901762882:
INFO:__main__:1: 19232: loss: 0.6899910841:
INFO:__main__:1: 22432: loss: 0.6896514035:
INFO:__main__:1: 25632: loss: 0.6893284004:
INFO:__main__:1: 28832: loss: 0.6889199091:
INFO:__main__:1: 32032: loss: 0.6885687533:
INFO:__main__:1: 35232: loss: 0.6882421346:
INFO:__main__:1: 38432: loss: 0.6879445742:
INFO:__main__:1: 41632: loss: 0.6876361748:
INFO:__main__:1: 44832: loss: 0.6873123932:
INFO:__main__:1: 48032: loss: 0.6869745048:
INFO:__main__:1: 51232: loss: 0.6866572177:
INFO:__main__:1: 54432: loss: 0.6863794806:
INFO:__main__:1: 57632: loss: 0.6859966629:
INFO:__main__:1: 60832: loss: 0.6856620264:
INFO:__main__:1: 64032: loss: 0.6853033249:
INFO:__main__:1: 67232: loss: 0.6849979687:
INFO:__main__:1: 70432: loss: 0.6846606819:
INFO:__main__:1: 73632: loss: 0.6843217921:
INFO:__main__:1: 76832: loss: 0.6840137328:
INFO:__main__:1: 80032: loss: 0.6837177135:
INFO:__main__:1: 83232: loss: 0.6833887492:
INFO:__main__:1: 86432: loss: 0.6830702402:
INFO:__main__:1: 89632: loss: 0.6827355975:
INFO:__main__:1: 92832: loss: 0.6824432268:
INFO:__main__:1: 96032: loss: 0.6821155465:
INFO:__main__:1: 99232: loss: 0.6817712802:
INFO:__main__:1: 102432: loss: 0.6814634624:
INFO:__main__:1: 105632: loss: 0.6811491219:
INFO:__main__:1: 108832: loss: 0.6808596202:
INFO:__main__:1: 112032: loss: 0.6805606132:
INFO:__main__:1: 115232: loss: 0.6802596573:
INFO:__main__:1: 118432: loss: 0.6799110765:
INFO:__main__:1: 121632: loss: 0.6795776409:
INFO:__main__:1: 124832: loss: 0.6792464538:
INFO:__main__:1: 128032: loss: 0.6789375511:
INFO:__main__:1: 131232: loss: 0.6786292708:
INFO:__main__:1: 134432: loss: 0.6783172128:
INFO:__main__:1: 137632: loss: 0.6780149202:
INFO:__main__:1: 140832: loss: 0.6776797544:
INFO:__main__:1: 144032: loss: 0.6773887718:
INFO:__main__:1: 147232: loss: 0.6770602368:
INFO:__main__:1: 150432: loss: 0.6767173875:
INFO:__main__:1: 153632: loss: 0.6764016459:
INFO:__main__:1: 156832: loss: 0.6760795839:
INFO:__main__:1: 160032: loss: 0.6757579450:
INFO:__main__:1: 163232: loss: 0.6754218221:
INFO:__main__:1: 166432: loss: 0.6751110796:
INFO:__main__:1: 169632: loss: 0.6747949089:
INFO:__main__:1: 172832: loss: 0.6744591032:
INFO:__main__:1: 176032: loss: 0.6741387125:
INFO:__main__:1: 179232: loss: 0.6738217292:
INFO:__main__:1: 182432: loss: 0.6735243998:
INFO:__main__:1: 185632: loss: 0.6732154869:
INFO:__main__:1: 188832: loss: 0.6728923668:
INFO:__main__:1: 192032: loss: 0.6725813579:
INFO:__main__:1: 195232: loss: 0.6722096892:
INFO:__main__:1: 198432: loss: 0.6718748900:
INFO:__main__:1: 201632: loss: 0.6715472820:
INFO:__main__:1: 204832: loss: 0.6712044651:
INFO:__main__:1: 208032: loss: 0.6708768330:
INFO:__main__:1: 211232: loss: 0.6705620638:
INFO:__main__:1: 214432: loss: 0.6702441873:
INFO:__main__:1: 217632: loss: 0.6699054369:
INFO:__main__:1: 220832: loss: 0.6695948751:
INFO:__main__:1: 224032: loss: 0.6692604608:
INFO:__main__:1: 227232: loss: 0.6689143989:
INFO:__main__:1: 230432: loss: 0.6685962193:
INFO:__main__:1: 233632: loss: 0.6682575790:
INFO:__main__:1: 236832: loss: 0.6679246451:
INFO:__main__:1: 240032: loss: 0.6676366262:
INFO:__main__:1: 243232: loss: 0.6672988452:
INFO:__main__:1: 246432: loss: 0.6669468216:
INFO:__main__:1: 249632: loss: 0.6666380997:
INFO:__main__:1: 252832: loss: 0.6662935870:
INFO:__main__:1: 256032: loss: 0.6659742004:
INFO:__main__:1: 259232: loss: 0.6656430624:
INFO:__main__:1: 262432: loss: 0.6652971257:
INFO:__main__:1: 265632: loss: 0.6649579937:
INFO:__main__:1: 268832: loss: 0.6646396294:
INFO:__main__:1: 272032: loss: 0.6643019677:
INFO:__main__:1: 275232: loss: 0.6639590509:
INFO:__main__:1: 278432: loss: 0.6636284379:
INFO:__main__:1: 281632: loss: 0.6633066037:
INFO:__main__:1: 284832: loss: 0.6629509040:
INFO:__main__:1: 288032: loss: 0.6626296312:
INFO:__main__:1: 291232: loss: 0.6622887261:
INFO:__main__:1: 294432: loss: 0.6619711710:
INFO:__main__:1: 297632: loss: 0.6616407895:
INFO:__main__:1: 300832: loss: 0.6612999728:
INFO:__main__:1: 304032: loss: 0.6609583899:
INFO:__main__:1: 307232: loss: 0.6606282175:
INFO:__main__:1: 310432: loss: 0.6603023843:
INFO:__main__:1: 313632: loss: 0.6599708883:
INFO:__main__:1: 316832: loss: 0.6596554440:
INFO:__main__:1: 320032: loss: 0.6593185286:
INFO:__main__:1: 323232: loss: 0.6589890823:
INFO:__main__:1: 326432: loss: 0.6586482641:
INFO:__main__:1: 329632: loss: 0.6583193401:
INFO:__main__:Dev-Acc: 1: Accuracy: 0.6579696536: precision: 0.6279149169: recall: 0.7240551181: f1: 0.6725666953
INFO:__main__:Train-Acc: 1: Accuracy: 0.8534829617: precision: 0.9400318804: recall: 0.7455100262: f1: 0.8315464469
INFO:__main__:2: 3232: loss: 0.6247252983:
INFO:__main__:2: 6432: loss: 0.6221564576:
INFO:__main__:2: 9632: loss: 0.6218997991:
INFO:__main__:2: 12832: loss: 0.6208064573:
INFO:__main__:2: 16032: loss: 0.6205323673:
INFO:__main__:2: 19232: loss: 0.6205313737:
INFO:__main__:2: 22432: loss: 0.6201958299:
INFO:__main__:2: 25632: loss: 0.6196756774:
INFO:__main__:2: 28832: loss: 0.6195507862:
INFO:__main__:2: 32032: loss: 0.6192877641:
INFO:__main__:2: 35232: loss: 0.6189719805:
INFO:__main__:2: 38432: loss: 0.6186179062:
INFO:__main__:2: 41632: loss: 0.6184238266:
INFO:__main__:2: 44832: loss: 0.6180497445:
INFO:__main__:2: 48032: loss: 0.6175283175:
INFO:__main__:2: 51232: loss: 0.6173217640:
INFO:__main__:2: 54432: loss: 0.6169904834:
INFO:__main__:2: 57632: loss: 0.6165119916:
INFO:__main__:2: 60832: loss: 0.6160853473:
INFO:__main__:2: 64032: loss: 0.6156010071:
INFO:__main__:2: 67232: loss: 0.6151401689:
INFO:__main__:2: 70432: loss: 0.6147560404:
INFO:__main__:2: 73632: loss: 0.6143863164:
INFO:__main__:2: 76832: loss: 0.6139796863:
INFO:__main__:2: 80032: loss: 0.6135730121:
INFO:__main__:2: 83232: loss: 0.6131070636:
INFO:__main__:2: 86432: loss: 0.6127582422:
INFO:__main__:2: 89632: loss: 0.6122545837:
INFO:__main__:2: 92832: loss: 0.6119064831:
INFO:__main__:2: 96032: loss: 0.6115523440:
INFO:__main__:2: 99232: loss: 0.6110905266:
INFO:__main__:2: 102432: loss: 0.6106667573:
INFO:__main__:2: 105632: loss: 0.6102805874:
INFO:__main__:2: 108832: loss: 0.6098796968:
INFO:__main__:2: 112032: loss: 0.6094597169:
INFO:__main__:2: 115232: loss: 0.6089801769:
INFO:__main__:2: 118432: loss: 0.6085933397:
INFO:__main__:2: 121632: loss: 0.6082345813:
INFO:__main__:2: 124832: loss: 0.6077998577:
INFO:__main__:2: 128032: loss: 0.6073435141:
INFO:__main__:2: 131232: loss: 0.6068913228:
INFO:__main__:2: 134432: loss: 0.6064723222:
INFO:__main__:2: 137632: loss: 0.6060511480:
INFO:__main__:2: 140832: loss: 0.6056483937:
INFO:__main__:2: 144032: loss: 0.6052043919:
INFO:__main__:2: 147232: loss: 0.6047439509:
INFO:__main__:2: 150432: loss: 0.6044216616:
INFO:__main__:2: 153632: loss: 0.6039870799:
INFO:__main__:2: 156832: loss: 0.6036441551:
INFO:__main__:2: 160032: loss: 0.6032310346:
INFO:__main__:2: 163232: loss: 0.6028779897:
INFO:__main__:2: 166432: loss: 0.6024150934:
INFO:__main__:2: 169632: loss: 0.6019392465:
INFO:__main__:2: 172832: loss: 0.6015592392:
INFO:__main__:2: 176032: loss: 0.6011828713:
INFO:__main__:2: 179232: loss: 0.6007932867:
INFO:__main__:2: 182432: loss: 0.6004495212:
INFO:__main__:2: 185632: loss: 0.6000405019:
INFO:__main__:2: 188832: loss: 0.5995905882:
INFO:__main__:2: 192032: loss: 0.5991914774:
INFO:__main__:2: 195232: loss: 0.5988186440:
INFO:__main__:2: 198432: loss: 0.5984180397:
INFO:__main__:2: 201632: loss: 0.5981101348:
INFO:__main__:2: 204832: loss: 0.5977049312:
INFO:__main__:2: 208032: loss: 0.5973419819:
INFO:__main__:2: 211232: loss: 0.5969506348:
INFO:__main__:2: 214432: loss: 0.5965524611:
INFO:__main__:2: 217632: loss: 0.5961459966:
INFO:__main__:2: 220832: loss: 0.5957430478:
INFO:__main__:2: 224032: loss: 0.5953172177:
INFO:__main__:2: 227232: loss: 0.5949609768:
INFO:__main__:2: 230432: loss: 0.5945625404:
INFO:__main__:2: 233632: loss: 0.5941039841:
INFO:__main__:2: 236832: loss: 0.5937086005:
INFO:__main__:2: 240032: loss: 0.5932864399:
INFO:__main__:2: 243232: loss: 0.5928462839:
INFO:__main__:2: 246432: loss: 0.5923793834:
INFO:__main__:2: 249632: loss: 0.5920050065:
INFO:__main__:2: 252832: loss: 0.5915343605:
INFO:__main__:2: 256032: loss: 0.5910328275:
INFO:__main__:2: 259232: loss: 0.5905920831:
INFO:__main__:2: 262432: loss: 0.5901755007:
INFO:__main__:2: 265632: loss: 0.5897359330:
INFO:__main__:2: 268832: loss: 0.5893176156:
INFO:__main__:2: 272032: loss: 0.5889433792:
INFO:__main__:2: 275232: loss: 0.5884730433:
INFO:__main__:2: 278432: loss: 0.5880257507:
INFO:__main__:2: 281632: loss: 0.5876279330:
INFO:__main__:2: 284832: loss: 0.5871610082:
INFO:__main__:2: 288032: loss: 0.5867626068:
INFO:__main__:2: 291232: loss: 0.5863471438:
INFO:__main__:2: 294432: loss: 0.5859375630:
INFO:__main__:2: 297632: loss: 0.5855472499:
INFO:__main__:2: 300832: loss: 0.5851372769:
INFO:__main__:2: 304032: loss: 0.5847310238:
INFO:__main__:2: 307232: loss: 0.5842796767:
INFO:__main__:2: 310432: loss: 0.5838718681:
INFO:__main__:2: 313632: loss: 0.5834223354:
INFO:__main__:2: 316832: loss: 0.5829651044:
INFO:__main__:2: 320032: loss: 0.5825617135:
INFO:__main__:2: 323232: loss: 0.5821369750:
INFO:__main__:2: 326432: loss: 0.5817514393:
INFO:__main__:2: 329632: loss: 0.5813287009:
INFO:__main__:Dev-Acc: 2: Accuracy: 0.6304078102: precision: 0.5866067339: recall: 0.8066535433: f1: 0.6792534147
INFO:__main__:Train-Acc: 2: Accuracy: 0.8876479864: precision: 0.9334569905: recall: 0.8273633080: f1: 0.8772139487
INFO:__main__:3: 3232: loss: 0.5417818549:
INFO:__main__:3: 6432: loss: 0.5388437906:
INFO:__main__:3: 9632: loss: 0.5369596148:
INFO:__main__:3: 12832: loss: 0.5359749229:
INFO:__main__:3: 16032: loss: 0.5351784176:
INFO:__main__:3: 19232: loss: 0.5345375468:
INFO:__main__:3: 22432: loss: 0.5341482661:
INFO:__main__:3: 25632: loss: 0.5334003548:
INFO:__main__:3: 28832: loss: 0.5330731882:
INFO:__main__:3: 32032: loss: 0.5332254325:
INFO:__main__:3: 35232: loss: 0.5330261939:
INFO:__main__:3: 38432: loss: 0.5322929357:
INFO:__main__:3: 41632: loss: 0.5318021634:
INFO:__main__:3: 44832: loss: 0.5313923756:
INFO:__main__:3: 48032: loss: 0.5305463099:
INFO:__main__:3: 51232: loss: 0.5300442255:
INFO:__main__:3: 54432: loss: 0.5299322582:
INFO:__main__:3: 57632: loss: 0.5294338616:
INFO:__main__:3: 60832: loss: 0.5287580761:
INFO:__main__:3: 64032: loss: 0.5283124997:
INFO:__main__:3: 67232: loss: 0.5278676833:
INFO:__main__:3: 70432: loss: 0.5272716150:
INFO:__main__:3: 73632: loss: 0.5268982527:
INFO:__main__:3: 76832: loss: 0.5264218237:
INFO:__main__:3: 80032: loss: 0.5258552147:
INFO:__main__:3: 83232: loss: 0.5251215392:
INFO:__main__:3: 86432: loss: 0.5246357595:
INFO:__main__:3: 89632: loss: 0.5241460590:
INFO:__main__:3: 92832: loss: 0.5236338676:
INFO:__main__:3: 96032: loss: 0.5231171886:
INFO:__main__:3: 99232: loss: 0.5226664175:
INFO:__main__:3: 102432: loss: 0.5223196347:
INFO:__main__:3: 105632: loss: 0.5216891953:
INFO:__main__:3: 108832: loss: 0.5212345751:
INFO:__main__:3: 112032: loss: 0.5207930601:
INFO:__main__:3: 115232: loss: 0.5204333806:
INFO:__main__:3: 118432: loss: 0.5201070722:
INFO:__main__:3: 121632: loss: 0.5196004207:
INFO:__main__:3: 124832: loss: 0.5190698130:
INFO:__main__:3: 128032: loss: 0.5185708267:
INFO:__main__:3: 131232: loss: 0.5182175051:
INFO:__main__:3: 134432: loss: 0.5178483076:
INFO:__main__:3: 137632: loss: 0.5173789573:
INFO:__main__:3: 140832: loss: 0.5170009146:
INFO:__main__:3: 144032: loss: 0.5166102670:
INFO:__main__:3: 147232: loss: 0.5161222647:
INFO:__main__:3: 150432: loss: 0.5156033135:
INFO:__main__:3: 153632: loss: 0.5152124290:
INFO:__main__:3: 156832: loss: 0.5147413522:
INFO:__main__:3: 160032: loss: 0.5142133212:
INFO:__main__:3: 163232: loss: 0.5137465727:
INFO:__main__:3: 166432: loss: 0.5132612560:
INFO:__main__:3: 169632: loss: 0.5127637572:
INFO:__main__:3: 172832: loss: 0.5123919472:
INFO:__main__:3: 176032: loss: 0.5120720529:
INFO:__main__:3: 179232: loss: 0.5116094261:
INFO:__main__:3: 182432: loss: 0.5110939583:
INFO:__main__:3: 185632: loss: 0.5106346237:
INFO:__main__:3: 188832: loss: 0.5101682831:
INFO:__main__:3: 192032: loss: 0.5096781684:
INFO:__main__:3: 195232: loss: 0.5091157924:
INFO:__main__:3: 198432: loss: 0.5086954008:
INFO:__main__:3: 201632: loss: 0.5081575076:
INFO:__main__:3: 204832: loss: 0.5077210966:
INFO:__main__:3: 208032: loss: 0.5072611767:
INFO:__main__:3: 211232: loss: 0.5067909773:
INFO:__main__:3: 214432: loss: 0.5063457928:
INFO:__main__:3: 217632: loss: 0.5058529081:
INFO:__main__:3: 220832: loss: 0.5054587293:
INFO:__main__:3: 224032: loss: 0.5050677497:
INFO:__main__:3: 227232: loss: 0.5045849443:
INFO:__main__:3: 230432: loss: 0.5041506197:
INFO:__main__:3: 233632: loss: 0.5036707637:
INFO:__main__:3: 236832: loss: 0.5032407409:
INFO:__main__:3: 240032: loss: 0.5027941381:
INFO:__main__:3: 243232: loss: 0.5023247393:
INFO:__main__:3: 246432: loss: 0.5018243752:
INFO:__main__:3: 249632: loss: 0.5014136767:
INFO:__main__:3: 252832: loss: 0.5009230094:
INFO:__main__:3: 256032: loss: 0.5005048694:
INFO:__main__:3: 259232: loss: 0.5000316427:
INFO:__main__:3: 262432: loss: 0.4995115168:
INFO:__main__:3: 265632: loss: 0.4990728533:
INFO:__main__:3: 268832: loss: 0.4986345881:
INFO:__main__:3: 272032: loss: 0.4982460519:
INFO:__main__:3: 275232: loss: 0.4978198990:
INFO:__main__:3: 278432: loss: 0.4973510008:
INFO:__main__:3: 281632: loss: 0.4969103799:
INFO:__main__:3: 284832: loss: 0.4963997053:
INFO:__main__:3: 288032: loss: 0.4959202078:
INFO:__main__:3: 291232: loss: 0.4954707771:
INFO:__main__:3: 294432: loss: 0.4949485115:
INFO:__main__:3: 297632: loss: 0.4945936421:
INFO:__main__:3: 300832: loss: 0.4941910379:
INFO:__main__:3: 304032: loss: 0.4937101378:
INFO:__main__:3: 307232: loss: 0.4932385712:
INFO:__main__:3: 310432: loss: 0.4927879558:
INFO:__main__:3: 313632: loss: 0.4923408498:
INFO:__main__:3: 316832: loss: 0.4918866489:
INFO:__main__:3: 320032: loss: 0.4914424131:
INFO:__main__:3: 323232: loss: 0.4910585145:
INFO:__main__:3: 326432: loss: 0.4905562597:
INFO:__main__:3: 329632: loss: 0.4900757320:
INFO:__main__:Dev-Acc: 3: Accuracy: 0.6047942042: precision: 0.5605970918: recall: 0.8575590551: f1: 0.6779861489
INFO:__main__:Train-Acc: 3: Accuracy: 0.9064523578: precision: 0.9275873900: recall: 0.8754950803: f1: 0.9007887436

Process finished with exit code 0
