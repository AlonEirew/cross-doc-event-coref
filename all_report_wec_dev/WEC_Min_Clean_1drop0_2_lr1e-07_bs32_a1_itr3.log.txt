ssh://alon_nlp@MLU-01-TitanXP.iil.intel.com:22/home/alon_nlp/venv/bin/python -u /home/alon_nlp/cross-doc-coref/src/pairwize_model/train.py
2019-11-15 11:22:21.746621: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-15 11:22:21.780005: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999980000 Hz
2019-11-15 11:22:21.783438: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50f89c0 executing computations on platform Host. Devices:
2019-11-15 11:22:21.783483: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
INFO:src.utils.dataset_utils:Create Features:SPLIT.TRAIN
INFO:src.utils.json_utils:Loading mentions from-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/CleanMin_WEC_Train_Event_gold_mentions.json
INFO:src.utils.json_utils:Mentions file-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/CleanMin_WEC_Train_Event_gold_mentions.json, took:1.4056 sec to load
INFO:src.utils.dataset_utils:Create pos/neg examples
INFO:src.utils.dataset_utils:pos-152565
INFO:src.utils.dataset_utils:neg-169272
INFO:src.utils.dataset_utils:Create Features:SPLIT.VALIDATION
INFO:src.utils.json_utils:Loading mentions from-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/CleanMin_WEC_Dev_Event_gold_mentions.json
INFO:src.utils.json_utils:Mentions file-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/CleanMin_WEC_Dev_Event_gold_mentions.json, took:0.1715 sec to load
INFO:src.utils.dataset_utils:Create pos/neg examples
INFO:src.utils.dataset_utils:pos-23999
INFO:src.utils.dataset_utils:neg-26775
INFO:__main__:1: 3232: loss: 0.6922223961:
INFO:__main__:1: 6432: loss: 0.6916425782:
INFO:__main__:1: 9632: loss: 0.6915595390:
INFO:__main__:1: 12832: loss: 0.6913027798:
INFO:__main__:1: 16032: loss: 0.6910316260:
INFO:__main__:1: 19232: loss: 0.6907995170:
INFO:__main__:1: 22432: loss: 0.6906016346:
INFO:__main__:1: 25632: loss: 0.6902759898:
INFO:__main__:1: 28832: loss: 0.6900218831:
INFO:__main__:1: 32032: loss: 0.6896336692:
INFO:__main__:1: 35232: loss: 0.6893535130:
INFO:__main__:1: 38432: loss: 0.6891520549:
INFO:__main__:1: 41632: loss: 0.6888561296:
INFO:__main__:1: 44832: loss: 0.6885727406:
INFO:__main__:1: 48032: loss: 0.6882474274:
INFO:__main__:1: 51232: loss: 0.6879290332:
INFO:__main__:1: 54432: loss: 0.6876534778:
INFO:__main__:1: 57632: loss: 0.6873742656:
INFO:__main__:1: 60832: loss: 0.6871076398:
INFO:__main__:1: 64032: loss: 0.6868505927:
INFO:__main__:1: 67232: loss: 0.6865704659:
INFO:__main__:1: 70432: loss: 0.6862738984:
INFO:__main__:1: 73632: loss: 0.6860315901:
INFO:__main__:1: 76832: loss: 0.6857402394:
INFO:__main__:1: 80032: loss: 0.6854581659:
INFO:__main__:1: 83232: loss: 0.6851966928:
INFO:__main__:1: 86432: loss: 0.6848933800:
INFO:__main__:1: 89632: loss: 0.6846125491:
INFO:__main__:1: 92832: loss: 0.6843595256:
INFO:__main__:1: 96032: loss: 0.6841107766:
INFO:__main__:1: 99232: loss: 0.6838229256:
INFO:__main__:1: 102432: loss: 0.6835523091:
INFO:__main__:1: 105632: loss: 0.6832563339:
INFO:__main__:1: 108832: loss: 0.6829470328:
INFO:__main__:1: 112032: loss: 0.6826720102:
INFO:__main__:1: 115232: loss: 0.6824185664:
INFO:__main__:1: 118432: loss: 0.6821245683:
INFO:__main__:1: 121632: loss: 0.6818657335:
INFO:__main__:1: 124832: loss: 0.6815853641:
INFO:__main__:1: 128032: loss: 0.6813071039:
INFO:__main__:1: 131232: loss: 0.6810627693:
INFO:__main__:1: 134432: loss: 0.6807790771:
INFO:__main__:1: 137632: loss: 0.6805402558:
INFO:__main__:1: 140832: loss: 0.6802547424:
INFO:__main__:1: 144032: loss: 0.6799873492:
INFO:__main__:1: 147232: loss: 0.6797240530:
INFO:__main__:1: 150432: loss: 0.6794656691:
INFO:__main__:1: 153632: loss: 0.6791723417:
INFO:__main__:1: 156832: loss: 0.6789378672:
INFO:__main__:1: 160032: loss: 0.6786596508:
INFO:__main__:1: 163232: loss: 0.6783955972:
INFO:__main__:1: 166432: loss: 0.6781047099:
INFO:__main__:1: 169632: loss: 0.6778148408:
INFO:__main__:1: 172832: loss: 0.6775392362:
INFO:__main__:1: 176032: loss: 0.6772628389:
INFO:__main__:1: 179232: loss: 0.6769639852:
INFO:__main__:1: 182432: loss: 0.6766710439:
INFO:__main__:1: 185632: loss: 0.6764134901:
INFO:__main__:1: 188832: loss: 0.6761314308:
INFO:__main__:1: 192032: loss: 0.6758561472:
INFO:__main__:1: 195232: loss: 0.6756029689:
INFO:__main__:1: 198432: loss: 0.6753449132:
INFO:__main__:1: 201632: loss: 0.6750629394:
INFO:__main__:1: 204832: loss: 0.6748088501:
INFO:__main__:1: 208032: loss: 0.6745520765:
INFO:__main__:1: 211232: loss: 0.6742998689:
INFO:__main__:1: 214432: loss: 0.6740322251:
INFO:__main__:1: 217632: loss: 0.6737643772:
INFO:__main__:1: 220832: loss: 0.6735028478:
INFO:__main__:1: 224032: loss: 0.6732313761:
INFO:__main__:1: 227232: loss: 0.6729595338:
INFO:__main__:1: 230432: loss: 0.6727011897:
INFO:__main__:1: 233632: loss: 0.6724265875:
INFO:__main__:1: 236832: loss: 0.6721425545:
INFO:__main__:1: 240032: loss: 0.6718822396:
INFO:__main__:1: 243232: loss: 0.6716332757:
INFO:__main__:1: 246432: loss: 0.6713419705:
INFO:__main__:1: 249632: loss: 0.6710867558:
INFO:__main__:1: 252832: loss: 0.6708223498:
INFO:__main__:1: 256032: loss: 0.6705631610:
INFO:__main__:1: 259232: loss: 0.6702879240:
INFO:__main__:1: 262432: loss: 0.6699886895:
INFO:__main__:1: 265632: loss: 0.6697168885:
INFO:__main__:1: 268832: loss: 0.6694606894:
INFO:__main__:1: 272032: loss: 0.6691595814:
INFO:__main__:1: 275232: loss: 0.6688952489:
INFO:__main__:1: 278432: loss: 0.6686300907:
INFO:__main__:1: 281632: loss: 0.6683603224:
INFO:__main__:1: 284832: loss: 0.6680707478:
INFO:__main__:1: 288032: loss: 0.6677995134:
INFO:__main__:1: 291232: loss: 0.6675279431:
INFO:__main__:1: 294432: loss: 0.6672423792:
INFO:__main__:1: 297632: loss: 0.6669824795:
INFO:__main__:1: 300832: loss: 0.6667212384:
INFO:__main__:1: 304032: loss: 0.6664420663:
INFO:__main__:1: 307232: loss: 0.6661865133:
INFO:__main__:1: 310432: loss: 0.6659013089:
INFO:__main__:1: 313632: loss: 0.6656158124:
INFO:__main__:1: 316832: loss: 0.6653311072:
INFO:__main__:1: 320032: loss: 0.6650714824:
INFO:__main__:Dev-Acc: 1: Accuracy: 0.6585850716: precision: 0.6398824517: recall: 0.6351097962: f1: 0.6374871913
INFO:__main__:Train-Acc: 1: Accuracy: 0.8225747943: precision: 0.9554488984: recall: 0.6563235342: f1: 0.7781292012
INFO:__main__:2: 3232: loss: 0.6368994540:
INFO:__main__:2: 6432: loss: 0.6357795265:
INFO:__main__:2: 9632: loss: 0.6355204542:
INFO:__main__:2: 12832: loss: 0.6350331685:
INFO:__main__:2: 16032: loss: 0.6348232785:
INFO:__main__:2: 19232: loss: 0.6346921254:
INFO:__main__:2: 22432: loss: 0.6345446409:
INFO:__main__:2: 25632: loss: 0.6345291158:
INFO:__main__:2: 28832: loss: 0.6344307554:
INFO:__main__:2: 32032: loss: 0.6346743872:
INFO:__main__:2: 35232: loss: 0.6344564037:
INFO:__main__:2: 38432: loss: 0.6338179703:
INFO:__main__:2: 41632: loss: 0.6333994912:
INFO:__main__:2: 44832: loss: 0.6331508983:
INFO:__main__:2: 48032: loss: 0.6328835922:
INFO:__main__:2: 51232: loss: 0.6324663633:
INFO:__main__:2: 54432: loss: 0.6321584244:
INFO:__main__:2: 57632: loss: 0.6319307266:
INFO:__main__:2: 60832: loss: 0.6316142113:
INFO:__main__:2: 64032: loss: 0.6313821318:
INFO:__main__:2: 67232: loss: 0.6310413370:
INFO:__main__:2: 70432: loss: 0.6305640880:
INFO:__main__:2: 73632: loss: 0.6302561497:
INFO:__main__:2: 76832: loss: 0.6299083554:
INFO:__main__:2: 80032: loss: 0.6296848600:
INFO:__main__:2: 83232: loss: 0.6293021299:
INFO:__main__:2: 86432: loss: 0.6290141773:
INFO:__main__:2: 89632: loss: 0.6286361493:
INFO:__main__:2: 92832: loss: 0.6282890142:
INFO:__main__:2: 96032: loss: 0.6279296743:
INFO:__main__:2: 99232: loss: 0.6277181582:
INFO:__main__:2: 102432: loss: 0.6274147177:
INFO:__main__:2: 105632: loss: 0.6270027054:
INFO:__main__:2: 108832: loss: 0.6266445617:
INFO:__main__:2: 112032: loss: 0.6263214693:
INFO:__main__:2: 115232: loss: 0.6259790081:
INFO:__main__:2: 118432: loss: 0.6256220746:
INFO:__main__:2: 121632: loss: 0.6252665812:
INFO:__main__:2: 124832: loss: 0.6248344854:
INFO:__main__:2: 128032: loss: 0.6245260309:
INFO:__main__:2: 131232: loss: 0.6242344417:
INFO:__main__:2: 134432: loss: 0.6239016316:
INFO:__main__:2: 137632: loss: 0.6235218397:
INFO:__main__:2: 140832: loss: 0.6232372939:
INFO:__main__:2: 144032: loss: 0.6229284165:
INFO:__main__:2: 147232: loss: 0.6226201267:
INFO:__main__:2: 150432: loss: 0.6222796974:
INFO:__main__:2: 153632: loss: 0.6219742865:
INFO:__main__:2: 156832: loss: 0.6216492320:
INFO:__main__:2: 160032: loss: 0.6213343931:
INFO:__main__:2: 163232: loss: 0.6210378347:
INFO:__main__:2: 166432: loss: 0.6207016277:
INFO:__main__:2: 169632: loss: 0.6203553401:
INFO:__main__:2: 172832: loss: 0.6200300792:
INFO:__main__:2: 176032: loss: 0.6196926116:
INFO:__main__:2: 179232: loss: 0.6193844151:
INFO:__main__:2: 182432: loss: 0.6190292820:
INFO:__main__:2: 185632: loss: 0.6187291700:
INFO:__main__:2: 188832: loss: 0.6184810645:
INFO:__main__:2: 192032: loss: 0.6181357980:
INFO:__main__:2: 195232: loss: 0.6178277118:
INFO:__main__:2: 198432: loss: 0.6175219413:
INFO:__main__:2: 201632: loss: 0.6172289377:
INFO:__main__:2: 204832: loss: 0.6169462714:
INFO:__main__:2: 208032: loss: 0.6165754777:
INFO:__main__:2: 211232: loss: 0.6162434249:
INFO:__main__:2: 214432: loss: 0.6159359447:
INFO:__main__:2: 217632: loss: 0.6156432832:
INFO:__main__:2: 220832: loss: 0.6152920738:
INFO:__main__:2: 224032: loss: 0.6149186088:
INFO:__main__:2: 227232: loss: 0.6145957805:
INFO:__main__:2: 230432: loss: 0.6142776358:
INFO:__main__:2: 233632: loss: 0.6138911131:
INFO:__main__:2: 236832: loss: 0.6136088536:
INFO:__main__:2: 240032: loss: 0.6132655778:
INFO:__main__:2: 243232: loss: 0.6129337082:
INFO:__main__:2: 246432: loss: 0.6126445250:
INFO:__main__:2: 249632: loss: 0.6123420026:
INFO:__main__:2: 252832: loss: 0.6120058064:
INFO:__main__:2: 256032: loss: 0.6116578870:
INFO:__main__:2: 259232: loss: 0.6113027467:
INFO:__main__:2: 262432: loss: 0.6109256542:
INFO:__main__:2: 265632: loss: 0.6106366507:
INFO:__main__:2: 268832: loss: 0.6102922525:
INFO:__main__:2: 272032: loss: 0.6099393253:
INFO:__main__:2: 275232: loss: 0.6096082830:
INFO:__main__:2: 278432: loss: 0.6091907361:
INFO:__main__:2: 281632: loss: 0.6088731691:
INFO:__main__:2: 284832: loss: 0.6085406885:
INFO:__main__:2: 288032: loss: 0.6082430107:
INFO:__main__:2: 291232: loss: 0.6078948009:
INFO:__main__:2: 294432: loss: 0.6075271543:
INFO:__main__:2: 297632: loss: 0.6071875043:
INFO:__main__:2: 300832: loss: 0.6068243681:
INFO:__main__:2: 304032: loss: 0.6065173630:
INFO:__main__:2: 307232: loss: 0.6061330532:
INFO:__main__:2: 310432: loss: 0.6058139493:
INFO:__main__:2: 313632: loss: 0.6054580783:
INFO:__main__:2: 316832: loss: 0.6051072148:
INFO:__main__:2: 320032: loss: 0.6047219999:
INFO:__main__:Dev-Acc: 2: Accuracy: 0.6647693515: precision: 0.6246338501: recall: 0.7286136922: f1: 0.6726290078
INFO:__main__:Train-Acc: 2: Accuracy: 0.8633252382: precision: 0.9499411560: recall: 0.7512732278: f1: 0.8390069650
INFO:__main__:3: 3232: loss: 0.5672217381:
INFO:__main__:3: 6432: loss: 0.5651159039:
INFO:__main__:3: 9632: loss: 0.5653036784:
INFO:__main__:3: 12832: loss: 0.5665201687:
INFO:__main__:3: 16032: loss: 0.5660348378:
INFO:__main__:3: 19232: loss: 0.5657307434:
INFO:__main__:3: 22432: loss: 0.5656811696:
INFO:__main__:3: 25632: loss: 0.5664139638:
INFO:__main__:3: 28832: loss: 0.5657052561:
INFO:__main__:3: 32032: loss: 0.5655041750:
INFO:__main__:3: 35232: loss: 0.5653247359:
INFO:__main__:3: 38432: loss: 0.5649585356:
INFO:__main__:3: 41632: loss: 0.5648821242:
INFO:__main__:3: 44832: loss: 0.5646819230:
INFO:__main__:3: 48032: loss: 0.5644407939:
INFO:__main__:3: 51232: loss: 0.5640659895:
INFO:__main__:3: 54432: loss: 0.5637584948:
INFO:__main__:3: 57632: loss: 0.5634684332:
INFO:__main__:3: 60832: loss: 0.5629568184:
INFO:__main__:3: 64032: loss: 0.5624536456:
INFO:__main__:3: 67232: loss: 0.5620602870:
INFO:__main__:3: 70432: loss: 0.5618037223:
INFO:__main__:3: 73632: loss: 0.5614444419:
INFO:__main__:3: 76832: loss: 0.5612904776:
INFO:__main__:3: 80032: loss: 0.5610455564:
INFO:__main__:3: 83232: loss: 0.5606025196:
INFO:__main__:3: 86432: loss: 0.5600492222:
INFO:__main__:3: 89632: loss: 0.5593058712:
INFO:__main__:3: 92832: loss: 0.5589236488:
INFO:__main__:3: 96032: loss: 0.5585071235:
INFO:__main__:3: 99232: loss: 0.5581720113:
INFO:__main__:3: 102432: loss: 0.5578271233:
INFO:__main__:3: 105632: loss: 0.5574237479:
INFO:__main__:3: 108832: loss: 0.5569153602:
INFO:__main__:3: 112032: loss: 0.5565625271:
INFO:__main__:3: 115232: loss: 0.5561568980:
INFO:__main__:3: 118432: loss: 0.5557117417:
INFO:__main__:3: 121632: loss: 0.5554791215:
INFO:__main__:3: 124832: loss: 0.5551012553:
INFO:__main__:3: 128032: loss: 0.5547691035:
INFO:__main__:3: 131232: loss: 0.5543654408:
INFO:__main__:3: 134432: loss: 0.5539761098:
INFO:__main__:3: 137632: loss: 0.5535443557:
INFO:__main__:3: 140832: loss: 0.5530257797:
INFO:__main__:3: 144032: loss: 0.5524534780:
INFO:__main__:3: 147232: loss: 0.5520112954:
INFO:__main__:3: 150432: loss: 0.5516063769:
INFO:__main__:3: 153632: loss: 0.5512541382:
INFO:__main__:3: 156832: loss: 0.5509634278:
INFO:__main__:3: 160032: loss: 0.5505358740:
INFO:__main__:3: 163232: loss: 0.5501889102:
INFO:__main__:3: 166432: loss: 0.5498688888:
INFO:__main__:3: 169632: loss: 0.5496149171:
INFO:__main__:3: 172832: loss: 0.5492807594:
INFO:__main__:3: 176032: loss: 0.5488458005:
INFO:__main__:3: 179232: loss: 0.5484330326:
INFO:__main__:3: 182432: loss: 0.5480436430:
INFO:__main__:3: 185632: loss: 0.5476897589:
INFO:__main__:3: 188832: loss: 0.5471724435:
INFO:__main__:3: 192032: loss: 0.5468611653:
INFO:__main__:3: 195232: loss: 0.5464198173:
INFO:__main__:3: 198432: loss: 0.5461039110:
INFO:__main__:3: 201632: loss: 0.5457710537:
INFO:__main__:3: 204832: loss: 0.5453024367:
INFO:__main__:3: 208032: loss: 0.5449273125:
INFO:__main__:3: 211232: loss: 0.5445585118:
INFO:__main__:3: 214432: loss: 0.5442624430:
INFO:__main__:3: 217632: loss: 0.5438805547:
INFO:__main__:3: 220832: loss: 0.5434290177:
INFO:__main__:3: 224032: loss: 0.5430653587:
INFO:__main__:3: 227232: loss: 0.5426905504:
INFO:__main__:3: 230432: loss: 0.5422039485:
INFO:__main__:3: 233632: loss: 0.5418599353:
INFO:__main__:3: 236832: loss: 0.5414028652:
INFO:__main__:3: 240032: loss: 0.5410406455:
INFO:__main__:3: 243232: loss: 0.5406216127:
INFO:__main__:3: 246432: loss: 0.5402588449:
INFO:__main__:3: 249632: loss: 0.5398598532:
INFO:__main__:3: 252832: loss: 0.5394749686:
INFO:__main__:3: 256032: loss: 0.5390697862:
INFO:__main__:3: 259232: loss: 0.5387607338:
INFO:__main__:3: 262432: loss: 0.5383288905:
INFO:__main__:3: 265632: loss: 0.5378455229:
INFO:__main__:3: 268832: loss: 0.5374769033:
INFO:__main__:3: 272032: loss: 0.5371027927:
INFO:__main__:3: 275232: loss: 0.5367220696:
INFO:__main__:3: 278432: loss: 0.5363475471:
INFO:__main__:3: 281632: loss: 0.5359241766:
INFO:__main__:3: 284832: loss: 0.5354942694:
INFO:__main__:3: 288032: loss: 0.5351303013:
INFO:__main__:3: 291232: loss: 0.5347978090:
INFO:__main__:3: 294432: loss: 0.5343710361:
INFO:__main__:3: 297632: loss: 0.5339766298:
INFO:__main__:3: 300832: loss: 0.5335737652:
INFO:__main__:3: 304032: loss: 0.5331082877:
INFO:__main__:3: 307232: loss: 0.5326871341:
INFO:__main__:3: 310432: loss: 0.5323283980:
INFO:__main__:3: 313632: loss: 0.5319133223:
INFO:__main__:3: 316832: loss: 0.5314645386:
INFO:__main__:3: 320032: loss: 0.5311235744:
INFO:__main__:Dev-Acc: 3: Accuracy: 0.6339858770: precision: 0.5818990290: recall: 0.8015750656: f1: 0.6742963301
INFO:__main__:Train-Acc: 3: Accuracy: 0.8913362622: precision: 0.9404331183: recall: 0.8228951594: f1: 0.8777467822

Process finished with exit code 0
