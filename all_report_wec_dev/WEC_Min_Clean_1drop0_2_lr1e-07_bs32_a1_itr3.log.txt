ssh://alon_nlp@MLU-01-TitanXP.iil.intel.com:22/home/alon_nlp/venv/bin/python -u /home/alon_nlp/cross-doc-coref/src/pairwize_model/train.py
2019-11-15 11:01:55.141809: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-15 11:01:55.172097: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999980000 Hz
2019-11-15 11:01:55.175618: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6c85ed0 executing computations on platform Host. Devices:
2019-11-15 11:01:55.175660: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
INFO:src.utils.dataset_utils:Create Features:SPLIT.TRAIN
INFO:src.utils.json_utils:Loading mentions from-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/CleanMin_WEC_Train_Event_gold_mentions.json
INFO:src.utils.json_utils:Mentions file-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/CleanMin_WEC_Train_Event_gold_mentions.json, took:1.4443 sec to load
INFO:src.utils.dataset_utils:Create pos/neg examples
INFO:src.utils.dataset_utils:pos-152565
INFO:src.utils.dataset_utils:neg-169272
INFO:src.utils.dataset_utils:Create Features:SPLIT.VALIDATION
INFO:src.utils.json_utils:Loading mentions from-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/CleanMin_WEC_Dev_Event_gold_mentions.json
INFO:src.utils.json_utils:Mentions file-/home/alon_nlp/cross-doc-coref/resources/corpora/single_sent_full_context_mean/CleanMin_WEC_Dev_Event_gold_mentions.json, took:0.1744 sec to load
INFO:src.utils.dataset_utils:Create pos/neg examples
INFO:src.utils.dataset_utils:pos-23999
INFO:src.utils.dataset_utils:neg-26775
INFO:__main__:1: 3232: loss: 0.6922129196:
INFO:__main__:1: 6432: loss: 0.6916238523:
INFO:__main__:1: 9632: loss: 0.6915321239:
INFO:__main__:1: 12832: loss: 0.6912666479:
INFO:__main__:1: 16032: loss: 0.6909857514:
INFO:__main__:1: 19232: loss: 0.6907444890:
INFO:__main__:1: 22432: loss: 0.6905368844:
INFO:__main__:1: 25632: loss: 0.6902016193:
INFO:__main__:1: 28832: loss: 0.6899387240:
INFO:__main__:1: 32032: loss: 0.6895411192:
INFO:__main__:1: 35232: loss: 0.6892513710:
INFO:__main__:1: 38432: loss: 0.6890404886:
INFO:__main__:1: 41632: loss: 0.6887353354:
INFO:__main__:1: 44832: loss: 0.6884427476:
INFO:__main__:1: 48032: loss: 0.6881071470:
INFO:__main__:1: 51232: loss: 0.6877789772:
INFO:__main__:1: 54432: loss: 0.6874943228:
INFO:__main__:1: 57632: loss: 0.6872065904:
INFO:__main__:1: 60832: loss: 0.6869300232:
INFO:__main__:1: 64032: loss: 0.6866648581:
INFO:__main__:1: 67232: loss: 0.6863749148:
INFO:__main__:1: 70432: loss: 0.6860689718:
INFO:__main__:1: 73632: loss: 0.6858174730:
INFO:__main__:1: 76832: loss: 0.6855164627:
INFO:__main__:1: 80032: loss: 0.6852247161:
INFO:__main__:1: 83232: loss: 0.6849532949:
INFO:__main__:1: 86432: loss: 0.6846399026:
INFO:__main__:1: 89632: loss: 0.6843491819:
INFO:__main__:1: 92832: loss: 0.6840865727:
INFO:__main__:1: 96032: loss: 0.6838276759:
INFO:__main__:1: 99232: loss: 0.6835295496:
INFO:__main__:1: 102432: loss: 0.6832491155:
INFO:__main__:1: 105632: loss: 0.6829425231:
INFO:__main__:1: 108832: loss: 0.6826223062:
INFO:__main__:1: 112032: loss: 0.6823366044:
INFO:__main__:1: 115232: loss: 0.6820736068:
INFO:__main__:1: 118432: loss: 0.6817698636:
INFO:__main__:1: 121632: loss: 0.6815009054:
INFO:__main__:1: 124832: loss: 0.6812101013:
INFO:__main__:1: 128032: loss: 0.6809215243:
INFO:__main__:1: 131232: loss: 0.6806673605:
INFO:__main__:1: 134432: loss: 0.6803734061:
INFO:__main__:1: 137632: loss: 0.6801247420:
INFO:__main__:1: 140832: loss: 0.6798284682:
INFO:__main__:1: 144032: loss: 0.6795509356:
INFO:__main__:1: 147232: loss: 0.6792769520:
INFO:__main__:1: 150432: loss: 0.6790085170:
INFO:__main__:1: 153632: loss: 0.6787036594:
INFO:__main__:1: 156832: loss: 0.6784595796:
INFO:__main__:1: 160032: loss: 0.6781702855:
INFO:__main__:1: 163232: loss: 0.6778956822:
INFO:__main__:1: 166432: loss: 0.6775936000:
INFO:__main__:1: 169632: loss: 0.6772926165:
INFO:__main__:1: 172832: loss: 0.6770057882:
INFO:__main__:1: 176032: loss: 0.6767181605:
INFO:__main__:1: 179232: loss: 0.6764074445:
INFO:__main__:1: 182432: loss: 0.6761026454:
INFO:__main__:1: 185632: loss: 0.6758348091:
INFO:__main__:1: 188832: loss: 0.6755414273:
INFO:__main__:1: 192032: loss: 0.6752548916:
INFO:__main__:1: 195232: loss: 0.6749905206:
INFO:__main__:1: 198432: loss: 0.6747212868:
INFO:__main__:1: 201632: loss: 0.6744278470:
INFO:__main__:1: 204832: loss: 0.6741626205:
INFO:__main__:1: 208032: loss: 0.6738946141:
INFO:__main__:1: 211232: loss: 0.6736311221:
INFO:__main__:1: 214432: loss: 0.6733518522:
INFO:__main__:1: 217632: loss: 0.6730722554:
INFO:__main__:1: 220832: loss: 0.6727993464:
INFO:__main__:1: 224032: loss: 0.6725163067:
INFO:__main__:1: 227232: loss: 0.6722324447:
INFO:__main__:1: 230432: loss: 0.6719621574:
INFO:__main__:1: 233632: loss: 0.6716754323:
INFO:__main__:1: 236832: loss: 0.6713789416:
INFO:__main__:1: 240032: loss: 0.6711067362:
INFO:__main__:1: 243232: loss: 0.6708462649:
INFO:__main__:1: 246432: loss: 0.6705419040:
INFO:__main__:1: 249632: loss: 0.6702745482:
INFO:__main__:1: 252832: loss: 0.6699978362:
INFO:__main__:1: 256032: loss: 0.6697266144:
INFO:__main__:1: 259232: loss: 0.6694387075:
INFO:__main__:1: 262432: loss: 0.6691258994:
INFO:__main__:1: 265632: loss: 0.6688415465:
INFO:__main__:1: 268832: loss: 0.6685731974:
INFO:__main__:1: 272032: loss: 0.6682586874:
INFO:__main__:1: 275232: loss: 0.6679819458:
INFO:__main__:1: 278432: loss: 0.6677041034:
INFO:__main__:1: 281632: loss: 0.6674217489:
INFO:__main__:1: 284832: loss: 0.6671187873:
INFO:__main__:1: 288032: loss: 0.6668341931:
INFO:__main__:1: 291232: loss: 0.6665495061:
INFO:__main__:1: 294432: loss: 0.6662502471:
INFO:__main__:1: 297632: loss: 0.6659775835:
INFO:__main__:1: 300832: loss: 0.6657036139:
INFO:__main__:1: 304032: loss: 0.6654107754:
INFO:__main__:1: 307232: loss: 0.6651424737:
INFO:__main__:1: 310432: loss: 0.6648435406:
INFO:__main__:1: 313632: loss: 0.6645440931:
INFO:__main__:1: 316832: loss: 0.6642454653:
INFO:__main__:1: 320032: loss: 0.6639722152:
INFO:__main__:Dev-Acc: 1: Accuracy: 0.6593137980: precision: 0.6393197222: recall: 0.6406516938: f1: 0.6399850150
INFO:__main__:Train-Acc: 1: Accuracy: 0.8253339529: precision: 0.9550740107: recall: 0.6627142529: f1: 0.7824771310
INFO:__main__:2: 3232: loss: 0.6343785018:
INFO:__main__:2: 6432: loss: 0.6332256907:
INFO:__main__:2: 9632: loss: 0.6329509308:
INFO:__main__:2: 12832: loss: 0.6324335620:
INFO:__main__:2: 16032: loss: 0.6321975288:
INFO:__main__:2: 19232: loss: 0.6320563592:
INFO:__main__:2: 22432: loss: 0.6318923732:
INFO:__main__:2: 25632: loss: 0.6318737774:
INFO:__main__:2: 28832: loss: 0.6317610247:
INFO:__main__:2: 32032: loss: 0.6320041091:
INFO:__main__:2: 35232: loss: 0.6317695223:
INFO:__main__:2: 38432: loss: 0.6311048095:
INFO:__main__:2: 41632: loss: 0.6306619767:
INFO:__main__:2: 44832: loss: 0.6303961805:
INFO:__main__:2: 48032: loss: 0.6301102179:
INFO:__main__:2: 51232: loss: 0.6296732594:
INFO:__main__:2: 54432: loss: 0.6293458440:
INFO:__main__:2: 57632: loss: 0.6290992338:
INFO:__main__:2: 60832: loss: 0.6287640601:
INFO:__main__:2: 64032: loss: 0.6285161754:
INFO:__main__:2: 67232: loss: 0.6281543616:
INFO:__main__:2: 70432: loss: 0.6276537288:
INFO:__main__:2: 73632: loss: 0.6273253698:
INFO:__main__:2: 76832: loss: 0.6269553088:
INFO:__main__:2: 80032: loss: 0.6267145344:
INFO:__main__:2: 83232: loss: 0.6263094328:
INFO:__main__:2: 86432: loss: 0.6260030357:
INFO:__main__:2: 89632: loss: 0.6256009964:
INFO:__main__:2: 92832: loss: 0.6252325359:
INFO:__main__:2: 96032: loss: 0.6248533378:
INFO:__main__:2: 99232: loss: 0.6246245377:
INFO:__main__:2: 102432: loss: 0.6243018865:
INFO:__main__:2: 105632: loss: 0.6238687786:
INFO:__main__:2: 108832: loss: 0.6234893552:
INFO:__main__:2: 112032: loss: 0.6231447512:
INFO:__main__:2: 115232: loss: 0.6227837708:
INFO:__main__:2: 118432: loss: 0.6224053692:
INFO:__main__:2: 121632: loss: 0.6220295857:
INFO:__main__:2: 124832: loss: 0.6215742905:
INFO:__main__:2: 128032: loss: 0.6212464293:
INFO:__main__:2: 131232: loss: 0.6209357978:
INFO:__main__:2: 134432: loss: 0.6205837162:
INFO:__main__:2: 137632: loss: 0.6201808978:
INFO:__main__:2: 140832: loss: 0.6198764298:
INFO:__main__:2: 144032: loss: 0.6195470127:
INFO:__main__:2: 147232: loss: 0.6192194070:
INFO:__main__:2: 150432: loss: 0.6188572119:
INFO:__main__:2: 153632: loss: 0.6185311050:
INFO:__main__:2: 156832: loss: 0.6181854311:
INFO:__main__:2: 160032: loss: 0.6178501707:
INFO:__main__:2: 163232: loss: 0.6175350527:
INFO:__main__:2: 166432: loss: 0.6171775030:
INFO:__main__:2: 169632: loss: 0.6168113284:
INFO:__main__:2: 172832: loss: 0.6164649311:
INFO:__main__:2: 176032: loss: 0.6161071615:
INFO:__main__:2: 179232: loss: 0.6157804472:
INFO:__main__:2: 182432: loss: 0.6154034405:
INFO:__main__:2: 185632: loss: 0.6150831590:
INFO:__main__:2: 188832: loss: 0.6148153857:
INFO:__main__:2: 192032: loss: 0.6144479059:
INFO:__main__:2: 195232: loss: 0.6141188902:
INFO:__main__:2: 198432: loss: 0.6137919902:
INFO:__main__:2: 201632: loss: 0.6134797158:
INFO:__main__:2: 204832: loss: 0.6131775681:
INFO:__main__:2: 208032: loss: 0.6127850466:
INFO:__main__:2: 211232: loss: 0.6124312382:
INFO:__main__:2: 214432: loss: 0.6121024519:
INFO:__main__:2: 217632: loss: 0.6117904247:
INFO:__main__:2: 220832: loss: 0.6114169774:
INFO:__main__:2: 224032: loss: 0.6110200464:
INFO:__main__:2: 227232: loss: 0.6106748531:
INFO:__main__:2: 230432: loss: 0.6103360027:
INFO:__main__:2: 233632: loss: 0.6099273632:
INFO:__main__:2: 236832: loss: 0.6096254696:
INFO:__main__:2: 240032: loss: 0.6092609880:
INFO:__main__:2: 243232: loss: 0.6089074542:
INFO:__main__:2: 246432: loss: 0.6085962457:
INFO:__main__:2: 249632: loss: 0.6082728875:
INFO:__main__:2: 252832: loss: 0.6079139236:
INFO:__main__:2: 256032: loss: 0.6075422270:
INFO:__main__:2: 259232: loss: 0.6071647882:
INFO:__main__:2: 262432: loss: 0.6067656307:
INFO:__main__:2: 265632: loss: 0.6064580023:
INFO:__main__:2: 268832: loss: 0.6060911590:
INFO:__main__:2: 272032: loss: 0.6057167539:
INFO:__main__:2: 275232: loss: 0.6053644882:
INFO:__main__:2: 278432: loss: 0.6049233527:
INFO:__main__:2: 281632: loss: 0.6045841201:
INFO:__main__:2: 284832: loss: 0.6042298585:
INFO:__main__:2: 288032: loss: 0.6039105347:
INFO:__main__:2: 291232: loss: 0.6035409211:
INFO:__main__:2: 294432: loss: 0.6031505768:
INFO:__main__:2: 297632: loss: 0.6027884339:
INFO:__main__:2: 300832: loss: 0.6024026042:
INFO:__main__:2: 304032: loss: 0.6020733164:
INFO:__main__:2: 307232: loss: 0.6016647851:
INFO:__main__:2: 310432: loss: 0.6013252694:
INFO:__main__:2: 313632: loss: 0.6009470836:
INFO:__main__:2: 316832: loss: 0.6005744667:
INFO:__main__:2: 320032: loss: 0.6001648521:
INFO:__main__:Dev-Acc: 2: Accuracy: 0.6644936204: precision: 0.6225279752: recall: 0.7371557148: f1: 0.6750100158
INFO:__main__:Train-Acc: 2: Accuracy: 0.8666157126: precision: 0.9484020842: recall: 0.7599711598: f1: 0.8437948024
INFO:__main__:3: 3232: loss: 0.5604032356:
INFO:__main__:3: 6432: loss: 0.5581549898:
INFO:__main__:3: 9632: loss: 0.5583261333:
INFO:__main__:3: 12832: loss: 0.5595853149:
INFO:__main__:3: 16032: loss: 0.5590563564:
INFO:__main__:3: 19232: loss: 0.5587458522:
INFO:__main__:3: 22432: loss: 0.5586806223:
INFO:__main__:3: 25632: loss: 0.5594161361:
INFO:__main__:3: 28832: loss: 0.5586867825:
INFO:__main__:3: 32032: loss: 0.5584683979:
INFO:__main__:3: 35232: loss: 0.5582721472:
INFO:__main__:3: 38432: loss: 0.5578775426:
INFO:__main__:3: 41632: loss: 0.5577772975:
INFO:__main__:3: 44832: loss: 0.5575551519:
INFO:__main__:3: 48032: loss: 0.5572908571:
INFO:__main__:3: 51232: loss: 0.5568835255:
INFO:__main__:3: 54432: loss: 0.5565538064:
INFO:__main__:3: 57632: loss: 0.5562424190:
INFO:__main__:3: 60832: loss: 0.5557033444:
INFO:__main__:3: 64032: loss: 0.5551745552:
INFO:__main__:3: 67232: loss: 0.5547534902:
INFO:__main__:3: 70432: loss: 0.5544754320:
INFO:__main__:3: 73632: loss: 0.5540919462:
INFO:__main__:3: 76832: loss: 0.5539237251:
INFO:__main__:3: 80032: loss: 0.5536616843:
INFO:__main__:3: 83232: loss: 0.5531869261:
INFO:__main__:3: 86432: loss: 0.5526040050:
INFO:__main__:3: 89632: loss: 0.5518179020:
INFO:__main__:3: 92832: loss: 0.5514046235:
INFO:__main__:3: 96032: loss: 0.5509695812:
INFO:__main__:3: 99232: loss: 0.5506098218:
INFO:__main__:3: 102432: loss: 0.5502391350:
INFO:__main__:3: 105632: loss: 0.5498074800:
INFO:__main__:3: 108832: loss: 0.5492715120:
INFO:__main__:3: 112032: loss: 0.5488956673:
INFO:__main__:3: 115232: loss: 0.5484674983:
INFO:__main__:3: 118432: loss: 0.5479996983:
INFO:__main__:3: 121632: loss: 0.5477461012:
INFO:__main__:3: 124832: loss: 0.5473410031:
INFO:__main__:3: 128032: loss: 0.5469876688:
INFO:__main__:3: 131232: loss: 0.5465575986:
INFO:__main__:3: 134432: loss: 0.5461455887:
INFO:__main__:3: 137632: loss: 0.5456903636:
INFO:__main__:3: 140832: loss: 0.5451468397:
INFO:__main__:3: 144032: loss: 0.5445464195:
INFO:__main__:3: 147232: loss: 0.5440805164:
INFO:__main__:3: 150432: loss: 0.5436504841:
INFO:__main__:3: 153632: loss: 0.5432801252:
INFO:__main__:3: 156832: loss: 0.5429701670:
INFO:__main__:3: 160032: loss: 0.5425183274:
INFO:__main__:3: 163232: loss: 0.5421515717:
INFO:__main__:3: 166432: loss: 0.5418097732:
INFO:__main__:3: 169632: loss: 0.5415368630:
INFO:__main__:3: 172832: loss: 0.5411807627:
INFO:__main__:3: 176032: loss: 0.5407244362:
INFO:__main__:3: 179232: loss: 0.5402877567:
INFO:__main__:3: 182432: loss: 0.5398760842:
INFO:__main__:3: 185632: loss: 0.5394994162:
INFO:__main__:3: 188832: loss: 0.5389542397:
INFO:__main__:3: 192032: loss: 0.5386200828:
INFO:__main__:3: 195232: loss: 0.5381567590:
INFO:__main__:3: 198432: loss: 0.5378211487:
INFO:__main__:3: 201632: loss: 0.5374685338:
INFO:__main__:3: 204832: loss: 0.5369711019:
INFO:__main__:3: 208032: loss: 0.5365732944:
INFO:__main__:3: 211232: loss: 0.5361828648:
INFO:__main__:3: 214432: loss: 0.5358684845:
INFO:__main__:3: 217632: loss: 0.5354634591:
INFO:__main__:3: 220832: loss: 0.5349848433:
INFO:__main__:3: 224032: loss: 0.5346012379:
INFO:__main__:3: 227232: loss: 0.5342021193:
INFO:__main__:3: 230432: loss: 0.5336890418:
INFO:__main__:3: 233632: loss: 0.5333245282:
INFO:__main__:3: 236832: loss: 0.5328405583:
INFO:__main__:3: 240032: loss: 0.5324573485:
INFO:__main__:3: 243232: loss: 0.5320138488:
INFO:__main__:3: 246432: loss: 0.5316312800:
INFO:__main__:3: 249632: loss: 0.5312122456:
INFO:__main__:3: 252832: loss: 0.5308085380:
INFO:__main__:3: 256032: loss: 0.5303821990:
INFO:__main__:3: 259232: loss: 0.5300524285:
INFO:__main__:3: 262432: loss: 0.5295969692:
INFO:__main__:3: 265632: loss: 0.5290894949:
INFO:__main__:3: 268832: loss: 0.5286991539:
INFO:__main__:3: 272032: loss: 0.5283024089:
INFO:__main__:3: 275232: loss: 0.5279005342:
INFO:__main__:3: 278432: loss: 0.5275035452:
INFO:__main__:3: 281632: loss: 0.5270549280:
INFO:__main__:3: 284832: loss: 0.5266029732:
INFO:__main__:3: 288032: loss: 0.5262178827:
INFO:__main__:3: 291232: loss: 0.5258641209:
INFO:__main__:3: 294432: loss: 0.5254148776:
INFO:__main__:3: 297632: loss: 0.5249990579:
INFO:__main__:3: 300832: loss: 0.5245729687:
INFO:__main__:3: 304032: loss: 0.5240824687:
INFO:__main__:3: 307232: loss: 0.5236378702:
INFO:__main__:3: 310432: loss: 0.5232585296:
INFO:__main__:3: 313632: loss: 0.5228204146:
INFO:__main__:3: 316832: loss: 0.5223480845:
INFO:__main__:3: 320032: loss: 0.5219893311:
INFO:__main__:Dev-Acc: 3: Accuracy: 0.6286878586: precision: 0.5765045195: recall: 0.8079086629: f1: 0.6728670334
INFO:__main__:Train-Acc: 3: Accuracy: 0.8937630057: precision: 0.9398296772: recall: 0.8289647036: f1: 0.8809227779

Process finished with exit code 0
