1: 6464: loss: 0.6912449580:
1: 12864: loss: 0.6857274491:
1: 19264: loss: 0.6804232262:
1: 25664: loss: 0.6751482607:
1: 32064: loss: 0.6695058088:
1: 38464: loss: 0.6643587429:
1: 44864: loss: 0.6590721279:
Dev-Acc: 1: Accuracy: 0.8047209978: precision: 0.1607669617: recall: 0.5560278864: f1: 0.2494184051
Train-Acc: 1: Accuracy: 0.8434028029: precision: 0.9614372354: recall: 0.5523634212: f1: 0.7016283925
2: 6464: loss: 0.6130531824:
2: 12864: loss: 0.6080298388:
2: 19264: loss: 0.6025707904:
2: 25664: loss: 0.5963162832:
2: 32064: loss: 0.5910418075:
2: 38464: loss: 0.5851936363:
2: 44864: loss: 0.5790825007:
Dev-Acc: 2: Accuracy: 0.8231762648: precision: 0.1808851828: recall: 0.5754123448: f1: 0.2752450283
Train-Acc: 2: Accuracy: 0.8537899852: precision: 0.9701574716: recall: 0.5791861153: f1: 0.7253416763
3: 6464: loss: 0.5300536582:
3: 12864: loss: 0.5243132520:
3: 19264: loss: 0.5174455354:
3: 25664: loss: 0.5105034447:
3: 32064: loss: 0.5036824537:
3: 38464: loss: 0.4966735016:
3: 44864: loss: 0.4900173699:
Dev-Acc: 3: Accuracy: 0.7798956037: precision: 0.1548234098: recall: 0.6216629825: f1: 0.2479064248
Train-Acc: 3: Accuracy: 0.8718032837: precision: 0.9690349734: recall: 0.6357241470: f1: 0.7677649861
4: 6464: loss: 0.4387768510:
4: 12864: loss: 0.4335371830:
4: 19264: loss: 0.4252105951:
4: 25664: loss: 0.4200901774:
4: 32064: loss: 0.4165847593:
4: 38464: loss: 0.4102709743:
4: 44864: loss: 0.4045568505:
Dev-Acc: 4: Accuracy: 0.7265537977: precision: 0.1319524618: recall: 0.6607719776: f1: 0.2199767909
Train-Acc: 4: Accuracy: 0.8924900889: precision: 0.9655733261: recall: 0.7025179147: f1: 0.8133039044
5: 6464: loss: 0.3640119490:
5: 12864: loss: 0.3591340624:
5: 19264: loss: 0.3543143286:
5: 25664: loss: 0.3508704402:
5: 32064: loss: 0.3465207018:
5: 38464: loss: 0.3430584729:
5: 44864: loss: 0.3386726091:
Dev-Acc: 5: Accuracy: 0.6729341745: precision: 0.1162283188: recall: 0.6973303860: f1: 0.1992469331
Train-Acc: 5: Accuracy: 0.9091446996: precision: 0.9667594702: recall: 0.7533364013: f1: 0.8468075672
6: 6464: loss: 0.3106895825:
6: 12864: loss: 0.3068290944:
6: 19264: loss: 0.3021574418:
6: 25664: loss: 0.3012526013:
6: 32064: loss: 0.2970789571:
6: 38464: loss: 0.2934774572:
6: 44864: loss: 0.2906049117:
Dev-Acc: 6: Accuracy: 0.6206640005: precision: 0.1053337888: recall: 0.7340588335: f1: 0.1842313027
Train-Acc: 6: Accuracy: 0.9199263453: precision: 0.9662712822: recall: 0.7872592203: f1: 0.8676278800
7: 6464: loss: 0.2677511263:
7: 12864: loss: 0.2658312068:
7: 19264: loss: 0.2655129844:
7: 25664: loss: 0.2617080588:
7: 32064: loss: 0.2602767796:
7: 38464: loss: 0.2578577852:
7: 44864: loss: 0.2555861002:
Dev-Acc: 7: Accuracy: 0.5781275034: precision: 0.0987976084: recall: 0.7670464207: f1: 0.1750485060
Train-Acc: 7: Accuracy: 0.9286043048: precision: 0.9654957551: recall: 0.8149365591: f1: 0.8838502674
8: 6464: loss: 0.2357329690:
8: 12864: loss: 0.2301182154:
8: 19264: loss: 0.2326326315:
8: 25664: loss: 0.2350534675:
8: 32064: loss: 0.2334812330:
8: 38464: loss: 0.2310681990:
8: 44864: loss: 0.2299070657:
Dev-Acc: 8: Accuracy: 0.5507619977: precision: 0.0949433466: recall: 0.7850705662: f1: 0.1694001101
Train-Acc: 8: Accuracy: 0.9333158135: precision: 0.9657072872: recall: 0.8293997765: f1: 0.8923784262
9: 6464: loss: 0.2208421481:
9: 12864: loss: 0.2160202880:
9: 19264: loss: 0.2130245735:
9: 25664: loss: 0.2132575446:
9: 32064: loss: 0.2123326429:
9: 38464: loss: 0.2106776446:
9: 44864: loss: 0.2089238637:
Dev-Acc: 9: Accuracy: 0.5314732194: precision: 0.0925827371: recall: 0.7986736949: f1: 0.1659306885
Train-Acc: 9: Accuracy: 0.9372383952: precision: 0.9651872504: recall: 0.8420879627: f1: 0.8994452637
10: 6464: loss: 0.2039105151:
10: 12864: loss: 0.1979945232:
10: 19264: loss: 0.1964587827:
10: 25664: loss: 0.1952190992:
10: 32064: loss: 0.1955400818:
10: 38464: loss: 0.1942716128:
10: 44864: loss: 0.1936587220:
Dev-Acc: 10: Accuracy: 0.5106167793: precision: 0.0895829790: recall: 0.8061554157: f1: 0.1612475342
Train-Acc: 10: Accuracy: 0.9408980012: precision: 0.9641691395: recall: 0.8544474394: f1: 0.9059983967
11: 6464: loss: 0.1917078622:
11: 12864: loss: 0.1867243461:
11: 19264: loss: 0.1850650678:
11: 25664: loss: 0.1810490953:
11: 32064: loss: 0.1807717200:
11: 38464: loss: 0.1814690694:
11: 44864: loss: 0.1800324216:
Dev-Acc: 11: Accuracy: 0.4934414029: precision: 0.0877413938: recall: 0.8173779969: f1: 0.1584716567
Train-Acc: 11: Accuracy: 0.9441412687: precision: 0.9614431487: recall: 0.8672013674: f1: 0.9118938163
12: 6464: loss: 0.1732171054:
12: 12864: loss: 0.1734094869:
12: 19264: loss: 0.1707482878:
12: 25664: loss: 0.1715589522:
12: 32064: loss: 0.1712021768:
12: 38464: loss: 0.1711748695:
12: 44864: loss: 0.1705732905:
Dev-Acc: 12: Accuracy: 0.4762065411: precision: 0.0861725215: recall: 0.8304710083: f1: 0.1561430992
Train-Acc: 12: Accuracy: 0.9476256371: precision: 0.9616852719: recall: 0.8778515548: f1: 0.9178581248
13: 6464: loss: 0.1719529579:
13: 12864: loss: 0.1648831368:
13: 19264: loss: 0.1614015054:
13: 25664: loss: 0.1605635380:
13: 32064: loss: 0.1608201059:
13: 38464: loss: 0.1616189717:
13: 44864: loss: 0.1615377270:
Dev-Acc: 13: Accuracy: 0.4651730359: precision: 0.0850600536: recall: 0.8369324945: f1: 0.1544253757
Train-Acc: 13: Accuracy: 0.9499922991: precision: 0.9624436655: recall: 0.8844914864: f1: 0.9218225420
14: 6464: loss: 0.1571621007:
14: 12864: loss: 0.1597900582:
14: 19264: loss: 0.1588658863:
14: 25664: loss: 0.1560114767:
14: 32064: loss: 0.1555960145:
14: 38464: loss: 0.1560958927:
14: 44864: loss: 0.1542493540:
Dev-Acc: 14: Accuracy: 0.4547447860: precision: 0.0840227858: recall: 0.8427138242: f1: 0.1528096816
Train-Acc: 14: Accuracy: 0.9521617889: precision: 0.9631025167: recall: 0.8906054829: f1: 0.9254363494
15: 6464: loss: 0.1564247768:
15: 12864: loss: 0.1520879956:
15: 19264: loss: 0.1504547804:
15: 25664: loss: 0.1508597177:
15: 32064: loss: 0.1506681860:
15: 38464: loss: 0.1504347135:
15: 44864: loss: 0.1490488534:
Dev-Acc: 15: Accuracy: 0.4417070150: precision: 0.0832974958: recall: 0.8563169529: f1: 0.1518262259
Train-Acc: 15: Accuracy: 0.9542655349: precision: 0.9624383369: recall: 0.8978370916: f1: 0.9290160199
16: 6464: loss: 0.1420610133:
16: 12864: loss: 0.1413361573:
16: 19264: loss: 0.1426734035:
16: 25664: loss: 0.1431864245:
16: 32064: loss: 0.1435447845:
16: 38464: loss: 0.1429667866:
16: 44864: loss: 0.1433531331:
Dev-Acc: 16: Accuracy: 0.4322313070: precision: 0.0827766672: recall: 0.8660091821: f1: 0.1511096606
Train-Acc: 16: Accuracy: 0.9557118416: precision: 0.9628719820: recall: 0.9019130892: f1: 0.9313961777
17: 6464: loss: 0.1427970038:
17: 12864: loss: 0.1394269378:
17: 19264: loss: 0.1393277430:
17: 25664: loss: 0.1404653328:
17: 32064: loss: 0.1395013098:
17: 38464: loss: 0.1391628498:
17: 44864: loss: 0.1389060488:
Dev-Acc: 17: Accuracy: 0.4266847968: precision: 0.0824349113: recall: 0.8711103554: f1: 0.1506166669
Train-Acc: 17: Accuracy: 0.9565445781: precision: 0.9636197953: recall: 0.9037538623: f1: 0.9327272110
18: 6464: loss: 0.1296328856:
18: 12864: loss: 0.1310881993:
18: 19264: loss: 0.1355173018:
18: 25664: loss: 0.1325737950:
18: 32064: loss: 0.1324071020:
18: 38464: loss: 0.1351373354:
18: 44864: loss: 0.1344996297:
Dev-Acc: 18: Accuracy: 0.4173380733: precision: 0.0819462025: recall: 0.8806325455: f1: 0.1499399256
Train-Acc: 18: Accuracy: 0.9580347538: precision: 0.9635336773: recall: 0.9084872789: f1: 0.9352011640
19: 6464: loss: 0.1261686513:
19: 12864: loss: 0.1293573569:
19: 19264: loss: 0.1303007910:
19: 25664: loss: 0.1324968568:
19: 32064: loss: 0.1344860394:
19: 38464: loss: 0.1326089359:
19: 44864: loss: 0.1318326968:
Dev-Acc: 19: Accuracy: 0.4174075127: precision: 0.0814253799: recall: 0.8738309811: f1: 0.1489694756
Train-Acc: 19: Accuracy: 0.9577498436: precision: 0.9644730401: recall: 0.9066465058: f1: 0.9346662148
20: 6464: loss: 0.1466545336:
20: 12864: loss: 0.1388600429:
20: 19264: loss: 0.1363382175:
20: 25664: loss: 0.1345057565:
20: 32064: loss: 0.1336290941:
20: 38464: loss: 0.1306076659:
20: 44864: loss: 0.1289948204:
Dev-Acc: 20: Accuracy: 0.4092613757: precision: 0.0811266550: recall: 0.8835232103: f1: 0.1486078737
Train-Acc: 20: Accuracy: 0.9588674903: precision: 0.9643404374: recall: 0.9102623102: f1: 0.9365213568
