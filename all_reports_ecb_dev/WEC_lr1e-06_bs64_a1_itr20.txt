1: 6464: loss: 0.6870983851:
1: 12864: loss: 0.6844234303:
1: 19264: loss: 0.6813665523:
1: 25664: loss: 0.6785573651:
1: 32064: loss: 0.6758988860:
1: 38464: loss: 0.6727691123:
1: 44864: loss: 0.6698381002:
1: 51264: loss: 0.6669461641:
1: 57664: loss: 0.6639782088:
1: 64064: loss: 0.6609775956:
1: 70464: loss: 0.6576540822:
1: 76864: loss: 0.6542247772:
1: 83264: loss: 0.6507778067:
1: 89664: loss: 0.6475208222:
1: 96064: loss: 0.6439931995:
1: 102464: loss: 0.6405003752:
1: 108864: loss: 0.6369152803:
1: 115264: loss: 0.6330950732:
1: 121664: loss: 0.6293045997:
1: 128064: loss: 0.6254286475:
1: 134464: loss: 0.6215445979:
1: 140864: loss: 0.6175323964:
1: 147264: loss: 0.6135562025:
1: 153664: loss: 0.6093633017:
1: 160064: loss: 0.6051518193:
1: 166464: loss: 0.6008642477:
1: 172864: loss: 0.5967082930:
1: 179264: loss: 0.5923395633:
1: 185664: loss: 0.5877316637:
1: 192064: loss: 0.5833696658:
1: 198464: loss: 0.5788902149:
1: 204864: loss: 0.5743856910:
1: 211264: loss: 0.5697632719:
1: 217664: loss: 0.5652729475:
1: 224064: loss: 0.5608771144:
1: 230464: loss: 0.5563790587:
1: 236864: loss: 0.5521049010:
1: 243264: loss: 0.5478608214:
1: 249664: loss: 0.5434935235:
1: 256064: loss: 0.5390935942:
1: 262464: loss: 0.5346845116:
1: 268864: loss: 0.5302632059:
1: 275264: loss: 0.5259911228:
1: 281664: loss: 0.5217099213:
1: 288064: loss: 0.5174363723:
1: 294464: loss: 0.5131841826:
1: 300864: loss: 0.5089273969:
1: 307264: loss: 0.5048218936:
1: 313664: loss: 0.5007236771:
1: 320064: loss: 0.4967006267:
1: 326464: loss: 0.4926869921:
Dev-Acc: 1: Accuracy: 0.1724579334: precision: 0.0598342040: recall: 0.8959360653: f1: 0.1121767918
Train-Acc: 1: Accuracy: 0.9364880323: precision: 0.9722385777: recall: 0.8946132769: f1: 0.9318120640
2: 6464: loss: 0.2836950734:
2: 12864: loss: 0.2824760799:
2: 19264: loss: 0.2779588118:
2: 25664: loss: 0.2750183666:
2: 32064: loss: 0.2723009434:
2: 38464: loss: 0.2701809428:
2: 44864: loss: 0.2668817949:
2: 51264: loss: 0.2645516835:
2: 57664: loss: 0.2619934890:
2: 64064: loss: 0.2593094227:
2: 70464: loss: 0.2570498692:
2: 76864: loss: 0.2542894301:
2: 83264: loss: 0.2523880130:
2: 89664: loss: 0.2499693337:
2: 96064: loss: 0.2473245844:
2: 102464: loss: 0.2452042863:
2: 108864: loss: 0.2429568203:
2: 115264: loss: 0.2411460811:
2: 121664: loss: 0.2390527759:
2: 128064: loss: 0.2373125732:
2: 134464: loss: 0.2355836866:
2: 140864: loss: 0.2338664334:
2: 147264: loss: 0.2322234831:
2: 153664: loss: 0.2304778608:
2: 160064: loss: 0.2285363943:
2: 166464: loss: 0.2268210678:
2: 172864: loss: 0.2251491630:
2: 179264: loss: 0.2234334824:
2: 185664: loss: 0.2217351425:
2: 192064: loss: 0.2204624527:
2: 198464: loss: 0.2190582873:
2: 204864: loss: 0.2173509674:
2: 211264: loss: 0.2157734469:
2: 217664: loss: 0.2143208635:
2: 224064: loss: 0.2130115191:
2: 230464: loss: 0.2114003531:
2: 236864: loss: 0.2100451738:
2: 243264: loss: 0.2087572546:
2: 249664: loss: 0.2074408938:
2: 256064: loss: 0.2060543515:
2: 262464: loss: 0.2047901947:
2: 268864: loss: 0.2033755285:
2: 275264: loss: 0.2022143838:
2: 281664: loss: 0.2008811932:
2: 288064: loss: 0.1996186876:
2: 294464: loss: 0.1984576764:
2: 300864: loss: 0.1972598457:
2: 307264: loss: 0.1961264009:
2: 313664: loss: 0.1949304970:
2: 320064: loss: 0.1937393937:
2: 326464: loss: 0.1924885909:
Dev-Acc: 2: Accuracy: 0.1431080252: precision: 0.0586073756: recall: 0.9085189594: f1: 0.1101115954
Train-Acc: 2: Accuracy: 0.9612584710: precision: 0.9898876032: recall: 0.9296300909: f1: 0.9588130449
3: 6464: loss: 0.1363886518:
3: 12864: loss: 0.1310490027:
3: 19264: loss: 0.1314664107:
3: 25664: loss: 0.1310847348:
3: 32064: loss: 0.1303233812:
3: 38464: loss: 0.1300457169:
3: 44864: loss: 0.1297581096:
3: 51264: loss: 0.1294877759:
3: 57664: loss: 0.1293486454:
3: 64064: loss: 0.1286444555:
3: 70464: loss: 0.1282993180:
3: 76864: loss: 0.1282379307:
3: 83264: loss: 0.1278374676:
3: 89664: loss: 0.1274527134:
3: 96064: loss: 0.1270958757:
3: 102464: loss: 0.1267053985:
3: 108864: loss: 0.1263644511:
3: 115264: loss: 0.1259518434:
3: 121664: loss: 0.1255907149:
3: 128064: loss: 0.1253171482:
3: 134464: loss: 0.1249801608:
3: 140864: loss: 0.1242686159:
3: 147264: loss: 0.1234484764:
3: 153664: loss: 0.1228307708:
3: 160064: loss: 0.1224934357:
3: 166464: loss: 0.1217029872:
3: 172864: loss: 0.1213510695:
3: 179264: loss: 0.1206418996:
3: 185664: loss: 0.1202901690:
3: 192064: loss: 0.1199065024:
3: 198464: loss: 0.1196349978:
3: 204864: loss: 0.1191628081:
3: 211264: loss: 0.1187165502:
3: 217664: loss: 0.1183379843:
3: 224064: loss: 0.1178498933:
3: 230464: loss: 0.1173459266:
3: 236864: loss: 0.1169081587:
3: 243264: loss: 0.1164054545:
3: 249664: loss: 0.1160826442:
3: 256064: loss: 0.1155878261:
3: 262464: loss: 0.1152025862:
3: 268864: loss: 0.1149081846:
3: 275264: loss: 0.1145224574:
3: 281664: loss: 0.1141740410:
3: 288064: loss: 0.1140159510:
3: 294464: loss: 0.1138053893:
3: 300864: loss: 0.1135017165:
3: 307264: loss: 0.1130657294:
3: 313664: loss: 0.1126368013:
3: 320064: loss: 0.1122266437:
3: 326464: loss: 0.1118791446:
Dev-Acc: 3: Accuracy: 0.1249305457: precision: 0.0569955437: recall: 0.9003570821: f1: 0.1072046810
Train-Acc: 3: Accuracy: 0.9729368091: precision: 0.9969388090: recall: 0.9471167020: f1: 0.9713893369
4: 6464: loss: 0.0944294168:
4: 12864: loss: 0.0944072823:
4: 19264: loss: 0.0951061934:
4: 25664: loss: 0.0945423382:
4: 32064: loss: 0.0948937810:
4: 38464: loss: 0.0941925423:
4: 44864: loss: 0.0945178653:
4: 51264: loss: 0.0944002369:
4: 57664: loss: 0.0940439066:
4: 64064: loss: 0.0933287211:
4: 70464: loss: 0.0924863077:
4: 76864: loss: 0.0923976535:
4: 83264: loss: 0.0920532111:
4: 89664: loss: 0.0913940976:
4: 96064: loss: 0.0910308699:
4: 102464: loss: 0.0911215041:
4: 108864: loss: 0.0908083410:
4: 115264: loss: 0.0906437137:
4: 121664: loss: 0.0901627993:
4: 128064: loss: 0.0900918024:
4: 134464: loss: 0.0898487524:
4: 140864: loss: 0.0895851861:
4: 147264: loss: 0.0890087180:
4: 153664: loss: 0.0886027398:
4: 160064: loss: 0.0885018712:
4: 166464: loss: 0.0882030998:
4: 172864: loss: 0.0878006388:
4: 179264: loss: 0.0878646954:
4: 185664: loss: 0.0878580089:
4: 192064: loss: 0.0874472246:
4: 198464: loss: 0.0870452586:
4: 204864: loss: 0.0867858114:
4: 211264: loss: 0.0865979630:
4: 217664: loss: 0.0863938270:
4: 224064: loss: 0.0863561816:
4: 230464: loss: 0.0861059237:
4: 236864: loss: 0.0859149794:
4: 243264: loss: 0.0858269868:
4: 249664: loss: 0.0855287795:
4: 256064: loss: 0.0853409475:
4: 262464: loss: 0.0851314108:
4: 268864: loss: 0.0849343886:
4: 275264: loss: 0.0847272817:
4: 281664: loss: 0.0843946122:
4: 288064: loss: 0.0841501939:
4: 294464: loss: 0.0839287797:
4: 300864: loss: 0.0837002466:
4: 307264: loss: 0.0836677255:
4: 313664: loss: 0.0835744744:
4: 320064: loss: 0.0834478617:
4: 326464: loss: 0.0832536275:
Dev-Acc: 4: Accuracy: 0.1107318625: precision: 0.0564882585: recall: 0.9068185683: f1: 0.1063515804
Train-Acc: 4: Accuracy: 0.9795855284: precision: 0.9975675081: recall: 0.9602565699: f1: 0.9785565152
5: 6464: loss: 0.0762607813:
5: 12864: loss: 0.0761073338:
5: 19264: loss: 0.0739429862:
5: 25664: loss: 0.0736229142:
5: 32064: loss: 0.0741082672:
5: 38464: loss: 0.0738398679:
5: 44864: loss: 0.0731897390:
5: 51264: loss: 0.0727448307:
5: 57664: loss: 0.0729209924:
5: 64064: loss: 0.0726723325:
5: 70464: loss: 0.0720329793:
5: 76864: loss: 0.0720110367:
5: 83264: loss: 0.0715616457:
5: 89664: loss: 0.0714602716:
5: 96064: loss: 0.0716233436:
5: 102464: loss: 0.0713690367:
5: 108864: loss: 0.0709909814:
5: 115264: loss: 0.0706550990:
5: 121664: loss: 0.0703048421:
5: 128064: loss: 0.0705829937:
5: 134464: loss: 0.0705106253:
5: 140864: loss: 0.0703748711:
5: 147264: loss: 0.0703203214:
5: 153664: loss: 0.0702889166:
5: 160064: loss: 0.0703110526:
5: 166464: loss: 0.0701818382:
5: 172864: loss: 0.0701534478:
5: 179264: loss: 0.0701396826:
5: 185664: loss: 0.0701933736:
5: 192064: loss: 0.0700821447:
5: 198464: loss: 0.0700025401:
5: 204864: loss: 0.0698953687:
5: 211264: loss: 0.0699094265:
5: 217664: loss: 0.0698347307:
5: 224064: loss: 0.0699438151:
5: 230464: loss: 0.0699100641:
5: 236864: loss: 0.0697249478:
5: 243264: loss: 0.0696168189:
5: 249664: loss: 0.0694378063:
5: 256064: loss: 0.0693309882:
5: 262464: loss: 0.0692123850:
5: 268864: loss: 0.0692145755:
5: 275264: loss: 0.0692877561:
5: 281664: loss: 0.0691423767:
5: 288064: loss: 0.0690382504:
5: 294464: loss: 0.0688221136:
5: 300864: loss: 0.0687994922:
5: 307264: loss: 0.0685826843:
5: 313664: loss: 0.0684461889:
5: 320064: loss: 0.0683931894:
5: 326464: loss: 0.0682903769:
Dev-Acc: 5: Accuracy: 0.1039252281: precision: 0.0565750360: recall: 0.9158306410: f1: 0.1065669456
Train-Acc: 5: Accuracy: 0.9828751683: precision: 0.9986031092: recall: 0.9660480757: f1: 0.9820558677
6: 6464: loss: 0.0662314964:
6: 12864: loss: 0.0634390478:
6: 19264: loss: 0.0626758625:
6: 25664: loss: 0.0630055440:
6: 32064: loss: 0.0628363040:
6: 38464: loss: 0.0629679014:
6: 44864: loss: 0.0630830520:
6: 51264: loss: 0.0628411578:
6: 57664: loss: 0.0631694172:
6: 64064: loss: 0.0627091814:
6: 70464: loss: 0.0626219448:
6: 76864: loss: 0.0622783317:
6: 83264: loss: 0.0625401896:
6: 89664: loss: 0.0619515619:
6: 96064: loss: 0.0619828392:
6: 102464: loss: 0.0619338000:
6: 108864: loss: 0.0619402500:
6: 115264: loss: 0.0616311533:
6: 121664: loss: 0.0614167174:
6: 128064: loss: 0.0612511103:
6: 134464: loss: 0.0610800806:
6: 140864: loss: 0.0611334204:
6: 147264: loss: 0.0613589297:
6: 153664: loss: 0.0611893016:
6: 160064: loss: 0.0608557604:
6: 166464: loss: 0.0607109712:
6: 172864: loss: 0.0605270321:
6: 179264: loss: 0.0605249007:
6: 185664: loss: 0.0605036965:
6: 192064: loss: 0.0603097757:
6: 198464: loss: 0.0601688047:
6: 204864: loss: 0.0600952135:
6: 211264: loss: 0.0601191294:
6: 217664: loss: 0.0600480387:
6: 224064: loss: 0.0599061311:
6: 230464: loss: 0.0599270680:
6: 236864: loss: 0.0597688417:
6: 243264: loss: 0.0595608739:
6: 249664: loss: 0.0593012420:
6: 256064: loss: 0.0592965017:
6: 262464: loss: 0.0593096373:
6: 268864: loss: 0.0592702325:
6: 275264: loss: 0.0591837998:
6: 281664: loss: 0.0590751402:
6: 288064: loss: 0.0588733076:
6: 294464: loss: 0.0588979965:
6: 300864: loss: 0.0589316141:
6: 307264: loss: 0.0589689770:
6: 313664: loss: 0.0589286401:
6: 320064: loss: 0.0588462796:
6: 326464: loss: 0.0588340554:
Dev-Acc: 6: Accuracy: 0.0997578949: precision: 0.0565027859: recall: 0.9190613841: f1: 0.1064605082
Train-Acc: 6: Accuracy: 0.9846453667: precision: 0.9992358815: recall: 0.9690870594: f1: 0.9839305755
7: 6464: loss: 0.0558760115:
7: 12864: loss: 0.0551209782:
7: 19264: loss: 0.0561503699:
7: 25664: loss: 0.0560367663:
7: 32064: loss: 0.0554482347:
7: 38464: loss: 0.0551152972:
7: 44864: loss: 0.0546057054:
7: 51264: loss: 0.0544940116:
7: 57664: loss: 0.0538252720:
7: 64064: loss: 0.0538592224:
7: 70464: loss: 0.0535466222:
7: 76864: loss: 0.0534072272:
7: 83264: loss: 0.0531330512:
7: 89664: loss: 0.0528079471:
7: 96064: loss: 0.0532132395:
7: 102464: loss: 0.0536596289:
7: 108864: loss: 0.0534489696:
7: 115264: loss: 0.0531547905:
7: 121664: loss: 0.0530866317:
7: 128064: loss: 0.0530521453:
7: 134464: loss: 0.0531482638:
7: 140864: loss: 0.0530074079:
7: 147264: loss: 0.0528591677:
7: 153664: loss: 0.0529820931:
7: 160064: loss: 0.0529347561:
7: 166464: loss: 0.0528874508:
7: 172864: loss: 0.0528275947:
7: 179264: loss: 0.0527717299:
7: 185664: loss: 0.0526782869:
7: 192064: loss: 0.0527532080:
7: 198464: loss: 0.0525884469:
7: 204864: loss: 0.0527000595:
7: 211264: loss: 0.0527727007:
7: 217664: loss: 0.0527387874:
7: 224064: loss: 0.0525516636:
7: 230464: loss: 0.0525205905:
7: 236864: loss: 0.0525032197:
7: 243264: loss: 0.0525496615:
7: 249664: loss: 0.0525374476:
7: 256064: loss: 0.0524260548:
7: 262464: loss: 0.0523148706:
7: 268864: loss: 0.0524340961:
7: 275264: loss: 0.0524050655:
7: 281664: loss: 0.0522641900:
7: 288064: loss: 0.0522936030:
7: 294464: loss: 0.0522596265:
7: 300864: loss: 0.0522573646:
7: 307264: loss: 0.0521443572:
7: 313664: loss: 0.0521184244:
7: 320064: loss: 0.0520799506:
7: 326464: loss: 0.0520691690:
Dev-Acc: 7: Accuracy: 0.0941022336: precision: 0.0564907216: recall: 0.9250127529: f1: 0.1064787630
Train-Acc: 7: Accuracy: 0.9868535399: precision: 0.9991309904: recall: 0.9737451737: f1: 0.9862747572
8: 6464: loss: 0.0496426902:
8: 12864: loss: 0.0461162223:
8: 19264: loss: 0.0464137820:
8: 25664: loss: 0.0467634047:
8: 32064: loss: 0.0477782680:
8: 38464: loss: 0.0478074917:
8: 44864: loss: 0.0482116768:
8: 51264: loss: 0.0479810897:
8: 57664: loss: 0.0474125995:
8: 64064: loss: 0.0476261793:
8: 70464: loss: 0.0477730774:
8: 76864: loss: 0.0477896483:
8: 83264: loss: 0.0479309356:
8: 89664: loss: 0.0481502942:
8: 96064: loss: 0.0482119447:
8: 102464: loss: 0.0479408870:
8: 108864: loss: 0.0477866960:
8: 115264: loss: 0.0478991637:
8: 121664: loss: 0.0477245242:
8: 128064: loss: 0.0476157288:
8: 134464: loss: 0.0474880154:
8: 140864: loss: 0.0472180560:
8: 147264: loss: 0.0472905401:
8: 153664: loss: 0.0472628210:
8: 160064: loss: 0.0471550439:
8: 166464: loss: 0.0474573387:
8: 172864: loss: 0.0475138805:
8: 179264: loss: 0.0473491041:
8: 185664: loss: 0.0472355003:
8: 192064: loss: 0.0472264041:
8: 198464: loss: 0.0471506347:
8: 204864: loss: 0.0470843077:
8: 211264: loss: 0.0472261232:
8: 217664: loss: 0.0472040216:
8: 224064: loss: 0.0473218297:
8: 230464: loss: 0.0472564801:
8: 236864: loss: 0.0473256343:
8: 243264: loss: 0.0475030168:
8: 249664: loss: 0.0474495783:
8: 256064: loss: 0.0474353614:
8: 262464: loss: 0.0474417686:
8: 268864: loss: 0.0474717290:
8: 275264: loss: 0.0474066709:
8: 281664: loss: 0.0472878351:
8: 288064: loss: 0.0472795858:
8: 294464: loss: 0.0473339906:
8: 300864: loss: 0.0471795178:
8: 307264: loss: 0.0471733666:
8: 313664: loss: 0.0471771640:
8: 320064: loss: 0.0470505241:
8: 326464: loss: 0.0470885761:
Dev-Acc: 8: Accuracy: 0.0908874422: precision: 0.0564757244: recall: 0.9282434960: f1: 0.1064734450
Train-Acc: 8: Accuracy: 0.9883276820: precision: 0.9993563517: recall: 0.9765661975: f1: 0.9878298446
9: 6464: loss: 0.0463172066:
9: 12864: loss: 0.0432697249:
9: 19264: loss: 0.0431634114:
9: 25664: loss: 0.0439085106:
9: 32064: loss: 0.0443284548:
9: 38464: loss: 0.0439865471:
9: 44864: loss: 0.0440214216:
9: 51264: loss: 0.0436658836:
9: 57664: loss: 0.0439236082:
9: 64064: loss: 0.0442446003:
9: 70464: loss: 0.0443336423:
9: 76864: loss: 0.0438416400:
9: 83264: loss: 0.0438543218:
9: 89664: loss: 0.0440852970:
9: 96064: loss: 0.0441253561:
9: 102464: loss: 0.0442757821:
9: 108864: loss: 0.0441054963:
9: 115264: loss: 0.0443901754:
9: 121664: loss: 0.0441529154:
9: 128064: loss: 0.0440720483:
9: 134464: loss: 0.0440224741:
9: 140864: loss: 0.0439712583:
9: 147264: loss: 0.0441845918:
9: 153664: loss: 0.0443515680:
9: 160064: loss: 0.0441777557:
9: 166464: loss: 0.0442488627:
9: 172864: loss: 0.0442669890:
9: 179264: loss: 0.0441981666:
9: 185664: loss: 0.0442738614:
9: 192064: loss: 0.0442179763:
9: 198464: loss: 0.0441789223:
9: 204864: loss: 0.0441040415:
9: 211264: loss: 0.0441417049:
9: 217664: loss: 0.0441622148:
9: 224064: loss: 0.0441822064:
9: 230464: loss: 0.0439959999:
9: 236864: loss: 0.0439254447:
9: 243264: loss: 0.0438564900:
9: 249664: loss: 0.0438267914:
9: 256064: loss: 0.0437643664:
9: 262464: loss: 0.0436986748:
9: 268864: loss: 0.0437020424:
9: 275264: loss: 0.0437014576:
9: 281664: loss: 0.0437050345:
9: 288064: loss: 0.0436284327:
9: 294464: loss: 0.0434485728:
9: 300864: loss: 0.0435163153:
9: 307264: loss: 0.0435129743:
9: 313664: loss: 0.0434823419:
9: 320064: loss: 0.0434220593:
9: 326464: loss: 0.0433620105:
Dev-Acc: 9: Accuracy: 0.0891808197: precision: 0.0564853340: recall: 0.9302839653: f1: 0.1065039226
Train-Acc: 9: Accuracy: 0.9890707731: precision: 0.9995607893: recall: 0.9778988666: f1: 0.9886111811
10: 6464: loss: 0.0368331384:
10: 12864: loss: 0.0405948913:
10: 19264: loss: 0.0421022064:
10: 25664: loss: 0.0414209179:
10: 32064: loss: 0.0413330493:
10: 38464: loss: 0.0414020024:
10: 44864: loss: 0.0416668917:
10: 51264: loss: 0.0411151036:
10: 57664: loss: 0.0412738085:
10: 64064: loss: 0.0414507330:
10: 70464: loss: 0.0409805320:
10: 76864: loss: 0.0407927662:
10: 83264: loss: 0.0404904871:
10: 89664: loss: 0.0401352628:
10: 96064: loss: 0.0400852707:
10: 102464: loss: 0.0400855818:
10: 108864: loss: 0.0398508337:
10: 115264: loss: 0.0399125317:
10: 121664: loss: 0.0399660161:
10: 128064: loss: 0.0403134982:
10: 134464: loss: 0.0403381638:
10: 140864: loss: 0.0402381910:
10: 147264: loss: 0.0403603940:
10: 153664: loss: 0.0404940951:
10: 160064: loss: 0.0405352469:
10: 166464: loss: 0.0404517288:
10: 172864: loss: 0.0403962652:
10: 179264: loss: 0.0403476581:
10: 185664: loss: 0.0404029412:
10: 192064: loss: 0.0403819349:
10: 198464: loss: 0.0403876761:
10: 204864: loss: 0.0402292568:
10: 211264: loss: 0.0402022650:
10: 217664: loss: 0.0401034619:
10: 224064: loss: 0.0398055374:
10: 230464: loss: 0.0398846749:
10: 236864: loss: 0.0398308082:
10: 243264: loss: 0.0399174853:
10: 249664: loss: 0.0398761958:
10: 256064: loss: 0.0398793505:
10: 262464: loss: 0.0399887451:
10: 268864: loss: 0.0399198398:
10: 275264: loss: 0.0398442160:
10: 281664: loss: 0.0397743287:
10: 288064: loss: 0.0398091718:
10: 294464: loss: 0.0398926604:
10: 300864: loss: 0.0399908730:
10: 307264: loss: 0.0400435718:
10: 313664: loss: 0.0399848978:
10: 320064: loss: 0.0399041706:
10: 326464: loss: 0.0398874643:
Dev-Acc: 10: Accuracy: 0.0880794525: precision: 0.0564664151: recall: 0.9311341609: f1: 0.1064758553
Train-Acc: 10: Accuracy: 0.9896447659: precision: 0.9997011053: recall: 0.9789450741: f1: 0.9892142241
11: 6464: loss: 0.0355526196:
11: 12864: loss: 0.0382745596:
11: 19264: loss: 0.0389487369:
11: 25664: loss: 0.0394698976:
11: 32064: loss: 0.0389700605:
11: 38464: loss: 0.0377226669:
11: 44864: loss: 0.0373322462:
11: 51264: loss: 0.0380243189:
11: 57664: loss: 0.0387046527:
11: 64064: loss: 0.0385325377:
11: 70464: loss: 0.0382692076:
11: 76864: loss: 0.0380330458:
11: 83264: loss: 0.0380109865:
11: 89664: loss: 0.0380412537:
11: 96064: loss: 0.0379089388:
11: 102464: loss: 0.0383726956:
11: 108864: loss: 0.0382774938:
11: 115264: loss: 0.0383183804:
11: 121664: loss: 0.0381661807:
11: 128064: loss: 0.0382429008:
11: 134464: loss: 0.0383621939:
11: 140864: loss: 0.0382476264:
11: 147264: loss: 0.0382254177:
11: 153664: loss: 0.0382413320:
11: 160064: loss: 0.0382709716:
11: 166464: loss: 0.0383152290:
11: 172864: loss: 0.0383910467:
11: 179264: loss: 0.0382136119:
11: 185664: loss: 0.0381491411:
11: 192064: loss: 0.0381231867:
11: 198464: loss: 0.0381715288:
11: 204864: loss: 0.0381899997:
11: 211264: loss: 0.0380001980:
11: 217664: loss: 0.0379149937:
11: 224064: loss: 0.0378202242:
11: 230464: loss: 0.0376721587:
11: 236864: loss: 0.0377322106:
11: 243264: loss: 0.0377710849:
11: 249664: loss: 0.0377608021:
11: 256064: loss: 0.0376487864:
11: 262464: loss: 0.0377223303:
11: 268864: loss: 0.0376252110:
11: 275264: loss: 0.0374679451:
11: 281664: loss: 0.0373660490:
11: 288064: loss: 0.0372919673:
11: 294464: loss: 0.0372958091:
11: 300864: loss: 0.0373237250:
11: 307264: loss: 0.0373079457:
11: 313664: loss: 0.0371748049:
11: 320064: loss: 0.0371279701:
11: 326464: loss: 0.0370106167:
Dev-Acc: 11: Accuracy: 0.0862835348: precision: 0.0563703544: recall: 0.9313042000: f1: 0.1063061664
Train-Acc: 11: Accuracy: 0.9901854992: precision: 0.9997395387: recall: 0.9800224187: f1: 0.9897827940
12: 6464: loss: 0.0370230406:
12: 12864: loss: 0.0359632289:
12: 19264: loss: 0.0361445505:
12: 25664: loss: 0.0353409860:
12: 32064: loss: 0.0355550106:
12: 38464: loss: 0.0362676787:
12: 44864: loss: 0.0360052093:
12: 51264: loss: 0.0357280293:
12: 57664: loss: 0.0355277043:
12: 64064: loss: 0.0357656107:
12: 70464: loss: 0.0357020365:
12: 76864: loss: 0.0353141356:
12: 83264: loss: 0.0350461538:
12: 89664: loss: 0.0354104985:
12: 96064: loss: 0.0354124705:
12: 102464: loss: 0.0352522436:
12: 108864: loss: 0.0350304213:
12: 115264: loss: 0.0349828109:
12: 121664: loss: 0.0352131748:
12: 128064: loss: 0.0353248194:
12: 134464: loss: 0.0351933222:
12: 140864: loss: 0.0349431167:
12: 147264: loss: 0.0352161614:
12: 153664: loss: 0.0353633805:
12: 160064: loss: 0.0354102213:
12: 166464: loss: 0.0352951739:
12: 172864: loss: 0.0354666250:
12: 179264: loss: 0.0355876139:
12: 185664: loss: 0.0355861878:
12: 192064: loss: 0.0357424206:
12: 198464: loss: 0.0356127272:
12: 204864: loss: 0.0355993139:
12: 211264: loss: 0.0355532796:
12: 217664: loss: 0.0354214666:
12: 224064: loss: 0.0355803252:
12: 230464: loss: 0.0354452665:
12: 236864: loss: 0.0353493066:
12: 243264: loss: 0.0352906185:
12: 249664: loss: 0.0352985060:
12: 256064: loss: 0.0351981001:
12: 262464: loss: 0.0352709853:
12: 268864: loss: 0.0352900111:
12: 275264: loss: 0.0352914412:
12: 281664: loss: 0.0352559871:
12: 288064: loss: 0.0351080161:
12: 294464: loss: 0.0350332952:
12: 300864: loss: 0.0348636429:
12: 307264: loss: 0.0349215840:
12: 313664: loss: 0.0349184051:
12: 320064: loss: 0.0347946235:
12: 326464: loss: 0.0347736694:
Dev-Acc: 12: Accuracy: 0.0855096057: precision: 0.0564710962: recall: 0.9340248257: f1: 0.1065030247
Train-Acc: 12: Accuracy: 0.9906778336: precision: 0.9997905560: recall: 0.9809876697: f1: 0.9902998680
13: 6464: loss: 0.0332394640:
13: 12864: loss: 0.0339049154:
13: 19264: loss: 0.0332511702:
13: 25664: loss: 0.0347893409:
13: 32064: loss: 0.0352039863:
13: 38464: loss: 0.0347134363:
13: 44864: loss: 0.0344534020:
13: 51264: loss: 0.0339013839:
13: 57664: loss: 0.0345051008:
13: 64064: loss: 0.0345192283:
13: 70464: loss: 0.0341840832:
13: 76864: loss: 0.0339412778:
13: 83264: loss: 0.0339708894:
13: 89664: loss: 0.0339193039:
13: 96064: loss: 0.0339139902:
13: 102464: loss: 0.0340002258:
13: 108864: loss: 0.0337746407:
13: 115264: loss: 0.0338754912:
13: 121664: loss: 0.0337505483:
13: 128064: loss: 0.0333027418:
13: 134464: loss: 0.0332248699:
13: 140864: loss: 0.0331378231:
13: 147264: loss: 0.0331810567:
13: 153664: loss: 0.0332262768:
13: 160064: loss: 0.0330796838:
13: 166464: loss: 0.0330559636:
13: 172864: loss: 0.0330885819:
13: 179264: loss: 0.0331763432:
13: 185664: loss: 0.0330532513:
13: 192064: loss: 0.0332840063:
13: 198464: loss: 0.0332163671:
13: 204864: loss: 0.0330783027:
13: 211264: loss: 0.0329921330:
13: 217664: loss: 0.0330413467:
13: 224064: loss: 0.0332682253:
13: 230464: loss: 0.0332136528:
13: 236864: loss: 0.0331625330:
13: 243264: loss: 0.0331350425:
13: 249664: loss: 0.0331025592:
13: 256064: loss: 0.0330229545:
13: 262464: loss: 0.0329388264:
13: 268864: loss: 0.0329619350:
13: 275264: loss: 0.0329796413:
13: 281664: loss: 0.0328766091:
13: 288064: loss: 0.0328145376:
13: 294464: loss: 0.0328499487:
13: 300864: loss: 0.0327702827:
13: 307264: loss: 0.0327231440:
13: 313664: loss: 0.0326895665:
13: 320064: loss: 0.0326670228:
13: 326464: loss: 0.0326269019:
Dev-Acc: 13: Accuracy: 0.0841800272: precision: 0.0567574499: recall: 0.9408263901: f1: 0.1070564789
Train-Acc: 13: Accuracy: 0.9914572239: precision: 0.9998225692: recall: 0.9825632084: f1: 0.9911177557
14: 6464: loss: 0.0300302318:
14: 12864: loss: 0.0337065820:
14: 19264: loss: 0.0323522759:
14: 25664: loss: 0.0324251316:
14: 32064: loss: 0.0331139366:
14: 38464: loss: 0.0326103670:
14: 44864: loss: 0.0324041462:
14: 51264: loss: 0.0327843754:
14: 57664: loss: 0.0327742316:
14: 64064: loss: 0.0328630901:
14: 70464: loss: 0.0325800067:
14: 76864: loss: 0.0323605185:
14: 83264: loss: 0.0321861063:
14: 89664: loss: 0.0324678958:
14: 96064: loss: 0.0327155597:
14: 102464: loss: 0.0326559058:
14: 108864: loss: 0.0326815038:
14: 115264: loss: 0.0327685521:
14: 121664: loss: 0.0327298376:
14: 128064: loss: 0.0326698752:
14: 134464: loss: 0.0323084221:
14: 140864: loss: 0.0324715238:
14: 147264: loss: 0.0324301321:
14: 153664: loss: 0.0323132582:
14: 160064: loss: 0.0323592391:
14: 166464: loss: 0.0322886983:
14: 172864: loss: 0.0320313545:
14: 179264: loss: 0.0317939769:
14: 185664: loss: 0.0315911079:
14: 192064: loss: 0.0315813073:
14: 198464: loss: 0.0316640865:
14: 204864: loss: 0.0314743068:
14: 211264: loss: 0.0315417876:
14: 217664: loss: 0.0313828279:
14: 224064: loss: 0.0313052479:
14: 230464: loss: 0.0312950848:
14: 236864: loss: 0.0312486057:
14: 243264: loss: 0.0312684573:
14: 249664: loss: 0.0312587855:
14: 256064: loss: 0.0311820825:
14: 262464: loss: 0.0311584948:
14: 268864: loss: 0.0311613674:
14: 275264: loss: 0.0310445371:
14: 281664: loss: 0.0310408432:
14: 288064: loss: 0.0309990816:
14: 294464: loss: 0.0309463678:
14: 300864: loss: 0.0309151455:
14: 307264: loss: 0.0308987305:
14: 313664: loss: 0.0308980938:
14: 320064: loss: 0.0307798995:
14: 326464: loss: 0.0307239378:
Dev-Acc: 14: Accuracy: 0.0816895515: precision: 0.0571735132: recall: 0.9513688148: f1: 0.1078647786
Train-Acc: 14: Accuracy: 0.9923785329: precision: 0.9997660168: recall: 0.9845186200: f1: 0.9920837373
15: 6464: loss: 0.0273540380:
15: 12864: loss: 0.0290935351:
15: 19264: loss: 0.0297669231:
15: 25664: loss: 0.0289630715:
15: 32064: loss: 0.0286885175:
15: 38464: loss: 0.0284926580:
15: 44864: loss: 0.0286770723:
15: 51264: loss: 0.0292054597:
15: 57664: loss: 0.0293584417:
15: 64064: loss: 0.0291872155:
15: 70464: loss: 0.0295568601:
15: 76864: loss: 0.0296681445:
15: 83264: loss: 0.0293563070:
15: 89664: loss: 0.0295907001:
15: 96064: loss: 0.0296068828:
15: 102464: loss: 0.0295939203:
15: 108864: loss: 0.0291553936:
15: 115264: loss: 0.0290862293:
15: 121664: loss: 0.0293178851:
15: 128064: loss: 0.0296713071:
15: 134464: loss: 0.0295999320:
15: 140864: loss: 0.0296115758:
15: 147264: loss: 0.0297543548:
15: 153664: loss: 0.0296968849:
15: 160064: loss: 0.0299535965:
15: 166464: loss: 0.0298693070:
15: 172864: loss: 0.0297767581:
15: 179264: loss: 0.0298627419:
15: 185664: loss: 0.0297694906:
15: 192064: loss: 0.0298020551:
15: 198464: loss: 0.0297047875:
15: 204864: loss: 0.0297541498:
15: 211264: loss: 0.0297709478:
15: 217664: loss: 0.0298451033:
15: 224064: loss: 0.0298652231:
15: 230464: loss: 0.0298508539:
15: 236864: loss: 0.0297641898:
15: 243264: loss: 0.0297251380:
15: 249664: loss: 0.0296854033:
15: 256064: loss: 0.0295696846:
15: 262464: loss: 0.0296500003:
15: 268864: loss: 0.0296545955:
15: 275264: loss: 0.0296788471:
15: 281664: loss: 0.0296374577:
15: 288064: loss: 0.0296256391:
15: 294464: loss: 0.0295249170:
15: 300864: loss: 0.0295425628:
15: 307264: loss: 0.0295168077:
15: 313664: loss: 0.0293527205:
15: 320064: loss: 0.0292916539:
15: 326464: loss: 0.0293615170:
Dev-Acc: 15: Accuracy: 0.0810446069: precision: 0.0571898261: recall: 0.9523890495: f1: 0.1079003641
Train-Acc: 15: Accuracy: 0.9926595092: precision: 0.9997787890: recall: 0.9850853157: f1: 0.9923776662
16: 6464: loss: 0.0254415165:
16: 12864: loss: 0.0273126538:
16: 19264: loss: 0.0278893656:
16: 25664: loss: 0.0265760463:
16: 32064: loss: 0.0265917291:
16: 38464: loss: 0.0266873006:
16: 44864: loss: 0.0278619110:
16: 51264: loss: 0.0276592471:
16: 57664: loss: 0.0278451611:
16: 64064: loss: 0.0282146694:
16: 70464: loss: 0.0281535312:
16: 76864: loss: 0.0278826555:
16: 83264: loss: 0.0280674154:
16: 89664: loss: 0.0276243069:
16: 96064: loss: 0.0278518256:
16: 102464: loss: 0.0279021119:
16: 108864: loss: 0.0281157397:
16: 115264: loss: 0.0283413744:
16: 121664: loss: 0.0282655321:
16: 128064: loss: 0.0283658720:
16: 134464: loss: 0.0282540683:
16: 140864: loss: 0.0280795613:
16: 147264: loss: 0.0280181831:
16: 153664: loss: 0.0279674741:
16: 160064: loss: 0.0280388371:
16: 166464: loss: 0.0281369524:
16: 172864: loss: 0.0283195108:
16: 179264: loss: 0.0281479563:
16: 185664: loss: 0.0280060345:
16: 192064: loss: 0.0280732509:
16: 198464: loss: 0.0279696288:
16: 204864: loss: 0.0280467800:
16: 211264: loss: 0.0282229516:
16: 217664: loss: 0.0281809078:
16: 224064: loss: 0.0280963910:
16: 230464: loss: 0.0280703898:
16: 236864: loss: 0.0280176003:
16: 243264: loss: 0.0279378849:
16: 249664: loss: 0.0278743388:
16: 256064: loss: 0.0277395480:
16: 262464: loss: 0.0277172144:
16: 268864: loss: 0.0277565134:
16: 275264: loss: 0.0277480733:
16: 281664: loss: 0.0277424576:
16: 288064: loss: 0.0278245760:
16: 294464: loss: 0.0278881073:
16: 300864: loss: 0.0278988740:
16: 307264: loss: 0.0278632346:
16: 313664: loss: 0.0278389205:
16: 320064: loss: 0.0276980307:
16: 326464: loss: 0.0276198890:
Dev-Acc: 16: Accuracy: 0.0789807886: precision: 0.0572580892: recall: 0.9559598708: f1: 0.1080447400
Train-Acc: 16: Accuracy: 0.9932787418: precision: 0.9997853847: recall: 0.9863557105: f1: 0.9930251440
17: 6464: loss: 0.0292106175:
17: 12864: loss: 0.0295603490:
17: 19264: loss: 0.0272900519:
17: 25664: loss: 0.0263799938:
17: 32064: loss: 0.0269154308:
17: 38464: loss: 0.0274503926:
17: 44864: loss: 0.0277429760:
17: 51264: loss: 0.0283737045:
17: 57664: loss: 0.0277882527:
17: 64064: loss: 0.0279258426:
17: 70464: loss: 0.0280536222:
17: 76864: loss: 0.0281848247:
17: 83264: loss: 0.0279892311:
17: 89664: loss: 0.0276703086:
17: 96064: loss: 0.0272939088:
17: 102464: loss: 0.0269950020:
17: 108864: loss: 0.0271003544:
17: 115264: loss: 0.0269593404:
17: 121664: loss: 0.0270011936:
17: 128064: loss: 0.0271239369:
17: 134464: loss: 0.0268538389:
17: 140864: loss: 0.0267654341:
17: 147264: loss: 0.0266835555:
17: 153664: loss: 0.0265055813:
17: 160064: loss: 0.0265615301:
17: 166464: loss: 0.0263755875:
17: 172864: loss: 0.0263285965:
17: 179264: loss: 0.0263698756:
17: 185664: loss: 0.0263160489:
17: 192064: loss: 0.0262322864:
17: 198464: loss: 0.0261459662:
17: 204864: loss: 0.0261511582:
17: 211264: loss: 0.0261126678:
17: 217664: loss: 0.0260553929:
17: 224064: loss: 0.0261287878:
17: 230464: loss: 0.0261186358:
17: 236864: loss: 0.0261197264:
17: 243264: loss: 0.0261162658:
17: 249664: loss: 0.0261027666:
17: 256064: loss: 0.0261244938:
17: 262464: loss: 0.0261385113:
17: 268864: loss: 0.0261639475:
17: 275264: loss: 0.0261604441:
17: 281664: loss: 0.0261710780:
17: 288064: loss: 0.0262149121:
17: 294464: loss: 0.0262579740:
17: 300864: loss: 0.0262691930:
17: 307264: loss: 0.0262287343:
17: 313664: loss: 0.0262663619:
17: 320064: loss: 0.0263140318:
17: 326464: loss: 0.0263407860:
Dev-Acc: 17: Accuracy: 0.0776512101: precision: 0.0572521025: recall: 0.9573201836: f1: 0.1080427565
Train-Acc: 17: Accuracy: 0.9936140776: precision: 0.9997729251: recall: 0.9870594096: f1: 0.9933754912
18: 6464: loss: 0.0231632250:
18: 12864: loss: 0.0255757128:
18: 19264: loss: 0.0245180262:
18: 25664: loss: 0.0249914651:
18: 32064: loss: 0.0246862907:
18: 38464: loss: 0.0240544618:
18: 44864: loss: 0.0236728268:
18: 51264: loss: 0.0241774484:
18: 57664: loss: 0.0240668142:
18: 64064: loss: 0.0242412443:
18: 70464: loss: 0.0243013949:
18: 76864: loss: 0.0244725980:
18: 83264: loss: 0.0245219456:
18: 89664: loss: 0.0244853633:
18: 96064: loss: 0.0247043862:
18: 102464: loss: 0.0246504335:
18: 108864: loss: 0.0247266418:
18: 115264: loss: 0.0248535753:
18: 121664: loss: 0.0250773737:
18: 128064: loss: 0.0249595010:
18: 134464: loss: 0.0252725904:
18: 140864: loss: 0.0251763123:
18: 147264: loss: 0.0251256306:
18: 153664: loss: 0.0249790187:
18: 160064: loss: 0.0250449305:
18: 166464: loss: 0.0251400678:
18: 172864: loss: 0.0251664704:
18: 179264: loss: 0.0252465904:
18: 185664: loss: 0.0251415259:
18: 192064: loss: 0.0251570917:
18: 198464: loss: 0.0250696527:
18: 204864: loss: 0.0251568897:
18: 211264: loss: 0.0252775184:
18: 217664: loss: 0.0253009271:
18: 224064: loss: 0.0252233636:
18: 230464: loss: 0.0251913989:
18: 236864: loss: 0.0251930064:
18: 243264: loss: 0.0251863270:
18: 249664: loss: 0.0252100573:
18: 256064: loss: 0.0250399096:
18: 262464: loss: 0.0251308115:
18: 268864: loss: 0.0252431670:
18: 275264: loss: 0.0252420235:
18: 281664: loss: 0.0252301320:
18: 288064: loss: 0.0252476391:
18: 294464: loss: 0.0252210877:
18: 300864: loss: 0.0252153009:
18: 307264: loss: 0.0252934193:
18: 313664: loss: 0.0252052962:
18: 320064: loss: 0.0251889963:
18: 326464: loss: 0.0252177543:
Dev-Acc: 18: Accuracy: 0.0772741660: precision: 0.0572749911: recall: 0.9581703792: f1: 0.1080889264
Train-Acc: 18: Accuracy: 0.9938678145: precision: 0.9997856485: recall: 0.9875700585: f1: 0.9936403110
19: 6464: loss: 0.0245231903:
19: 12864: loss: 0.0249016170:
19: 19264: loss: 0.0266148740:
19: 25664: loss: 0.0263360195:
19: 32064: loss: 0.0258993097:
19: 38464: loss: 0.0264763189:
19: 44864: loss: 0.0264455031:
19: 51264: loss: 0.0259677306:
19: 57664: loss: 0.0260885610:
19: 64064: loss: 0.0258060358:
19: 70464: loss: 0.0255228445:
19: 76864: loss: 0.0253516645:
19: 83264: loss: 0.0254393830:
19: 89664: loss: 0.0252555086:
19: 96064: loss: 0.0252417888:
19: 102464: loss: 0.0249636949:
19: 108864: loss: 0.0250974692:
19: 115264: loss: 0.0250801472:
19: 121664: loss: 0.0247487847:
19: 128064: loss: 0.0247257463:
19: 134464: loss: 0.0246378333:
19: 140864: loss: 0.0246611412:
19: 147264: loss: 0.0245910347:
19: 153664: loss: 0.0244696577:
19: 160064: loss: 0.0245999830:
19: 166464: loss: 0.0245475857:
19: 172864: loss: 0.0246562237:
19: 179264: loss: 0.0247228823:
19: 185664: loss: 0.0246414067:
19: 192064: loss: 0.0245701084:
19: 198464: loss: 0.0245813591:
19: 204864: loss: 0.0245472882:
19: 211264: loss: 0.0245756747:
19: 217664: loss: 0.0244650235:
19: 224064: loss: 0.0244416835:
19: 230464: loss: 0.0243410762:
19: 236864: loss: 0.0243930492:
19: 243264: loss: 0.0243000392:
19: 249664: loss: 0.0242466850:
19: 256064: loss: 0.0241573687:
19: 262464: loss: 0.0241959977:
19: 268864: loss: 0.0240794802:
19: 275264: loss: 0.0240830861:
19: 281664: loss: 0.0240971285:
19: 288064: loss: 0.0241784998:
19: 294464: loss: 0.0241249973:
19: 300864: loss: 0.0241115931:
19: 307264: loss: 0.0240490212:
19: 313664: loss: 0.0239906316:
19: 320064: loss: 0.0240264184:
19: 326464: loss: 0.0240400946:
Dev-Acc: 19: Accuracy: 0.0764407068: precision: 0.0573160454: recall: 0.9598707703: f1: 0.1081728466
Train-Acc: 19: Accuracy: 0.9942091703: precision: 0.9997795039: recall: 0.9882799851: f1: 0.9939964862
20: 6464: loss: 0.0258792068:
20: 12864: loss: 0.0258214183:
20: 19264: loss: 0.0242933123:
20: 25664: loss: 0.0244916616:
20: 32064: loss: 0.0248063254:
20: 38464: loss: 0.0254063987:
20: 44864: loss: 0.0249010908:
20: 51264: loss: 0.0243265725:
20: 57664: loss: 0.0237614874:
20: 64064: loss: 0.0234143734:
20: 70464: loss: 0.0232588986:
20: 76864: loss: 0.0228716652:
20: 83264: loss: 0.0227493362:
20: 89664: loss: 0.0227151309:
20: 96064: loss: 0.0225352642:
20: 102464: loss: 0.0226671264:
20: 108864: loss: 0.0227621840:
20: 115264: loss: 0.0228071074:
20: 121664: loss: 0.0228117507:
20: 128064: loss: 0.0228240453:
20: 134464: loss: 0.0230634027:
20: 140864: loss: 0.0231596105:
20: 147264: loss: 0.0230811997:
20: 153664: loss: 0.0232674819:
20: 160064: loss: 0.0232276501:
20: 166464: loss: 0.0231910546:
20: 172864: loss: 0.0231055349:
20: 179264: loss: 0.0230038774:
20: 185664: loss: 0.0230357403:
20: 192064: loss: 0.0229946229:
20: 198464: loss: 0.0229708231:
20: 204864: loss: 0.0228774203:
20: 211264: loss: 0.0228023585:
20: 217664: loss: 0.0227996470:
20: 224064: loss: 0.0229332279:
20: 230464: loss: 0.0230963398:
20: 236864: loss: 0.0232037507:
20: 243264: loss: 0.0232555779:
20: 249664: loss: 0.0232513693:
20: 256064: loss: 0.0233414701:
20: 262464: loss: 0.0233687028:
20: 268864: loss: 0.0233944392:
20: 275264: loss: 0.0234349988:
20: 281664: loss: 0.0234152176:
20: 288064: loss: 0.0233724296:
20: 294464: loss: 0.0232810491:
20: 300864: loss: 0.0232467122:
20: 307264: loss: 0.0231793569:
20: 313664: loss: 0.0231442912:
20: 320064: loss: 0.0231601964:
20: 326464: loss: 0.0231022816:
Dev-Acc: 20: Accuracy: 0.0760041252: precision: 0.0572904509: recall: 0.9598707703: f1: 0.1081272626
Train-Acc: 20: Accuracy: 0.9942967296: precision: 0.9998047293: recall: 0.9884356707: f1: 0.9940876950
