1: 6464: loss: 0.7051697975:
1: 12864: loss: 0.7039860907:
1: 19264: loss: 0.7039009903:
1: 25664: loss: 0.7036977553:
1: 32064: loss: 0.7037707511:
1: 38464: loss: 0.7037670939:
1: 44864: loss: 0.7037905587:
1: 51264: loss: 0.7037238371:
1: 57664: loss: 0.7036459293:
1: 64064: loss: 0.7034958264:
1: 70464: loss: 0.7034318875:
Dev-Acc: 1: Accuracy: 0.2751924992: precision: 0.0634813351: recall: 0.8304710083: f1: 0.1179467984
Train-Acc: 1: Accuracy: 0.3711130023: precision: 0.2191648730: recall: 0.8367628690: f1: 0.3473514723
2: 6464: loss: 0.7019201112:
2: 12864: loss: 0.7019983053:
2: 19264: loss: 0.7023975885:
2: 25664: loss: 0.7022267231:
2: 32064: loss: 0.7019807175:
2: 38464: loss: 0.7018728608:
2: 44864: loss: 0.7017757529:
2: 51264: loss: 0.7017292450:
2: 57664: loss: 0.7016696852:
2: 64064: loss: 0.7015322950:
2: 70464: loss: 0.7013886173:
Dev-Acc: 2: Accuracy: 0.2969618142: precision: 0.0643188585: recall: 0.8155075667: f1: 0.1192337812
Train-Acc: 2: Accuracy: 0.3897048235: precision: 0.2220341701: recall: 0.8193412662: f1: 0.3493874576
3: 6464: loss: 0.6999799830:
3: 12864: loss: 0.7001674905:
3: 19264: loss: 0.7002419641:
3: 25664: loss: 0.7001833028:
3: 32064: loss: 0.7000649648:
3: 38464: loss: 0.7001231881:
3: 44864: loss: 0.6999997411:
3: 51264: loss: 0.6999547857:
3: 57664: loss: 0.6997555740:
3: 64064: loss: 0.6997232804:
3: 70464: loss: 0.6996701374:
Dev-Acc: 3: Accuracy: 0.3197432160: precision: 0.0648932330: recall: 0.7947627954: f1: 0.1199892179
Train-Acc: 3: Accuracy: 0.4114785194: precision: 0.2261395021: recall: 0.8020511472: f1: 0.3528050896
4: 6464: loss: 0.6982792020:
4: 12864: loss: 0.6985134444:
4: 19264: loss: 0.6984210805:
4: 25664: loss: 0.6984059215:
4: 32064: loss: 0.6981767989:
4: 38464: loss: 0.6979564065:
4: 44864: loss: 0.6979410509:
4: 51264: loss: 0.6979185550:
4: 57664: loss: 0.6977941733:
4: 64064: loss: 0.6976534254:
4: 70464: loss: 0.6976067042:
Dev-Acc: 4: Accuracy: 0.3463843465: precision: 0.0660040222: recall: 0.7757184152: f1: 0.1216565775
Train-Acc: 4: Accuracy: 0.4334626198: precision: 0.2300675872: recall: 0.7810137401: f1: 0.3554332216
5: 6464: loss: 0.6964683926:
5: 12864: loss: 0.6962313393:
5: 19264: loss: 0.6965274698:
5: 25664: loss: 0.6967932640:
5: 32064: loss: 0.6965668205:
5: 38464: loss: 0.6964098328:
5: 44864: loss: 0.6964211783:
5: 51264: loss: 0.6962806235:
5: 57664: loss: 0.6962427330:
5: 64064: loss: 0.6960644105:
5: 70464: loss: 0.6960451887:
Dev-Acc: 5: Accuracy: 0.3733132184: precision: 0.0672549523: recall: 0.7568440741: f1: 0.1235325137
Train-Acc: 5: Accuracy: 0.4577608407: precision: 0.2353323979: recall: 0.7607652357: f1: 0.3594681909
6: 6464: loss: 0.6940631384:
6: 12864: loss: 0.6944742170:
6: 19264: loss: 0.6948357413:
6: 25664: loss: 0.6948142974:
6: 32064: loss: 0.6945078217:
6: 38464: loss: 0.6944966044:
6: 44864: loss: 0.6943182076:
6: 51264: loss: 0.6943983261:
6: 57664: loss: 0.6944049658:
6: 64064: loss: 0.6943549817:
6: 70464: loss: 0.6941736556:
Dev-Acc: 6: Accuracy: 0.4002023935: precision: 0.0680303342: recall: 0.7306580514: f1: 0.1244713516
Train-Acc: 6: Accuracy: 0.4834264517: precision: 0.2412800069: recall: 0.7380842811: f1: 0.3636746461
7: 6464: loss: 0.6937422246:
7: 12864: loss: 0.6930777815:
7: 19264: loss: 0.6930958066:
7: 25664: loss: 0.6929642464:
7: 32064: loss: 0.6928282245:
7: 38464: loss: 0.6926491408:
7: 44864: loss: 0.6925214341:
7: 51264: loss: 0.6925571731:
7: 57664: loss: 0.6924642270:
7: 64064: loss: 0.6924615067:
7: 70464: loss: 0.6923961013:
Dev-Acc: 7: Accuracy: 0.4275380969: precision: 0.0682022734: recall: 0.6958000340: f1: 0.1242277509
Train-Acc: 7: Accuracy: 0.5099467635: precision: 0.2472965543: recall: 0.7096180396: f1: 0.3667748348
8: 6464: loss: 0.6911508793:
8: 12864: loss: 0.6913539785:
8: 19264: loss: 0.6908957197:
8: 25664: loss: 0.6907925160:
8: 32064: loss: 0.6908911186:
8: 38464: loss: 0.6908353472:
8: 44864: loss: 0.6907246913:
8: 51264: loss: 0.6906528552:
8: 57664: loss: 0.6906346624:
8: 64064: loss: 0.6905933052:
8: 70464: loss: 0.6905873499:
Dev-Acc: 8: Accuracy: 0.4564514160: precision: 0.0687678578: recall: 0.6629824860: f1: 0.1246104923
Train-Acc: 8: Accuracy: 0.5377687216: precision: 0.2545293423: recall: 0.6797712182: f1: 0.3703770037
9: 6464: loss: 0.6894370109:
9: 12864: loss: 0.6893920687:
9: 19264: loss: 0.6893098470:
9: 25664: loss: 0.6893869385:
9: 32064: loss: 0.6893890047:
9: 38464: loss: 0.6894896308:
9: 44864: loss: 0.6893215338:
9: 51264: loss: 0.6892326582:
9: 57664: loss: 0.6893221735:
9: 64064: loss: 0.6891288828:
9: 70464: loss: 0.6890420715:
Dev-Acc: 9: Accuracy: 0.4834497571: precision: 0.0690408198: recall: 0.6289746642: f1: 0.1244239631
Train-Acc: 9: Accuracy: 0.5647097230: precision: 0.2626091109: recall: 0.6507132996: f1: 0.3742013534
10: 6464: loss: 0.6878722328:
10: 12864: loss: 0.6871488523:
10: 19264: loss: 0.6872873410:
10: 25664: loss: 0.6873798645:
10: 32064: loss: 0.6873311797:
10: 38464: loss: 0.6872660982:
10: 44864: loss: 0.6872262002:
10: 51264: loss: 0.6870983780:
10: 57664: loss: 0.6870094896:
10: 64064: loss: 0.6869669652:
10: 70464: loss: 0.6869175826:
Dev-Acc: 10: Accuracy: 0.5116387606: precision: 0.0686559439: recall: 0.5864648869: f1: 0.1229217528
Train-Acc: 10: Accuracy: 0.5918743014: precision: 0.2714619849: recall: 0.6180395766: f1: 0.3772320533
11: 6464: loss: 0.6863942844:
11: 12864: loss: 0.6859780183:
11: 19264: loss: 0.6859631240:
11: 25664: loss: 0.6858720011:
11: 32064: loss: 0.6857722461:
11: 38464: loss: 0.6856600235:
11: 44864: loss: 0.6855581364:
11: 51264: loss: 0.6854385275:
11: 57664: loss: 0.6853271039:
11: 64064: loss: 0.6852246040:
11: 70464: loss: 0.6852048616:
Dev-Acc: 11: Accuracy: 0.5404528379: precision: 0.0697595233: recall: 0.5573881993: f1: 0.1239999243
Train-Acc: 11: Accuracy: 0.6169482470: precision: 0.2809766535: recall: 0.5870751430: f1: 0.3800566042
12: 6464: loss: 0.6835025126:
12: 12864: loss: 0.6830479214:
12: 19264: loss: 0.6832729208:
12: 25664: loss: 0.6835406741:
12: 32064: loss: 0.6834747818:
12: 38464: loss: 0.6834435704:
12: 44864: loss: 0.6835270402:
12: 51264: loss: 0.6834097642:
12: 57664: loss: 0.6834093459:
12: 64064: loss: 0.6833612164:
12: 70464: loss: 0.6833536194:
Dev-Acc: 12: Accuracy: 0.5701996088: precision: 0.0697802703: recall: 0.5162387349: f1: 0.1229423556
Train-Acc: 12: Accuracy: 0.6422720551: precision: 0.2928438212: recall: 0.5574255473: f1: 0.3839692064
13: 6464: loss: 0.6826895422:
13: 12864: loss: 0.6823571879:
13: 19264: loss: 0.6819347076:
13: 25664: loss: 0.6821359217:
13: 32064: loss: 0.6821244804:
13: 38464: loss: 0.6820923234:
13: 44864: loss: 0.6820338452:
13: 51264: loss: 0.6820030271:
13: 57664: loss: 0.6819062530:
13: 64064: loss: 0.6818339863:
13: 70464: loss: 0.6817486889:
Dev-Acc: 13: Accuracy: 0.5979719162: precision: 0.0707398686: recall: 0.4852916171: f1: 0.1234802925
Train-Acc: 13: Accuracy: 0.6653737426: precision: 0.3054531636: recall: 0.5284333706: f1: 0.3871309541
14: 6464: loss: 0.6815377945:
14: 12864: loss: 0.6808824885:
14: 19264: loss: 0.6803474311:
14: 25664: loss: 0.6806087227:
14: 32064: loss: 0.6804865957:
14: 38464: loss: 0.6804589657:
14: 44864: loss: 0.6804030046:
14: 51264: loss: 0.6802547832:
14: 57664: loss: 0.6801239452:
14: 64064: loss: 0.6800605234:
14: 70464: loss: 0.6799955963:
Dev-Acc: 14: Accuracy: 0.6236307025: precision: 0.0715221518: recall: 0.4548546166: f1: 0.1236079664
Train-Acc: 14: Accuracy: 0.6864768863: precision: 0.3194932263: recall: 0.5023338374: f1: 0.3905742837
15: 6464: loss: 0.6783661991:
15: 12864: loss: 0.6784449205:
15: 19264: loss: 0.6786486763:
15: 25664: loss: 0.6786167215:
15: 32064: loss: 0.6786695938:
15: 38464: loss: 0.6785677533:
15: 44864: loss: 0.6785038837:
15: 51264: loss: 0.6783473964:
15: 57664: loss: 0.6782384598:
15: 64064: loss: 0.6781428876:
15: 70464: loss: 0.6780181080:
Dev-Acc: 15: Accuracy: 0.6483072639: precision: 0.0728239510: recall: 0.4284985547: f1: 0.1244905521
Train-Acc: 15: Accuracy: 0.7045164704: precision: 0.3338215103: recall: 0.4795213990: f1: 0.3936213270
16: 6464: loss: 0.6764414567:
16: 12864: loss: 0.6765242580:
16: 19264: loss: 0.6765360955:
16: 25664: loss: 0.6765878649:
16: 32064: loss: 0.6765939951:
16: 38464: loss: 0.6766178244:
16: 44864: loss: 0.6766909606:
16: 51264: loss: 0.6767172103:
16: 57664: loss: 0.6766753887:
16: 64064: loss: 0.6765784798:
16: 70464: loss: 0.6764767825:
Dev-Acc: 16: Accuracy: 0.6728349924: precision: 0.0750674446: recall: 0.4069035878: f1: 0.1267512381
Train-Acc: 16: Accuracy: 0.7236736417: precision: 0.3531940721: recall: 0.4590756689: f1: 0.3992338917
17: 6464: loss: 0.6756971872:
17: 12864: loss: 0.6751793411:
17: 19264: loss: 0.6751984833:
17: 25664: loss: 0.6749764921:
17: 32064: loss: 0.6749648415:
17: 38464: loss: 0.6748959355:
17: 44864: loss: 0.6749159470:
17: 51264: loss: 0.6748436584:
17: 57664: loss: 0.6747532751:
17: 64064: loss: 0.6746819547:
17: 70464: loss: 0.6745924398:
Dev-Acc: 17: Accuracy: 0.6959040761: precision: 0.0773982186: recall: 0.3856486992: f1: 0.1289222374
Train-Acc: 17: Accuracy: 0.7408585548: precision: 0.3738642737: recall: 0.4382354875: f1: 0.4034986835
18: 6464: loss: 0.6742031080:
18: 12864: loss: 0.6739839002:
18: 19264: loss: 0.6736898790:
18: 25664: loss: 0.6735029003:
18: 32064: loss: 0.6734933680:
18: 38464: loss: 0.6732925963:
18: 44864: loss: 0.6731995174:
18: 51264: loss: 0.6731186959:
18: 57664: loss: 0.6731004849:
18: 64064: loss: 0.6730072587:
18: 70464: loss: 0.6730093167:
Dev-Acc: 18: Accuracy: 0.7178619504: precision: 0.0801251024: recall: 0.3659241626: f1: 0.1314640032
Train-Acc: 18: Accuracy: 0.7548090219: precision: 0.3936374327: recall: 0.4181184669: f1: 0.4055087988
19: 6464: loss: 0.6716836375:
19: 12864: loss: 0.6717186496:
19: 19264: loss: 0.6716542145:
19: 25664: loss: 0.6715377730:
19: 32064: loss: 0.6713822457:
19: 38464: loss: 0.6714270160:
19: 44864: loss: 0.6713637675:
19: 51264: loss: 0.6712419464:
19: 57664: loss: 0.6712906548:
19: 64064: loss: 0.6713300323:
19: 70464: loss: 0.6712634193:
Dev-Acc: 19: Accuracy: 0.7362180352: precision: 0.0820075911: recall: 0.3453494304: f1: 0.1325415212
Train-Acc: 19: Accuracy: 0.7680888772: precision: 0.4169688676: recall: 0.4006311222: f1: 0.4086367599
20: 6464: loss: 0.6705758190:
20: 12864: loss: 0.6709054509:
20: 19264: loss: 0.6702986866:
20: 25664: loss: 0.6702672571:
20: 32064: loss: 0.6700961264:
20: 38464: loss: 0.6699049193:
20: 44864: loss: 0.6697539107:
20: 51264: loss: 0.6696114033:
20: 57664: loss: 0.6695591811:
20: 64064: loss: 0.6694920204:
20: 70464: loss: 0.6694409549:
Dev-Acc: 20: Accuracy: 0.7539291978: precision: 0.0853097190: recall: 0.3308961061: f1: 0.1356475673
Train-Acc: 20: Accuracy: 0.7787784934: precision: 0.4389191644: recall: 0.3812372625: f1: 0.4080498188
