1: 3232: loss: 0.7057842040:
1: 6432: loss: 0.7043076333:
1: 9632: loss: 0.7036511594:
1: 12832: loss: 0.7033386086:
1: 16032: loss: 0.7025046688:
1: 19232: loss: 0.7016794334:
1: 22432: loss: 0.7008250430:
1: 25632: loss: 0.7001362472:
1: 28832: loss: 0.6994917807:
1: 32032: loss: 0.6985109109:
1: 35232: loss: 0.6977911420:
1: 38432: loss: 0.6972402188:
1: 41632: loss: 0.6965860430:
1: 44832: loss: 0.6958670280:
1: 48032: loss: 0.6952398328:
1: 51232: loss: 0.6945120179:
1: 54432: loss: 0.6937996821:
1: 57632: loss: 0.6931081373:
1: 60832: loss: 0.6924540873:
1: 64032: loss: 0.6917728174:
1: 67232: loss: 0.6911503521:
1: 70432: loss: 0.6905583465:
1: 73632: loss: 0.6898979481:
1: 76832: loss: 0.6892005379:
1: 80032: loss: 0.6885209924:
1: 83232: loss: 0.6878564452:
1: 86432: loss: 0.6871275057:
1: 89632: loss: 0.6864584122:
Dev-Acc: 1: Accuracy: 0.7959398031: precision: 0.0798764090: recall: 0.2373745962: f1: 0.1195307817
Train-Acc: 1: Accuracy: 0.7861415744: precision: 0.3378510654: recall: 0.2949838932: f1: 0.3149656044
2: 3232: loss: 0.6688944215:
2: 6432: loss: 0.6671119860:
2: 9632: loss: 0.6660527160:
2: 12832: loss: 0.6651758781:
2: 16032: loss: 0.6644013994:
2: 19232: loss: 0.6634101392:
2: 22432: loss: 0.6629451739:
2: 25632: loss: 0.6622439877:
2: 28832: loss: 0.6614982182:
2: 32032: loss: 0.6607381576:
2: 35232: loss: 0.6600985309:
2: 38432: loss: 0.6594538544:
2: 41632: loss: 0.6588619659:
2: 44832: loss: 0.6582417426:
2: 48032: loss: 0.6577481253:
2: 51232: loss: 0.6572296495:
2: 54432: loss: 0.6565951829:
2: 57632: loss: 0.6559072293:
2: 60832: loss: 0.6552156281:
2: 64032: loss: 0.6546180242:
2: 67232: loss: 0.6539507313:
2: 70432: loss: 0.6533098977:
2: 73632: loss: 0.6525911925:
2: 76832: loss: 0.6519656031:
2: 80032: loss: 0.6514195209:
2: 83232: loss: 0.6507912462:
2: 86432: loss: 0.6501487206:
2: 89632: loss: 0.6496566051:
Dev-Acc: 2: Accuracy: 0.9342058301: precision: 0.1687279152: recall: 0.0324774698: f1: 0.0544702695
Train-Acc: 2: Accuracy: 0.8393377662: precision: 0.6841397849: recall: 0.0669252515: f1: 0.1219234685
3: 3232: loss: 0.6293685031:
3: 6432: loss: 0.6295385480:
3: 9632: loss: 0.6293741796:
3: 12832: loss: 0.6286639565:
3: 16032: loss: 0.6286415706:
3: 19232: loss: 0.6281488298:
3: 22432: loss: 0.6277265997:
3: 25632: loss: 0.6271804912:
3: 28832: loss: 0.6268002048:
3: 32032: loss: 0.6262453102:
3: 35232: loss: 0.6257039958:
3: 38432: loss: 0.6251742196:
3: 41632: loss: 0.6245342910:
3: 44832: loss: 0.6238712287:
3: 48032: loss: 0.6232409333:
3: 51232: loss: 0.6226817180:
3: 54432: loss: 0.6220791301:
3: 57632: loss: 0.6214575022:
3: 60832: loss: 0.6208626964:
3: 64032: loss: 0.6202541217:
3: 67232: loss: 0.6196069610:
3: 70432: loss: 0.6189300850:
3: 73632: loss: 0.6183992766:
3: 76832: loss: 0.6178378687:
3: 80032: loss: 0.6172256137:
3: 83232: loss: 0.6165813898:
3: 86432: loss: 0.6159977490:
3: 89632: loss: 0.6153806442:
Dev-Acc: 3: Accuracy: 0.9415184855: precision: 0.2173913043: recall: 0.0008501955: f1: 0.0016937669
Train-Acc: 3: Accuracy: 0.8345167041: precision: 0.9354838710: recall: 0.0076260601: f1: 0.0151287903
4: 3232: loss: 0.5982389212:
4: 6432: loss: 0.5982936183:
4: 9632: loss: 0.5973033776:
4: 12832: loss: 0.5964277214:
4: 16032: loss: 0.5960408026:
4: 19232: loss: 0.5957728891:
4: 22432: loss: 0.5952613069:
4: 25632: loss: 0.5947806361:
4: 28832: loss: 0.5937822569:
4: 32032: loss: 0.5929926913:
4: 35232: loss: 0.5921903521:
4: 38432: loss: 0.5918160895:
4: 41632: loss: 0.5910888774:
4: 44832: loss: 0.5904809059:
4: 48032: loss: 0.5900961629:
4: 51232: loss: 0.5896294459:
4: 54432: loss: 0.5891189311:
4: 57632: loss: 0.5885701454:
4: 60832: loss: 0.5880829718:
4: 64032: loss: 0.5874104917:
4: 67232: loss: 0.5869979714:
4: 70432: loss: 0.5864064205:
4: 73632: loss: 0.5856911370:
4: 76832: loss: 0.5850773303:
4: 80032: loss: 0.5845811961:
4: 83232: loss: 0.5839373165:
4: 86432: loss: 0.5833374548:
4: 89632: loss: 0.5828123434:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8334757686: precision: 1.0000000000: recall: 0.0008546447: f1: 0.0017078297
5: 3232: loss: 0.5647611409:
5: 6432: loss: 0.5647628248:
5: 9632: loss: 0.5643628212:
5: 12832: loss: 0.5641697764:
5: 16032: loss: 0.5640128899:
5: 19232: loss: 0.5639778792:
5: 22432: loss: 0.5628990511:
5: 25632: loss: 0.5627633541:
5: 28832: loss: 0.5621112841:
5: 32032: loss: 0.5619837182:
5: 35232: loss: 0.5611522568:
5: 38432: loss: 0.5605965307:
5: 41632: loss: 0.5599218076:
5: 44832: loss: 0.5593572794:
5: 48032: loss: 0.5586355448:
5: 51232: loss: 0.5582848011:
5: 54432: loss: 0.5577438414:
5: 57632: loss: 0.5571493113:
5: 60832: loss: 0.5564154092:
5: 64032: loss: 0.5560263104:
5: 67232: loss: 0.5554008224:
5: 70432: loss: 0.5549507986:
5: 73632: loss: 0.5544941918:
5: 76832: loss: 0.5540142531:
5: 80032: loss: 0.5536988806:
5: 83232: loss: 0.5532539718:
5: 86432: loss: 0.5527830946:
5: 89632: loss: 0.5523434987:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.5404741374:
6: 6432: loss: 0.5391964673:
6: 9632: loss: 0.5381241044:
6: 12832: loss: 0.5359076155:
6: 16032: loss: 0.5348869755:
6: 19232: loss: 0.5339136941:
6: 22432: loss: 0.5335706996:
6: 25632: loss: 0.5336383215:
6: 28832: loss: 0.5326138707:
6: 32032: loss: 0.5319139607:
6: 35232: loss: 0.5313857396:
6: 38432: loss: 0.5306141801:
6: 41632: loss: 0.5310651784:
6: 44832: loss: 0.5301996100:
6: 48032: loss: 0.5299838808:
6: 51232: loss: 0.5297069040:
6: 54432: loss: 0.5289677365:
6: 57632: loss: 0.5286244009:
6: 60832: loss: 0.5282241409:
6: 64032: loss: 0.5275526288:
6: 67232: loss: 0.5268468014:
6: 70432: loss: 0.5266463574:
6: 73632: loss: 0.5263109500:
6: 76832: loss: 0.5259733907:
6: 80032: loss: 0.5256845057:
6: 83232: loss: 0.5252306476:
6: 86432: loss: 0.5245417493:
6: 89632: loss: 0.5237876146:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.5135271013:
7: 6432: loss: 0.5129389592:
7: 9632: loss: 0.5145278208:
7: 12832: loss: 0.5128032616:
7: 16032: loss: 0.5115547762:
7: 19232: loss: 0.5106246628:
7: 22432: loss: 0.5092289175:
7: 25632: loss: 0.5082095824:
7: 28832: loss: 0.5077229911:
7: 32032: loss: 0.5071654066:
7: 35232: loss: 0.5065088186:
7: 38432: loss: 0.5054506975:
7: 41632: loss: 0.5047156486:
7: 44832: loss: 0.5040195227:
7: 48032: loss: 0.5031692344:
7: 51232: loss: 0.5028915346:
7: 54432: loss: 0.5022314347:
7: 57632: loss: 0.5015225598:
7: 60832: loss: 0.5008120121:
7: 64032: loss: 0.5001780280:
7: 67232: loss: 0.4997464761:
7: 70432: loss: 0.4995734559:
7: 73632: loss: 0.4992392957:
7: 76832: loss: 0.4987797884:
7: 80032: loss: 0.4982400176:
7: 83232: loss: 0.4978183393:
7: 86432: loss: 0.4975876805:
7: 89632: loss: 0.4970444133:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.4753348020:
8: 6432: loss: 0.4786903267:
8: 9632: loss: 0.4804281357:
8: 12832: loss: 0.4800939190:
8: 16032: loss: 0.4796852309:
8: 19232: loss: 0.4799545097:
8: 22432: loss: 0.4792877989:
8: 25632: loss: 0.4798974258:
8: 28832: loss: 0.4799538567:
8: 32032: loss: 0.4795732526:
8: 35232: loss: 0.4794127636:
8: 38432: loss: 0.4788283430:
8: 41632: loss: 0.4788881394:
8: 44832: loss: 0.4781385205:
8: 48032: loss: 0.4778554067:
8: 51232: loss: 0.4777776070:
8: 54432: loss: 0.4774575814:
8: 57632: loss: 0.4771779904:
8: 60832: loss: 0.4764493498:
8: 64032: loss: 0.4762932351:
8: 67232: loss: 0.4757568564:
8: 70432: loss: 0.4753824681:
8: 73632: loss: 0.4754519775:
8: 76832: loss: 0.4751411272:
8: 80032: loss: 0.4741479420:
8: 83232: loss: 0.4738377384:
8: 86432: loss: 0.4735734983:
8: 89632: loss: 0.4729777572:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.4704354191:
9: 6432: loss: 0.4650535563:
9: 9632: loss: 0.4617267169:
9: 12832: loss: 0.4598503642:
9: 16032: loss: 0.4596204357:
9: 19232: loss: 0.4593071255:
9: 22432: loss: 0.4592677572:
9: 25632: loss: 0.4596939251:
9: 28832: loss: 0.4594717866:
9: 32032: loss: 0.4586980315:
9: 35232: loss: 0.4572761941:
9: 38432: loss: 0.4573860011:
9: 41632: loss: 0.4570566593:
9: 44832: loss: 0.4574356857:
9: 48032: loss: 0.4566084322:
9: 51232: loss: 0.4560049805:
9: 54432: loss: 0.4553238932:
9: 57632: loss: 0.4544721366:
9: 60832: loss: 0.4538506320:
9: 64032: loss: 0.4537360575:
9: 67232: loss: 0.4530323672:
9: 70432: loss: 0.4527428616:
9: 73632: loss: 0.4527307926:
9: 76832: loss: 0.4521578424:
9: 80032: loss: 0.4519542570:
9: 83232: loss: 0.4515290026:
9: 86432: loss: 0.4509416634:
9: 89632: loss: 0.4505991688:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.4297407085:
10: 6432: loss: 0.4310905473:
10: 9632: loss: 0.4340729338:
10: 12832: loss: 0.4364196571:
10: 16032: loss: 0.4381716099:
10: 19232: loss: 0.4355628335:
10: 22432: loss: 0.4345434523:
10: 25632: loss: 0.4346480843:
10: 28832: loss: 0.4345537532:
10: 32032: loss: 0.4340394955:
10: 35232: loss: 0.4343262735:
10: 38432: loss: 0.4339860937:
10: 41632: loss: 0.4337379260:
10: 44832: loss: 0.4335673484:
10: 48032: loss: 0.4337808892:
10: 51232: loss: 0.4334494209:
10: 54432: loss: 0.4328112387:
10: 57632: loss: 0.4330035921:
10: 60832: loss: 0.4329643579:
10: 64032: loss: 0.4330312875:
10: 67232: loss: 0.4332487196:
10: 70432: loss: 0.4334108195:
10: 73632: loss: 0.4328666873:
10: 76832: loss: 0.4326763803:
10: 80032: loss: 0.4321543598:
10: 83232: loss: 0.4317467618:
10: 86432: loss: 0.4307459238:
10: 89632: loss: 0.4306701152:
Dev-Acc: 10: Accuracy: 0.9416574240: precision: 0.6666666667: recall: 0.0003400782: f1: 0.0006798097
Train-Acc: 10: Accuracy: 0.8351193070: precision: 1.0000000000: recall: 0.0107159293: f1: 0.0212046312
11: 3232: loss: 0.4169706121:
11: 6432: loss: 0.4128347088:
11: 9632: loss: 0.4138658930:
11: 12832: loss: 0.4151492944:
11: 16032: loss: 0.4179579943:
11: 19232: loss: 0.4166486564:
11: 22432: loss: 0.4178318493:
11: 25632: loss: 0.4191927053:
11: 28832: loss: 0.4197493740:
11: 32032: loss: 0.4192964363:
11: 35232: loss: 0.4192227246:
11: 38432: loss: 0.4191198157:
11: 41632: loss: 0.4189352747:
11: 44832: loss: 0.4182907855:
11: 48032: loss: 0.4180034666:
11: 51232: loss: 0.4173394628:
11: 54432: loss: 0.4167296099:
11: 57632: loss: 0.4163357861:
11: 60832: loss: 0.4154388281:
11: 64032: loss: 0.4150531680:
11: 67232: loss: 0.4152415268:
11: 70432: loss: 0.4150606586:
11: 73632: loss: 0.4147682509:
11: 76832: loss: 0.4145484737:
11: 80032: loss: 0.4147045645:
11: 83232: loss: 0.4141594329:
11: 86432: loss: 0.4134474607:
11: 89632: loss: 0.4127687116:
Dev-Acc: 11: Accuracy: 0.9417169094: precision: 0.8888888889: recall: 0.0013603129: f1: 0.0027164686
Train-Acc: 11: Accuracy: 0.8381763101: precision: 1.0000000000: recall: 0.0290579186: f1: 0.0564747972
12: 3232: loss: 0.4150278947:
12: 6432: loss: 0.4080184807:
12: 9632: loss: 0.4078930479:
12: 12832: loss: 0.4061385092:
12: 16032: loss: 0.4022872401:
12: 19232: loss: 0.4015581746:
12: 22432: loss: 0.4036938877:
12: 25632: loss: 0.4020128426:
12: 28832: loss: 0.4024975190:
12: 32032: loss: 0.4028058108:
12: 35232: loss: 0.4032578797:
12: 38432: loss: 0.4020743800:
12: 41632: loss: 0.4012233380:
12: 44832: loss: 0.4016114649:
12: 48032: loss: 0.4013957079:
12: 51232: loss: 0.4009595313:
12: 54432: loss: 0.4004486263:
12: 57632: loss: 0.4000520356:
12: 60832: loss: 0.3994794101:
12: 64032: loss: 0.3989618230:
12: 67232: loss: 0.3985221680:
12: 70432: loss: 0.3984266994:
12: 73632: loss: 0.3983711355:
12: 76832: loss: 0.3982304119:
12: 80032: loss: 0.3980176197:
12: 83232: loss: 0.3976268552:
12: 86432: loss: 0.3973309580:
12: 89632: loss: 0.3971422470:
Dev-Acc: 12: Accuracy: 0.9418756962: precision: 0.6236559140: recall: 0.0098622683: f1: 0.0194174757
Train-Acc: 12: Accuracy: 0.8404992223: precision: 0.9698275862: recall: 0.0443757807: f1: 0.0848682970
13: 3232: loss: 0.3825259939:
13: 6432: loss: 0.3802911390:
13: 9632: loss: 0.3841789171:
13: 12832: loss: 0.3861343030:
13: 16032: loss: 0.3857849568:
13: 19232: loss: 0.3846411920:
13: 22432: loss: 0.3846249255:
13: 25632: loss: 0.3850647530:
13: 28832: loss: 0.3859910692:
13: 32032: loss: 0.3861075783:
13: 35232: loss: 0.3858323447:
13: 38432: loss: 0.3865046288:
13: 41632: loss: 0.3863356318:
13: 44832: loss: 0.3852379735:
13: 48032: loss: 0.3845512613:
13: 51232: loss: 0.3845228208:
13: 54432: loss: 0.3840484945:
13: 57632: loss: 0.3842280597:
13: 60832: loss: 0.3844177087:
13: 64032: loss: 0.3843071982:
13: 67232: loss: 0.3840018571:
13: 70432: loss: 0.3842283199:
13: 73632: loss: 0.3838810148:
13: 76832: loss: 0.3834390555:
13: 80032: loss: 0.3833067775:
13: 83232: loss: 0.3829509141:
13: 86432: loss: 0.3826964834:
13: 89632: loss: 0.3826592045:
Dev-Acc: 13: Accuracy: 0.9425107241: precision: 0.6407766990: recall: 0.0336677436: f1: 0.0639741519
Train-Acc: 13: Accuracy: 0.8450244069: precision: 0.9038607116: recall: 0.0784958254: f1: 0.1444471328
14: 3232: loss: 0.3535040693:
14: 6432: loss: 0.3638656590:
14: 9632: loss: 0.3651767365:
14: 12832: loss: 0.3670661250:
14: 16032: loss: 0.3697837704:
14: 19232: loss: 0.3684471103:
14: 22432: loss: 0.3701234414:
14: 25632: loss: 0.3704535744:
14: 28832: loss: 0.3716717133:
14: 32032: loss: 0.3714491373:
14: 35232: loss: 0.3710410086:
14: 38432: loss: 0.3719711040:
14: 41632: loss: 0.3716930645:
14: 44832: loss: 0.3717350359:
14: 48032: loss: 0.3707544450:
14: 51232: loss: 0.3710587272:
14: 54432: loss: 0.3707707826:
14: 57632: loss: 0.3708433105:
14: 60832: loss: 0.3718187740:
14: 64032: loss: 0.3718216158:
14: 67232: loss: 0.3708489054:
14: 70432: loss: 0.3701848135:
14: 73632: loss: 0.3701015049:
14: 76832: loss: 0.3706949021:
14: 80032: loss: 0.3704021467:
14: 83232: loss: 0.3700890881:
14: 86432: loss: 0.3695510200:
14: 89632: loss: 0.3692422548:
Dev-Acc: 14: Accuracy: 0.9423817396: precision: 0.5553892216: recall: 0.0630845094: f1: 0.1132997404
Train-Acc: 14: Accuracy: 0.8486950397: precision: 0.9157769870: recall: 0.1015054894: f1: 0.1827543351
15: 3232: loss: 0.3668524939:
15: 6432: loss: 0.3635802555:
15: 9632: loss: 0.3621157792:
15: 12832: loss: 0.3602773418:
15: 16032: loss: 0.3593185329:
15: 19232: loss: 0.3602760362:
15: 22432: loss: 0.3600886235:
15: 25632: loss: 0.3603434054:
15: 28832: loss: 0.3594108857:
15: 32032: loss: 0.3607275993:
15: 35232: loss: 0.3611670276:
15: 38432: loss: 0.3603504155:
15: 41632: loss: 0.3590280363:
15: 44832: loss: 0.3590944448:
15: 48032: loss: 0.3594697533:
15: 51232: loss: 0.3582939190:
15: 54432: loss: 0.3580882921:
15: 57632: loss: 0.3574006193:
15: 60832: loss: 0.3579323769:
15: 64032: loss: 0.3570144569:
15: 67232: loss: 0.3569515343:
15: 70432: loss: 0.3571914978:
15: 73632: loss: 0.3565913952:
15: 76832: loss: 0.3567357907:
15: 80032: loss: 0.3569704468:
15: 83232: loss: 0.3571612603:
15: 86432: loss: 0.3573273183:
15: 89632: loss: 0.3572588854:
Dev-Acc: 15: Accuracy: 0.9424412251: precision: 0.5478468900: recall: 0.0778779119: f1: 0.1363704035
Train-Acc: 15: Accuracy: 0.8537023664: precision: 0.9265718219: recall: 0.1327328907: f1: 0.2322024152
16: 3232: loss: 0.3590247019:
16: 6432: loss: 0.3567991886:
16: 9632: loss: 0.3546470870:
16: 12832: loss: 0.3541652779:
16: 16032: loss: 0.3532440339:
16: 19232: loss: 0.3525719545:
16: 22432: loss: 0.3525534222:
16: 25632: loss: 0.3531798752:
16: 28832: loss: 0.3518778493:
16: 32032: loss: 0.3513410264:
16: 35232: loss: 0.3505294007:
16: 38432: loss: 0.3491207788:
16: 41632: loss: 0.3491320171:
16: 44832: loss: 0.3488659192:
16: 48032: loss: 0.3490781750:
16: 51232: loss: 0.3486788905:
16: 54432: loss: 0.3489793639:
16: 57632: loss: 0.3480743603:
16: 60832: loss: 0.3469600150:
16: 64032: loss: 0.3471907054:
16: 67232: loss: 0.3477205848:
16: 70432: loss: 0.3475389146:
16: 73632: loss: 0.3474773253:
16: 76832: loss: 0.3476424231:
16: 80032: loss: 0.3469344074:
16: 83232: loss: 0.3470179684:
16: 86432: loss: 0.3471802022:
16: 89632: loss: 0.3467442205:
Dev-Acc: 16: Accuracy: 0.9424412251: precision: 0.5299401198: recall: 0.1203876892: f1: 0.1962034086
Train-Acc: 16: Accuracy: 0.8588631153: precision: 0.9276798825: recall: 0.1661297745: f1: 0.2817953722
17: 3232: loss: 0.3418999998:
17: 6432: loss: 0.3434117508:
17: 9632: loss: 0.3402497193:
17: 12832: loss: 0.3407205783:
17: 16032: loss: 0.3410687022:
17: 19232: loss: 0.3434617085:
17: 22432: loss: 0.3422969087:
17: 25632: loss: 0.3421114182:
17: 28832: loss: 0.3432806623:
17: 32032: loss: 0.3433490407:
17: 35232: loss: 0.3428953476:
17: 38432: loss: 0.3433786291:
17: 41632: loss: 0.3429451673:
17: 44832: loss: 0.3428939826:
17: 48032: loss: 0.3431700061:
17: 51232: loss: 0.3433391425:
17: 54432: loss: 0.3425908778:
17: 57632: loss: 0.3419274816:
17: 60832: loss: 0.3406177416:
17: 64032: loss: 0.3404817944:
17: 67232: loss: 0.3399767426:
17: 70432: loss: 0.3390899269:
17: 73632: loss: 0.3388048653:
17: 76832: loss: 0.3382688047:
17: 80032: loss: 0.3379214381:
17: 83232: loss: 0.3374556950:
17: 86432: loss: 0.3372305167:
17: 89632: loss: 0.3369839551:
Dev-Acc: 17: Accuracy: 0.9419947267: precision: 0.5090439276: recall: 0.1674885224: f1: 0.2520470829
Train-Acc: 17: Accuracy: 0.8645826578: precision: 0.9342265530: recall: 0.2016961410: f1: 0.3317653420
18: 3232: loss: 0.3339113940:
18: 6432: loss: 0.3341133172:
18: 9632: loss: 0.3315888701:
18: 12832: loss: 0.3299948919:
18: 16032: loss: 0.3296657521:
18: 19232: loss: 0.3295726692:
18: 22432: loss: 0.3285657245:
18: 25632: loss: 0.3289411912:
18: 28832: loss: 0.3269853827:
18: 32032: loss: 0.3283356906:
18: 35232: loss: 0.3283181168:
18: 38432: loss: 0.3288649431:
18: 41632: loss: 0.3278929753:
18: 44832: loss: 0.3279383064:
18: 48032: loss: 0.3279113809:
18: 51232: loss: 0.3279625831:
18: 54432: loss: 0.3277338132:
18: 57632: loss: 0.3283081155:
18: 60832: loss: 0.3283935795:
18: 64032: loss: 0.3288300512:
18: 67232: loss: 0.3287022204:
18: 70432: loss: 0.3284951024:
18: 73632: loss: 0.3282229216:
18: 76832: loss: 0.3278434867:
18: 80032: loss: 0.3277406294:
18: 83232: loss: 0.3274869494:
18: 86432: loss: 0.3273199943:
18: 89632: loss: 0.3274382960:
Dev-Acc: 18: Accuracy: 0.9397622347: precision: 0.4668296089: recall: 0.2273422887: f1: 0.3057747284
Train-Acc: 18: Accuracy: 0.8720443249: precision: 0.9354202613: recall: 0.2494905003: f1: 0.3939173760
19: 3232: loss: 0.3224736601:
19: 6432: loss: 0.3273394279:
19: 9632: loss: 0.3262325650:
19: 12832: loss: 0.3243885155:
19: 16032: loss: 0.3224570697:
19: 19232: loss: 0.3214860935:
19: 22432: loss: 0.3232400952:
19: 25632: loss: 0.3238832327:
19: 28832: loss: 0.3233079840:
19: 32032: loss: 0.3233485606:
19: 35232: loss: 0.3223410080:
19: 38432: loss: 0.3223507463:
19: 41632: loss: 0.3219406307:
19: 44832: loss: 0.3226063928:
19: 48032: loss: 0.3228583286:
19: 51232: loss: 0.3224901315:
19: 54432: loss: 0.3216456788:
19: 57632: loss: 0.3214172721:
19: 60832: loss: 0.3210339732:
19: 64032: loss: 0.3208208768:
19: 67232: loss: 0.3203364809:
19: 70432: loss: 0.3202731423:
19: 73632: loss: 0.3202662811:
19: 76832: loss: 0.3204163830:
19: 80032: loss: 0.3205821639:
19: 83232: loss: 0.3198913964:
19: 86432: loss: 0.3192738059:
19: 89632: loss: 0.3189214578:
Dev-Acc: 19: Accuracy: 0.9292446971: precision: 0.3893413598: recall: 0.3739160007: f1: 0.3814728077
Train-Acc: 19: Accuracy: 0.8804264665: precision: 0.8782118972: recall: 0.3280520676: f1: 0.4776719475
20: 3232: loss: 0.3302867962:
20: 6432: loss: 0.3223507615:
20: 9632: loss: 0.3219136280:
20: 12832: loss: 0.3182782128:
20: 16032: loss: 0.3192775540:
20: 19232: loss: 0.3170165250:
20: 22432: loss: 0.3185959043:
20: 25632: loss: 0.3179153452:
20: 28832: loss: 0.3169922495:
20: 32032: loss: 0.3177717928:
20: 35232: loss: 0.3162495949:
20: 38432: loss: 0.3133056434:
20: 41632: loss: 0.3140179205:
20: 44832: loss: 0.3130169301:
20: 48032: loss: 0.3117301688:
20: 51232: loss: 0.3123231988:
20: 54432: loss: 0.3117779489:
20: 57632: loss: 0.3115403999:
20: 60832: loss: 0.3121312736:
20: 64032: loss: 0.3117614871:
20: 67232: loss: 0.3112891789:
20: 70432: loss: 0.3118925483:
20: 73632: loss: 0.3116402789:
20: 76832: loss: 0.3120676911:
20: 80032: loss: 0.3114561905:
20: 83232: loss: 0.3114694703:
20: 86432: loss: 0.3113647400:
20: 89632: loss: 0.3110786400:
Dev-Acc: 20: Accuracy: 0.9214359522: precision: 0.3540203526: recall: 0.4199965992: f1: 0.3841966091
Train-Acc: 20: Accuracy: 0.8830999136: precision: 0.8394618834: recall: 0.3692064953: f1: 0.5128532944
21: 3232: loss: 0.2943898633:
21: 6432: loss: 0.3021529005:
21: 9632: loss: 0.3045997603:
21: 12832: loss: 0.3052657554:
21: 16032: loss: 0.3061907501:
21: 19232: loss: 0.3083005065:
21: 22432: loss: 0.3078919062:
21: 25632: loss: 0.3096578888:
21: 28832: loss: 0.3088569462:
21: 32032: loss: 0.3066519487:
21: 35232: loss: 0.3044081680:
21: 38432: loss: 0.3043580106:
21: 41632: loss: 0.3042432145:
21: 44832: loss: 0.3037467008:
21: 48032: loss: 0.3030723126:
21: 51232: loss: 0.3030965323:
21: 54432: loss: 0.3032914668:
21: 57632: loss: 0.3037574091:
21: 60832: loss: 0.3034838469:
21: 64032: loss: 0.3042008402:
21: 67232: loss: 0.3040967138:
21: 70432: loss: 0.3039754118:
21: 73632: loss: 0.3042510028:
21: 76832: loss: 0.3038826072:
21: 80032: loss: 0.3034047810:
21: 83232: loss: 0.3034973567:
21: 86432: loss: 0.3032958861:
21: 89632: loss: 0.3031556193:
Dev-Acc: 21: Accuracy: 0.9187867045: precision: 0.3460716195: recall: 0.4404012923: f1: 0.3875794987
Train-Acc: 21: Accuracy: 0.8871540427: precision: 0.8396957123: recall: 0.3991190586: f1: 0.5410632325
22: 3232: loss: 0.2966513376:
22: 6432: loss: 0.3013406057:
22: 9632: loss: 0.2999645090:
22: 12832: loss: 0.2985402605:
22: 16032: loss: 0.2956367897:
22: 19232: loss: 0.2941222072:
22: 22432: loss: 0.2950173953:
22: 25632: loss: 0.2959498656:
22: 28832: loss: 0.2955972986:
22: 32032: loss: 0.2955062637:
22: 35232: loss: 0.2963392395:
22: 38432: loss: 0.2969115815:
22: 41632: loss: 0.2965383317:
22: 44832: loss: 0.2965019775:
22: 48032: loss: 0.2964824948:
22: 51232: loss: 0.2961153694:
22: 54432: loss: 0.2961945975:
22: 57632: loss: 0.2962126917:
22: 60832: loss: 0.2959467409:
22: 64032: loss: 0.2963651827:
22: 67232: loss: 0.2965140931:
22: 70432: loss: 0.2963363401:
22: 73632: loss: 0.2959850065:
22: 76832: loss: 0.2962811311:
22: 80032: loss: 0.2966330856:
22: 83232: loss: 0.2963465131:
22: 86432: loss: 0.2966932207:
22: 89632: loss: 0.2966352290:
Dev-Acc: 22: Accuracy: 0.9168022871: precision: 0.3412778905: recall: 0.4577452814: f1: 0.3910233132
Train-Acc: 22: Accuracy: 0.8897289038: precision: 0.8373312361: recall: 0.4199592400: f1: 0.5593695271
23: 3232: loss: 0.3003605121:
23: 6432: loss: 0.2944000682:
23: 9632: loss: 0.2947511793:
23: 12832: loss: 0.2942289624:
23: 16032: loss: 0.2930408786:
23: 19232: loss: 0.2932027707:
23: 22432: loss: 0.2929062660:
23: 25632: loss: 0.2933807996:
23: 28832: loss: 0.2938104362:
23: 32032: loss: 0.2924459300:
23: 35232: loss: 0.2925803685:
23: 38432: loss: 0.2925397892:
23: 41632: loss: 0.2922692433:
23: 44832: loss: 0.2921643786:
23: 48032: loss: 0.2931643313:
23: 51232: loss: 0.2926182250:
23: 54432: loss: 0.2920712813:
23: 57632: loss: 0.2920771477:
23: 60832: loss: 0.2921924998:
23: 64032: loss: 0.2925816085:
23: 67232: loss: 0.2927113005:
23: 70432: loss: 0.2923668363:
23: 73632: loss: 0.2920916712:
23: 76832: loss: 0.2917740802:
23: 80032: loss: 0.2918191371:
23: 83232: loss: 0.2916230531:
23: 86432: loss: 0.2913533380:
23: 89632: loss: 0.2912528065:
Dev-Acc: 23: Accuracy: 0.9152544141: precision: 0.3400673401: recall: 0.4808706002: f1: 0.3983940269
Train-Acc: 23: Accuracy: 0.8926982880: precision: 0.8382022472: recall: 0.4413910985: f1: 0.5782696697
24: 3232: loss: 0.2993497763:
24: 6432: loss: 0.2956389873:
24: 9632: loss: 0.2943718979:
24: 12832: loss: 0.2917577817:
24: 16032: loss: 0.2895477696:
24: 19232: loss: 0.2879124035:
24: 22432: loss: 0.2881901117:
24: 25632: loss: 0.2900150492:
24: 28832: loss: 0.2909724384:
24: 32032: loss: 0.2919830634:
24: 35232: loss: 0.2902480410:
24: 38432: loss: 0.2893260515:
24: 41632: loss: 0.2884384900:
24: 44832: loss: 0.2870759541:
24: 48032: loss: 0.2862823268:
24: 51232: loss: 0.2855399926:
24: 54432: loss: 0.2863936019:
24: 57632: loss: 0.2864818906:
24: 60832: loss: 0.2863627067:
24: 64032: loss: 0.2860729893:
24: 67232: loss: 0.2855173867:
24: 70432: loss: 0.2855829199:
24: 73632: loss: 0.2852097866:
24: 76832: loss: 0.2853880590:
24: 80032: loss: 0.2851549590:
24: 83232: loss: 0.2851264508:
24: 86432: loss: 0.2848262019:
24: 89632: loss: 0.2848830392:
Dev-Acc: 24: Accuracy: 0.9130715132: precision: 0.3341395992: recall: 0.4932834552: f1: 0.3984069217
Train-Acc: 24: Accuracy: 0.8949115872: precision: 0.8384726572: recall: 0.4576293472: f1: 0.5920979883
25: 3232: loss: 0.2846654703:
25: 6432: loss: 0.2840217621:
25: 9632: loss: 0.2791257875:
25: 12832: loss: 0.2772850264:
25: 16032: loss: 0.2787931937:
25: 19232: loss: 0.2774217722:
25: 22432: loss: 0.2786738194:
25: 25632: loss: 0.2778263155:
25: 28832: loss: 0.2793521356:
25: 32032: loss: 0.2809614352:
25: 35232: loss: 0.2815342474:
25: 38432: loss: 0.2809967779:
25: 41632: loss: 0.2797690608:
25: 44832: loss: 0.2787155713:
25: 48032: loss: 0.2783996261:
25: 51232: loss: 0.2779227427:
25: 54432: loss: 0.2778850775:
25: 57632: loss: 0.2778425819:
25: 60832: loss: 0.2779671147:
25: 64032: loss: 0.2789139856:
25: 67232: loss: 0.2788975042:
25: 70432: loss: 0.2791582484:
25: 73632: loss: 0.2786509172:
25: 76832: loss: 0.2786753440:
25: 80032: loss: 0.2791570898:
25: 83232: loss: 0.2791385988:
25: 86432: loss: 0.2795312099:
25: 89632: loss: 0.2794962565:
Dev-Acc: 25: Accuracy: 0.9108092189: precision: 0.3272949544: recall: 0.5007651760: f1: 0.3958599368
Train-Acc: 25: Accuracy: 0.8973549604: precision: 0.8395908404: recall: 0.4748537243: f1: 0.6066179558
26: 3232: loss: 0.2668999321:
26: 6432: loss: 0.2772229462:
26: 9632: loss: 0.2765344015:
26: 12832: loss: 0.2748162119:
26: 16032: loss: 0.2773669339:
26: 19232: loss: 0.2783863041:
26: 22432: loss: 0.2784534049:
26: 25632: loss: 0.2790053384:
26: 28832: loss: 0.2786662336:
26: 32032: loss: 0.2790154784:
26: 35232: loss: 0.2795848853:
26: 38432: loss: 0.2791994822:
26: 41632: loss: 0.2786378703:
26: 44832: loss: 0.2784743955:
26: 48032: loss: 0.2772529022:
26: 51232: loss: 0.2777346012:
26: 54432: loss: 0.2780107628:
26: 57632: loss: 0.2784703973:
26: 60832: loss: 0.2773833961:
26: 64032: loss: 0.2775018680:
26: 67232: loss: 0.2765393950:
26: 70432: loss: 0.2762455843:
26: 73632: loss: 0.2760284730:
26: 76832: loss: 0.2751715928:
26: 80032: loss: 0.2751634563:
26: 83232: loss: 0.2748965256:
26: 86432: loss: 0.2747024444:
26: 89632: loss: 0.2744758064:
Dev-Acc: 26: Accuracy: 0.9084874392: precision: 0.3205155747: recall: 0.5073967012: f1: 0.3928641959
Train-Acc: 26: Accuracy: 0.8995025754: precision: 0.8415337631: recall: 0.4891197160: f1: 0.6186595709
27: 3232: loss: 0.2742722623:
27: 6432: loss: 0.2758346568:
27: 9632: loss: 0.2778271225:
27: 12832: loss: 0.2734431209:
27: 16032: loss: 0.2744343092:
27: 19232: loss: 0.2746058394:
27: 22432: loss: 0.2739270493:
27: 25632: loss: 0.2732009437:
27: 28832: loss: 0.2740620604:
27: 32032: loss: 0.2745875303:
27: 35232: loss: 0.2737217056:
27: 38432: loss: 0.2739498796:
27: 41632: loss: 0.2736846722:
27: 44832: loss: 0.2723805425:
27: 48032: loss: 0.2720678810:
27: 51232: loss: 0.2716536116:
27: 54432: loss: 0.2719472110:
27: 57632: loss: 0.2706758060:
27: 60832: loss: 0.2700791256:
27: 64032: loss: 0.2699452717:
27: 67232: loss: 0.2697494396:
27: 70432: loss: 0.2693014135:
27: 73632: loss: 0.2702707189:
27: 76832: loss: 0.2702943813:
27: 80032: loss: 0.2701269493:
27: 83232: loss: 0.2699451809:
27: 86432: loss: 0.2699177834:
27: 89632: loss: 0.2695857494:
Dev-Acc: 27: Accuracy: 0.9058580399: precision: 0.3138610796: recall: 0.5170889305: f1: 0.3906229929
Train-Acc: 27: Accuracy: 0.9010584354: precision: 0.8426654840: recall: 0.4996384196: f1: 0.6273215023
28: 3232: loss: 0.2587376744:
28: 6432: loss: 0.2668133908:
28: 9632: loss: 0.2722360961:
28: 12832: loss: 0.2725504039:
28: 16032: loss: 0.2696181689:
28: 19232: loss: 0.2669335980:
28: 22432: loss: 0.2662475223:
28: 25632: loss: 0.2684798814:
28: 28832: loss: 0.2680976762:
28: 32032: loss: 0.2668451464:
28: 35232: loss: 0.2654932366:
28: 38432: loss: 0.2663913007:
28: 41632: loss: 0.2657716150:
28: 44832: loss: 0.2658345973:
28: 48032: loss: 0.2654678517:
28: 51232: loss: 0.2655552174:
28: 54432: loss: 0.2659166138:
28: 57632: loss: 0.2655423914:
28: 60832: loss: 0.2655279810:
28: 64032: loss: 0.2661745784:
28: 67232: loss: 0.2660747045:
28: 70432: loss: 0.2657560668:
28: 73632: loss: 0.2656882160:
28: 76832: loss: 0.2651973108:
28: 80032: loss: 0.2650093823:
28: 83232: loss: 0.2651121337:
28: 86432: loss: 0.2652398635:
28: 89632: loss: 0.2652917294:
Dev-Acc: 28: Accuracy: 0.9033278823: precision: 0.3078989256: recall: 0.5262710423: f1: 0.3885018515
Train-Acc: 28: Accuracy: 0.9026033878: precision: 0.8431393834: recall: 0.5106173164: f1: 0.6360397986
29: 3232: loss: 0.2671333788:
29: 6432: loss: 0.2584325878:
29: 9632: loss: 0.2586458911:
29: 12832: loss: 0.2589233013:
29: 16032: loss: 0.2584848646:
29: 19232: loss: 0.2599918345:
29: 22432: loss: 0.2588209241:
29: 25632: loss: 0.2589933109:
29: 28832: loss: 0.2586985817:
29: 32032: loss: 0.2598446687:
29: 35232: loss: 0.2603380818:
29: 38432: loss: 0.2609523722:
29: 41632: loss: 0.2610353215:
29: 44832: loss: 0.2611527112:
29: 48032: loss: 0.2602422160:
29: 51232: loss: 0.2598567398:
29: 54432: loss: 0.2609558851:
29: 57632: loss: 0.2611524192:
29: 60832: loss: 0.2612113045:
29: 64032: loss: 0.2610679309:
29: 67232: loss: 0.2619838659:
29: 70432: loss: 0.2617400065:
29: 73632: loss: 0.2615534201:
29: 76832: loss: 0.2614565932:
29: 80032: loss: 0.2608544692:
29: 83232: loss: 0.2605169868:
29: 86432: loss: 0.2607350830:
29: 89632: loss: 0.2610656750:
Dev-Acc: 29: Accuracy: 0.9009664059: precision: 0.3022380861: recall: 0.5327325285: f1: 0.3856712008
Train-Acc: 29: Accuracy: 0.9038634300: precision: 0.8429408631: recall: 0.5200841496: f1: 0.6432753293
30: 3232: loss: 0.2629033460:
30: 6432: loss: 0.2560751169:
30: 9632: loss: 0.2620660992:
30: 12832: loss: 0.2612908207:
30: 16032: loss: 0.2579077164:
30: 19232: loss: 0.2571767355:
30: 22432: loss: 0.2572016705:
30: 25632: loss: 0.2558736535:
30: 28832: loss: 0.2554181548:
30: 32032: loss: 0.2564765904:
30: 35232: loss: 0.2560996530:
30: 38432: loss: 0.2556642349:
30: 41632: loss: 0.2558038206:
30: 44832: loss: 0.2555729789:
30: 48032: loss: 0.2559255860:
30: 51232: loss: 0.2572456740:
30: 54432: loss: 0.2563552641:
30: 57632: loss: 0.2567645387:
30: 60832: loss: 0.2574320596:
30: 64032: loss: 0.2577014368:
30: 67232: loss: 0.2570830472:
30: 70432: loss: 0.2573527796:
30: 73632: loss: 0.2572246636:
30: 76832: loss: 0.2566000896:
30: 80032: loss: 0.2566725286:
30: 83232: loss: 0.2574826239:
30: 86432: loss: 0.2568647327:
30: 89632: loss: 0.2573158439:
Dev-Acc: 30: Accuracy: 0.8989422917: precision: 0.2980480480: recall: 0.5400442102: f1: 0.3841083631
Train-Acc: 30: Accuracy: 0.9052221179: precision: 0.8435438266: recall: 0.5295509828: f1: 0.6506462036
31: 3232: loss: 0.2549540931:
31: 6432: loss: 0.2615504665:
31: 9632: loss: 0.2611124598:
31: 12832: loss: 0.2570879823:
31: 16032: loss: 0.2552947075:
31: 19232: loss: 0.2532596511:
31: 22432: loss: 0.2531454057:
31: 25632: loss: 0.2523950417:
31: 28832: loss: 0.2520698434:
31: 32032: loss: 0.2527660332:
31: 35232: loss: 0.2528217201:
31: 38432: loss: 0.2526893441:
31: 41632: loss: 0.2538155486:
31: 44832: loss: 0.2537790864:
31: 48032: loss: 0.2536693268:
31: 51232: loss: 0.2526475985:
31: 54432: loss: 0.2526430693:
31: 57632: loss: 0.2538597349:
31: 60832: loss: 0.2538573218:
31: 64032: loss: 0.2539236557:
31: 67232: loss: 0.2539774371:
31: 70432: loss: 0.2537305774:
31: 73632: loss: 0.2537631327:
31: 76832: loss: 0.2539304816:
31: 80032: loss: 0.2533826426:
31: 83232: loss: 0.2535991638:
31: 86432: loss: 0.2538106196:
31: 89632: loss: 0.2538241067:
Dev-Acc: 31: Accuracy: 0.8961740136: precision: 0.2924177915: recall: 0.5488862438: f1: 0.3815602837
Train-Acc: 31: Accuracy: 0.9062411189: precision: 0.8432728023: recall: 0.5373085267: f1: 0.6563867807
32: 3232: loss: 0.2653387503:
32: 6432: loss: 0.2516084654:
32: 9632: loss: 0.2530977645:
32: 12832: loss: 0.2550144422:
32: 16032: loss: 0.2565044766:
32: 19232: loss: 0.2570437701:
32: 22432: loss: 0.2557690445:
32: 25632: loss: 0.2554287531:
32: 28832: loss: 0.2540607360:
32: 32032: loss: 0.2533226828:
32: 35232: loss: 0.2509260778:
32: 38432: loss: 0.2523482509:
32: 41632: loss: 0.2523478596:
32: 44832: loss: 0.2527078707:
32: 48032: loss: 0.2528296556:
32: 51232: loss: 0.2525449641:
32: 54432: loss: 0.2526286963:
32: 57632: loss: 0.2523693813:
32: 60832: loss: 0.2521476146:
32: 64032: loss: 0.2521662964:
32: 67232: loss: 0.2515303657:
32: 70432: loss: 0.2513631347:
32: 73632: loss: 0.2511450805:
32: 76832: loss: 0.2513254974:
32: 80032: loss: 0.2516309731:
32: 83232: loss: 0.2514007081:
32: 86432: loss: 0.2514068693:
32: 89632: loss: 0.2510688685:
Dev-Acc: 32: Accuracy: 0.8939315677: precision: 0.2872688667: recall: 0.5521169869: f1: 0.3779096834
Train-Acc: 32: Accuracy: 0.9072272182: precision: 0.8434508046: recall: 0.5444086516: f1: 0.6617124136
33: 3232: loss: 0.2362500771:
33: 6432: loss: 0.2411366628:
33: 9632: loss: 0.2441167791:
33: 12832: loss: 0.2467963548:
33: 16032: loss: 0.2492305112:
33: 19232: loss: 0.2507125424:
33: 22432: loss: 0.2530858506:
33: 25632: loss: 0.2521909671:
33: 28832: loss: 0.2516312329:
33: 32032: loss: 0.2513277258:
33: 35232: loss: 0.2502764921:
33: 38432: loss: 0.2507455421:
33: 41632: loss: 0.2499755855:
33: 44832: loss: 0.2502153553:
33: 48032: loss: 0.2501584660:
33: 51232: loss: 0.2497841657:
33: 54432: loss: 0.2490099833:
33: 57632: loss: 0.2488609297:
33: 60832: loss: 0.2485876796:
33: 64032: loss: 0.2488521601:
33: 67232: loss: 0.2486339366:
33: 70432: loss: 0.2487363951:
33: 73632: loss: 0.2495267261:
33: 76832: loss: 0.2484611038:
33: 80032: loss: 0.2483847024:
33: 83232: loss: 0.2476285089:
33: 86432: loss: 0.2475741836:
33: 89632: loss: 0.2481796183:
Dev-Acc: 33: Accuracy: 0.8920066357: precision: 0.2834386633: recall: 0.5567080428: f1: 0.3756310234
Train-Acc: 33: Accuracy: 0.9080489874: precision: 0.8427666633: recall: 0.5511143252: f1: 0.6664281739
34: 3232: loss: 0.2406747667:
34: 6432: loss: 0.2475633003:
34: 9632: loss: 0.2422039212:
34: 12832: loss: 0.2454520566:
34: 16032: loss: 0.2450495786:
34: 19232: loss: 0.2452963724:
34: 22432: loss: 0.2456120942:
34: 25632: loss: 0.2452365646:
34: 28832: loss: 0.2442438800:
34: 32032: loss: 0.2437007409:
34: 35232: loss: 0.2440280963:
34: 38432: loss: 0.2436799777:
34: 41632: loss: 0.2433364432:
34: 44832: loss: 0.2437319011:
34: 48032: loss: 0.2432572089:
34: 51232: loss: 0.2432764103:
34: 54432: loss: 0.2432606481:
34: 57632: loss: 0.2432426304:
34: 60832: loss: 0.2425586574:
34: 64032: loss: 0.2433072796:
34: 67232: loss: 0.2440496665:
34: 70432: loss: 0.2433941060:
34: 73632: loss: 0.2437668169:
34: 76832: loss: 0.2444598314:
34: 80032: loss: 0.2452265404:
34: 83232: loss: 0.2448405976:
34: 86432: loss: 0.2450948894:
34: 89632: loss: 0.2450495846:
Dev-Acc: 34: Accuracy: 0.8899229765: precision: 0.2793158920: recall: 0.5609590206: f1: 0.3729369206
Train-Acc: 34: Accuracy: 0.9087721705: precision: 0.8425032335: recall: 0.5567023864: f1: 0.6704140606
35: 3232: loss: 0.2379119810:
35: 6432: loss: 0.2366536549:
35: 9632: loss: 0.2384436527:
35: 12832: loss: 0.2401355960:
35: 16032: loss: 0.2392877167:
35: 19232: loss: 0.2367004235:
35: 22432: loss: 0.2378667080:
35: 25632: loss: 0.2382691341:
35: 28832: loss: 0.2401188274:
35: 32032: loss: 0.2403230317:
35: 35232: loss: 0.2405149003:
35: 38432: loss: 0.2408813017:
35: 41632: loss: 0.2407458639:
35: 44832: loss: 0.2408728103:
35: 48032: loss: 0.2405427988:
35: 51232: loss: 0.2410446579:
35: 54432: loss: 0.2406993083:
35: 57632: loss: 0.2400449879:
35: 60832: loss: 0.2403006863:
35: 64032: loss: 0.2408959616:
35: 67232: loss: 0.2410431106:
35: 70432: loss: 0.2410202681:
35: 73632: loss: 0.2415529025:
35: 76832: loss: 0.2414196267:
35: 80032: loss: 0.2415184368:
35: 83232: loss: 0.2415559002:
35: 86432: loss: 0.2416346650:
35: 89632: loss: 0.2415131565:
Dev-Acc: 35: Accuracy: 0.8881270885: precision: 0.2755119028: recall: 0.5628294508: f1: 0.3699357362
Train-Acc: 35: Accuracy: 0.9093967080: precision: 0.8420378400: recall: 0.5617645125: f1: 0.6739224733
36: 3232: loss: 0.2300569266:
36: 6432: loss: 0.2319453337:
36: 9632: loss: 0.2370906591:
36: 12832: loss: 0.2388176274:
36: 16032: loss: 0.2410043356:
36: 19232: loss: 0.2414993100:
36: 22432: loss: 0.2424357575:
36: 25632: loss: 0.2416655769:
36: 28832: loss: 0.2395308425:
36: 32032: loss: 0.2395980659:
36: 35232: loss: 0.2394251867:
36: 38432: loss: 0.2391153339:
36: 41632: loss: 0.2381660002:
36: 44832: loss: 0.2389229103:
36: 48032: loss: 0.2388909058:
36: 51232: loss: 0.2393533687:
36: 54432: loss: 0.2404052442:
36: 57632: loss: 0.2405525365:
36: 60832: loss: 0.2405303344:
36: 64032: loss: 0.2403889662:
36: 67232: loss: 0.2397527036:
36: 70432: loss: 0.2392703378:
36: 73632: loss: 0.2394414285:
36: 76832: loss: 0.2395084582:
36: 80032: loss: 0.2394944322:
36: 83232: loss: 0.2392538959:
36: 86432: loss: 0.2392289194:
36: 89632: loss: 0.2397309768:
Dev-Acc: 36: Accuracy: 0.8863311410: precision: 0.2722072403: recall: 0.5664002721: f1: 0.3677006292
Train-Acc: 36: Accuracy: 0.9099774361: precision: 0.8414526994: recall: 0.5666294129: f1: 0.6772216547
37: 3232: loss: 0.2307824154:
37: 6432: loss: 0.2343459409:
37: 9632: loss: 0.2325142193:
37: 12832: loss: 0.2328743410:
37: 16032: loss: 0.2332350852:
37: 19232: loss: 0.2358582210:
37: 22432: loss: 0.2347488314:
37: 25632: loss: 0.2380004317:
37: 28832: loss: 0.2392563284:
37: 32032: loss: 0.2390340492:
37: 35232: loss: 0.2385618779:
37: 38432: loss: 0.2375435439:
37: 41632: loss: 0.2378402732:
37: 44832: loss: 0.2376299764:
37: 48032: loss: 0.2377389803:
37: 51232: loss: 0.2384029758:
37: 54432: loss: 0.2378509645:
37: 57632: loss: 0.2387202159:
37: 60832: loss: 0.2383158589:
37: 64032: loss: 0.2381858875:
37: 67232: loss: 0.2382011827:
37: 70432: loss: 0.2376847656:
37: 73632: loss: 0.2376527391:
37: 76832: loss: 0.2372594298:
37: 80032: loss: 0.2369277056:
37: 83232: loss: 0.2370205269:
37: 86432: loss: 0.2371584379:
37: 89632: loss: 0.2372740070:
Dev-Acc: 37: Accuracy: 0.8843864202: precision: 0.2685860935: recall: 0.5694609760: f1: 0.3650136240
Train-Acc: 37: Accuracy: 0.9105471969: precision: 0.8403361345: recall: 0.5719545066: f1: 0.6806446565
38: 3232: loss: 0.2236862728:
38: 6432: loss: 0.2346792988:
38: 9632: loss: 0.2313776405:
38: 12832: loss: 0.2295045680:
38: 16032: loss: 0.2307733999:
38: 19232: loss: 0.2302092490:
38: 22432: loss: 0.2289059741:
38: 25632: loss: 0.2302994607:
38: 28832: loss: 0.2309335288:
38: 32032: loss: 0.2319884065:
38: 35232: loss: 0.2319490038:
38: 38432: loss: 0.2314416840:
38: 41632: loss: 0.2317533581:
38: 44832: loss: 0.2319726387:
38: 48032: loss: 0.2330882893:
38: 51232: loss: 0.2330958016:
38: 54432: loss: 0.2341497443:
38: 57632: loss: 0.2338174274:
38: 60832: loss: 0.2336932357:
38: 64032: loss: 0.2338937140:
38: 67232: loss: 0.2336900835:
38: 70432: loss: 0.2335193720:
38: 73632: loss: 0.2339570978:
38: 76832: loss: 0.2341203019:
38: 80032: loss: 0.2342076697:
38: 83232: loss: 0.2339788644:
38: 86432: loss: 0.2337756254:
38: 89632: loss: 0.2338491342:
Dev-Acc: 38: Accuracy: 0.8822829127: precision: 0.2643189159: recall: 0.5704812107: f1: 0.3612576720
Train-Acc: 38: Accuracy: 0.9114127755: precision: 0.8407612854: recall: 0.5779370193: f1: 0.6850040909
39: 3232: loss: 0.2340907963:
39: 6432: loss: 0.2393688790:
39: 9632: loss: 0.2377976663:
39: 12832: loss: 0.2366229443:
39: 16032: loss: 0.2371330649:
39: 19232: loss: 0.2362285015:
39: 22432: loss: 0.2363045287:
39: 25632: loss: 0.2360979967:
39: 28832: loss: 0.2342918846:
39: 32032: loss: 0.2324155829:
39: 35232: loss: 0.2312479217:
39: 38432: loss: 0.2308217716:
39: 41632: loss: 0.2306495127:
39: 44832: loss: 0.2312390866:
39: 48032: loss: 0.2319172873:
39: 51232: loss: 0.2316832191:
39: 54432: loss: 0.2313491800:
39: 57632: loss: 0.2308154702:
39: 60832: loss: 0.2307996884:
39: 64032: loss: 0.2309651764:
39: 67232: loss: 0.2319936374:
39: 70432: loss: 0.2320075377:
39: 73632: loss: 0.2323787281:
39: 76832: loss: 0.2326642634:
39: 80032: loss: 0.2329696531:
39: 83232: loss: 0.2331522361:
39: 86432: loss: 0.2323678008:
39: 89632: loss: 0.2320131235:
Dev-Acc: 39: Accuracy: 0.8799908757: precision: 0.2602253434: recall: 0.5733718755: f1: 0.3579807845
Train-Acc: 39: Accuracy: 0.9119715691: precision: 0.8411445955: recall: 0.5816843074: f1: 0.6877574815
40: 3232: loss: 0.2262506264:
40: 6432: loss: 0.2303163442:
40: 9632: loss: 0.2239335812:
40: 12832: loss: 0.2264017264:
40: 16032: loss: 0.2278954222:
40: 19232: loss: 0.2283942676:
40: 22432: loss: 0.2274024094:
40: 25632: loss: 0.2269158127:
40: 28832: loss: 0.2272034291:
40: 32032: loss: 0.2285502753:
40: 35232: loss: 0.2275124436:
40: 38432: loss: 0.2267518859:
40: 41632: loss: 0.2267653447:
40: 44832: loss: 0.2265352838:
40: 48032: loss: 0.2282687496:
40: 51232: loss: 0.2279604347:
40: 54432: loss: 0.2282208601:
40: 57632: loss: 0.2281121285:
40: 60832: loss: 0.2282223816:
40: 64032: loss: 0.2282273122:
40: 67232: loss: 0.2276966745:
40: 70432: loss: 0.2272665808:
40: 73632: loss: 0.2279395166:
40: 76832: loss: 0.2280429706:
40: 80032: loss: 0.2283375789:
40: 83232: loss: 0.2289494292:
40: 86432: loss: 0.2290829975:
40: 89632: loss: 0.2292381629:
Dev-Acc: 40: Accuracy: 0.8783437610: precision: 0.2573775479: recall: 0.5754123448: f1: 0.3556676651
Train-Acc: 40: Accuracy: 0.9124099016: precision: 0.8403924158: recall: 0.5856945631: f1: 0.6902990857
41: 3232: loss: 0.2174029969:
41: 6432: loss: 0.2252084337:
41: 9632: loss: 0.2234959017:
41: 12832: loss: 0.2262948696:
41: 16032: loss: 0.2268470159:
41: 19232: loss: 0.2270058155:
41: 22432: loss: 0.2274028990:
41: 25632: loss: 0.2275010613:
41: 28832: loss: 0.2273651497:
41: 32032: loss: 0.2266334917:
41: 35232: loss: 0.2275489129:
41: 38432: loss: 0.2278374230:
41: 41632: loss: 0.2274461130:
41: 44832: loss: 0.2271464934:
41: 48032: loss: 0.2265722465:
41: 51232: loss: 0.2264484130:
41: 54432: loss: 0.2266761502:
41: 57632: loss: 0.2272590904:
41: 60832: loss: 0.2275781024:
41: 64032: loss: 0.2272881797:
41: 67232: loss: 0.2273855269:
41: 70432: loss: 0.2280232820:
41: 73632: loss: 0.2279367867:
41: 76832: loss: 0.2277245759:
41: 80032: loss: 0.2274028641:
41: 83232: loss: 0.2275410800:
41: 86432: loss: 0.2278344775:
41: 89632: loss: 0.2274075950:
Dev-Acc: 41: Accuracy: 0.8771729469: precision: 0.2557877330: recall: 0.5786430879: f1: 0.3547563200
Train-Acc: 41: Accuracy: 0.9131110907: precision: 0.8411582794: recall: 0.5900992703: f1: 0.6936094583
42: 3232: loss: 0.2171587006:
42: 6432: loss: 0.2184287301:
42: 9632: loss: 0.2180839262:
42: 12832: loss: 0.2207048869:
42: 16032: loss: 0.2229681901:
42: 19232: loss: 0.2239408318:
42: 22432: loss: 0.2246797066:
42: 25632: loss: 0.2245801193:
42: 28832: loss: 0.2247579082:
42: 32032: loss: 0.2271127995:
42: 35232: loss: 0.2256035787:
42: 38432: loss: 0.2246927254:
42: 41632: loss: 0.2245352653:
42: 44832: loss: 0.2246546151:
42: 48032: loss: 0.2253042621:
42: 51232: loss: 0.2250540336:
42: 54432: loss: 0.2246013842:
42: 57632: loss: 0.2248248163:
42: 60832: loss: 0.2251949155:
42: 64032: loss: 0.2251805698:
42: 67232: loss: 0.2256977670:
42: 70432: loss: 0.2264605873:
42: 73632: loss: 0.2262972347:
42: 76832: loss: 0.2254872429:
42: 80032: loss: 0.2252585091:
42: 83232: loss: 0.2252283476:
42: 86432: loss: 0.2250507117:
42: 89632: loss: 0.2252943999:
Dev-Acc: 42: Accuracy: 0.8755754828: precision: 0.2532058409: recall: 0.5808535963: f1: 0.3526739624
Train-Acc: 42: Accuracy: 0.9137247205: precision: 0.8409069789: recall: 0.5948984288: f1: 0.6968273525
43: 3232: loss: 0.2206718623:
43: 6432: loss: 0.2260101779:
43: 9632: loss: 0.2301174056:
43: 12832: loss: 0.2253388976:
43: 16032: loss: 0.2240766313:
43: 19232: loss: 0.2225039516:
43: 22432: loss: 0.2214980901:
43: 25632: loss: 0.2251588600:
43: 28832: loss: 0.2237274215:
43: 32032: loss: 0.2240740344:
43: 35232: loss: 0.2228979531:
43: 38432: loss: 0.2238918794:
43: 41632: loss: 0.2233139638:
43: 44832: loss: 0.2226589935:
43: 48032: loss: 0.2228598446:
43: 51232: loss: 0.2233903346:
43: 54432: loss: 0.2230825846:
43: 57632: loss: 0.2229223968:
43: 60832: loss: 0.2226561856:
43: 64032: loss: 0.2226197221:
43: 67232: loss: 0.2230065212:
43: 70432: loss: 0.2231414164:
43: 73632: loss: 0.2228161679:
43: 76832: loss: 0.2229300852:
43: 80032: loss: 0.2230299865:
43: 83232: loss: 0.2231769581:
43: 86432: loss: 0.2234954159:
43: 89632: loss: 0.2238031316:
Dev-Acc: 43: Accuracy: 0.8739482164: precision: 0.2503110591: recall: 0.5815337528: f1: 0.3499795334
Train-Acc: 43: Accuracy: 0.9145902991: precision: 0.8416252073: recall: 0.6005522319: f1: 0.7009399578
44: 3232: loss: 0.2270980626:
44: 6432: loss: 0.2322061666:
44: 9632: loss: 0.2282646506:
44: 12832: loss: 0.2244511598:
44: 16032: loss: 0.2258948874:
44: 19232: loss: 0.2234376985:
44: 22432: loss: 0.2227844962:
44: 25632: loss: 0.2214479741:
44: 28832: loss: 0.2210905716:
44: 32032: loss: 0.2208340521:
44: 35232: loss: 0.2197816083:
44: 38432: loss: 0.2194813035:
44: 41632: loss: 0.2204513873:
44: 44832: loss: 0.2203176866:
44: 48032: loss: 0.2190851315:
44: 51232: loss: 0.2197971603:
44: 54432: loss: 0.2206508880:
44: 57632: loss: 0.2215109524:
44: 60832: loss: 0.2221794177:
44: 64032: loss: 0.2222340086:
44: 67232: loss: 0.2230434068:
44: 70432: loss: 0.2226052221:
44: 73632: loss: 0.2229271029:
44: 76832: loss: 0.2223236948:
44: 80032: loss: 0.2218336076:
44: 83232: loss: 0.2223145714:
44: 86432: loss: 0.2220615844:
44: 89632: loss: 0.2222090870:
Dev-Acc: 44: Accuracy: 0.8726583719: precision: 0.2483532392: recall: 0.5834041830: f1: 0.3483956133
Train-Acc: 44: Accuracy: 0.9153463244: precision: 0.8425002288: recall: 0.6052199066: f1: 0.7044150279
45: 3232: loss: 0.2213859598:
45: 6432: loss: 0.2180420010:
45: 9632: loss: 0.2185185330:
45: 12832: loss: 0.2157177178:
45: 16032: loss: 0.2186245672:
45: 19232: loss: 0.2201279155:
45: 22432: loss: 0.2202882246:
45: 25632: loss: 0.2203483416:
45: 28832: loss: 0.2214711386:
45: 32032: loss: 0.2209228027:
45: 35232: loss: 0.2202423317:
45: 38432: loss: 0.2189560393:
45: 41632: loss: 0.2189325266:
45: 44832: loss: 0.2201901000:
45: 48032: loss: 0.2199451749:
45: 51232: loss: 0.2195509338:
45: 54432: loss: 0.2194018430:
45: 57632: loss: 0.2199856200:
45: 60832: loss: 0.2195648382:
45: 64032: loss: 0.2199829532:
45: 67232: loss: 0.2194384850:
45: 70432: loss: 0.2199087354:
45: 73632: loss: 0.2200806095:
45: 76832: loss: 0.2198131800:
45: 80032: loss: 0.2202430223:
45: 83232: loss: 0.2204682640:
45: 86432: loss: 0.2206085898:
45: 89632: loss: 0.2200775738:
Dev-Acc: 45: Accuracy: 0.8716363311: precision: 0.2467336683: recall: 0.5844244176: f1: 0.3469789511
Train-Acc: 45: Accuracy: 0.9158503413: precision: 0.8420383323: recall: 0.6094273881: f1: 0.7070938215
46: 3232: loss: 0.2293407258:
46: 6432: loss: 0.2226074986:
46: 9632: loss: 0.2237887139:
46: 12832: loss: 0.2220601255:
46: 16032: loss: 0.2203384461:
46: 19232: loss: 0.2188582751:
46: 22432: loss: 0.2181588799:
46: 25632: loss: 0.2172314805:
46: 28832: loss: 0.2175075287:
46: 32032: loss: 0.2176498996:
46: 35232: loss: 0.2185900534:
46: 38432: loss: 0.2194751022:
46: 41632: loss: 0.2192423401:
46: 44832: loss: 0.2188254371:
46: 48032: loss: 0.2180195777:
46: 51232: loss: 0.2182933071:
46: 54432: loss: 0.2185139849:
46: 57632: loss: 0.2182617599:
46: 60832: loss: 0.2182914152:
46: 64032: loss: 0.2181964728:
46: 67232: loss: 0.2181811475:
46: 70432: loss: 0.2185474572:
46: 73632: loss: 0.2187155798:
46: 76832: loss: 0.2184120944:
46: 80032: loss: 0.2185455145:
46: 83232: loss: 0.2180688627:
46: 86432: loss: 0.2183942163:
46: 89632: loss: 0.2178030286:
Dev-Acc: 46: Accuracy: 0.8705151677: precision: 0.2450024899: recall: 0.5856146914: f1: 0.3454709600
Train-Acc: 46: Accuracy: 0.9163982272: precision: 0.8423809954: recall: 0.6131089343: f1: 0.7096872384
47: 3232: loss: 0.2127998292:
47: 6432: loss: 0.2118826413:
47: 9632: loss: 0.2104177158:
47: 12832: loss: 0.2084241624:
47: 16032: loss: 0.2116498337:
47: 19232: loss: 0.2117053135:
47: 22432: loss: 0.2124954242:
47: 25632: loss: 0.2141437014:
47: 28832: loss: 0.2144861670:
47: 32032: loss: 0.2150270936:
47: 35232: loss: 0.2146529963:
47: 38432: loss: 0.2156724981:
47: 41632: loss: 0.2153342251:
47: 44832: loss: 0.2158818839:
47: 48032: loss: 0.2157829478:
47: 51232: loss: 0.2157617603:
47: 54432: loss: 0.2156287174:
47: 57632: loss: 0.2162186882:
47: 60832: loss: 0.2166828634:
47: 64032: loss: 0.2168116094:
47: 67232: loss: 0.2163868646:
47: 70432: loss: 0.2164584599:
47: 73632: loss: 0.2166545097:
47: 76832: loss: 0.2171422532:
47: 80032: loss: 0.2167215743:
47: 83232: loss: 0.2168710209:
47: 86432: loss: 0.2167490358:
47: 89632: loss: 0.2169964008:
Dev-Acc: 47: Accuracy: 0.8693046570: precision: 0.2431842198: recall: 0.5869750043: f1: 0.3438932058
Train-Acc: 47: Accuracy: 0.9169132113: precision: 0.8424313162: recall: 0.6168562225: f1: 0.7122091920
48: 3232: loss: 0.2314870080:
48: 6432: loss: 0.2271704243:
48: 9632: loss: 0.2196095665:
48: 12832: loss: 0.2195759121:
48: 16032: loss: 0.2189552823:
48: 19232: loss: 0.2170738442:
48: 22432: loss: 0.2163441243:
48: 25632: loss: 0.2177373726:
48: 28832: loss: 0.2180394663:
48: 32032: loss: 0.2170996084:
48: 35232: loss: 0.2159043600:
48: 38432: loss: 0.2152867699:
48: 41632: loss: 0.2154261228:
48: 44832: loss: 0.2157370042:
48: 48032: loss: 0.2153399914:
48: 51232: loss: 0.2152063861:
48: 54432: loss: 0.2155153564:
48: 57632: loss: 0.2152932645:
48: 60832: loss: 0.2147734608:
48: 64032: loss: 0.2156841860:
48: 67232: loss: 0.2158342705:
48: 70432: loss: 0.2158388290:
48: 73632: loss: 0.2153540466:
48: 76832: loss: 0.2152420214:
48: 80032: loss: 0.2147867015:
48: 83232: loss: 0.2154751104:
48: 86432: loss: 0.2152674110:
48: 89632: loss: 0.2151030833:
Dev-Acc: 48: Accuracy: 0.8681536913: precision: 0.2415020590: recall: 0.5883353171: f1: 0.3424386382
Train-Acc: 48: Accuracy: 0.9176473022: precision: 0.8434347943: recall: 0.6211951877: f1: 0.7154539259
49: 3232: loss: 0.2146722545:
49: 6432: loss: 0.2133699829:
49: 9632: loss: 0.2130081176:
49: 12832: loss: 0.2099440239:
49: 16032: loss: 0.2099489986:
49: 19232: loss: 0.2105798863:
49: 22432: loss: 0.2096240883:
49: 25632: loss: 0.2104941483:
49: 28832: loss: 0.2123421369:
49: 32032: loss: 0.2120586571:
49: 35232: loss: 0.2115282257:
49: 38432: loss: 0.2128874585:
49: 41632: loss: 0.2122238076:
49: 44832: loss: 0.2132790445:
49: 48032: loss: 0.2131201136:
49: 51232: loss: 0.2131642522:
49: 54432: loss: 0.2129208413:
49: 57632: loss: 0.2131174044:
49: 60832: loss: 0.2130931791:
49: 64032: loss: 0.2132351889:
49: 67232: loss: 0.2130973815:
49: 70432: loss: 0.2130232933:
49: 73632: loss: 0.2138116858:
49: 76832: loss: 0.2136294711:
49: 80032: loss: 0.2135152097:
49: 83232: loss: 0.2133132763:
49: 86432: loss: 0.2131788847:
49: 89632: loss: 0.2128096506:
Dev-Acc: 49: Accuracy: 0.8670920134: precision: 0.2399640089: recall: 0.5895255909: f1: 0.3410890846
Train-Acc: 49: Accuracy: 0.9181184769: precision: 0.8434836648: recall: 0.6246137664: f1: 0.7177337110
50: 3232: loss: 0.1997942301:
50: 6432: loss: 0.2043757169:
50: 9632: loss: 0.2042828191:
50: 12832: loss: 0.2042889692:
50: 16032: loss: 0.2082590043:
50: 19232: loss: 0.2097062697:
50: 22432: loss: 0.2105253895:
50: 25632: loss: 0.2121870397:
50: 28832: loss: 0.2116808461:
50: 32032: loss: 0.2112338181:
50: 35232: loss: 0.2113474961:
50: 38432: loss: 0.2119209636:
50: 41632: loss: 0.2106157620:
50: 44832: loss: 0.2117256846:
50: 48032: loss: 0.2121590081:
50: 51232: loss: 0.2112640029:
50: 54432: loss: 0.2115502620:
50: 57632: loss: 0.2117161039:
50: 60832: loss: 0.2116428514:
50: 64032: loss: 0.2114131687:
50: 67232: loss: 0.2107117085:
50: 70432: loss: 0.2122099098:
50: 73632: loss: 0.2122235043:
50: 76832: loss: 0.2121824846:
50: 80032: loss: 0.2123830962:
50: 83232: loss: 0.2120253369:
50: 86432: loss: 0.2119744657:
50: 89632: loss: 0.2119745132:
Dev-Acc: 50: Accuracy: 0.8655838370: precision: 0.2377172574: recall: 0.5907158646: f1: 0.3390095145
Train-Acc: 50: Accuracy: 0.9187320471: precision: 0.8435296192: recall: 0.6290842154: f1: 0.7206929015
