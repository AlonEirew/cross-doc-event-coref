1: 3232: loss: 0.7112611806:
1: 6432: loss: 0.7097432712:
1: 9632: loss: 0.7084852558:
1: 12832: loss: 0.7071429537:
1: 16032: loss: 0.7061985493:
1: 19232: loss: 0.7053276958:
1: 22432: loss: 0.7043118258:
1: 25632: loss: 0.7035348297:
1: 28832: loss: 0.7026002464:
1: 32032: loss: 0.7015433731:
1: 35232: loss: 0.7005933997:
1: 38432: loss: 0.6997306822:
1: 41632: loss: 0.6988281319:
1: 44832: loss: 0.6978609248:
1: 48032: loss: 0.6969125580:
1: 51232: loss: 0.6959290968:
1: 54432: loss: 0.6949804320:
1: 57632: loss: 0.6940334151:
1: 60832: loss: 0.6931383503:
1: 64032: loss: 0.6921623945:
1: 67232: loss: 0.6911905331:
1: 70432: loss: 0.6902488118:
1: 73632: loss: 0.6893569468:
1: 76832: loss: 0.6884352072:
1: 80032: loss: 0.6875398663:
1: 83232: loss: 0.6866327399:
1: 86432: loss: 0.6857047199:
1: 89632: loss: 0.6848422311:
1: 92832: loss: 0.6838995577:
1: 96032: loss: 0.6830377509:
1: 99232: loss: 0.6821181798:
1: 102432: loss: 0.6812077560:
1: 105632: loss: 0.6802884248:
1: 108832: loss: 0.6793707551:
1: 112032: loss: 0.6784982296:
1: 115232: loss: 0.6775766188:
1: 118432: loss: 0.6767018696:
1: 121632: loss: 0.6758170935:
1: 124832: loss: 0.6749233205:
1: 128032: loss: 0.6740477829:
1: 131232: loss: 0.6731839255:
1: 134432: loss: 0.6722877227:
1: 137632: loss: 0.6714026846:
1: 140832: loss: 0.6704848356:
1: 144032: loss: 0.6695820709:
1: 147232: loss: 0.6686933795:
1: 150432: loss: 0.6678274344:
1: 153632: loss: 0.6669630949:
1: 156832: loss: 0.6661089480:
1: 160032: loss: 0.6652039704:
1: 163232: loss: 0.6643022808:
1: 166432: loss: 0.6634888526:
Dev-Acc: 1: Accuracy: 0.9392264485: precision: 0.0960264901: recall: 0.0049311342: f1: 0.0093805596
Train-Acc: 1: Accuracy: 0.9069274068: precision: 0.2189440994: recall: 0.0092696075: f1: 0.0177861873
2: 3232: loss: 0.6176242077:
2: 6432: loss: 0.6177148038:
2: 9632: loss: 0.6159027708:
2: 12832: loss: 0.6150143029:
2: 16032: loss: 0.6140921875:
2: 19232: loss: 0.6135586536:
2: 22432: loss: 0.6128076198:
2: 25632: loss: 0.6119092467:
2: 28832: loss: 0.6109855837:
2: 32032: loss: 0.6100474994:
2: 35232: loss: 0.6091333382:
2: 38432: loss: 0.6082783175:
2: 41632: loss: 0.6076051418:
2: 44832: loss: 0.6066463295:
2: 48032: loss: 0.6058010809:
2: 51232: loss: 0.6049322071:
2: 54432: loss: 0.6041742979:
2: 57632: loss: 0.6034443866:
2: 60832: loss: 0.6028583143:
2: 64032: loss: 0.6020060028:
2: 67232: loss: 0.6012304881:
2: 70432: loss: 0.6005009481:
2: 73632: loss: 0.5996030311:
2: 76832: loss: 0.5988984134:
2: 80032: loss: 0.5980345382:
2: 83232: loss: 0.5971754073:
2: 86432: loss: 0.5963806740:
2: 89632: loss: 0.5955419912:
2: 92832: loss: 0.5947722282:
2: 96032: loss: 0.5939438224:
2: 99232: loss: 0.5931671056:
2: 102432: loss: 0.5923183392:
2: 105632: loss: 0.5915165666:
2: 108832: loss: 0.5908496621:
2: 112032: loss: 0.5900473552:
2: 115232: loss: 0.5893132832:
2: 118432: loss: 0.5884049184:
2: 121632: loss: 0.5876413628:
2: 124832: loss: 0.5868272346:
2: 128032: loss: 0.5861696402:
2: 131232: loss: 0.5854245764:
2: 134432: loss: 0.5845809420:
2: 137632: loss: 0.5838180621:
2: 140832: loss: 0.5829572519:
2: 144032: loss: 0.5821003547:
2: 147232: loss: 0.5812381383:
2: 150432: loss: 0.5804245246:
2: 153632: loss: 0.5797014088:
2: 156832: loss: 0.5789030584:
2: 160032: loss: 0.5782045595:
2: 163232: loss: 0.5774016899:
2: 166432: loss: 0.5765848769:
Dev-Acc: 2: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 2: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
3: 3232: loss: 0.5341464862:
3: 6432: loss: 0.5343040407:
3: 9632: loss: 0.5343658143:
3: 12832: loss: 0.5337754379:
3: 16032: loss: 0.5323493320:
3: 19232: loss: 0.5313974428:
3: 22432: loss: 0.5304170208:
3: 25632: loss: 0.5303221112:
3: 28832: loss: 0.5291613202:
3: 32032: loss: 0.5287497957:
3: 35232: loss: 0.5280997976:
3: 38432: loss: 0.5269104557:
3: 41632: loss: 0.5261724131:
3: 44832: loss: 0.5256210800:
3: 48032: loss: 0.5249332897:
3: 51232: loss: 0.5239647226:
3: 54432: loss: 0.5231733070:
3: 57632: loss: 0.5224424581:
3: 60832: loss: 0.5217967190:
3: 64032: loss: 0.5208504600:
3: 67232: loss: 0.5204852483:
3: 70432: loss: 0.5198394993:
3: 73632: loss: 0.5190364264:
3: 76832: loss: 0.5185264436:
3: 80032: loss: 0.5175790011:
3: 83232: loss: 0.5169506656:
3: 86432: loss: 0.5163878940:
3: 89632: loss: 0.5155827367:
3: 92832: loss: 0.5149445214:
3: 96032: loss: 0.5142974510:
3: 99232: loss: 0.5134588433:
3: 102432: loss: 0.5127009202:
3: 105632: loss: 0.5121334364:
3: 108832: loss: 0.5116281989:
3: 112032: loss: 0.5107553037:
3: 115232: loss: 0.5101268708:
3: 118432: loss: 0.5095982552:
3: 121632: loss: 0.5089623982:
3: 124832: loss: 0.5082976334:
3: 128032: loss: 0.5075667909:
3: 131232: loss: 0.5066501293:
3: 134432: loss: 0.5061427723:
3: 137632: loss: 0.5054974408:
3: 140832: loss: 0.5049403416:
3: 144032: loss: 0.5042719564:
3: 147232: loss: 0.5037450700:
3: 150432: loss: 0.5031214105:
3: 153632: loss: 0.5024302365:
3: 156832: loss: 0.5015943032:
3: 160032: loss: 0.5009512543:
3: 163232: loss: 0.5003062068:
3: 166432: loss: 0.4994418406:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.4602072278:
4: 6432: loss: 0.4598050711:
4: 9632: loss: 0.4598430326:
4: 12832: loss: 0.4605297347:
4: 16032: loss: 0.4593037827:
4: 19232: loss: 0.4584557288:
4: 22432: loss: 0.4574457143:
4: 25632: loss: 0.4576212151:
4: 28832: loss: 0.4576276073:
4: 32032: loss: 0.4574535792:
4: 35232: loss: 0.4569069166:
4: 38432: loss: 0.4560141691:
4: 41632: loss: 0.4553661878:
4: 44832: loss: 0.4550986308:
4: 48032: loss: 0.4545019643:
4: 51232: loss: 0.4537484303:
4: 54432: loss: 0.4532521909:
4: 57632: loss: 0.4525564096:
4: 60832: loss: 0.4523782824:
4: 64032: loss: 0.4519194218:
4: 67232: loss: 0.4516233653:
4: 70432: loss: 0.4510376940:
4: 73632: loss: 0.4505070895:
4: 76832: loss: 0.4500115030:
4: 80032: loss: 0.4496177770:
4: 83232: loss: 0.4491033691:
4: 86432: loss: 0.4483117442:
4: 89632: loss: 0.4474864620:
4: 92832: loss: 0.4465819996:
4: 96032: loss: 0.4460987840:
4: 99232: loss: 0.4454716956:
4: 102432: loss: 0.4447480482:
4: 105632: loss: 0.4442535437:
4: 108832: loss: 0.4441200189:
4: 112032: loss: 0.4435527752:
4: 115232: loss: 0.4432046033:
4: 118432: loss: 0.4427656743:
4: 121632: loss: 0.4423146684:
4: 124832: loss: 0.4416178055:
4: 128032: loss: 0.4408180117:
4: 131232: loss: 0.4403779233:
4: 134432: loss: 0.4397539541:
4: 137632: loss: 0.4389259609:
4: 140832: loss: 0.4384094129:
4: 144032: loss: 0.4380420940:
4: 147232: loss: 0.4375269289:
4: 150432: loss: 0.4368666131:
4: 153632: loss: 0.4360739636:
4: 156832: loss: 0.4354246147:
4: 160032: loss: 0.4347791512:
4: 163232: loss: 0.4341479198:
4: 166432: loss: 0.4335591441:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4008821326:
5: 6432: loss: 0.4050012395:
5: 9632: loss: 0.4045754439:
5: 12832: loss: 0.4019764914:
5: 16032: loss: 0.4019102100:
5: 19232: loss: 0.4001309077:
5: 22432: loss: 0.4004462555:
5: 25632: loss: 0.3998069543:
5: 28832: loss: 0.3993936720:
5: 32032: loss: 0.3982922009:
5: 35232: loss: 0.3973008976:
5: 38432: loss: 0.3973520795:
5: 41632: loss: 0.3963136141:
5: 44832: loss: 0.3958038843:
5: 48032: loss: 0.3961067354:
5: 51232: loss: 0.3957867972:
5: 54432: loss: 0.3957672196:
5: 57632: loss: 0.3953736485:
5: 60832: loss: 0.3948121879:
5: 64032: loss: 0.3944971442:
5: 67232: loss: 0.3936662471:
5: 70432: loss: 0.3934120740:
5: 73632: loss: 0.3933198259:
5: 76832: loss: 0.3931707590:
5: 80032: loss: 0.3927099455:
5: 83232: loss: 0.3921504242:
5: 86432: loss: 0.3916761024:
5: 89632: loss: 0.3914402977:
5: 92832: loss: 0.3909145886:
5: 96032: loss: 0.3906513781:
5: 99232: loss: 0.3901687390:
5: 102432: loss: 0.3899117822:
5: 105632: loss: 0.3897101518:
5: 108832: loss: 0.3893161281:
5: 112032: loss: 0.3889562348:
5: 115232: loss: 0.3884032610:
5: 118432: loss: 0.3878997896:
5: 121632: loss: 0.3872442271:
5: 124832: loss: 0.3869525471:
5: 128032: loss: 0.3866387366:
5: 131232: loss: 0.3865116455:
5: 134432: loss: 0.3863191281:
5: 137632: loss: 0.3856557172:
5: 140832: loss: 0.3850498585:
5: 144032: loss: 0.3844960495:
5: 147232: loss: 0.3841205153:
5: 150432: loss: 0.3836032050:
5: 153632: loss: 0.3831904639:
5: 156832: loss: 0.3828511916:
5: 160032: loss: 0.3824433122:
5: 163232: loss: 0.3820596414:
5: 166432: loss: 0.3818178630:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.3611842607:
6: 6432: loss: 0.3611181985:
6: 9632: loss: 0.3653124517:
6: 12832: loss: 0.3665210989:
6: 16032: loss: 0.3670206808:
6: 19232: loss: 0.3653385315:
6: 22432: loss: 0.3646759547:
6: 25632: loss: 0.3618423820:
6: 28832: loss: 0.3612937571:
6: 32032: loss: 0.3595066428:
6: 35232: loss: 0.3594196373:
6: 38432: loss: 0.3592826335:
6: 41632: loss: 0.3584990600:
6: 44832: loss: 0.3586640380:
6: 48032: loss: 0.3584985905:
6: 51232: loss: 0.3576664119:
6: 54432: loss: 0.3570222855:
6: 57632: loss: 0.3568387121:
6: 60832: loss: 0.3559785701:
6: 64032: loss: 0.3559642749:
6: 67232: loss: 0.3551299432:
6: 70432: loss: 0.3547059689:
6: 73632: loss: 0.3542875996:
6: 76832: loss: 0.3538148626:
6: 80032: loss: 0.3537074869:
6: 83232: loss: 0.3534686371:
6: 86432: loss: 0.3531846869:
6: 89632: loss: 0.3526395930:
6: 92832: loss: 0.3525749917:
6: 96032: loss: 0.3522818634:
6: 99232: loss: 0.3519189597:
6: 102432: loss: 0.3511604461:
6: 105632: loss: 0.3506025120:
6: 108832: loss: 0.3506198583:
6: 112032: loss: 0.3504603915:
6: 115232: loss: 0.3496106231:
6: 118432: loss: 0.3492901983:
6: 121632: loss: 0.3489797524:
6: 124832: loss: 0.3484878150:
6: 128032: loss: 0.3480126552:
6: 131232: loss: 0.3475433175:
6: 134432: loss: 0.3470349277:
6: 137632: loss: 0.3469572293:
6: 140832: loss: 0.3467092887:
6: 144032: loss: 0.3462120101:
6: 147232: loss: 0.3460500781:
6: 150432: loss: 0.3454784882:
6: 153632: loss: 0.3453652857:
6: 156832: loss: 0.3452000594:
6: 160032: loss: 0.3448262728:
6: 163232: loss: 0.3445821931:
6: 166432: loss: 0.3441817107:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.3427736610:
7: 6432: loss: 0.3379549015:
7: 9632: loss: 0.3338615339:
7: 12832: loss: 0.3296037031:
7: 16032: loss: 0.3298675950:
7: 19232: loss: 0.3343391966:
7: 22432: loss: 0.3329057540:
7: 25632: loss: 0.3324999789:
7: 28832: loss: 0.3309193875:
7: 32032: loss: 0.3285589602:
7: 35232: loss: 0.3283845429:
7: 38432: loss: 0.3277853466:
7: 41632: loss: 0.3267106535:
7: 44832: loss: 0.3253861674:
7: 48032: loss: 0.3257893903:
7: 51232: loss: 0.3263882908:
7: 54432: loss: 0.3267839297:
7: 57632: loss: 0.3269429744:
7: 60832: loss: 0.3263711209:
7: 64032: loss: 0.3262692661:
7: 67232: loss: 0.3257780617:
7: 70432: loss: 0.3253413306:
7: 73632: loss: 0.3248022486:
7: 76832: loss: 0.3243335887:
7: 80032: loss: 0.3241300441:
7: 83232: loss: 0.3241068798:
7: 86432: loss: 0.3236386954:
7: 89632: loss: 0.3236872682:
7: 92832: loss: 0.3238989298:
7: 96032: loss: 0.3238222238:
7: 99232: loss: 0.3235410789:
7: 102432: loss: 0.3233852527:
7: 105632: loss: 0.3224296856:
7: 108832: loss: 0.3222488631:
7: 112032: loss: 0.3218185801:
7: 115232: loss: 0.3219857781:
7: 118432: loss: 0.3215474099:
7: 121632: loss: 0.3209669996:
7: 124832: loss: 0.3207115288:
7: 128032: loss: 0.3204382777:
7: 131232: loss: 0.3202426589:
7: 134432: loss: 0.3200875488:
7: 137632: loss: 0.3197892522:
7: 140832: loss: 0.3194806225:
7: 144032: loss: 0.3191946712:
7: 147232: loss: 0.3191504110:
7: 150432: loss: 0.3187149853:
7: 153632: loss: 0.3184008068:
7: 156832: loss: 0.3182885645:
7: 160032: loss: 0.3185399470:
7: 163232: loss: 0.3181838765:
7: 166432: loss: 0.3181672922:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3064984752:
8: 6432: loss: 0.3086067788:
8: 9632: loss: 0.3094005804:
8: 12832: loss: 0.3063627038:
8: 16032: loss: 0.3069139097:
8: 19232: loss: 0.3066068557:
8: 22432: loss: 0.3036129140:
8: 25632: loss: 0.3036659537:
8: 28832: loss: 0.3049741214:
8: 32032: loss: 0.3048726288:
8: 35232: loss: 0.3038217919:
8: 38432: loss: 0.3043855033:
8: 41632: loss: 0.3045312056:
8: 44832: loss: 0.3041729428:
8: 48032: loss: 0.3040294000:
8: 51232: loss: 0.3048506869:
8: 54432: loss: 0.3044015731:
8: 57632: loss: 0.3044724572:
8: 60832: loss: 0.3045399051:
8: 64032: loss: 0.3043267774:
8: 67232: loss: 0.3040202749:
8: 70432: loss: 0.3037416903:
8: 73632: loss: 0.3033734244:
8: 76832: loss: 0.3036951746:
8: 80032: loss: 0.3044613354:
8: 83232: loss: 0.3042138742:
8: 86432: loss: 0.3044754513:
8: 89632: loss: 0.3040157878:
8: 92832: loss: 0.3032949097:
8: 96032: loss: 0.3028294327:
8: 99232: loss: 0.3026062908:
8: 102432: loss: 0.3020812273:
8: 105632: loss: 0.3019609552:
8: 108832: loss: 0.3016820252:
8: 112032: loss: 0.3012537943:
8: 115232: loss: 0.3010593095:
8: 118432: loss: 0.3012725667:
8: 121632: loss: 0.3008020449:
8: 124832: loss: 0.3006218304:
8: 128032: loss: 0.3005071429:
8: 131232: loss: 0.3004138810:
8: 134432: loss: 0.3002621255:
8: 137632: loss: 0.3000257389:
8: 140832: loss: 0.2998404352:
8: 144032: loss: 0.2997944262:
8: 147232: loss: 0.2996335560:
8: 150432: loss: 0.2996147188:
8: 153632: loss: 0.2996317860:
8: 156832: loss: 0.2995521791:
8: 160032: loss: 0.2998669722:
8: 163232: loss: 0.2999573001:
8: 166432: loss: 0.3000412808:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.2801035039:
9: 6432: loss: 0.2847698764:
9: 9632: loss: 0.2872940315:
9: 12832: loss: 0.2930272645:
9: 16032: loss: 0.2899470701:
9: 19232: loss: 0.2883596615:
9: 22432: loss: 0.2885406491:
9: 25632: loss: 0.2876373960:
9: 28832: loss: 0.2872359295:
9: 32032: loss: 0.2875808939:
9: 35232: loss: 0.2878397866:
9: 38432: loss: 0.2883891152:
9: 41632: loss: 0.2893360952:
9: 44832: loss: 0.2899014127:
9: 48032: loss: 0.2897637125:
9: 51232: loss: 0.2901352512:
9: 54432: loss: 0.2901742313:
9: 57632: loss: 0.2913326762:
9: 60832: loss: 0.2911581017:
9: 64032: loss: 0.2912457778:
9: 67232: loss: 0.2911130709:
9: 70432: loss: 0.2906763243:
9: 73632: loss: 0.2907733988:
9: 76832: loss: 0.2896054214:
9: 80032: loss: 0.2894745195:
9: 83232: loss: 0.2888938469:
9: 86432: loss: 0.2888875036:
9: 89632: loss: 0.2889040595:
9: 92832: loss: 0.2889207834:
9: 96032: loss: 0.2886452503:
9: 99232: loss: 0.2882045909:
9: 102432: loss: 0.2880463164:
9: 105632: loss: 0.2880593490:
9: 108832: loss: 0.2879056048:
9: 112032: loss: 0.2877641826:
9: 115232: loss: 0.2876859567:
9: 118432: loss: 0.2875002768:
9: 121632: loss: 0.2880730919:
9: 124832: loss: 0.2878989399:
9: 128032: loss: 0.2881107594:
9: 131232: loss: 0.2880528152:
9: 134432: loss: 0.2875903034:
9: 137632: loss: 0.2875429536:
9: 140832: loss: 0.2875530061:
9: 144032: loss: 0.2873137888:
9: 147232: loss: 0.2872176480:
9: 150432: loss: 0.2870657708:
9: 153632: loss: 0.2866720721:
9: 156832: loss: 0.2866068649:
9: 160032: loss: 0.2865981842:
9: 163232: loss: 0.2865917956:
9: 166432: loss: 0.2863811982:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.2617894167:
10: 6432: loss: 0.2709736247:
10: 9632: loss: 0.2753865610:
10: 12832: loss: 0.2762602125:
10: 16032: loss: 0.2778608782:
10: 19232: loss: 0.2774624698:
10: 22432: loss: 0.2758517750:
10: 25632: loss: 0.2745855886:
10: 28832: loss: 0.2775462128:
10: 32032: loss: 0.2779138966:
10: 35232: loss: 0.2783498760:
10: 38432: loss: 0.2774378879:
10: 41632: loss: 0.2780741422:
10: 44832: loss: 0.2778610584:
10: 48032: loss: 0.2777368878:
10: 51232: loss: 0.2780521162:
10: 54432: loss: 0.2774082286:
10: 57632: loss: 0.2767480382:
10: 60832: loss: 0.2775374834:
10: 64032: loss: 0.2778246050:
10: 67232: loss: 0.2774657900:
10: 70432: loss: 0.2765726796:
10: 73632: loss: 0.2764474905:
10: 76832: loss: 0.2759957783:
10: 80032: loss: 0.2762409405:
10: 83232: loss: 0.2763327671:
10: 86432: loss: 0.2760327327:
10: 89632: loss: 0.2756450263:
10: 92832: loss: 0.2758334784:
10: 96032: loss: 0.2761876235:
10: 99232: loss: 0.2763618067:
10: 102432: loss: 0.2764356824:
10: 105632: loss: 0.2771495596:
10: 108832: loss: 0.2770671148:
10: 112032: loss: 0.2773853806:
10: 115232: loss: 0.2772470056:
10: 118432: loss: 0.2769397546:
10: 121632: loss: 0.2767230697:
10: 124832: loss: 0.2767532377:
10: 128032: loss: 0.2763922406:
10: 131232: loss: 0.2762094501:
10: 134432: loss: 0.2759531054:
10: 137632: loss: 0.2761234320:
10: 140832: loss: 0.2759228382:
10: 144032: loss: 0.2758903380:
10: 147232: loss: 0.2755038699:
10: 150432: loss: 0.2754388743:
10: 153632: loss: 0.2752002214:
10: 156832: loss: 0.2751808809:
10: 160032: loss: 0.2752914447:
10: 163232: loss: 0.2754221918:
10: 166432: loss: 0.2754970449:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.2678352639:
11: 6432: loss: 0.2676925294:
11: 9632: loss: 0.2732976530:
11: 12832: loss: 0.2727909932:
11: 16032: loss: 0.2718745456:
11: 19232: loss: 0.2681961085:
11: 22432: loss: 0.2692999696:
11: 25632: loss: 0.2692136611:
11: 28832: loss: 0.2674535606:
11: 32032: loss: 0.2667483249:
11: 35232: loss: 0.2682421889:
11: 38432: loss: 0.2675239536:
11: 41632: loss: 0.2677828825:
11: 44832: loss: 0.2685134806:
11: 48032: loss: 0.2676360884:
11: 51232: loss: 0.2677720256:
11: 54432: loss: 0.2668070341:
11: 57632: loss: 0.2669502513:
11: 60832: loss: 0.2675421318:
11: 64032: loss: 0.2680689205:
11: 67232: loss: 0.2681694272:
11: 70432: loss: 0.2684972559:
11: 73632: loss: 0.2680505124:
11: 76832: loss: 0.2685529988:
11: 80032: loss: 0.2682622458:
11: 83232: loss: 0.2684991922:
11: 86432: loss: 0.2688333328:
11: 89632: loss: 0.2690013785:
11: 92832: loss: 0.2687493327:
11: 96032: loss: 0.2683582405:
11: 99232: loss: 0.2689739201:
11: 102432: loss: 0.2691084582:
11: 105632: loss: 0.2692058469:
11: 108832: loss: 0.2693028164:
11: 112032: loss: 0.2690510586:
11: 115232: loss: 0.2685703152:
11: 118432: loss: 0.2686406381:
11: 121632: loss: 0.2679634431:
11: 124832: loss: 0.2683350150:
11: 128032: loss: 0.2681491899:
11: 131232: loss: 0.2680494650:
11: 134432: loss: 0.2682468168:
11: 137632: loss: 0.2678843844:
11: 140832: loss: 0.2676996100:
11: 144032: loss: 0.2674621907:
11: 147232: loss: 0.2672881880:
11: 150432: loss: 0.2672317660:
11: 153632: loss: 0.2667867930:
11: 156832: loss: 0.2665576608:
11: 160032: loss: 0.2667426408:
11: 163232: loss: 0.2664821131:
11: 166432: loss: 0.2662222322:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.2645723457:
12: 6432: loss: 0.2569098076:
12: 9632: loss: 0.2585812088:
12: 12832: loss: 0.2585330380:
12: 16032: loss: 0.2604210625:
12: 19232: loss: 0.2606862654:
12: 22432: loss: 0.2617301480:
12: 25632: loss: 0.2592273378:
12: 28832: loss: 0.2574909506:
12: 32032: loss: 0.2576706591:
12: 35232: loss: 0.2584704391:
12: 38432: loss: 0.2580774276:
12: 41632: loss: 0.2597137654:
12: 44832: loss: 0.2581015335:
12: 48032: loss: 0.2585371958:
12: 51232: loss: 0.2582564472:
12: 54432: loss: 0.2578584371:
12: 57632: loss: 0.2579508026:
12: 60832: loss: 0.2578888088:
12: 64032: loss: 0.2574303043:
12: 67232: loss: 0.2571517474:
12: 70432: loss: 0.2575113643:
12: 73632: loss: 0.2569860378:
12: 76832: loss: 0.2575616023:
12: 80032: loss: 0.2574854981:
12: 83232: loss: 0.2579137849:
12: 86432: loss: 0.2578542133:
12: 89632: loss: 0.2574040256:
12: 92832: loss: 0.2572490955:
12: 96032: loss: 0.2571231094:
12: 99232: loss: 0.2574926790:
12: 102432: loss: 0.2575260979:
12: 105632: loss: 0.2574936693:
12: 108832: loss: 0.2578224550:
12: 112032: loss: 0.2584763817:
12: 115232: loss: 0.2592226321:
12: 118432: loss: 0.2589859549:
12: 121632: loss: 0.2586374292:
12: 124832: loss: 0.2585961583:
12: 128032: loss: 0.2584128999:
12: 131232: loss: 0.2586697418:
12: 134432: loss: 0.2584335589:
12: 137632: loss: 0.2584030721:
12: 140832: loss: 0.2587882820:
12: 144032: loss: 0.2584262783:
12: 147232: loss: 0.2581437856:
12: 150432: loss: 0.2581514375:
12: 153632: loss: 0.2580744872:
12: 156832: loss: 0.2579459740:
12: 160032: loss: 0.2579365383:
12: 163232: loss: 0.2577569632:
12: 166432: loss: 0.2578239990:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2596737207:
13: 6432: loss: 0.2619295051:
13: 9632: loss: 0.2629307402:
13: 12832: loss: 0.2587271609:
13: 16032: loss: 0.2537972225:
13: 19232: loss: 0.2516951214:
13: 22432: loss: 0.2526081262:
13: 25632: loss: 0.2516906071:
13: 28832: loss: 0.2518854222:
13: 32032: loss: 0.2524103229:
13: 35232: loss: 0.2533922183:
13: 38432: loss: 0.2542645968:
13: 41632: loss: 0.2542179005:
13: 44832: loss: 0.2534350587:
13: 48032: loss: 0.2526780865:
13: 51232: loss: 0.2526212651:
13: 54432: loss: 0.2523969403:
13: 57632: loss: 0.2517835926:
13: 60832: loss: 0.2521980379:
13: 64032: loss: 0.2513124521:
13: 67232: loss: 0.2510945500:
13: 70432: loss: 0.2511106781:
13: 73632: loss: 0.2509132222:
13: 76832: loss: 0.2505726918:
13: 80032: loss: 0.2503950505:
13: 83232: loss: 0.2499446071:
13: 86432: loss: 0.2502481657:
13: 89632: loss: 0.2503083811:
13: 92832: loss: 0.2505105970:
13: 96032: loss: 0.2512776539:
13: 99232: loss: 0.2512986110:
13: 102432: loss: 0.2516514534:
13: 105632: loss: 0.2513192086:
13: 108832: loss: 0.2508692537:
13: 112032: loss: 0.2505118401:
13: 115232: loss: 0.2508628039:
13: 118432: loss: 0.2508329039:
13: 121632: loss: 0.2507351120:
13: 124832: loss: 0.2508261572:
13: 128032: loss: 0.2507722738:
13: 131232: loss: 0.2511598996:
13: 134432: loss: 0.2514111367:
13: 137632: loss: 0.2516454977:
13: 140832: loss: 0.2517055670:
13: 144032: loss: 0.2511798207:
13: 147232: loss: 0.2508983871:
13: 150432: loss: 0.2509048589:
13: 153632: loss: 0.2509847213:
13: 156832: loss: 0.2508465549:
13: 160032: loss: 0.2510277124:
13: 163232: loss: 0.2509851062:
13: 166432: loss: 0.2509699670:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.9094973207: precision: 1.0000000000: recall: 0.0044704490: f1: 0.0089011061
14: 3232: loss: 0.2401652895:
14: 6432: loss: 0.2447462367:
14: 9632: loss: 0.2507624433:
14: 12832: loss: 0.2456395140:
14: 16032: loss: 0.2450021133:
14: 19232: loss: 0.2453274437:
14: 22432: loss: 0.2454028114:
14: 25632: loss: 0.2471409292:
14: 28832: loss: 0.2473919894:
14: 32032: loss: 0.2473140381:
14: 35232: loss: 0.2477065229:
14: 38432: loss: 0.2473612639:
14: 41632: loss: 0.2471121511:
14: 44832: loss: 0.2476015612:
14: 48032: loss: 0.2481163732:
14: 51232: loss: 0.2474176078:
14: 54432: loss: 0.2473906468:
14: 57632: loss: 0.2469295489:
14: 60832: loss: 0.2464598042:
14: 64032: loss: 0.2455362754:
14: 67232: loss: 0.2448825974:
14: 70432: loss: 0.2451180982:
14: 73632: loss: 0.2456415298:
14: 76832: loss: 0.2457689317:
14: 80032: loss: 0.2451238040:
14: 83232: loss: 0.2455410656:
14: 86432: loss: 0.2457468488:
14: 89632: loss: 0.2461707327:
14: 92832: loss: 0.2467855547:
14: 96032: loss: 0.2462494793:
14: 99232: loss: 0.2459431547:
14: 102432: loss: 0.2460804892:
14: 105632: loss: 0.2457374847:
14: 108832: loss: 0.2453396956:
14: 112032: loss: 0.2453484716:
14: 115232: loss: 0.2456287160:
14: 118432: loss: 0.2457143029:
14: 121632: loss: 0.2457434427:
14: 124832: loss: 0.2460314523:
14: 128032: loss: 0.2459775283:
14: 131232: loss: 0.2457937023:
14: 134432: loss: 0.2457811694:
14: 137632: loss: 0.2458460913:
14: 140832: loss: 0.2455891847:
14: 144032: loss: 0.2453934351:
14: 147232: loss: 0.2456444467:
14: 150432: loss: 0.2456105352:
14: 153632: loss: 0.2456632127:
14: 156832: loss: 0.2455126838:
14: 160032: loss: 0.2452235235:
14: 163232: loss: 0.2449988119:
14: 166432: loss: 0.2450181867:
Dev-Acc: 14: Accuracy: 0.9418558478: precision: 0.6567164179: recall: 0.0074817208: f1: 0.0147948890
Train-Acc: 14: Accuracy: 0.9109556079: precision: 0.8095238095: recall: 0.0268226941: f1: 0.0519249125
15: 3232: loss: 0.2405529891:
15: 6432: loss: 0.2420241298:
15: 9632: loss: 0.2387944730:
15: 12832: loss: 0.2395692375:
15: 16032: loss: 0.2386925288:
15: 19232: loss: 0.2396493886:
15: 22432: loss: 0.2419920023:
15: 25632: loss: 0.2423551980:
15: 28832: loss: 0.2413228600:
15: 32032: loss: 0.2404620721:
15: 35232: loss: 0.2401527384:
15: 38432: loss: 0.2404237185:
15: 41632: loss: 0.2398480388:
15: 44832: loss: 0.2397302793:
15: 48032: loss: 0.2393677184:
15: 51232: loss: 0.2388014026:
15: 54432: loss: 0.2395463017:
15: 57632: loss: 0.2394428401:
15: 60832: loss: 0.2393484378:
15: 64032: loss: 0.2398970482:
15: 67232: loss: 0.2394397993:
15: 70432: loss: 0.2387312761:
15: 73632: loss: 0.2390691935:
15: 76832: loss: 0.2388379225:
15: 80032: loss: 0.2392955091:
15: 83232: loss: 0.2397835491:
15: 86432: loss: 0.2395958864:
15: 89632: loss: 0.2401144842:
15: 92832: loss: 0.2404913265:
15: 96032: loss: 0.2405193238:
15: 99232: loss: 0.2404132058:
15: 102432: loss: 0.2403276513:
15: 105632: loss: 0.2400051514:
15: 108832: loss: 0.2397700630:
15: 112032: loss: 0.2396475950:
15: 115232: loss: 0.2393164538:
15: 118432: loss: 0.2396311207:
15: 121632: loss: 0.2398176908:
15: 124832: loss: 0.2396592622:
15: 128032: loss: 0.2395275848:
15: 131232: loss: 0.2395131582:
15: 134432: loss: 0.2394838309:
15: 137632: loss: 0.2394786984:
15: 140832: loss: 0.2397596691:
15: 144032: loss: 0.2398077671:
15: 147232: loss: 0.2400289141:
15: 150432: loss: 0.2398707718:
15: 153632: loss: 0.2396275821:
15: 156832: loss: 0.2395184203:
15: 160032: loss: 0.2395489752:
15: 163232: loss: 0.2393002226:
15: 166432: loss: 0.2394128242:
Dev-Acc: 15: Accuracy: 0.9420840740: precision: 0.7244897959: recall: 0.0120727767: f1: 0.0237497909
Train-Acc: 15: Accuracy: 0.9114695787: precision: 0.8119122257: recall: 0.0340543028: f1: 0.0653669001
16: 3232: loss: 0.2439832942:
16: 6432: loss: 0.2380255203:
16: 9632: loss: 0.2399846190:
16: 12832: loss: 0.2407783911:
16: 16032: loss: 0.2396249901:
16: 19232: loss: 0.2403509659:
16: 22432: loss: 0.2391644002:
16: 25632: loss: 0.2386044606:
16: 28832: loss: 0.2381735441:
16: 32032: loss: 0.2397643483:
16: 35232: loss: 0.2397146917:
16: 38432: loss: 0.2383613102:
16: 41632: loss: 0.2393765446:
16: 44832: loss: 0.2398012840:
16: 48032: loss: 0.2378117776:
16: 51232: loss: 0.2372424568:
16: 54432: loss: 0.2367368089:
16: 57632: loss: 0.2360088493:
16: 60832: loss: 0.2354713436:
16: 64032: loss: 0.2361771140:
16: 67232: loss: 0.2358092482:
16: 70432: loss: 0.2355649612:
16: 73632: loss: 0.2351086042:
16: 76832: loss: 0.2347340670:
16: 80032: loss: 0.2344384299:
16: 83232: loss: 0.2346930943:
16: 86432: loss: 0.2349274567:
16: 89632: loss: 0.2349330485:
16: 92832: loss: 0.2349256479:
16: 96032: loss: 0.2347272226:
16: 99232: loss: 0.2349809170:
16: 102432: loss: 0.2348549171:
16: 105632: loss: 0.2342328523:
16: 108832: loss: 0.2340696529:
16: 112032: loss: 0.2338212397:
16: 115232: loss: 0.2338124014:
16: 118432: loss: 0.2340890427:
16: 121632: loss: 0.2339330563:
16: 124832: loss: 0.2339868171:
16: 128032: loss: 0.2340902044:
16: 131232: loss: 0.2338203990:
16: 134432: loss: 0.2337525639:
16: 137632: loss: 0.2338243414:
16: 140832: loss: 0.2339231344:
16: 144032: loss: 0.2341351411:
16: 147232: loss: 0.2341277038:
16: 150432: loss: 0.2340723245:
16: 153632: loss: 0.2340480309:
16: 156832: loss: 0.2340782188:
16: 160032: loss: 0.2345349392:
16: 163232: loss: 0.2347478699:
16: 166432: loss: 0.2347106238:
Dev-Acc: 16: Accuracy: 0.9424114823: precision: 0.6370106762: recall: 0.0304370005: f1: 0.0580980201
Train-Acc: 16: Accuracy: 0.9130653143: precision: 0.8474399164: recall: 0.0533166787: f1: 0.1003216230
17: 3232: loss: 0.2191870544:
17: 6432: loss: 0.2376218126:
17: 9632: loss: 0.2387371298:
17: 12832: loss: 0.2339951050:
17: 16032: loss: 0.2365784193:
17: 19232: loss: 0.2358725771:
17: 22432: loss: 0.2355050646:
17: 25632: loss: 0.2351662847:
17: 28832: loss: 0.2361340353:
17: 32032: loss: 0.2361682996:
17: 35232: loss: 0.2364448918:
17: 38432: loss: 0.2351892929:
17: 41632: loss: 0.2356938114:
17: 44832: loss: 0.2359654730:
17: 48032: loss: 0.2356467005:
17: 51232: loss: 0.2354437037:
17: 54432: loss: 0.2343779726:
17: 57632: loss: 0.2335314169:
17: 60832: loss: 0.2335675290:
17: 64032: loss: 0.2329628373:
17: 67232: loss: 0.2327338368:
17: 70432: loss: 0.2325779918:
17: 73632: loss: 0.2326781193:
17: 76832: loss: 0.2325234606:
17: 80032: loss: 0.2323669648:
17: 83232: loss: 0.2321992247:
17: 86432: loss: 0.2319286863:
17: 89632: loss: 0.2314389963:
17: 92832: loss: 0.2315489698:
17: 96032: loss: 0.2310140672:
17: 99232: loss: 0.2315254242:
17: 102432: loss: 0.2311775618:
17: 105632: loss: 0.2313196312:
17: 108832: loss: 0.2317486164:
17: 112032: loss: 0.2314702679:
17: 115232: loss: 0.2316773252:
17: 118432: loss: 0.2317437045:
17: 121632: loss: 0.2321175509:
17: 124832: loss: 0.2324369265:
17: 128032: loss: 0.2322898413:
17: 131232: loss: 0.2320970423:
17: 134432: loss: 0.2316399504:
17: 137632: loss: 0.2316539036:
17: 140832: loss: 0.2312789250:
17: 144032: loss: 0.2310060183:
17: 147232: loss: 0.2309584772:
17: 150432: loss: 0.2310108338:
17: 153632: loss: 0.2305982509:
17: 156832: loss: 0.2301741534:
17: 160032: loss: 0.2303659643:
17: 163232: loss: 0.2305308578:
17: 166432: loss: 0.2303457541:
Dev-Acc: 17: Accuracy: 0.9420642257: precision: 0.5483870968: recall: 0.0404693079: f1: 0.0753760887
Train-Acc: 17: Accuracy: 0.9140813351: precision: 0.8430566968: recall: 0.0674511866: f1: 0.1249086925
18: 3232: loss: 0.2258099861:
18: 6432: loss: 0.2306106512:
18: 9632: loss: 0.2281182733:
18: 12832: loss: 0.2282480170:
18: 16032: loss: 0.2276208784:
18: 19232: loss: 0.2286417049:
18: 22432: loss: 0.2285649451:
18: 25632: loss: 0.2296080617:
18: 28832: loss: 0.2296048142:
18: 32032: loss: 0.2286727941:
18: 35232: loss: 0.2285802223:
18: 38432: loss: 0.2288191209:
18: 41632: loss: 0.2281315272:
18: 44832: loss: 0.2287198056:
18: 48032: loss: 0.2278162319:
18: 51232: loss: 0.2270401977:
18: 54432: loss: 0.2273349840:
18: 57632: loss: 0.2282514289:
18: 60832: loss: 0.2272322306:
18: 64032: loss: 0.2266876218:
18: 67232: loss: 0.2267113471:
18: 70432: loss: 0.2267594176:
18: 73632: loss: 0.2261081148:
18: 76832: loss: 0.2259301474:
18: 80032: loss: 0.2256583835:
18: 83232: loss: 0.2258624599:
18: 86432: loss: 0.2253141860:
18: 89632: loss: 0.2257959667:
18: 92832: loss: 0.2265967727:
18: 96032: loss: 0.2266267577:
18: 99232: loss: 0.2269245316:
18: 102432: loss: 0.2270810654:
18: 105632: loss: 0.2270015640:
18: 108832: loss: 0.2266216349:
18: 112032: loss: 0.2267265803:
18: 115232: loss: 0.2264996351:
18: 118432: loss: 0.2264201716:
18: 121632: loss: 0.2261216084:
18: 124832: loss: 0.2261318430:
18: 128032: loss: 0.2258370553:
18: 131232: loss: 0.2259416109:
18: 134432: loss: 0.2261330378:
18: 137632: loss: 0.2257085836:
18: 140832: loss: 0.2258786693:
18: 144032: loss: 0.2257213794:
18: 147232: loss: 0.2255057702:
18: 150432: loss: 0.2255950608:
18: 153632: loss: 0.2259034080:
18: 156832: loss: 0.2261281274:
18: 160032: loss: 0.2261178533:
18: 163232: loss: 0.2262355205:
18: 166432: loss: 0.2263574427:
Dev-Acc: 18: Accuracy: 0.9430365562: precision: 0.5940860215: recall: 0.0751572862: f1: 0.1334339623
Train-Acc: 18: Accuracy: 0.9159938097: precision: 0.8419182948: recall: 0.0934849780: f1: 0.1682840237
19: 3232: loss: 0.2359708828:
19: 6432: loss: 0.2272717504:
19: 9632: loss: 0.2252728639:
19: 12832: loss: 0.2267961530:
19: 16032: loss: 0.2265969281:
19: 19232: loss: 0.2271206375:
19: 22432: loss: 0.2270360634:
19: 25632: loss: 0.2264066234:
19: 28832: loss: 0.2259351167:
19: 32032: loss: 0.2249052984:
19: 35232: loss: 0.2242357512:
19: 38432: loss: 0.2242570504:
19: 41632: loss: 0.2234991673:
19: 44832: loss: 0.2243124122:
19: 48032: loss: 0.2239515888:
19: 51232: loss: 0.2234029831:
19: 54432: loss: 0.2239269006:
19: 57632: loss: 0.2235130134:
19: 60832: loss: 0.2230250412:
19: 64032: loss: 0.2224247093:
19: 67232: loss: 0.2227020887:
19: 70432: loss: 0.2226830764:
19: 73632: loss: 0.2223436412:
19: 76832: loss: 0.2228622404:
19: 80032: loss: 0.2233733918:
19: 83232: loss: 0.2234792446:
19: 86432: loss: 0.2239748837:
19: 89632: loss: 0.2237028652:
19: 92832: loss: 0.2231415802:
19: 96032: loss: 0.2233116687:
19: 99232: loss: 0.2229106116:
19: 102432: loss: 0.2230213567:
19: 105632: loss: 0.2228178250:
19: 108832: loss: 0.2232028127:
19: 112032: loss: 0.2230438022:
19: 115232: loss: 0.2230983421:
19: 118432: loss: 0.2229379311:
19: 121632: loss: 0.2231567018:
19: 124832: loss: 0.2232471066:
19: 128032: loss: 0.2235390797:
19: 131232: loss: 0.2235751059:
19: 134432: loss: 0.2234001775:
19: 137632: loss: 0.2237484055:
19: 140832: loss: 0.2235588138:
19: 144032: loss: 0.2235525623:
19: 147232: loss: 0.2232708574:
19: 150432: loss: 0.2231619257:
19: 153632: loss: 0.2232259453:
19: 156832: loss: 0.2232671320:
19: 160032: loss: 0.2230071651:
19: 163232: loss: 0.2229916772:
19: 166432: loss: 0.2228496757:
Dev-Acc: 19: Accuracy: 0.9433243275: precision: 0.5780240074: recall: 0.1064444822: f1: 0.1797817346
Train-Acc: 19: Accuracy: 0.9180377722: precision: 0.8354997759: recall: 0.1225428966: f1: 0.2137369568
20: 3232: loss: 0.2275374868:
20: 6432: loss: 0.2294340112:
20: 9632: loss: 0.2267297416:
20: 12832: loss: 0.2260001178:
20: 16032: loss: 0.2230294378:
20: 19232: loss: 0.2194040106:
20: 22432: loss: 0.2171917862:
20: 25632: loss: 0.2187312088:
20: 28832: loss: 0.2201694972:
20: 32032: loss: 0.2197916856:
20: 35232: loss: 0.2199367821:
20: 38432: loss: 0.2196607141:
20: 41632: loss: 0.2195713293:
20: 44832: loss: 0.2199134015:
20: 48032: loss: 0.2202058289:
20: 51232: loss: 0.2197279308:
20: 54432: loss: 0.2199380207:
20: 57632: loss: 0.2202703481:
20: 60832: loss: 0.2206214844:
20: 64032: loss: 0.2200395735:
20: 67232: loss: 0.2194654944:
20: 70432: loss: 0.2195779532:
20: 73632: loss: 0.2194722871:
20: 76832: loss: 0.2198148207:
20: 80032: loss: 0.2196341636:
20: 83232: loss: 0.2194322577:
20: 86432: loss: 0.2189354634:
20: 89632: loss: 0.2188087385:
20: 92832: loss: 0.2187726338:
20: 96032: loss: 0.2188036366:
20: 99232: loss: 0.2189957719:
20: 102432: loss: 0.2190143459:
20: 105632: loss: 0.2189548026:
20: 108832: loss: 0.2192633989:
20: 112032: loss: 0.2194550916:
20: 115232: loss: 0.2193906112:
20: 118432: loss: 0.2195881616:
20: 121632: loss: 0.2196091309:
20: 124832: loss: 0.2196460001:
20: 128032: loss: 0.2195015018:
20: 131232: loss: 0.2193059820:
20: 134432: loss: 0.2191862478:
20: 137632: loss: 0.2190677654:
20: 140832: loss: 0.2191984392:
20: 144032: loss: 0.2194483926:
20: 147232: loss: 0.2194915103:
20: 150432: loss: 0.2199071323:
20: 153632: loss: 0.2195499076:
20: 156832: loss: 0.2198533285:
20: 160032: loss: 0.2194473807:
20: 163232: loss: 0.2193975151:
20: 166432: loss: 0.2197331893:
Dev-Acc: 20: Accuracy: 0.9434037209: precision: 0.5666917860: recall: 0.1278694100: f1: 0.2086570477
Train-Acc: 20: Accuracy: 0.9204881787: precision: 0.8192166053: recall: 0.1608704227: f1: 0.2689306517
21: 3232: loss: 0.2094817438:
21: 6432: loss: 0.2196429650:
21: 9632: loss: 0.2200949730:
21: 12832: loss: 0.2183120313:
21: 16032: loss: 0.2202219672:
21: 19232: loss: 0.2207080559:
21: 22432: loss: 0.2186594927:
21: 25632: loss: 0.2185578999:
21: 28832: loss: 0.2176393083:
21: 32032: loss: 0.2164372191:
21: 35232: loss: 0.2158131395:
21: 38432: loss: 0.2158096106:
21: 41632: loss: 0.2159254421:
21: 44832: loss: 0.2168335405:
21: 48032: loss: 0.2157554848:
21: 51232: loss: 0.2153377646:
21: 54432: loss: 0.2148443832:
21: 57632: loss: 0.2145959033:
21: 60832: loss: 0.2147073962:
21: 64032: loss: 0.2145803282:
21: 67232: loss: 0.2150535257:
21: 70432: loss: 0.2151523058:
21: 73632: loss: 0.2164547887:
21: 76832: loss: 0.2169076346:
21: 80032: loss: 0.2164277668:
21: 83232: loss: 0.2163283563:
21: 86432: loss: 0.2161451616:
21: 89632: loss: 0.2168337059:
21: 92832: loss: 0.2162861568:
21: 96032: loss: 0.2157999379:
21: 99232: loss: 0.2160838330:
21: 102432: loss: 0.2158484124:
21: 105632: loss: 0.2158890491:
21: 108832: loss: 0.2162459165:
21: 112032: loss: 0.2160942392:
21: 115232: loss: 0.2164028047:
21: 118432: loss: 0.2159245396:
21: 121632: loss: 0.2159192350:
21: 124832: loss: 0.2161965852:
21: 128032: loss: 0.2160559633:
21: 131232: loss: 0.2161475800:
21: 134432: loss: 0.2163691795:
21: 137632: loss: 0.2164619378:
21: 140832: loss: 0.2161205677:
21: 144032: loss: 0.2164024724:
21: 147232: loss: 0.2164236618:
21: 150432: loss: 0.2164160580:
21: 153632: loss: 0.2166745350:
21: 156832: loss: 0.2167934120:
21: 160032: loss: 0.2167001512:
21: 163232: loss: 0.2166729518:
21: 166432: loss: 0.2167095641:
Dev-Acc: 21: Accuracy: 0.9438303709: precision: 0.5727513228: recall: 0.1472538684: f1: 0.2342756662
Train-Acc: 21: Accuracy: 0.9216476083: precision: 0.7902735562: recall: 0.1880218263: f1: 0.3037705789
22: 3232: loss: 0.2153499872:
22: 6432: loss: 0.2203971094:
22: 9632: loss: 0.2180102751:
22: 12832: loss: 0.2160810396:
22: 16032: loss: 0.2180730655:
22: 19232: loss: 0.2166865695:
22: 22432: loss: 0.2181428414:
22: 25632: loss: 0.2170171352:
22: 28832: loss: 0.2170149006:
22: 32032: loss: 0.2168232663:
22: 35232: loss: 0.2177204287:
22: 38432: loss: 0.2174279680:
22: 41632: loss: 0.2167902645:
22: 44832: loss: 0.2176861532:
22: 48032: loss: 0.2178746187:
22: 51232: loss: 0.2177758037:
22: 54432: loss: 0.2171524376:
22: 57632: loss: 0.2166481110:
22: 60832: loss: 0.2162863489:
22: 64032: loss: 0.2165452578:
22: 67232: loss: 0.2155035993:
22: 70432: loss: 0.2167878368:
22: 73632: loss: 0.2160227268:
22: 76832: loss: 0.2162985480:
22: 80032: loss: 0.2159602141:
22: 83232: loss: 0.2155631896:
22: 86432: loss: 0.2150159261:
22: 89632: loss: 0.2150952580:
22: 92832: loss: 0.2145104807:
22: 96032: loss: 0.2142608504:
22: 99232: loss: 0.2141600922:
22: 102432: loss: 0.2136888534:
22: 105632: loss: 0.2142779182:
22: 108832: loss: 0.2141757523:
22: 112032: loss: 0.2141372564:
22: 115232: loss: 0.2139234986:
22: 118432: loss: 0.2140341429:
22: 121632: loss: 0.2140374115:
22: 124832: loss: 0.2136211554:
22: 128032: loss: 0.2140038896:
22: 131232: loss: 0.2141293141:
22: 134432: loss: 0.2143145949:
22: 137632: loss: 0.2139957652:
22: 140832: loss: 0.2138794118:
22: 144032: loss: 0.2138397336:
22: 147232: loss: 0.2140448892:
22: 150432: loss: 0.2142224440:
22: 153632: loss: 0.2140204977:
22: 156832: loss: 0.2139266638:
22: 160032: loss: 0.2138219508:
22: 163232: loss: 0.2138255720:
22: 166432: loss: 0.2136836084:
Dev-Acc: 22: Accuracy: 0.9438105226: precision: 0.5613048369: recall: 0.1696990308: f1: 0.2606084345
Train-Acc: 22: Accuracy: 0.9220719337: precision: 0.7586946165: recall: 0.2093879429: f1: 0.3281982585
23: 3232: loss: 0.2217310765:
23: 6432: loss: 0.2145039892:
23: 9632: loss: 0.2104495656:
23: 12832: loss: 0.2105355815:
23: 16032: loss: 0.2090703619:
23: 19232: loss: 0.2082958807:
23: 22432: loss: 0.2103302246:
23: 25632: loss: 0.2104434208:
23: 28832: loss: 0.2114429034:
23: 32032: loss: 0.2119295902:
23: 35232: loss: 0.2100912682:
23: 38432: loss: 0.2100988928:
23: 41632: loss: 0.2100257443:
23: 44832: loss: 0.2094908289:
23: 48032: loss: 0.2089199305:
23: 51232: loss: 0.2098767819:
23: 54432: loss: 0.2099906045:
23: 57632: loss: 0.2100525597:
23: 60832: loss: 0.2099772700:
23: 64032: loss: 0.2095681670:
23: 67232: loss: 0.2092673765:
23: 70432: loss: 0.2091242043:
23: 73632: loss: 0.2096828301:
23: 76832: loss: 0.2097794232:
23: 80032: loss: 0.2096161575:
23: 83232: loss: 0.2104363761:
23: 86432: loss: 0.2108615679:
23: 89632: loss: 0.2108313596:
23: 92832: loss: 0.2104925653:
23: 96032: loss: 0.2103867917:
23: 99232: loss: 0.2106074071:
23: 102432: loss: 0.2108562038:
23: 105632: loss: 0.2106032956:
23: 108832: loss: 0.2110184364:
23: 112032: loss: 0.2108408124:
23: 115232: loss: 0.2105889562:
23: 118432: loss: 0.2105741581:
23: 121632: loss: 0.2107204253:
23: 124832: loss: 0.2105963709:
23: 128032: loss: 0.2108571989:
23: 131232: loss: 0.2110836529:
23: 134432: loss: 0.2109124666:
23: 137632: loss: 0.2106984559:
23: 140832: loss: 0.2107692778:
23: 144032: loss: 0.2109660292:
23: 147232: loss: 0.2107666365:
23: 150432: loss: 0.2107312611:
23: 153632: loss: 0.2109143858:
23: 156832: loss: 0.2108208246:
23: 160032: loss: 0.2109366632:
23: 163232: loss: 0.2110429622:
23: 166432: loss: 0.2112279523:
Dev-Acc: 23: Accuracy: 0.9428579807: precision: 0.5253955037: recall: 0.2145893556: f1: 0.3047205119
Train-Acc: 23: Accuracy: 0.9233688712: precision: 0.7548538511: recall: 0.2325948327: f1: 0.3556136295
24: 3232: loss: 0.2151787019:
24: 6432: loss: 0.2135976977:
24: 9632: loss: 0.2179185008:
24: 12832: loss: 0.2213236566:
24: 16032: loss: 0.2187078873:
24: 19232: loss: 0.2154376789:
24: 22432: loss: 0.2149735285:
24: 25632: loss: 0.2151766623:
24: 28832: loss: 0.2133815207:
24: 32032: loss: 0.2142858208:
24: 35232: loss: 0.2146757006:
24: 38432: loss: 0.2143880055:
24: 41632: loss: 0.2134944924:
24: 44832: loss: 0.2135248885:
24: 48032: loss: 0.2133817991:
24: 51232: loss: 0.2135407174:
24: 54432: loss: 0.2133128497:
24: 57632: loss: 0.2120214132:
24: 60832: loss: 0.2114630861:
24: 64032: loss: 0.2120344642:
24: 67232: loss: 0.2121612116:
24: 70432: loss: 0.2115828234:
24: 73632: loss: 0.2120208796:
24: 76832: loss: 0.2110324413:
24: 80032: loss: 0.2110296806:
24: 83232: loss: 0.2108470564:
24: 86432: loss: 0.2107162215:
24: 89632: loss: 0.2110101932:
24: 92832: loss: 0.2107362281:
24: 96032: loss: 0.2110476987:
24: 99232: loss: 0.2112689637:
24: 102432: loss: 0.2110532700:
24: 105632: loss: 0.2110158264:
24: 108832: loss: 0.2105991496:
24: 112032: loss: 0.2104557033:
24: 115232: loss: 0.2100896090:
24: 118432: loss: 0.2102444507:
24: 121632: loss: 0.2100951192:
24: 124832: loss: 0.2098954018:
24: 128032: loss: 0.2096482798:
24: 131232: loss: 0.2094268714:
24: 134432: loss: 0.2092253477:
24: 137632: loss: 0.2094019905:
24: 140832: loss: 0.2092805124:
24: 144032: loss: 0.2093810667:
24: 147232: loss: 0.2092408084:
24: 150432: loss: 0.2093116133:
24: 153632: loss: 0.2090930120:
24: 156832: loss: 0.2091557117:
24: 160032: loss: 0.2089675625:
24: 163232: loss: 0.2088744920:
24: 166432: loss: 0.2087449294:
Dev-Acc: 24: Accuracy: 0.9415680766: precision: 0.4986486486: recall: 0.2509777249: f1: 0.3338988802
Train-Acc: 24: Accuracy: 0.9248629808: precision: 0.7584720862: recall: 0.2545526264: f1: 0.3811773971
25: 3232: loss: 0.1897531580:
25: 6432: loss: 0.2033635052:
25: 9632: loss: 0.2061123737:
25: 12832: loss: 0.2040123827:
25: 16032: loss: 0.2023375278:
25: 19232: loss: 0.2055222079:
25: 22432: loss: 0.2049081623:
25: 25632: loss: 0.2041540743:
25: 28832: loss: 0.2049876028:
25: 32032: loss: 0.2058366968:
25: 35232: loss: 0.2047123107:
25: 38432: loss: 0.2047072433:
25: 41632: loss: 0.2043028190:
25: 44832: loss: 0.2049815879:
25: 48032: loss: 0.2045036967:
25: 51232: loss: 0.2049205588:
25: 54432: loss: 0.2047086900:
25: 57632: loss: 0.2049312907:
25: 60832: loss: 0.2053346743:
25: 64032: loss: 0.2052041158:
25: 67232: loss: 0.2049064909:
25: 70432: loss: 0.2050654702:
25: 73632: loss: 0.2051487453:
25: 76832: loss: 0.2055743064:
25: 80032: loss: 0.2055206627:
25: 83232: loss: 0.2056814892:
25: 86432: loss: 0.2057628151:
25: 89632: loss: 0.2053397148:
25: 92832: loss: 0.2055958299:
25: 96032: loss: 0.2060640506:
25: 99232: loss: 0.2064645182:
25: 102432: loss: 0.2061581614:
25: 105632: loss: 0.2058578137:
25: 108832: loss: 0.2057344092:
25: 112032: loss: 0.2059118888:
25: 115232: loss: 0.2062020359:
25: 118432: loss: 0.2062271612:
25: 121632: loss: 0.2064583743:
25: 124832: loss: 0.2065199672:
25: 128032: loss: 0.2065768666:
25: 131232: loss: 0.2066546164:
25: 134432: loss: 0.2066672869:
25: 137632: loss: 0.2069421225:
25: 140832: loss: 0.2066930244:
25: 144032: loss: 0.2067848520:
25: 147232: loss: 0.2067384379:
25: 150432: loss: 0.2068130032:
25: 153632: loss: 0.2066991163:
25: 156832: loss: 0.2066531765:
25: 160032: loss: 0.2066535796:
25: 163232: loss: 0.2068248812:
25: 166432: loss: 0.2064494341:
Dev-Acc: 25: Accuracy: 0.9413596988: precision: 0.4956153614: recall: 0.2786940996: f1: 0.3567696996
Train-Acc: 25: Accuracy: 0.9260881543: precision: 0.7615894040: recall: 0.2721714549: f1: 0.4010267836
26: 3232: loss: 0.2035958109:
26: 6432: loss: 0.1944961282:
26: 9632: loss: 0.1971359820:
26: 12832: loss: 0.1971356729:
26: 16032: loss: 0.1988489659:
26: 19232: loss: 0.1993122492:
26: 22432: loss: 0.2014670253:
26: 25632: loss: 0.2017305145:
26: 28832: loss: 0.2031053779:
26: 32032: loss: 0.2033481294:
26: 35232: loss: 0.2038268686:
26: 38432: loss: 0.2032346019:
26: 41632: loss: 0.2022810052:
26: 44832: loss: 0.2027132064:
26: 48032: loss: 0.2029499435:
26: 51232: loss: 0.2029101232:
26: 54432: loss: 0.2028984133:
26: 57632: loss: 0.2031241044:
26: 60832: loss: 0.2032108396:
26: 64032: loss: 0.2034572351:
26: 67232: loss: 0.2039958941:
26: 70432: loss: 0.2035648658:
26: 73632: loss: 0.2033683367:
26: 76832: loss: 0.2035213382:
26: 80032: loss: 0.2034010918:
26: 83232: loss: 0.2031670862:
26: 86432: loss: 0.2031259492:
26: 89632: loss: 0.2035325268:
26: 92832: loss: 0.2034599007:
26: 96032: loss: 0.2034688793:
26: 99232: loss: 0.2034771417:
26: 102432: loss: 0.2034260262:
26: 105632: loss: 0.2038974647:
26: 108832: loss: 0.2038467020:
26: 112032: loss: 0.2041162198:
26: 115232: loss: 0.2039491034:
26: 118432: loss: 0.2035399812:
26: 121632: loss: 0.2038446111:
26: 124832: loss: 0.2037621818:
26: 128032: loss: 0.2039817926:
26: 131232: loss: 0.2040875467:
26: 134432: loss: 0.2044108793:
26: 137632: loss: 0.2046154009:
26: 140832: loss: 0.2044207928:
26: 144032: loss: 0.2043011647:
26: 147232: loss: 0.2041715826:
26: 150432: loss: 0.2043101503:
26: 153632: loss: 0.2042707655:
26: 156832: loss: 0.2044960375:
26: 160032: loss: 0.2042568729:
26: 163232: loss: 0.2044225376:
26: 166432: loss: 0.2042644796:
Dev-Acc: 26: Accuracy: 0.9408338666: precision: 0.4883720930: recall: 0.2928073457: f1: 0.3661103434
Train-Acc: 26: Accuracy: 0.9269069433: precision: 0.7608048994: recall: 0.2858457695: f1: 0.4155595909
27: 3232: loss: 0.1979758213:
27: 6432: loss: 0.2016723461:
27: 9632: loss: 0.2077624361:
27: 12832: loss: 0.2046947982:
27: 16032: loss: 0.2060592776:
27: 19232: loss: 0.2056113843:
27: 22432: loss: 0.2050098783:
27: 25632: loss: 0.2053740001:
27: 28832: loss: 0.2056300714:
27: 32032: loss: 0.2053464792:
27: 35232: loss: 0.2053456834:
27: 38432: loss: 0.2053153181:
27: 41632: loss: 0.2052287452:
27: 44832: loss: 0.2059664371:
27: 48032: loss: 0.2051447250:
27: 51232: loss: 0.2040227002:
27: 54432: loss: 0.2040895453:
27: 57632: loss: 0.2030108012:
27: 60832: loss: 0.2030111621:
27: 64032: loss: 0.2031137425:
27: 67232: loss: 0.2030373330:
27: 70432: loss: 0.2030930625:
27: 73632: loss: 0.2022945870:
27: 76832: loss: 0.2028596394:
27: 80032: loss: 0.2030655115:
27: 83232: loss: 0.2030531628:
27: 86432: loss: 0.2033120180:
27: 89632: loss: 0.2031511633:
27: 92832: loss: 0.2026586573:
27: 96032: loss: 0.2024229990:
27: 99232: loss: 0.2024260433:
27: 102432: loss: 0.2022505402:
27: 105632: loss: 0.2018892213:
27: 108832: loss: 0.2021127807:
27: 112032: loss: 0.2019873165:
27: 115232: loss: 0.2017483884:
27: 118432: loss: 0.2017635602:
27: 121632: loss: 0.2017296709:
27: 124832: loss: 0.2016831390:
27: 128032: loss: 0.2017689586:
27: 131232: loss: 0.2015611103:
27: 134432: loss: 0.2016217358:
27: 137632: loss: 0.2019770454:
27: 140832: loss: 0.2021627360:
27: 144032: loss: 0.2024758217:
27: 147232: loss: 0.2024999115:
27: 150432: loss: 0.2024068387:
27: 153632: loss: 0.2021673291:
27: 156832: loss: 0.2022542087:
27: 160032: loss: 0.2023757387:
27: 163232: loss: 0.2025662242:
27: 166432: loss: 0.2026014907:
Dev-Acc: 27: Accuracy: 0.9402583838: precision: 0.4809679173: recall: 0.3007991838: f1: 0.3701223977
Train-Acc: 27: Accuracy: 0.9274747372: precision: 0.7591843613: recall: 0.2961672474: f1: 0.4261054623
28: 3232: loss: 0.2086992707:
28: 6432: loss: 0.2030380228:
28: 9632: loss: 0.2010441359:
28: 12832: loss: 0.1978125819:
28: 16032: loss: 0.1989240065:
28: 19232: loss: 0.1978969799:
28: 22432: loss: 0.2005251242:
28: 25632: loss: 0.2015751426:
28: 28832: loss: 0.2009141207:
28: 32032: loss: 0.2007795774:
28: 35232: loss: 0.2008059365:
28: 38432: loss: 0.2011369250:
28: 41632: loss: 0.2016764312:
28: 44832: loss: 0.2026831238:
28: 48032: loss: 0.2027844676:
28: 51232: loss: 0.2026723030:
28: 54432: loss: 0.2017704141:
28: 57632: loss: 0.2022614013:
28: 60832: loss: 0.2021045396:
28: 64032: loss: 0.2019585730:
28: 67232: loss: 0.2017406616:
28: 70432: loss: 0.2021637823:
28: 73632: loss: 0.2020710045:
28: 76832: loss: 0.2019241780:
28: 80032: loss: 0.2024385818:
28: 83232: loss: 0.2025699643:
28: 86432: loss: 0.2021393863:
28: 89632: loss: 0.2025942986:
28: 92832: loss: 0.2024991811:
28: 96032: loss: 0.2021572233:
28: 99232: loss: 0.2023082157:
28: 102432: loss: 0.2024140690:
28: 105632: loss: 0.2023807891:
28: 108832: loss: 0.2019996342:
28: 112032: loss: 0.2017787457:
28: 115232: loss: 0.2016739593:
28: 118432: loss: 0.2017417246:
28: 121632: loss: 0.2018174242:
28: 124832: loss: 0.2020648017:
28: 128032: loss: 0.2021974330:
28: 131232: loss: 0.2018946655:
28: 134432: loss: 0.2016354403:
28: 137632: loss: 0.2016286228:
28: 140832: loss: 0.2014214850:
28: 144032: loss: 0.2012705002:
28: 147232: loss: 0.2012914948:
28: 150432: loss: 0.2014487536:
28: 153632: loss: 0.2009444700:
28: 156832: loss: 0.2011953354:
28: 160032: loss: 0.2010216062:
28: 163232: loss: 0.2007928981:
28: 166432: loss: 0.2004937470:
Dev-Acc: 28: Accuracy: 0.9400301576: precision: 0.4785469860: recall: 0.3091311002: f1: 0.3756198347
Train-Acc: 28: Accuracy: 0.9279468656: precision: 0.7558799676: recall: 0.3063572415: f1: 0.4360029940
29: 3232: loss: 0.1782621109:
29: 6432: loss: 0.1902440626:
29: 9632: loss: 0.1909780293:
29: 12832: loss: 0.1917045829:
29: 16032: loss: 0.1935914270:
29: 19232: loss: 0.1922807385:
29: 22432: loss: 0.1912782762:
29: 25632: loss: 0.1915406381:
29: 28832: loss: 0.1940099000:
29: 32032: loss: 0.1940611044:
29: 35232: loss: 0.1954800796:
29: 38432: loss: 0.1957082002:
29: 41632: loss: 0.1962605722:
29: 44832: loss: 0.1957844763:
29: 48032: loss: 0.1957626547:
29: 51232: loss: 0.1965331537:
29: 54432: loss: 0.1965240242:
29: 57632: loss: 0.1976345881:
29: 60832: loss: 0.1976627650:
29: 64032: loss: 0.1979170517:
29: 67232: loss: 0.1979850179:
29: 70432: loss: 0.1977577178:
29: 73632: loss: 0.1978220967:
29: 76832: loss: 0.1979512823:
29: 80032: loss: 0.1981891341:
29: 83232: loss: 0.1978597542:
29: 86432: loss: 0.1978634826:
29: 89632: loss: 0.1982346786:
29: 92832: loss: 0.1981944278:
29: 96032: loss: 0.1983956578:
29: 99232: loss: 0.1985298547:
29: 102432: loss: 0.1987153085:
29: 105632: loss: 0.1985314565:
29: 108832: loss: 0.1987869266:
29: 112032: loss: 0.1983404152:
29: 115232: loss: 0.1982356222:
29: 118432: loss: 0.1980867526:
29: 121632: loss: 0.1981245926:
29: 124832: loss: 0.1982250963:
29: 128032: loss: 0.1981591722:
29: 131232: loss: 0.1983735923:
29: 134432: loss: 0.1983841122:
29: 137632: loss: 0.1981703884:
29: 140832: loss: 0.1980309807:
29: 144032: loss: 0.1982740676:
29: 147232: loss: 0.1981432296:
29: 150432: loss: 0.1981605539:
29: 153632: loss: 0.1982344872:
29: 156832: loss: 0.1982220688:
29: 160032: loss: 0.1983261550:
29: 163232: loss: 0.1983591060:
29: 166432: loss: 0.1983297082:
Dev-Acc: 29: Accuracy: 0.9397423863: precision: 0.4753593429: recall: 0.3149124299: f1: 0.3788483175
Train-Acc: 29: Accuracy: 0.9283174276: precision: 0.7524721394: recall: 0.3151666557: f1: 0.4442591048
30: 3232: loss: 0.2021463171:
30: 6432: loss: 0.1922608675:
30: 9632: loss: 0.1961902786:
30: 12832: loss: 0.1926981171:
30: 16032: loss: 0.1939649870:
30: 19232: loss: 0.1945685731:
30: 22432: loss: 0.1938241303:
30: 25632: loss: 0.1935718755:
30: 28832: loss: 0.1943257654:
30: 32032: loss: 0.1942869188:
30: 35232: loss: 0.1950967372:
30: 38432: loss: 0.1945593978:
30: 41632: loss: 0.1955281510:
30: 44832: loss: 0.1957318888:
30: 48032: loss: 0.1959189790:
30: 51232: loss: 0.1962656624:
30: 54432: loss: 0.1963909144:
30: 57632: loss: 0.1961050836:
30: 60832: loss: 0.1957212428:
30: 64032: loss: 0.1960439318:
30: 67232: loss: 0.1955577624:
30: 70432: loss: 0.1955991550:
30: 73632: loss: 0.1953535632:
30: 76832: loss: 0.1950422865:
30: 80032: loss: 0.1954357756:
30: 83232: loss: 0.1949558530:
30: 86432: loss: 0.1952422529:
30: 89632: loss: 0.1950365678:
30: 92832: loss: 0.1950296624:
30: 96032: loss: 0.1953016749:
30: 99232: loss: 0.1955339563:
30: 102432: loss: 0.1959042488:
30: 105632: loss: 0.1957654579:
30: 108832: loss: 0.1956384760:
30: 112032: loss: 0.1960077050:
30: 115232: loss: 0.1962517696:
30: 118432: loss: 0.1967383516:
30: 121632: loss: 0.1970259293:
30: 124832: loss: 0.1966800750:
30: 128032: loss: 0.1963084821:
30: 131232: loss: 0.1969172216:
30: 134432: loss: 0.1970722731:
30: 137632: loss: 0.1969840406:
30: 140832: loss: 0.1968043339:
30: 144032: loss: 0.1969000533:
30: 147232: loss: 0.1971726379:
30: 150432: loss: 0.1970533197:
30: 153632: loss: 0.1968747128:
30: 156832: loss: 0.1964379882:
30: 160032: loss: 0.1963338678:
30: 163232: loss: 0.1966010453:
30: 166432: loss: 0.1969017070:
Dev-Acc: 30: Accuracy: 0.9394149780: precision: 0.4719101124: recall: 0.3213739160: f1: 0.3823588914
Train-Acc: 30: Accuracy: 0.9289270639: precision: 0.7517827340: recall: 0.3257511012: f1: 0.4545454545
31: 3232: loss: 0.2110090123:
31: 6432: loss: 0.2151779409:
31: 9632: loss: 0.2105393842:
31: 12832: loss: 0.2040881314:
31: 16032: loss: 0.2044823546:
31: 19232: loss: 0.2004488562:
31: 22432: loss: 0.1997329650:
31: 25632: loss: 0.1988444220:
31: 28832: loss: 0.1983824581:
31: 32032: loss: 0.1975310080:
31: 35232: loss: 0.1977996774:
31: 38432: loss: 0.1971202360:
31: 41632: loss: 0.1974284115:
31: 44832: loss: 0.1983060948:
31: 48032: loss: 0.1971155431:
31: 51232: loss: 0.1971154742:
31: 54432: loss: 0.1968482465:
31: 57632: loss: 0.1965042650:
31: 60832: loss: 0.1965381058:
31: 64032: loss: 0.1965841392:
31: 67232: loss: 0.1968028898:
31: 70432: loss: 0.1971051646:
31: 73632: loss: 0.1970687586:
31: 76832: loss: 0.1966628606:
31: 80032: loss: 0.1965115156:
31: 83232: loss: 0.1965726282:
31: 86432: loss: 0.1966424866:
31: 89632: loss: 0.1964804444:
31: 92832: loss: 0.1962279389:
31: 96032: loss: 0.1958978846:
31: 99232: loss: 0.1954305634:
31: 102432: loss: 0.1953413541:
31: 105632: loss: 0.1949359427:
31: 108832: loss: 0.1947658174:
31: 112032: loss: 0.1949540196:
31: 115232: loss: 0.1949682992:
31: 118432: loss: 0.1949692761:
31: 121632: loss: 0.1948935383:
31: 124832: loss: 0.1948560988:
31: 128032: loss: 0.1950241353:
31: 131232: loss: 0.1952721118:
31: 134432: loss: 0.1954099638:
31: 137632: loss: 0.1950477818:
31: 140832: loss: 0.1950163640:
31: 144032: loss: 0.1949684423:
31: 147232: loss: 0.1951517258:
31: 150432: loss: 0.1951410841:
31: 153632: loss: 0.1952930253:
31: 156832: loss: 0.1950296691:
31: 160032: loss: 0.1950028129:
31: 163232: loss: 0.1951075187:
31: 166432: loss: 0.1952452799:
Dev-Acc: 31: Accuracy: 0.9392561913: precision: 0.4704439539: recall: 0.3261350111: f1: 0.3852179152
Train-Acc: 31: Accuracy: 0.9294529557: precision: 0.7504778709: recall: 0.3355466439: f1: 0.4637470471
32: 3232: loss: 0.2064570319:
32: 6432: loss: 0.1978384327:
32: 9632: loss: 0.2027864466:
32: 12832: loss: 0.2019404173:
32: 16032: loss: 0.2002732413:
32: 19232: loss: 0.1989788628:
32: 22432: loss: 0.1974039620:
32: 25632: loss: 0.1975915423:
32: 28832: loss: 0.1974805054:
32: 32032: loss: 0.1981177926:
32: 35232: loss: 0.1982800477:
32: 38432: loss: 0.1981936446:
32: 41632: loss: 0.1987899864:
32: 44832: loss: 0.1988015082:
32: 48032: loss: 0.1979874822:
32: 51232: loss: 0.1981511324:
32: 54432: loss: 0.1983888069:
32: 57632: loss: 0.1976325309:
32: 60832: loss: 0.1976644345:
32: 64032: loss: 0.1971944104:
32: 67232: loss: 0.1971368719:
32: 70432: loss: 0.1968662493:
32: 73632: loss: 0.1956541179:
32: 76832: loss: 0.1951943527:
32: 80032: loss: 0.1957993138:
32: 83232: loss: 0.1952387559:
32: 86432: loss: 0.1954239285:
32: 89632: loss: 0.1954129691:
32: 92832: loss: 0.1956582271:
32: 96032: loss: 0.1948965787:
32: 99232: loss: 0.1948523145:
32: 102432: loss: 0.1950264641:
32: 105632: loss: 0.1947439731:
32: 108832: loss: 0.1946308378:
32: 112032: loss: 0.1945042071:
32: 115232: loss: 0.1946415132:
32: 118432: loss: 0.1945452012:
32: 121632: loss: 0.1944824366:
32: 124832: loss: 0.1946762433:
32: 128032: loss: 0.1949701634:
32: 131232: loss: 0.1952048287:
32: 134432: loss: 0.1946328360:
32: 137632: loss: 0.1945763424:
32: 140832: loss: 0.1943702113:
32: 144032: loss: 0.1945607895:
32: 147232: loss: 0.1944731892:
32: 150432: loss: 0.1942927785:
32: 153632: loss: 0.1941566404:
32: 156832: loss: 0.1941280872:
32: 160032: loss: 0.1942390380:
32: 163232: loss: 0.1941483365:
32: 166432: loss: 0.1938809337:
Dev-Acc: 32: Accuracy: 0.9389783740: precision: 0.4675825500: recall: 0.3298758715: f1: 0.3868394816
Train-Acc: 32: Accuracy: 0.9297996163: precision: 0.7484583393: recall: 0.3431069621: f1: 0.4705192932
33: 3232: loss: 0.1892595658:
33: 6432: loss: 0.1906645913:
33: 9632: loss: 0.1926935890:
33: 12832: loss: 0.1903791708:
33: 16032: loss: 0.1904879648:
33: 19232: loss: 0.1902455411:
33: 22432: loss: 0.1918469385:
33: 25632: loss: 0.1918788570:
33: 28832: loss: 0.1909668336:
33: 32032: loss: 0.1920878541:
33: 35232: loss: 0.1918907855:
33: 38432: loss: 0.1918416213:
33: 41632: loss: 0.1918496116:
33: 44832: loss: 0.1916609173:
33: 48032: loss: 0.1914146612:
33: 51232: loss: 0.1924234098:
33: 54432: loss: 0.1926350999:
33: 57632: loss: 0.1922786166:
33: 60832: loss: 0.1914984863:
33: 64032: loss: 0.1913617461:
33: 67232: loss: 0.1918164812:
33: 70432: loss: 0.1915230770:
33: 73632: loss: 0.1911492722:
33: 76832: loss: 0.1905184904:
33: 80032: loss: 0.1903320660:
33: 83232: loss: 0.1916680621:
33: 86432: loss: 0.1918305938:
33: 89632: loss: 0.1915035215:
33: 92832: loss: 0.1908795946:
33: 96032: loss: 0.1909130794:
33: 99232: loss: 0.1907810356:
33: 102432: loss: 0.1910504822:
33: 105632: loss: 0.1914501565:
33: 108832: loss: 0.1916058296:
33: 112032: loss: 0.1920143830:
33: 115232: loss: 0.1922695222:
33: 118432: loss: 0.1916492274:
33: 121632: loss: 0.1920945409:
33: 124832: loss: 0.1918956349:
33: 128032: loss: 0.1920509268:
33: 131232: loss: 0.1922331873:
33: 134432: loss: 0.1923981172:
33: 137632: loss: 0.1925008173:
33: 140832: loss: 0.1927462413:
33: 144032: loss: 0.1927277922:
33: 147232: loss: 0.1924918854:
33: 150432: loss: 0.1924566609:
33: 153632: loss: 0.1925298488:
33: 156832: loss: 0.1926136756:
33: 160032: loss: 0.1927805880:
33: 163232: loss: 0.1925874806:
33: 166432: loss: 0.1926381609:
Dev-Acc: 33: Accuracy: 0.9386807084: precision: 0.4649636747: recall: 0.3373575922: f1: 0.3910130075
Train-Acc: 33: Accuracy: 0.9300386906: precision: 0.7460339744: recall: 0.3493524423: f1: 0.4758663920
34: 3232: loss: 0.1949484739:
34: 6432: loss: 0.1944626640:
34: 9632: loss: 0.1940112717:
34: 12832: loss: 0.1891501292:
34: 16032: loss: 0.1886415091:
34: 19232: loss: 0.1879789580:
34: 22432: loss: 0.1878865310:
34: 25632: loss: 0.1883096626:
34: 28832: loss: 0.1873170013:
34: 32032: loss: 0.1866669761:
34: 35232: loss: 0.1883681740:
34: 38432: loss: 0.1885852231:
34: 41632: loss: 0.1866825103:
34: 44832: loss: 0.1871165448:
34: 48032: loss: 0.1871360748:
34: 51232: loss: 0.1877444289:
34: 54432: loss: 0.1875659079:
34: 57632: loss: 0.1876273785:
34: 60832: loss: 0.1883482895:
34: 64032: loss: 0.1885990628:
34: 67232: loss: 0.1895487695:
34: 70432: loss: 0.1897241210:
34: 73632: loss: 0.1898260131:
34: 76832: loss: 0.1900049843:
34: 80032: loss: 0.1907517474:
34: 83232: loss: 0.1905612696:
34: 86432: loss: 0.1906808500:
34: 89632: loss: 0.1908262623:
34: 92832: loss: 0.1910819197:
34: 96032: loss: 0.1917343680:
34: 99232: loss: 0.1915535996:
34: 102432: loss: 0.1911067219:
34: 105632: loss: 0.1909640732:
34: 108832: loss: 0.1909517497:
34: 112032: loss: 0.1907134838:
34: 115232: loss: 0.1909830609:
34: 118432: loss: 0.1911354049:
34: 121632: loss: 0.1911206961:
34: 124832: loss: 0.1911623576:
34: 128032: loss: 0.1915827059:
34: 131232: loss: 0.1914745207:
34: 134432: loss: 0.1913320453:
34: 137632: loss: 0.1910964695:
34: 140832: loss: 0.1910632652:
34: 144032: loss: 0.1910342345:
34: 147232: loss: 0.1910816068:
34: 150432: loss: 0.1911087936:
34: 153632: loss: 0.1909748596:
34: 156832: loss: 0.1912654572:
34: 160032: loss: 0.1911560712:
34: 163232: loss: 0.1912142795:
34: 166432: loss: 0.1911679231:
Dev-Acc: 34: Accuracy: 0.9383235574: precision: 0.4613253290: recall: 0.3397381398: f1: 0.3913043478
Train-Acc: 34: Accuracy: 0.9303434491: precision: 0.7449035813: recall: 0.3555321807: f1: 0.4813314939
35: 3232: loss: 0.1803730110:
35: 6432: loss: 0.1906638095:
35: 9632: loss: 0.1959052586:
35: 12832: loss: 0.1942505685:
35: 16032: loss: 0.1934827712:
35: 19232: loss: 0.1931352370:
35: 22432: loss: 0.1926094807:
35: 25632: loss: 0.1922073051:
35: 28832: loss: 0.1909463240:
35: 32032: loss: 0.1911545778:
35: 35232: loss: 0.1911684939:
35: 38432: loss: 0.1913866951:
35: 41632: loss: 0.1902739455:
35: 44832: loss: 0.1906741806:
35: 48032: loss: 0.1912058869:
35: 51232: loss: 0.1904237442:
35: 54432: loss: 0.1912834943:
35: 57632: loss: 0.1911461166:
35: 60832: loss: 0.1915452098:
35: 64032: loss: 0.1915933864:
35: 67232: loss: 0.1920491768:
35: 70432: loss: 0.1922346290:
35: 73632: loss: 0.1926290628:
35: 76832: loss: 0.1929586947:
35: 80032: loss: 0.1932793176:
35: 83232: loss: 0.1933163287:
35: 86432: loss: 0.1929796264:
35: 89632: loss: 0.1928677405:
35: 92832: loss: 0.1928454067:
35: 96032: loss: 0.1924748069:
35: 99232: loss: 0.1922984556:
35: 102432: loss: 0.1923392459:
35: 105632: loss: 0.1921921116:
35: 108832: loss: 0.1921301114:
35: 112032: loss: 0.1920640380:
35: 115232: loss: 0.1918852367:
35: 118432: loss: 0.1918414216:
35: 121632: loss: 0.1920215614:
35: 124832: loss: 0.1920928749:
35: 128032: loss: 0.1921495063:
35: 131232: loss: 0.1916780159:
35: 134432: loss: 0.1915575309:
35: 137632: loss: 0.1915048546:
35: 140832: loss: 0.1912726840:
35: 144032: loss: 0.1910274933:
35: 147232: loss: 0.1907719410:
35: 150432: loss: 0.1906257600:
35: 153632: loss: 0.1906165762:
35: 156832: loss: 0.1905423002:
35: 160032: loss: 0.1905414614:
35: 163232: loss: 0.1903936214:
35: 166432: loss: 0.1903206135:
Dev-Acc: 35: Accuracy: 0.9381945729: precision: 0.4602921041: recall: 0.3429688828: f1: 0.3930624574
Train-Acc: 35: Accuracy: 0.9307080507: precision: 0.7455532926: recall: 0.3609887581: f1: 0.4864457831
36: 3232: loss: 0.1885459380:
36: 6432: loss: 0.1852221731:
36: 9632: loss: 0.1913618462:
36: 12832: loss: 0.1903711877:
36: 16032: loss: 0.1894134295:
36: 19232: loss: 0.1899695693:
36: 22432: loss: 0.1883282237:
36: 25632: loss: 0.1892012044:
36: 28832: loss: 0.1892901160:
36: 32032: loss: 0.1888755628:
36: 35232: loss: 0.1883409757:
36: 38432: loss: 0.1889077657:
36: 41632: loss: 0.1896921894:
36: 44832: loss: 0.1883145324:
36: 48032: loss: 0.1883323656:
36: 51232: loss: 0.1880625398:
36: 54432: loss: 0.1885124269:
36: 57632: loss: 0.1880739796:
36: 60832: loss: 0.1879392662:
36: 64032: loss: 0.1874809884:
36: 67232: loss: 0.1873338918:
36: 70432: loss: 0.1875147655:
36: 73632: loss: 0.1875337750:
36: 76832: loss: 0.1876745254:
36: 80032: loss: 0.1881807400:
36: 83232: loss: 0.1885130482:
36: 86432: loss: 0.1886096782:
36: 89632: loss: 0.1879783371:
36: 92832: loss: 0.1881532192:
36: 96032: loss: 0.1884039594:
36: 99232: loss: 0.1883593743:
36: 102432: loss: 0.1884285001:
36: 105632: loss: 0.1882423678:
36: 108832: loss: 0.1882636503:
36: 112032: loss: 0.1881301997:
36: 115232: loss: 0.1884022152:
36: 118432: loss: 0.1886194671:
36: 121632: loss: 0.1887566514:
36: 124832: loss: 0.1890035053:
36: 128032: loss: 0.1887373610:
36: 131232: loss: 0.1887439546:
36: 134432: loss: 0.1886209908:
36: 137632: loss: 0.1885476849:
36: 140832: loss: 0.1883054743:
36: 144032: loss: 0.1884502223:
36: 147232: loss: 0.1883566434:
36: 150432: loss: 0.1886983231:
36: 153632: loss: 0.1886946740:
36: 156832: loss: 0.1887827907:
36: 160032: loss: 0.1885324243:
36: 163232: loss: 0.1887676372:
36: 166432: loss: 0.1886663854:
Dev-Acc: 36: Accuracy: 0.9378274083: precision: 0.4566734189: recall: 0.3450093522: f1: 0.3930647036
Train-Acc: 36: Accuracy: 0.9309112430: precision: 0.7440181794: recall: 0.3659194004: f1: 0.4905693637
37: 3232: loss: 0.1799970238:
37: 6432: loss: 0.1740345071:
37: 9632: loss: 0.1728229010:
37: 12832: loss: 0.1770110927:
37: 16032: loss: 0.1800744002:
37: 19232: loss: 0.1820389191:
37: 22432: loss: 0.1840195764:
37: 25632: loss: 0.1857304880:
37: 28832: loss: 0.1870167297:
37: 32032: loss: 0.1878723334:
37: 35232: loss: 0.1886503048:
37: 38432: loss: 0.1881231453:
37: 41632: loss: 0.1889427469:
37: 44832: loss: 0.1887301355:
37: 48032: loss: 0.1887369977:
37: 51232: loss: 0.1886413076:
37: 54432: loss: 0.1883098186:
37: 57632: loss: 0.1874185026:
37: 60832: loss: 0.1873357993:
37: 64032: loss: 0.1866323256:
37: 67232: loss: 0.1864029371:
37: 70432: loss: 0.1867696495:
37: 73632: loss: 0.1864863640:
37: 76832: loss: 0.1868885801:
37: 80032: loss: 0.1874418755:
37: 83232: loss: 0.1870399759:
37: 86432: loss: 0.1872995154:
37: 89632: loss: 0.1875055846:
37: 92832: loss: 0.1871832628:
37: 96032: loss: 0.1867235009:
37: 99232: loss: 0.1866594711:
37: 102432: loss: 0.1867261653:
37: 105632: loss: 0.1865038179:
37: 108832: loss: 0.1865426543:
37: 112032: loss: 0.1864029489:
37: 115232: loss: 0.1865100111:
37: 118432: loss: 0.1865359114:
37: 121632: loss: 0.1868428333:
37: 124832: loss: 0.1870821028:
37: 128032: loss: 0.1874925896:
37: 131232: loss: 0.1871487096:
37: 134432: loss: 0.1871437043:
37: 137632: loss: 0.1872647196:
37: 140832: loss: 0.1872346365:
37: 144032: loss: 0.1872320072:
37: 147232: loss: 0.1871853888:
37: 150432: loss: 0.1871290345:
37: 153632: loss: 0.1873203949:
37: 156832: loss: 0.1872525472:
37: 160032: loss: 0.1873035816:
37: 163232: loss: 0.1877616413:
37: 166432: loss: 0.1877047125:
Dev-Acc: 37: Accuracy: 0.9377381206: precision: 0.4562222222: recall: 0.3490902908: f1: 0.3955302957
Train-Acc: 37: Accuracy: 0.9311443567: precision: 0.7439185616: recall: 0.3699296562: f1: 0.4941383095
38: 3232: loss: 0.1984956537:
38: 6432: loss: 0.1793532567:
38: 9632: loss: 0.1881112765:
38: 12832: loss: 0.1889601940:
38: 16032: loss: 0.1867572970:
38: 19232: loss: 0.1854007073:
38: 22432: loss: 0.1865674869:
38: 25632: loss: 0.1883728905:
38: 28832: loss: 0.1876296296:
38: 32032: loss: 0.1875064931:
38: 35232: loss: 0.1868910767:
38: 38432: loss: 0.1861225930:
38: 41632: loss: 0.1854629462:
38: 44832: loss: 0.1852580300:
38: 48032: loss: 0.1857896780:
38: 51232: loss: 0.1860647322:
38: 54432: loss: 0.1867158061:
38: 57632: loss: 0.1868924136:
38: 60832: loss: 0.1871540127:
38: 64032: loss: 0.1869479559:
38: 67232: loss: 0.1865357078:
38: 70432: loss: 0.1864179005:
38: 73632: loss: 0.1868898242:
38: 76832: loss: 0.1871623473:
38: 80032: loss: 0.1869183438:
38: 83232: loss: 0.1871290243:
38: 86432: loss: 0.1870887453:
38: 89632: loss: 0.1870794758:
38: 92832: loss: 0.1873960960:
38: 96032: loss: 0.1872600974:
38: 99232: loss: 0.1868323458:
38: 102432: loss: 0.1869332458:
38: 105632: loss: 0.1870523489:
38: 108832: loss: 0.1868946941:
38: 112032: loss: 0.1869441729:
38: 115232: loss: 0.1868928702:
38: 118432: loss: 0.1872774983:
38: 121632: loss: 0.1870744260:
38: 124832: loss: 0.1871313838:
38: 128032: loss: 0.1874372657:
38: 131232: loss: 0.1874974549:
38: 134432: loss: 0.1872854874:
38: 137632: loss: 0.1875817915:
38: 140832: loss: 0.1871277179:
38: 144032: loss: 0.1870628816:
38: 147232: loss: 0.1867974315:
38: 150432: loss: 0.1867080199:
38: 153632: loss: 0.1864677914:
38: 156832: loss: 0.1863317482:
38: 160032: loss: 0.1862031426:
38: 163232: loss: 0.1862376877:
38: 166432: loss: 0.1862623665:
Dev-Acc: 38: Accuracy: 0.9376984239: precision: 0.4561480829: recall: 0.3519809556: f1: 0.3973509934
Train-Acc: 38: Accuracy: 0.9315686822: precision: 0.7453999739: recall: 0.3755177174: f1: 0.4994316691
39: 3232: loss: 0.1805714636:
39: 6432: loss: 0.1844602858:
39: 9632: loss: 0.1847162467:
39: 12832: loss: 0.1861176068:
39: 16032: loss: 0.1883066732:
39: 19232: loss: 0.1860299285:
39: 22432: loss: 0.1860046460:
39: 25632: loss: 0.1859668554:
39: 28832: loss: 0.1844087849:
39: 32032: loss: 0.1851785953:
39: 35232: loss: 0.1859438078:
39: 38432: loss: 0.1857397133:
39: 41632: loss: 0.1860622106:
39: 44832: loss: 0.1872549674:
39: 48032: loss: 0.1871984914:
39: 51232: loss: 0.1866568125:
39: 54432: loss: 0.1855664548:
39: 57632: loss: 0.1853016486:
39: 60832: loss: 0.1845556239:
39: 64032: loss: 0.1849941536:
39: 67232: loss: 0.1854044605:
39: 70432: loss: 0.1854558894:
39: 73632: loss: 0.1860149833:
39: 76832: loss: 0.1859033557:
39: 80032: loss: 0.1855543267:
39: 83232: loss: 0.1856080818:
39: 86432: loss: 0.1851207012:
39: 89632: loss: 0.1854907177:
39: 92832: loss: 0.1857907587:
39: 96032: loss: 0.1862374232:
39: 99232: loss: 0.1867457188:
39: 102432: loss: 0.1869130331:
39: 105632: loss: 0.1868004435:
39: 108832: loss: 0.1866267056:
39: 112032: loss: 0.1865183036:
39: 115232: loss: 0.1862221839:
39: 118432: loss: 0.1861716729:
39: 121632: loss: 0.1859751142:
39: 124832: loss: 0.1860489500:
39: 128032: loss: 0.1860176420:
39: 131232: loss: 0.1863807282:
39: 134432: loss: 0.1863375800:
39: 137632: loss: 0.1860687146:
39: 140832: loss: 0.1860477019:
39: 144032: loss: 0.1859998022:
39: 147232: loss: 0.1855616671:
39: 150432: loss: 0.1857027145:
39: 153632: loss: 0.1860958822:
39: 156832: loss: 0.1860507855:
39: 160032: loss: 0.1859775747:
39: 163232: loss: 0.1858345026:
39: 166432: loss: 0.1858409737:
Dev-Acc: 39: Accuracy: 0.9373114705: precision: 0.4525309581: recall: 0.3541914640: f1: 0.3973674170
Train-Acc: 39: Accuracy: 0.9317420125: precision: 0.7447687936: recall: 0.3790677799: f1: 0.5024179846
40: 3232: loss: 0.1813264859:
40: 6432: loss: 0.1838634053:
40: 9632: loss: 0.1767273964:
40: 12832: loss: 0.1817900271:
40: 16032: loss: 0.1845660358:
40: 19232: loss: 0.1852186676:
40: 22432: loss: 0.1848772885:
40: 25632: loss: 0.1866276479:
40: 28832: loss: 0.1868687112:
40: 32032: loss: 0.1870426357:
40: 35232: loss: 0.1869226759:
40: 38432: loss: 0.1856566423:
40: 41632: loss: 0.1849422019:
40: 44832: loss: 0.1853216996:
40: 48032: loss: 0.1855055583:
40: 51232: loss: 0.1862525371:
40: 54432: loss: 0.1859304607:
40: 57632: loss: 0.1858547986:
40: 60832: loss: 0.1858253955:
40: 64032: loss: 0.1855299608:
40: 67232: loss: 0.1846247847:
40: 70432: loss: 0.1846200301:
40: 73632: loss: 0.1846637085:
40: 76832: loss: 0.1846364468:
40: 80032: loss: 0.1849321195:
40: 83232: loss: 0.1849201193:
40: 86432: loss: 0.1848551499:
40: 89632: loss: 0.1846409185:
40: 92832: loss: 0.1846754058:
40: 96032: loss: 0.1843127863:
40: 99232: loss: 0.1848351982:
40: 102432: loss: 0.1851820841:
40: 105632: loss: 0.1851227991:
40: 108832: loss: 0.1850851866:
40: 112032: loss: 0.1850570222:
40: 115232: loss: 0.1849900777:
40: 118432: loss: 0.1850584558:
40: 121632: loss: 0.1849096834:
40: 124832: loss: 0.1847738816:
40: 128032: loss: 0.1848883446:
40: 131232: loss: 0.1849602511:
40: 134432: loss: 0.1848313026:
40: 137632: loss: 0.1845521951:
40: 140832: loss: 0.1844236288:
40: 144032: loss: 0.1844855518:
40: 147232: loss: 0.1846065445:
40: 150432: loss: 0.1841768718:
40: 153632: loss: 0.1841861681:
40: 156832: loss: 0.1841528518:
40: 160032: loss: 0.1844099920:
40: 163232: loss: 0.1843637343:
40: 166432: loss: 0.1845215522:
Dev-Acc: 40: Accuracy: 0.9372717738: precision: 0.4526112186: recall: 0.3581023635: f1: 0.3998481109
Train-Acc: 40: Accuracy: 0.9320706725: precision: 0.7449356606: recall: 0.3843928736: f1: 0.5071118820
41: 3232: loss: 0.1880688107:
41: 6432: loss: 0.1884067227:
41: 9632: loss: 0.1845524448:
41: 12832: loss: 0.1859989371:
41: 16032: loss: 0.1855046083:
41: 19232: loss: 0.1873632135:
41: 22432: loss: 0.1863117222:
41: 25632: loss: 0.1861841558:
41: 28832: loss: 0.1873553250:
41: 32032: loss: 0.1861966399:
41: 35232: loss: 0.1863294618:
41: 38432: loss: 0.1849284742:
41: 41632: loss: 0.1851526322:
41: 44832: loss: 0.1855201490:
41: 48032: loss: 0.1860074667:
41: 51232: loss: 0.1862238309:
41: 54432: loss: 0.1863303091:
41: 57632: loss: 0.1867074762:
41: 60832: loss: 0.1873909282:
41: 64032: loss: 0.1872737070:
41: 67232: loss: 0.1873968914:
41: 70432: loss: 0.1870080580:
41: 73632: loss: 0.1870343965:
41: 76832: loss: 0.1867384004:
41: 80032: loss: 0.1862930789:
41: 83232: loss: 0.1858744545:
41: 86432: loss: 0.1851321016:
41: 89632: loss: 0.1847781324:
41: 92832: loss: 0.1850448448:
41: 96032: loss: 0.1846490758:
41: 99232: loss: 0.1848366082:
41: 102432: loss: 0.1848056342:
41: 105632: loss: 0.1847309067:
41: 108832: loss: 0.1849229766:
41: 112032: loss: 0.1849658744:
41: 115232: loss: 0.1849341941:
41: 118432: loss: 0.1847104601:
41: 121632: loss: 0.1848920851:
41: 124832: loss: 0.1847291957:
41: 128032: loss: 0.1849187452:
41: 131232: loss: 0.1846950288:
41: 134432: loss: 0.1844692429:
41: 137632: loss: 0.1849086092:
41: 140832: loss: 0.1850469754:
41: 144032: loss: 0.1850168454:
41: 147232: loss: 0.1849884969:
41: 150432: loss: 0.1843105621:
41: 153632: loss: 0.1843140304:
41: 156832: loss: 0.1838958972:
41: 160032: loss: 0.1843963647:
41: 163232: loss: 0.1841111975:
41: 166432: loss: 0.1840970117:
Dev-Acc: 41: Accuracy: 0.9369939566: precision: 0.4501594049: recall: 0.3601428329: f1: 0.4001511430
Train-Acc: 41: Accuracy: 0.9323396683: precision: 0.7450856855: recall: 0.3887318388: f1: 0.5109085411
42: 3232: loss: 0.1741480145:
42: 6432: loss: 0.1792091322:
42: 9632: loss: 0.1845961954:
42: 12832: loss: 0.1812368543:
42: 16032: loss: 0.1804792439:
42: 19232: loss: 0.1796640485:
42: 22432: loss: 0.1787273625:
42: 25632: loss: 0.1803563958:
42: 28832: loss: 0.1804839000:
42: 32032: loss: 0.1817307739:
42: 35232: loss: 0.1824718101:
42: 38432: loss: 0.1812317867:
42: 41632: loss: 0.1808627555:
42: 44832: loss: 0.1811073103:
42: 48032: loss: 0.1822292714:
42: 51232: loss: 0.1825616437:
42: 54432: loss: 0.1828365398:
42: 57632: loss: 0.1830550667:
42: 60832: loss: 0.1832981876:
42: 64032: loss: 0.1827690585:
42: 67232: loss: 0.1826974833:
42: 70432: loss: 0.1830640055:
42: 73632: loss: 0.1831355462:
42: 76832: loss: 0.1828814902:
42: 80032: loss: 0.1827340323:
42: 83232: loss: 0.1829683009:
42: 86432: loss: 0.1825709331:
42: 89632: loss: 0.1821333752:
42: 92832: loss: 0.1823482072:
42: 96032: loss: 0.1820802535:
42: 99232: loss: 0.1818767225:
42: 102432: loss: 0.1819248176:
42: 105632: loss: 0.1822898430:
42: 108832: loss: 0.1823908825:
42: 112032: loss: 0.1821677338:
42: 115232: loss: 0.1823673765:
42: 118432: loss: 0.1824114265:
42: 121632: loss: 0.1818883536:
42: 124832: loss: 0.1823635518:
42: 128032: loss: 0.1825459958:
42: 131232: loss: 0.1828475061:
42: 134432: loss: 0.1831961408:
42: 137632: loss: 0.1828147648:
42: 140832: loss: 0.1826210276:
42: 144032: loss: 0.1826050228:
42: 147232: loss: 0.1825382009:
42: 150432: loss: 0.1828879221:
42: 153632: loss: 0.1829801065:
42: 156832: loss: 0.1829230425:
42: 160032: loss: 0.1827646733:
42: 163232: loss: 0.1828225868:
42: 166432: loss: 0.1830536431:
Dev-Acc: 42: Accuracy: 0.9365871549: precision: 0.4466527197: recall: 0.3630334977: f1: 0.4005252791
Train-Acc: 42: Accuracy: 0.9326264858: precision: 0.7453888335: recall: 0.3932022878: f1: 0.5148267700
43: 3232: loss: 0.1842459111:
43: 6432: loss: 0.1804895654:
43: 9632: loss: 0.1844327659:
43: 12832: loss: 0.1843989297:
43: 16032: loss: 0.1849525066:
43: 19232: loss: 0.1858468692:
43: 22432: loss: 0.1863940847:
43: 25632: loss: 0.1868702120:
43: 28832: loss: 0.1855782464:
43: 32032: loss: 0.1858462031:
43: 35232: loss: 0.1861692973:
43: 38432: loss: 0.1859916849:
43: 41632: loss: 0.1859752956:
43: 44832: loss: 0.1855243385:
43: 48032: loss: 0.1848048161:
43: 51232: loss: 0.1843935434:
43: 54432: loss: 0.1841128485:
43: 57632: loss: 0.1837716169:
43: 60832: loss: 0.1839157611:
43: 64032: loss: 0.1834054436:
43: 67232: loss: 0.1833535448:
43: 70432: loss: 0.1835437047:
43: 73632: loss: 0.1830649185:
43: 76832: loss: 0.1829021815:
43: 80032: loss: 0.1826076744:
43: 83232: loss: 0.1828684587:
43: 86432: loss: 0.1828832247:
43: 89632: loss: 0.1834142242:
43: 92832: loss: 0.1831864801:
43: 96032: loss: 0.1830537816:
43: 99232: loss: 0.1832733116:
43: 102432: loss: 0.1828443346:
43: 105632: loss: 0.1831250538:
43: 108832: loss: 0.1834690510:
43: 112032: loss: 0.1832859228:
43: 115232: loss: 0.1833267598:
43: 118432: loss: 0.1831152420:
43: 121632: loss: 0.1831976034:
43: 124832: loss: 0.1830251397:
43: 128032: loss: 0.1830835281:
43: 131232: loss: 0.1831321118:
43: 134432: loss: 0.1828891057:
43: 137632: loss: 0.1830177916:
43: 140832: loss: 0.1828946106:
43: 144032: loss: 0.1828293227:
43: 147232: loss: 0.1828245597:
43: 150432: loss: 0.1827750897:
43: 153632: loss: 0.1825562017:
43: 156832: loss: 0.1825291091:
43: 160032: loss: 0.1825327986:
43: 163232: loss: 0.1824281695:
43: 166432: loss: 0.1822227745:
Dev-Acc: 43: Accuracy: 0.9365177155: precision: 0.4463804190: recall: 0.3659241626: f1: 0.4021678191
Train-Acc: 43: Accuracy: 0.9329253435: precision: 0.7460513327: recall: 0.3974755111: f1: 0.5186360712
44: 3232: loss: 0.1810753133:
44: 6432: loss: 0.1918400754:
44: 9632: loss: 0.1891104998:
44: 12832: loss: 0.1903190912:
44: 16032: loss: 0.1854390428:
44: 19232: loss: 0.1839811437:
44: 22432: loss: 0.1829847093:
44: 25632: loss: 0.1833757180:
44: 28832: loss: 0.1827776800:
44: 32032: loss: 0.1830608785:
44: 35232: loss: 0.1832546667:
44: 38432: loss: 0.1813940315:
44: 41632: loss: 0.1819843739:
44: 44832: loss: 0.1823735121:
44: 48032: loss: 0.1820997827:
44: 51232: loss: 0.1824389378:
44: 54432: loss: 0.1824927685:
44: 57632: loss: 0.1842843576:
44: 60832: loss: 0.1839213830:
44: 64032: loss: 0.1836069011:
44: 67232: loss: 0.1838095395:
44: 70432: loss: 0.1839073983:
44: 73632: loss: 0.1838736999:
44: 76832: loss: 0.1833762006:
44: 80032: loss: 0.1837752002:
44: 83232: loss: 0.1837889213:
44: 86432: loss: 0.1834277594:
44: 89632: loss: 0.1834683609:
44: 92832: loss: 0.1834369804:
44: 96032: loss: 0.1836763037:
44: 99232: loss: 0.1835471681:
44: 102432: loss: 0.1832892097:
44: 105632: loss: 0.1826157755:
44: 108832: loss: 0.1823179239:
44: 112032: loss: 0.1822897658:
44: 115232: loss: 0.1817419101:
44: 118432: loss: 0.1820076382:
44: 121632: loss: 0.1819632933:
44: 124832: loss: 0.1824291157:
44: 128032: loss: 0.1825523914:
44: 131232: loss: 0.1821401199:
44: 134432: loss: 0.1820497080:
44: 137632: loss: 0.1820825808:
44: 140832: loss: 0.1816739642:
44: 144032: loss: 0.1817709496:
44: 147232: loss: 0.1817095524:
44: 150432: loss: 0.1815870815:
44: 153632: loss: 0.1814478318:
44: 156832: loss: 0.1813941333:
44: 160032: loss: 0.1813230566:
44: 163232: loss: 0.1812281055:
44: 166432: loss: 0.1814801947:
Dev-Acc: 44: Accuracy: 0.9363887310: precision: 0.4453833471: recall: 0.3674545145: f1: 0.4026833131
Train-Acc: 44: Accuracy: 0.9331524372: precision: 0.7460278660: recall: 0.4012885412: f1: 0.5218655153
45: 3232: loss: 0.1796506428:
45: 6432: loss: 0.1857520502:
45: 9632: loss: 0.1818395175:
45: 12832: loss: 0.1819758101:
45: 16032: loss: 0.1815244674:
45: 19232: loss: 0.1819236618:
45: 22432: loss: 0.1814646756:
45: 25632: loss: 0.1804509735:
45: 28832: loss: 0.1815000680:
45: 32032: loss: 0.1815958328:
45: 35232: loss: 0.1820563947:
45: 38432: loss: 0.1808821668:
45: 41632: loss: 0.1819900725:
45: 44832: loss: 0.1819542208:
45: 48032: loss: 0.1817749167:
45: 51232: loss: 0.1819709577:
45: 54432: loss: 0.1820446042:
45: 57632: loss: 0.1816234764:
45: 60832: loss: 0.1814316598:
45: 64032: loss: 0.1811629379:
45: 67232: loss: 0.1806962032:
45: 70432: loss: 0.1808665790:
45: 73632: loss: 0.1800404241:
45: 76832: loss: 0.1806194047:
45: 80032: loss: 0.1804629918:
45: 83232: loss: 0.1800322834:
45: 86432: loss: 0.1803269054:
45: 89632: loss: 0.1801324310:
45: 92832: loss: 0.1798363511:
45: 96032: loss: 0.1800853412:
45: 99232: loss: 0.1801031117:
45: 102432: loss: 0.1797924130:
45: 105632: loss: 0.1800738056:
45: 108832: loss: 0.1802956818:
45: 112032: loss: 0.1805767364:
45: 115232: loss: 0.1803100892:
45: 118432: loss: 0.1802177232:
45: 121632: loss: 0.1802095205:
45: 124832: loss: 0.1798025843:
45: 128032: loss: 0.1799601138:
45: 131232: loss: 0.1800076857:
45: 134432: loss: 0.1797180391:
45: 137632: loss: 0.1797815621:
45: 140832: loss: 0.1799482778:
45: 144032: loss: 0.1799983747:
45: 147232: loss: 0.1799613517:
45: 150432: loss: 0.1799428859:
45: 153632: loss: 0.1802474671:
45: 156832: loss: 0.1802323051:
45: 160032: loss: 0.1803545385:
45: 163232: loss: 0.1804633833:
45: 166432: loss: 0.1803064574:
Dev-Acc: 45: Accuracy: 0.9364879131: precision: 0.4468520033: recall: 0.3717054923: f1: 0.4058293883
Train-Acc: 45: Accuracy: 0.9333377481: precision: 0.7458489880: recall: 0.4045756361: f1: 0.5245929588
46: 3232: loss: 0.1867470399:
46: 6432: loss: 0.1939647881:
46: 9632: loss: 0.1962679681:
46: 12832: loss: 0.1927360991:
46: 16032: loss: 0.1896978111:
46: 19232: loss: 0.1851819426:
46: 22432: loss: 0.1857152307:
46: 25632: loss: 0.1851078130:
46: 28832: loss: 0.1841414373:
46: 32032: loss: 0.1831686484:
46: 35232: loss: 0.1829918238:
46: 38432: loss: 0.1834255519:
46: 41632: loss: 0.1839989767:
46: 44832: loss: 0.1840544500:
46: 48032: loss: 0.1834586854:
46: 51232: loss: 0.1830360509:
46: 54432: loss: 0.1837635372:
46: 57632: loss: 0.1831204773:
46: 60832: loss: 0.1829662343:
46: 64032: loss: 0.1822371898:
46: 67232: loss: 0.1817820747:
46: 70432: loss: 0.1809411418:
46: 73632: loss: 0.1805427570:
46: 76832: loss: 0.1806942221:
46: 80032: loss: 0.1803636073:
46: 83232: loss: 0.1809241081:
46: 86432: loss: 0.1807055148:
46: 89632: loss: 0.1809643650:
46: 92832: loss: 0.1809808014:
46: 96032: loss: 0.1805999001:
46: 99232: loss: 0.1801766061:
46: 102432: loss: 0.1802843153:
46: 105632: loss: 0.1802260838:
46: 108832: loss: 0.1801769229:
46: 112032: loss: 0.1803080156:
46: 115232: loss: 0.1807980290:
46: 118432: loss: 0.1809565400:
46: 121632: loss: 0.1809496190:
46: 124832: loss: 0.1809435069:
46: 128032: loss: 0.1812022478:
46: 131232: loss: 0.1814633577:
46: 134432: loss: 0.1813065340:
46: 137632: loss: 0.1810670659:
46: 140832: loss: 0.1810590522:
46: 144032: loss: 0.1809054336:
46: 147232: loss: 0.1805571607:
46: 150432: loss: 0.1804560073:
46: 153632: loss: 0.1803561473:
46: 156832: loss: 0.1804770782:
46: 160032: loss: 0.1803470891:
46: 163232: loss: 0.1800476581:
46: 166432: loss: 0.1798849686:
Dev-Acc: 46: Accuracy: 0.9361902475: precision: 0.4442190669: recall: 0.3723856487: f1: 0.4051429100
Train-Acc: 46: Accuracy: 0.9336365461: precision: 0.7469039317: recall: 0.4083886661: f1: 0.5280516831
47: 3232: loss: 0.1580554856:
47: 6432: loss: 0.1628666211:
47: 9632: loss: 0.1692897551:
47: 12832: loss: 0.1730251330:
47: 16032: loss: 0.1728436216:
47: 19232: loss: 0.1731400290:
47: 22432: loss: 0.1745446455:
47: 25632: loss: 0.1776413040:
47: 28832: loss: 0.1778278550:
47: 32032: loss: 0.1766015476:
47: 35232: loss: 0.1776707346:
47: 38432: loss: 0.1774205729:
47: 41632: loss: 0.1768123773:
47: 44832: loss: 0.1773255573:
47: 48032: loss: 0.1783481046:
47: 51232: loss: 0.1779950558:
47: 54432: loss: 0.1778384490:
47: 57632: loss: 0.1782492865:
47: 60832: loss: 0.1784693185:
47: 64032: loss: 0.1784990669:
47: 67232: loss: 0.1786046241:
47: 70432: loss: 0.1793070700:
47: 73632: loss: 0.1791994571:
47: 76832: loss: 0.1792805095:
47: 80032: loss: 0.1796043381:
47: 83232: loss: 0.1796010871:
47: 86432: loss: 0.1794219696:
47: 89632: loss: 0.1794941280:
47: 92832: loss: 0.1794837071:
47: 96032: loss: 0.1790343218:
47: 99232: loss: 0.1796000757:
47: 102432: loss: 0.1799539308:
47: 105632: loss: 0.1795040855:
47: 108832: loss: 0.1797730426:
47: 112032: loss: 0.1800257887:
47: 115232: loss: 0.1798799424:
47: 118432: loss: 0.1801180926:
47: 121632: loss: 0.1799905202:
47: 124832: loss: 0.1794600901:
47: 128032: loss: 0.1793683766:
47: 131232: loss: 0.1794686010:
47: 134432: loss: 0.1792562699:
47: 137632: loss: 0.1793281432:
47: 140832: loss: 0.1793511036:
47: 144032: loss: 0.1794650564:
47: 147232: loss: 0.1794180677:
47: 150432: loss: 0.1792718106:
47: 153632: loss: 0.1794508376:
47: 156832: loss: 0.1792626772:
47: 160032: loss: 0.1791763296:
47: 163232: loss: 0.1792949370:
47: 166432: loss: 0.1792293449:
Dev-Acc: 47: Accuracy: 0.9361605048: precision: 0.4442652691: recall: 0.3747661962: f1: 0.4065670540
Train-Acc: 47: Accuracy: 0.9338636398: precision: 0.7475809342: recall: 0.4114127934: f1: 0.5307437876
48: 3232: loss: 0.1688232759:
48: 6432: loss: 0.1690771280:
48: 9632: loss: 0.1749837816:
48: 12832: loss: 0.1729990229:
48: 16032: loss: 0.1736384020:
48: 19232: loss: 0.1746132472:
48: 22432: loss: 0.1754145222:
48: 25632: loss: 0.1759385305:
48: 28832: loss: 0.1766634938:
48: 32032: loss: 0.1770047933:
48: 35232: loss: 0.1772320959:
48: 38432: loss: 0.1767629298:
48: 41632: loss: 0.1770640675:
48: 44832: loss: 0.1774969250:
48: 48032: loss: 0.1778165617:
48: 51232: loss: 0.1771023334:
48: 54432: loss: 0.1765206634:
48: 57632: loss: 0.1776755229:
48: 60832: loss: 0.1779104212:
48: 64032: loss: 0.1784253155:
48: 67232: loss: 0.1778243728:
48: 70432: loss: 0.1779657599:
48: 73632: loss: 0.1780341037:
48: 76832: loss: 0.1786814102:
48: 80032: loss: 0.1788684534:
48: 83232: loss: 0.1784955741:
48: 86432: loss: 0.1791074216:
48: 89632: loss: 0.1791375991:
48: 92832: loss: 0.1790602441:
48: 96032: loss: 0.1789657828:
48: 99232: loss: 0.1790645847:
48: 102432: loss: 0.1788773381:
48: 105632: loss: 0.1787997025:
48: 108832: loss: 0.1786360779:
48: 112032: loss: 0.1789766716:
48: 115232: loss: 0.1789790360:
48: 118432: loss: 0.1789051740:
48: 121632: loss: 0.1786882904:
48: 124832: loss: 0.1785162025:
48: 128032: loss: 0.1787197423:
48: 131232: loss: 0.1786280090:
48: 134432: loss: 0.1784184262:
48: 137632: loss: 0.1784917526:
48: 140832: loss: 0.1784364215:
48: 144032: loss: 0.1785954678:
48: 147232: loss: 0.1787693154:
48: 150432: loss: 0.1784602167:
48: 153632: loss: 0.1784204554:
48: 156832: loss: 0.1782017995:
48: 160032: loss: 0.1781302288:
48: 163232: loss: 0.1779900193:
48: 166432: loss: 0.1782490598:
Dev-Acc: 48: Accuracy: 0.9361703992: precision: 0.4445114596: recall: 0.3759564700: f1: 0.4073698756
Train-Acc: 48: Accuracy: 0.9340489507: precision: 0.7479221088: recall: 0.4141082112: f1: 0.5330681674
49: 3232: loss: 0.1934509307:
49: 6432: loss: 0.1857606040:
49: 9632: loss: 0.1842997811:
49: 12832: loss: 0.1818688018:
49: 16032: loss: 0.1833504631:
49: 19232: loss: 0.1817567797:
49: 22432: loss: 0.1834659308:
49: 25632: loss: 0.1819841878:
49: 28832: loss: 0.1813803037:
49: 32032: loss: 0.1793258184:
49: 35232: loss: 0.1799681790:
49: 38432: loss: 0.1799649443:
49: 41632: loss: 0.1798388732:
49: 44832: loss: 0.1802531499:
49: 48032: loss: 0.1790438984:
49: 51232: loss: 0.1786615100:
49: 54432: loss: 0.1780869110:
49: 57632: loss: 0.1779665388:
49: 60832: loss: 0.1771735415:
49: 64032: loss: 0.1771612652:
49: 67232: loss: 0.1766354733:
49: 70432: loss: 0.1763525423:
49: 73632: loss: 0.1761565794:
49: 76832: loss: 0.1761711824:
49: 80032: loss: 0.1756654032:
49: 83232: loss: 0.1757398731:
49: 86432: loss: 0.1768794947:
49: 89632: loss: 0.1767710205:
49: 92832: loss: 0.1771771117:
49: 96032: loss: 0.1769098318:
49: 99232: loss: 0.1768969362:
49: 102432: loss: 0.1763253180:
49: 105632: loss: 0.1765539018:
49: 108832: loss: 0.1769899508:
49: 112032: loss: 0.1773930510:
49: 115232: loss: 0.1771861573:
49: 118432: loss: 0.1771360150:
49: 121632: loss: 0.1774943372:
49: 124832: loss: 0.1771687889:
49: 128032: loss: 0.1773454697:
49: 131232: loss: 0.1769685612:
49: 134432: loss: 0.1774040161:
49: 137632: loss: 0.1771699751:
49: 140832: loss: 0.1772614366:
49: 144032: loss: 0.1770601660:
49: 147232: loss: 0.1769989974:
49: 150432: loss: 0.1769746969:
49: 153632: loss: 0.1771751731:
49: 156832: loss: 0.1771598499:
49: 160032: loss: 0.1769856724:
49: 163232: loss: 0.1773840909:
49: 166432: loss: 0.1772950812:
Dev-Acc: 49: Accuracy: 0.9360612631: precision: 0.4438011579: recall: 0.3779969393: f1: 0.4082644628
Train-Acc: 49: Accuracy: 0.9342760444: precision: 0.7481743227: recall: 0.4175925317: f1: 0.5360111388
50: 3232: loss: 0.1834036663:
50: 6432: loss: 0.1806356532:
50: 9632: loss: 0.1815027344:
50: 12832: loss: 0.1766417136:
50: 16032: loss: 0.1774590801:
50: 19232: loss: 0.1777722398:
50: 22432: loss: 0.1755399267:
50: 25632: loss: 0.1750766397:
50: 28832: loss: 0.1752580871:
50: 32032: loss: 0.1749259005:
50: 35232: loss: 0.1744697404:
50: 38432: loss: 0.1745701362:
50: 41632: loss: 0.1743197043:
50: 44832: loss: 0.1737215857:
50: 48032: loss: 0.1744308311:
50: 51232: loss: 0.1742675493:
50: 54432: loss: 0.1748675005:
50: 57632: loss: 0.1748192710:
50: 60832: loss: 0.1746366772:
50: 64032: loss: 0.1749635112:
50: 67232: loss: 0.1746578463:
50: 70432: loss: 0.1743997071:
50: 73632: loss: 0.1748180897:
50: 76832: loss: 0.1755616470:
50: 80032: loss: 0.1759364184:
50: 83232: loss: 0.1759355647:
50: 86432: loss: 0.1757734012:
50: 89632: loss: 0.1753070522:
50: 92832: loss: 0.1748266012:
50: 96032: loss: 0.1748335976:
50: 99232: loss: 0.1748950211:
50: 102432: loss: 0.1746410416:
50: 105632: loss: 0.1752837719:
50: 108832: loss: 0.1760501649:
50: 112032: loss: 0.1762147092:
50: 115232: loss: 0.1762137796:
50: 118432: loss: 0.1762960599:
50: 121632: loss: 0.1764498361:
50: 124832: loss: 0.1765799139:
50: 128032: loss: 0.1762910378:
50: 131232: loss: 0.1762131661:
50: 134432: loss: 0.1767285201:
50: 137632: loss: 0.1766709151:
50: 140832: loss: 0.1765774129:
50: 144032: loss: 0.1765200883:
50: 147232: loss: 0.1768109885:
50: 150432: loss: 0.1766231521:
50: 153632: loss: 0.1767871791:
50: 156832: loss: 0.1769586716:
50: 160032: loss: 0.1769351108:
50: 163232: loss: 0.1769850447:
50: 166432: loss: 0.1770370583:
Dev-Acc: 50: Accuracy: 0.9359918237: precision: 0.4434748116: recall: 0.3802074477: f1: 0.4094113339
Train-Acc: 50: Accuracy: 0.9344075322: precision: 0.7481255858: recall: 0.4198277562: f1: 0.5378363583
51: 3232: loss: 0.1782043048:
51: 6432: loss: 0.1780864666:
51: 9632: loss: 0.1784229528:
51: 12832: loss: 0.1782291906:
51: 16032: loss: 0.1758989917:
51: 19232: loss: 0.1806711778:
51: 22432: loss: 0.1789196565:
51: 25632: loss: 0.1792824725:
51: 28832: loss: 0.1776358733:
51: 32032: loss: 0.1777180572:
51: 35232: loss: 0.1780651919:
51: 38432: loss: 0.1779273161:
51: 41632: loss: 0.1783438799:
51: 44832: loss: 0.1776438980:
51: 48032: loss: 0.1783671764:
51: 51232: loss: 0.1784270285:
51: 54432: loss: 0.1788628067:
51: 57632: loss: 0.1789712477:
51: 60832: loss: 0.1785076347:
51: 64032: loss: 0.1784445376:
51: 67232: loss: 0.1781521134:
51: 70432: loss: 0.1776909719:
51: 73632: loss: 0.1771485526:
51: 76832: loss: 0.1765451311:
51: 80032: loss: 0.1762938718:
51: 83232: loss: 0.1757553771:
51: 86432: loss: 0.1758959380:
51: 89632: loss: 0.1759480998:
51: 92832: loss: 0.1762241492:
51: 96032: loss: 0.1763085738:
51: 99232: loss: 0.1762790170:
51: 102432: loss: 0.1771924609:
51: 105632: loss: 0.1773522429:
51: 108832: loss: 0.1772495638:
51: 112032: loss: 0.1777450414:
51: 115232: loss: 0.1774592608:
51: 118432: loss: 0.1770190986:
51: 121632: loss: 0.1763126970:
51: 124832: loss: 0.1761363847:
51: 128032: loss: 0.1760793314:
51: 131232: loss: 0.1761649898:
51: 134432: loss: 0.1765216156:
51: 137632: loss: 0.1765143536:
51: 140832: loss: 0.1766658594:
51: 144032: loss: 0.1767543002:
51: 147232: loss: 0.1770779979:
51: 150432: loss: 0.1772505717:
51: 153632: loss: 0.1769373061:
51: 156832: loss: 0.1767696448:
51: 160032: loss: 0.1765401953:
51: 163232: loss: 0.1764923936:
51: 166432: loss: 0.1761054365:
Dev-Acc: 51: Accuracy: 0.9360017180: precision: 0.4436745199: recall: 0.3810576433: f1: 0.4099890231
Train-Acc: 51: Accuracy: 0.9344971776: precision: 0.7477561487: recall: 0.4217342713: f1: 0.5393022278
52: 3232: loss: 0.1642204169:
52: 6432: loss: 0.1766723622:
52: 9632: loss: 0.1751458044:
52: 12832: loss: 0.1732372521:
52: 16032: loss: 0.1740192556:
52: 19232: loss: 0.1735640582:
52: 22432: loss: 0.1761914577:
52: 25632: loss: 0.1779998178:
52: 28832: loss: 0.1770152975:
52: 32032: loss: 0.1761385585:
52: 35232: loss: 0.1761147975:
52: 38432: loss: 0.1758332056:
52: 41632: loss: 0.1762216763:
52: 44832: loss: 0.1767672459:
52: 48032: loss: 0.1768147349:
52: 51232: loss: 0.1771600130:
52: 54432: loss: 0.1773294667:
52: 57632: loss: 0.1771094042:
52: 60832: loss: 0.1769270378:
52: 64032: loss: 0.1758154374:
52: 67232: loss: 0.1759528795:
52: 70432: loss: 0.1764284536:
52: 73632: loss: 0.1763917164:
52: 76832: loss: 0.1758531930:
52: 80032: loss: 0.1764437889:
52: 83232: loss: 0.1768613825:
52: 86432: loss: 0.1766444609:
52: 89632: loss: 0.1765196677:
52: 92832: loss: 0.1765083821:
52: 96032: loss: 0.1764085297:
52: 99232: loss: 0.1762534977:
52: 102432: loss: 0.1763839001:
52: 105632: loss: 0.1766866123:
52: 108832: loss: 0.1764289061:
52: 112032: loss: 0.1761437122:
52: 115232: loss: 0.1762085597:
52: 118432: loss: 0.1759354222:
52: 121632: loss: 0.1760743389:
52: 124832: loss: 0.1762631630:
52: 128032: loss: 0.1761232625:
52: 131232: loss: 0.1761787748:
52: 134432: loss: 0.1763521499:
52: 137632: loss: 0.1764787821:
52: 140832: loss: 0.1762692731:
52: 144032: loss: 0.1762277961:
52: 147232: loss: 0.1763184800:
52: 150432: loss: 0.1762453409:
52: 153632: loss: 0.1761700161:
52: 156832: loss: 0.1762864055:
52: 160032: loss: 0.1758621786:
52: 163232: loss: 0.1758322642:
52: 166432: loss: 0.1759188095:
Dev-Acc: 52: Accuracy: 0.9359025359: precision: 0.4427978660: recall: 0.3810576433: f1: 0.4096143301
Train-Acc: 52: Accuracy: 0.9346644878: precision: 0.7475413630: recall: 0.4247583985: f1: 0.5417120818
53: 3232: loss: 0.1776555211:
53: 6432: loss: 0.1796019735:
53: 9632: loss: 0.1774014522:
53: 12832: loss: 0.1780688417:
53: 16032: loss: 0.1773256432:
53: 19232: loss: 0.1745729665:
53: 22432: loss: 0.1747749167:
53: 25632: loss: 0.1754833122:
53: 28832: loss: 0.1747249432:
53: 32032: loss: 0.1747726007:
53: 35232: loss: 0.1744505484:
53: 38432: loss: 0.1749808337:
53: 41632: loss: 0.1746970032:
53: 44832: loss: 0.1756159981:
53: 48032: loss: 0.1767155343:
53: 51232: loss: 0.1768144766:
53: 54432: loss: 0.1761170556:
53: 57632: loss: 0.1757552247:
53: 60832: loss: 0.1761035843:
53: 64032: loss: 0.1762007818:
53: 67232: loss: 0.1766868244:
53: 70432: loss: 0.1764883360:
53: 73632: loss: 0.1759328497:
53: 76832: loss: 0.1758968175:
53: 80032: loss: 0.1755738904:
53: 83232: loss: 0.1754525516:
53: 86432: loss: 0.1757008420:
53: 89632: loss: 0.1754121290:
53: 92832: loss: 0.1752088852:
53: 96032: loss: 0.1750118228:
53: 99232: loss: 0.1753826407:
53: 102432: loss: 0.1754673105:
53: 105632: loss: 0.1748855672:
53: 108832: loss: 0.1749378737:
53: 112032: loss: 0.1750112497:
53: 115232: loss: 0.1750915096:
53: 118432: loss: 0.1749996141:
53: 121632: loss: 0.1748671014:
53: 124832: loss: 0.1748999484:
53: 128032: loss: 0.1745886331:
53: 131232: loss: 0.1743454746:
53: 134432: loss: 0.1747052974:
53: 137632: loss: 0.1747210686:
53: 140832: loss: 0.1747013353:
53: 144032: loss: 0.1747850127:
53: 147232: loss: 0.1750914383:
53: 150432: loss: 0.1750476203:
53: 153632: loss: 0.1749816147:
53: 156832: loss: 0.1750367094:
53: 160032: loss: 0.1754225507:
53: 163232: loss: 0.1751484600:
53: 166432: loss: 0.1751975281:
Dev-Acc: 53: Accuracy: 0.9358429909: precision: 0.4424099232: recall: 0.3820778779: f1: 0.4100364964
Train-Acc: 53: Accuracy: 0.9347840548: precision: 0.7474956822: recall: 0.4267963973: f1: 0.5433545363
54: 3232: loss: 0.1847570007:
54: 6432: loss: 0.1907736719:
54: 9632: loss: 0.1804744784:
54: 12832: loss: 0.1786448256:
54: 16032: loss: 0.1792174386:
54: 19232: loss: 0.1781806451:
54: 22432: loss: 0.1788881405:
54: 25632: loss: 0.1785785768:
54: 28832: loss: 0.1780185992:
54: 32032: loss: 0.1783383200:
54: 35232: loss: 0.1781945016:
54: 38432: loss: 0.1775662718:
54: 41632: loss: 0.1772874339:
54: 44832: loss: 0.1774419333:
54: 48032: loss: 0.1778513774:
54: 51232: loss: 0.1772612211:
54: 54432: loss: 0.1777307998:
54: 57632: loss: 0.1781518501:
54: 60832: loss: 0.1774106004:
54: 64032: loss: 0.1772724810:
54: 67232: loss: 0.1773503473:
54: 70432: loss: 0.1773764283:
54: 73632: loss: 0.1774065455:
54: 76832: loss: 0.1768438382:
54: 80032: loss: 0.1770062896:
54: 83232: loss: 0.1767367045:
54: 86432: loss: 0.1765809656:
54: 89632: loss: 0.1765692840:
54: 92832: loss: 0.1766051463:
54: 96032: loss: 0.1760037481:
54: 99232: loss: 0.1756116587:
54: 102432: loss: 0.1754985327:
54: 105632: loss: 0.1754038136:
54: 108832: loss: 0.1756129493:
54: 112032: loss: 0.1753635115:
54: 115232: loss: 0.1753398678:
54: 118432: loss: 0.1752049540:
54: 121632: loss: 0.1749829616:
54: 124832: loss: 0.1750477941:
54: 128032: loss: 0.1751671530:
54: 131232: loss: 0.1747184852:
54: 134432: loss: 0.1745094114:
54: 137632: loss: 0.1743968506:
54: 140832: loss: 0.1742708540:
54: 144032: loss: 0.1745259252:
54: 147232: loss: 0.1740119385:
54: 150432: loss: 0.1739730243:
54: 153632: loss: 0.1738614980:
54: 156832: loss: 0.1738569184:
54: 160032: loss: 0.1742062911:
54: 163232: loss: 0.1742359899:
54: 166432: loss: 0.1741733196:
Dev-Acc: 54: Accuracy: 0.9356445074: precision: 0.4407211444: recall: 0.3824179561: f1: 0.4095047342
Train-Acc: 54: Accuracy: 0.9349274635: precision: 0.7477931904: recall: 0.4288343962: f1: 0.5450823097
55: 3232: loss: 0.1894657826:
55: 6432: loss: 0.1762953070:
55: 9632: loss: 0.1778106578:
55: 12832: loss: 0.1773820955:
55: 16032: loss: 0.1746635767:
55: 19232: loss: 0.1750349530:
55: 22432: loss: 0.1741476274:
55: 25632: loss: 0.1738843964:
55: 28832: loss: 0.1745912580:
55: 32032: loss: 0.1747447808:
55: 35232: loss: 0.1761887699:
55: 38432: loss: 0.1762906003:
55: 41632: loss: 0.1774313508:
55: 44832: loss: 0.1772359625:
55: 48032: loss: 0.1763372002:
55: 51232: loss: 0.1751835820:
55: 54432: loss: 0.1744009447:
55: 57632: loss: 0.1741716458:
55: 60832: loss: 0.1738866476:
55: 64032: loss: 0.1738047895:
55: 67232: loss: 0.1729667804:
55: 70432: loss: 0.1731014174:
55: 73632: loss: 0.1730493981:
55: 76832: loss: 0.1731446862:
55: 80032: loss: 0.1723933328:
55: 83232: loss: 0.1719311808:
55: 86432: loss: 0.1716425670:
55: 89632: loss: 0.1724063578:
55: 92832: loss: 0.1723551102:
55: 96032: loss: 0.1723817356:
55: 99232: loss: 0.1729329225:
55: 102432: loss: 0.1730708562:
55: 105632: loss: 0.1732354638:
55: 108832: loss: 0.1737472172:
55: 112032: loss: 0.1737507652:
55: 115232: loss: 0.1734053703:
55: 118432: loss: 0.1731555115:
55: 121632: loss: 0.1729943198:
55: 124832: loss: 0.1731130113:
55: 128032: loss: 0.1731474582:
55: 131232: loss: 0.1731012760:
55: 134432: loss: 0.1729486188:
55: 137632: loss: 0.1733114844:
55: 140832: loss: 0.1736597445:
55: 144032: loss: 0.1735789891:
55: 147232: loss: 0.1736163359:
55: 150432: loss: 0.1736893536:
55: 153632: loss: 0.1735445984:
55: 156832: loss: 0.1735248643:
55: 160032: loss: 0.1732841271:
55: 163232: loss: 0.1736688480:
55: 166432: loss: 0.1737129439:
Dev-Acc: 55: Accuracy: 0.9355552197: precision: 0.4400156311: recall: 0.3829280735: f1: 0.4094917720
Train-Acc: 55: Accuracy: 0.9350948334: precision: 0.7482597284: recall: 0.4310696207: f1: 0.5470092600
56: 3232: loss: 0.1778769403:
56: 6432: loss: 0.1764484670:
56: 9632: loss: 0.1774596165:
56: 12832: loss: 0.1765121385:
56: 16032: loss: 0.1762284444:
56: 19232: loss: 0.1750176784:
56: 22432: loss: 0.1781107663:
56: 25632: loss: 0.1763201626:
56: 28832: loss: 0.1751565338:
56: 32032: loss: 0.1749245436:
56: 35232: loss: 0.1745511359:
56: 38432: loss: 0.1752680902:
56: 41632: loss: 0.1753483567:
56: 44832: loss: 0.1745917346:
56: 48032: loss: 0.1735446016:
56: 51232: loss: 0.1735792094:
56: 54432: loss: 0.1741745116:
56: 57632: loss: 0.1737292718:
56: 60832: loss: 0.1738575331:
56: 64032: loss: 0.1735112303:
56: 67232: loss: 0.1734949496:
56: 70432: loss: 0.1734587087:
56: 73632: loss: 0.1732141991:
56: 76832: loss: 0.1733771524:
56: 80032: loss: 0.1738419368:
56: 83232: loss: 0.1736234440:
56: 86432: loss: 0.1732598745:
56: 89632: loss: 0.1737971606:
56: 92832: loss: 0.1742811530:
56: 96032: loss: 0.1744105348:
56: 99232: loss: 0.1740806173:
56: 102432: loss: 0.1739644826:
56: 105632: loss: 0.1738353628:
56: 108832: loss: 0.1738876900:
56: 112032: loss: 0.1741684866:
56: 115232: loss: 0.1741971630:
56: 118432: loss: 0.1738027278:
56: 121632: loss: 0.1739694460:
56: 124832: loss: 0.1744148969:
56: 128032: loss: 0.1744448064:
56: 131232: loss: 0.1741938954:
56: 134432: loss: 0.1742892784:
56: 137632: loss: 0.1741429481:
56: 140832: loss: 0.1740427676:
56: 144032: loss: 0.1738385795:
56: 147232: loss: 0.1735770840:
56: 150432: loss: 0.1735167880:
56: 153632: loss: 0.1735396362:
56: 156832: loss: 0.1736572767:
56: 160032: loss: 0.1733834381:
56: 163232: loss: 0.1734563265:
56: 166432: loss: 0.1734003797:
Dev-Acc: 56: Accuracy: 0.9355056286: precision: 0.4397273612: recall: 0.3839483081: f1: 0.4099491649
Train-Acc: 56: Accuracy: 0.9353517890: precision: 0.7493191103: recall: 0.4340937479: f1: 0.5497231820
57: 3232: loss: 0.1692598097:
57: 6432: loss: 0.1681123675:
57: 9632: loss: 0.1726092255:
57: 12832: loss: 0.1754574948:
57: 16032: loss: 0.1742327807:
57: 19232: loss: 0.1735351344:
57: 22432: loss: 0.1727177337:
57: 25632: loss: 0.1731310728:
57: 28832: loss: 0.1719094630:
57: 32032: loss: 0.1729000009:
57: 35232: loss: 0.1733232559:
57: 38432: loss: 0.1732552999:
57: 41632: loss: 0.1735868599:
57: 44832: loss: 0.1730038609:
57: 48032: loss: 0.1736296347:
57: 51232: loss: 0.1737481928:
57: 54432: loss: 0.1740640257:
57: 57632: loss: 0.1748308105:
57: 60832: loss: 0.1751304279:
57: 64032: loss: 0.1759121079:
57: 67232: loss: 0.1761521727:
57: 70432: loss: 0.1760041974:
57: 73632: loss: 0.1767051387:
57: 76832: loss: 0.1763551015:
57: 80032: loss: 0.1755566481:
57: 83232: loss: 0.1752788448:
57: 86432: loss: 0.1749433516:
57: 89632: loss: 0.1752452941:
57: 92832: loss: 0.1752106220:
57: 96032: loss: 0.1749438144:
57: 99232: loss: 0.1745659787:
57: 102432: loss: 0.1747910702:
57: 105632: loss: 0.1750169594:
57: 108832: loss: 0.1746875367:
57: 112032: loss: 0.1745810496:
57: 115232: loss: 0.1741194494:
57: 118432: loss: 0.1737762177:
57: 121632: loss: 0.1732928178:
57: 124832: loss: 0.1731046272:
57: 128032: loss: 0.1731370026:
57: 131232: loss: 0.1730974514:
57: 134432: loss: 0.1729155239:
57: 137632: loss: 0.1728330346:
57: 140832: loss: 0.1727787334:
57: 144032: loss: 0.1732993409:
57: 147232: loss: 0.1733788286:
57: 150432: loss: 0.1731942055:
57: 153632: loss: 0.1729684395:
57: 156832: loss: 0.1730159198:
57: 160032: loss: 0.1728354667:
57: 163232: loss: 0.1728359382:
57: 166432: loss: 0.1725263001:
Dev-Acc: 57: Accuracy: 0.9355453253: precision: 0.4400935126: recall: 0.3841183472: f1: 0.4102051934
Train-Acc: 57: Accuracy: 0.9355370998: precision: 0.7501413228: recall: 0.4361974887: f1: 0.5516295311
58: 3232: loss: 0.1590986789:
58: 6432: loss: 0.1709518763:
58: 9632: loss: 0.1719304358:
58: 12832: loss: 0.1698977647:
58: 16032: loss: 0.1704342472:
58: 19232: loss: 0.1691895368:
58: 22432: loss: 0.1714707949:
58: 25632: loss: 0.1746760468:
58: 28832: loss: 0.1741427844:
58: 32032: loss: 0.1736494859:
58: 35232: loss: 0.1737993723:
58: 38432: loss: 0.1727689429:
58: 41632: loss: 0.1720967192:
58: 44832: loss: 0.1725112834:
58: 48032: loss: 0.1728559433:
58: 51232: loss: 0.1728829685:
58: 54432: loss: 0.1731064034:
58: 57632: loss: 0.1726283520:
58: 60832: loss: 0.1727834747:
58: 64032: loss: 0.1720685596:
58: 67232: loss: 0.1714958244:
58: 70432: loss: 0.1716374060:
58: 73632: loss: 0.1723071591:
58: 76832: loss: 0.1718434167:
58: 80032: loss: 0.1722017827:
58: 83232: loss: 0.1722984347:
58: 86432: loss: 0.1723113932:
58: 89632: loss: 0.1730768339:
58: 92832: loss: 0.1734488820:
58: 96032: loss: 0.1735528294:
58: 99232: loss: 0.1729002887:
58: 102432: loss: 0.1731042097:
58: 105632: loss: 0.1725337364:
58: 108832: loss: 0.1724889316:
58: 112032: loss: 0.1724306972:
58: 115232: loss: 0.1722065058:
58: 118432: loss: 0.1722453508:
58: 121632: loss: 0.1719587918:
58: 124832: loss: 0.1723490706:
58: 128032: loss: 0.1723079284:
58: 131232: loss: 0.1723256516:
58: 134432: loss: 0.1723864072:
58: 137632: loss: 0.1720431626:
58: 140832: loss: 0.1722157343:
58: 144032: loss: 0.1723355194:
58: 147232: loss: 0.1722838536:
58: 150432: loss: 0.1722991077:
58: 153632: loss: 0.1722027980:
58: 156832: loss: 0.1722044602:
58: 160032: loss: 0.1722322446:
58: 163232: loss: 0.1722329238:
58: 166432: loss: 0.1721573244:
Dev-Acc: 58: Accuracy: 0.9354063869: precision: 0.4389912706: recall: 0.3847985037: f1: 0.4101123596
Train-Acc: 58: Accuracy: 0.9356685877: precision: 0.7500843550: recall: 0.4384327132: f1: 0.5533980583
59: 3232: loss: 0.1708127331:
59: 6432: loss: 0.1735749115:
59: 9632: loss: 0.1722840130:
59: 12832: loss: 0.1708908752:
59: 16032: loss: 0.1698591214:
59: 19232: loss: 0.1699291998:
59: 22432: loss: 0.1731670860:
59: 25632: loss: 0.1711515917:
59: 28832: loss: 0.1717128541:
59: 32032: loss: 0.1717750338:
59: 35232: loss: 0.1711208505:
59: 38432: loss: 0.1711996414:
59: 41632: loss: 0.1710698556:
59: 44832: loss: 0.1709864236:
59: 48032: loss: 0.1704138101:
59: 51232: loss: 0.1707559953:
59: 54432: loss: 0.1706403514:
59: 57632: loss: 0.1707746484:
59: 60832: loss: 0.1711333121:
59: 64032: loss: 0.1713211577:
59: 67232: loss: 0.1713182353:
59: 70432: loss: 0.1708716541:
59: 73632: loss: 0.1707570876:
59: 76832: loss: 0.1711629112:
59: 80032: loss: 0.1707622783:
59: 83232: loss: 0.1707700162:
59: 86432: loss: 0.1705847699:
59: 89632: loss: 0.1706645479:
59: 92832: loss: 0.1703272448:
59: 96032: loss: 0.1707307533:
59: 99232: loss: 0.1706444232:
59: 102432: loss: 0.1708424920:
59: 105632: loss: 0.1709304446:
59: 108832: loss: 0.1710253782:
59: 112032: loss: 0.1709425044:
59: 115232: loss: 0.1705462067:
59: 118432: loss: 0.1705853213:
59: 121632: loss: 0.1704817455:
59: 124832: loss: 0.1704046961:
59: 128032: loss: 0.1703046635:
59: 131232: loss: 0.1702930012:
59: 134432: loss: 0.1705332441:
59: 137632: loss: 0.1706791091:
59: 140832: loss: 0.1710476441:
59: 144032: loss: 0.1715581946:
59: 147232: loss: 0.1714495887:
59: 150432: loss: 0.1712111055:
59: 153632: loss: 0.1714090892:
59: 156832: loss: 0.1715705518:
59: 160032: loss: 0.1717554307:
59: 163232: loss: 0.1718649134:
59: 166432: loss: 0.1716688353:
Dev-Acc: 59: Accuracy: 0.9352774024: precision: 0.4380069525: recall: 0.3856486992: f1: 0.4101636676
Train-Acc: 59: Accuracy: 0.9358717799: precision: 0.7501954216: recall: 0.4416540661: f1: 0.5559877514
60: 3232: loss: 0.1638854751:
60: 6432: loss: 0.1718843718:
60: 9632: loss: 0.1675961417:
60: 12832: loss: 0.1700397289:
60: 16032: loss: 0.1711091438:
60: 19232: loss: 0.1715808654:
60: 22432: loss: 0.1702523738:
60: 25632: loss: 0.1720947977:
60: 28832: loss: 0.1718032916:
60: 32032: loss: 0.1706838018:
60: 35232: loss: 0.1705392673:
60: 38432: loss: 0.1698972298:
60: 41632: loss: 0.1701590741:
60: 44832: loss: 0.1716616179:
60: 48032: loss: 0.1708674692:
60: 51232: loss: 0.1707302476:
60: 54432: loss: 0.1713178682:
60: 57632: loss: 0.1716442392:
60: 60832: loss: 0.1707953826:
60: 64032: loss: 0.1700331824:
60: 67232: loss: 0.1706023737:
60: 70432: loss: 0.1708969385:
60: 73632: loss: 0.1704992477:
60: 76832: loss: 0.1708845897:
60: 80032: loss: 0.1706886115:
60: 83232: loss: 0.1703439987:
60: 86432: loss: 0.1705710033:
60: 89632: loss: 0.1702722261:
60: 92832: loss: 0.1709940394:
60: 96032: loss: 0.1714928999:
60: 99232: loss: 0.1709945835:
60: 102432: loss: 0.1706968372:
60: 105632: loss: 0.1704942760:
60: 108832: loss: 0.1709210580:
60: 112032: loss: 0.1709799634:
60: 115232: loss: 0.1706938458:
60: 118432: loss: 0.1711047265:
60: 121632: loss: 0.1708348765:
60: 124832: loss: 0.1710228216:
60: 128032: loss: 0.1709749068:
60: 131232: loss: 0.1714240539:
60: 134432: loss: 0.1714142663:
60: 137632: loss: 0.1718492056:
60: 140832: loss: 0.1717745008:
60: 144032: loss: 0.1716282740:
60: 147232: loss: 0.1715904477:
60: 150432: loss: 0.1715009266:
60: 153632: loss: 0.1715865583:
60: 156832: loss: 0.1716789668:
60: 160032: loss: 0.1714400861:
60: 163232: loss: 0.1713096635:
60: 166432: loss: 0.1711266950:
Dev-Acc: 60: Accuracy: 0.9351484179: precision: 0.4370555449: recall: 0.3866689339: f1: 0.4103211837
Train-Acc: 60: Accuracy: 0.9360271692: precision: 0.7507510849: recall: 0.4435605812: f1: 0.5576493925
61: 3232: loss: 0.1760538384:
61: 6432: loss: 0.1717626961:
61: 9632: loss: 0.1735084189:
61: 12832: loss: 0.1707933208:
61: 16032: loss: 0.1718270659:
61: 19232: loss: 0.1724877430:
61: 22432: loss: 0.1698138456:
61: 25632: loss: 0.1710685480:
61: 28832: loss: 0.1705915916:
61: 32032: loss: 0.1715977837:
61: 35232: loss: 0.1709896091:
61: 38432: loss: 0.1704789609:
61: 41632: loss: 0.1698625663:
61: 44832: loss: 0.1706596241:
61: 48032: loss: 0.1705582587:
61: 51232: loss: 0.1701330163:
61: 54432: loss: 0.1704450726:
61: 57632: loss: 0.1708322983:
61: 60832: loss: 0.1706747449:
61: 64032: loss: 0.1711822755:
61: 67232: loss: 0.1714686649:
61: 70432: loss: 0.1719937207:
61: 73632: loss: 0.1716848917:
61: 76832: loss: 0.1718933347:
61: 80032: loss: 0.1720895465:
61: 83232: loss: 0.1718905762:
61: 86432: loss: 0.1713905962:
61: 89632: loss: 0.1718108381:
61: 92832: loss: 0.1720663504:
61: 96032: loss: 0.1721885084:
61: 99232: loss: 0.1723901732:
61: 102432: loss: 0.1718864239:
61: 105632: loss: 0.1713049794:
61: 108832: loss: 0.1715580737:
61: 112032: loss: 0.1712902234:
61: 115232: loss: 0.1710452502:
61: 118432: loss: 0.1707922268:
61: 121632: loss: 0.1710134142:
61: 124832: loss: 0.1709480930:
61: 128032: loss: 0.1706816110:
61: 131232: loss: 0.1705199513:
61: 134432: loss: 0.1707160749:
61: 137632: loss: 0.1706626249:
61: 140832: loss: 0.1708815080:
61: 144032: loss: 0.1706736172:
61: 147232: loss: 0.1706072550:
61: 150432: loss: 0.1704890529:
61: 153632: loss: 0.1706035449:
61: 156832: loss: 0.1701502022:
61: 160032: loss: 0.1700113786:
61: 163232: loss: 0.1703565024:
61: 166432: loss: 0.1705129767:
Dev-Acc: 61: Accuracy: 0.9351881146: precision: 0.4375359816: recall: 0.3876891685: f1: 0.4111071042
Train-Acc: 61: Accuracy: 0.9362542629: precision: 0.7518005540: recall: 0.4460587733: f1: 0.5599108764
62: 3232: loss: 0.1808835617:
62: 6432: loss: 0.1685789834:
62: 9632: loss: 0.1662951448:
62: 12832: loss: 0.1662798476:
62: 16032: loss: 0.1662137948:
62: 19232: loss: 0.1680074757:
62: 22432: loss: 0.1676988687:
62: 25632: loss: 0.1666895658:
62: 28832: loss: 0.1669563200:
62: 32032: loss: 0.1674860549:
62: 35232: loss: 0.1672033368:
62: 38432: loss: 0.1681229003:
62: 41632: loss: 0.1688916250:
62: 44832: loss: 0.1686566580:
62: 48032: loss: 0.1685011750:
62: 51232: loss: 0.1680032572:
62: 54432: loss: 0.1684937267:
62: 57632: loss: 0.1688658955:
62: 60832: loss: 0.1689813770:
62: 64032: loss: 0.1692392657:
62: 67232: loss: 0.1691011114:
62: 70432: loss: 0.1686543781:
62: 73632: loss: 0.1694455033:
62: 76832: loss: 0.1693964448:
62: 80032: loss: 0.1693486942:
62: 83232: loss: 0.1695799354:
62: 86432: loss: 0.1697053450:
62: 89632: loss: 0.1696356702:
62: 92832: loss: 0.1696972592:
62: 96032: loss: 0.1695131929:
62: 99232: loss: 0.1691346199:
62: 102432: loss: 0.1686492846:
62: 105632: loss: 0.1688021426:
62: 108832: loss: 0.1687793607:
62: 112032: loss: 0.1686250436:
62: 115232: loss: 0.1687119782:
62: 118432: loss: 0.1684564055:
62: 121632: loss: 0.1687370259:
62: 124832: loss: 0.1690303315:
62: 128032: loss: 0.1691072886:
62: 131232: loss: 0.1689613172:
62: 134432: loss: 0.1690463779:
62: 137632: loss: 0.1691033461:
62: 140832: loss: 0.1690130148:
62: 144032: loss: 0.1691659121:
62: 147232: loss: 0.1696323174:
62: 150432: loss: 0.1698533535:
62: 153632: loss: 0.1700102069:
62: 156832: loss: 0.1698881470:
62: 160032: loss: 0.1699868018:
62: 163232: loss: 0.1701031156:
62: 166432: loss: 0.1701042849:
Dev-Acc: 62: Accuracy: 0.9351087213: precision: 0.4369619284: recall: 0.3883693249: f1: 0.4112351458
Train-Acc: 62: Accuracy: 0.9365052581: precision: 0.7528387168: recall: 0.4489514167: f1: 0.5624742608
63: 3232: loss: 0.1581353597:
63: 6432: loss: 0.1625984366:
63: 9632: loss: 0.1638753720:
63: 12832: loss: 0.1634116771:
63: 16032: loss: 0.1612661375:
63: 19232: loss: 0.1613407947:
63: 22432: loss: 0.1602376144:
63: 25632: loss: 0.1618484478:
63: 28832: loss: 0.1629871129:
63: 32032: loss: 0.1649149079:
63: 35232: loss: 0.1647407108:
63: 38432: loss: 0.1652608937:
63: 41632: loss: 0.1654367570:
63: 44832: loss: 0.1649191542:
63: 48032: loss: 0.1650333089:
63: 51232: loss: 0.1653724909:
63: 54432: loss: 0.1661407211:
63: 57632: loss: 0.1667942453:
63: 60832: loss: 0.1672617955:
63: 64032: loss: 0.1671676831:
63: 67232: loss: 0.1669623187:
63: 70432: loss: 0.1672936179:
63: 73632: loss: 0.1668767113:
63: 76832: loss: 0.1668356846:
63: 80032: loss: 0.1671251717:
63: 83232: loss: 0.1676055061:
63: 86432: loss: 0.1681042821:
63: 89632: loss: 0.1682500203:
63: 92832: loss: 0.1683990072:
63: 96032: loss: 0.1681763738:
63: 99232: loss: 0.1682706282:
63: 102432: loss: 0.1681096876:
63: 105632: loss: 0.1681311263:
63: 108832: loss: 0.1681411499:
63: 112032: loss: 0.1679319508:
63: 115232: loss: 0.1678476792:
63: 118432: loss: 0.1681630481:
63: 121632: loss: 0.1679453364:
63: 124832: loss: 0.1682382487:
63: 128032: loss: 0.1683077352:
63: 131232: loss: 0.1683953966:
63: 134432: loss: 0.1685787213:
63: 137632: loss: 0.1688687168:
63: 140832: loss: 0.1688980087:
63: 144032: loss: 0.1686577632:
63: 147232: loss: 0.1687124844:
63: 150432: loss: 0.1687197277:
63: 153632: loss: 0.1687759195:
63: 156832: loss: 0.1689834842:
63: 160032: loss: 0.1691688338:
63: 163232: loss: 0.1689515173:
63: 166432: loss: 0.1692091419:
Dev-Acc: 63: Accuracy: 0.9350789785: precision: 0.4367112811: recall: 0.3883693249: f1: 0.4111241112
Train-Acc: 63: Accuracy: 0.9366367459: precision: 0.7531582995: recall: 0.4507264480: f1: 0.5639549231
64: 3232: loss: 0.1645367455:
64: 6432: loss: 0.1661387534:
64: 9632: loss: 0.1682129343:
64: 12832: loss: 0.1674895162:
64: 16032: loss: 0.1690945472:
64: 19232: loss: 0.1679209036:
64: 22432: loss: 0.1685710910:
64: 25632: loss: 0.1694705080:
64: 28832: loss: 0.1682109982:
64: 32032: loss: 0.1681656032:
64: 35232: loss: 0.1683761751:
64: 38432: loss: 0.1678775786:
64: 41632: loss: 0.1680318500:
64: 44832: loss: 0.1683388042:
64: 48032: loss: 0.1678582915:
64: 51232: loss: 0.1680734751:
64: 54432: loss: 0.1682777134:
64: 57632: loss: 0.1680342826:
64: 60832: loss: 0.1678672474:
64: 64032: loss: 0.1676966460:
64: 67232: loss: 0.1676917676:
64: 70432: loss: 0.1679598468:
64: 73632: loss: 0.1679406459:
64: 76832: loss: 0.1680493373:
64: 80032: loss: 0.1680881703:
64: 83232: loss: 0.1681495103:
64: 86432: loss: 0.1680293883:
64: 89632: loss: 0.1681414172:
64: 92832: loss: 0.1682142643:
64: 96032: loss: 0.1685227471:
64: 99232: loss: 0.1684379258:
64: 102432: loss: 0.1680589749:
64: 105632: loss: 0.1681783874:
64: 108832: loss: 0.1681749126:
64: 112032: loss: 0.1684388189:
64: 115232: loss: 0.1683595991:
64: 118432: loss: 0.1688427366:
64: 121632: loss: 0.1687224596:
64: 124832: loss: 0.1683963958:
64: 128032: loss: 0.1685497006:
64: 131232: loss: 0.1681664184:
64: 134432: loss: 0.1683046303:
64: 137632: loss: 0.1684119827:
64: 140832: loss: 0.1685238301:
64: 144032: loss: 0.1686417950:
64: 147232: loss: 0.1688594427:
64: 150432: loss: 0.1689236701:
64: 153632: loss: 0.1690666776:
64: 156832: loss: 0.1694109788:
64: 160032: loss: 0.1695407402:
64: 163232: loss: 0.1692692194:
64: 166432: loss: 0.1690657210:
Dev-Acc: 64: Accuracy: 0.9350194335: precision: 0.4363324438: recall: 0.3892195205: f1: 0.4114316527
Train-Acc: 64: Accuracy: 0.9368638992: precision: 0.7539621817: recall: 0.4534876077: f1: 0.5663382594
65: 3232: loss: 0.1567849419:
65: 6432: loss: 0.1576385494:
65: 9632: loss: 0.1613886459:
65: 12832: loss: 0.1624157424:
65: 16032: loss: 0.1664300288:
65: 19232: loss: 0.1667325978:
65: 22432: loss: 0.1670121197:
65: 25632: loss: 0.1676099067:
65: 28832: loss: 0.1703547013:
65: 32032: loss: 0.1710750881:
65: 35232: loss: 0.1707657629:
65: 38432: loss: 0.1701651824:
65: 41632: loss: 0.1696531356:
65: 44832: loss: 0.1688616772:
65: 48032: loss: 0.1687559817:
65: 51232: loss: 0.1685505991:
65: 54432: loss: 0.1678965319:
65: 57632: loss: 0.1672598822:
65: 60832: loss: 0.1679660983:
65: 64032: loss: 0.1680482286:
65: 67232: loss: 0.1673441543:
65: 70432: loss: 0.1671460039:
65: 73632: loss: 0.1678260429:
65: 76832: loss: 0.1674292780:
65: 80032: loss: 0.1678761918:
65: 83232: loss: 0.1683678372:
65: 86432: loss: 0.1686300814:
65: 89632: loss: 0.1686267370:
65: 92832: loss: 0.1687481448:
65: 96032: loss: 0.1687712398:
65: 99232: loss: 0.1689710297:
65: 102432: loss: 0.1692569834:
65: 105632: loss: 0.1690252734:
65: 108832: loss: 0.1689865668:
65: 112032: loss: 0.1690775633:
65: 115232: loss: 0.1689710240:
65: 118432: loss: 0.1690552297:
65: 121632: loss: 0.1687208615:
65: 124832: loss: 0.1688220495:
65: 128032: loss: 0.1688389337:
65: 131232: loss: 0.1690442830:
65: 134432: loss: 0.1693701658:
65: 137632: loss: 0.1690212960:
65: 140832: loss: 0.1689925588:
65: 144032: loss: 0.1687497809:
65: 147232: loss: 0.1685447545:
65: 150432: loss: 0.1687263398:
65: 153632: loss: 0.1688490560:
65: 156832: loss: 0.1688328999:
65: 160032: loss: 0.1686302457:
65: 163232: loss: 0.1686262496:
65: 166432: loss: 0.1684596338:
Dev-Acc: 65: Accuracy: 0.9350194335: precision: 0.4364051790: recall: 0.3897296378: f1: 0.4117488548
Train-Acc: 65: Accuracy: 0.9369893670: precision: 0.7541376307: recall: 0.4553283808: f1: 0.5678212749
66: 3232: loss: 0.1799141118:
66: 6432: loss: 0.1761535012:
66: 9632: loss: 0.1731346369:
66: 12832: loss: 0.1721842923:
66: 16032: loss: 0.1683196106:
66: 19232: loss: 0.1697394975:
66: 22432: loss: 0.1697387223:
66: 25632: loss: 0.1685280395:
66: 28832: loss: 0.1701981208:
66: 32032: loss: 0.1702866840:
66: 35232: loss: 0.1706909432:
66: 38432: loss: 0.1695180070:
66: 41632: loss: 0.1689217964:
66: 44832: loss: 0.1690614167:
66: 48032: loss: 0.1694334952:
66: 51232: loss: 0.1688075034:
66: 54432: loss: 0.1687686469:
66: 57632: loss: 0.1688001305:
66: 60832: loss: 0.1686771821:
66: 64032: loss: 0.1692034352:
66: 67232: loss: 0.1693370625:
66: 70432: loss: 0.1693563661:
66: 73632: loss: 0.1694456010:
66: 76832: loss: 0.1691188304:
66: 80032: loss: 0.1691363781:
66: 83232: loss: 0.1688034606:
66: 86432: loss: 0.1692005328:
66: 89632: loss: 0.1686751498:
66: 92832: loss: 0.1681836394:
66: 96032: loss: 0.1681637301:
66: 99232: loss: 0.1681529730:
66: 102432: loss: 0.1682698954:
66: 105632: loss: 0.1682165529:
66: 108832: loss: 0.1683203019:
66: 112032: loss: 0.1686644459:
66: 115232: loss: 0.1687967273:
66: 118432: loss: 0.1683798593:
66: 121632: loss: 0.1683290293:
66: 124832: loss: 0.1680928922:
66: 128032: loss: 0.1683741765:
66: 131232: loss: 0.1682129841:
66: 134432: loss: 0.1682540188:
66: 137632: loss: 0.1683963643:
66: 140832: loss: 0.1688146720:
66: 144032: loss: 0.1685867355:
66: 147232: loss: 0.1684422040:
66: 150432: loss: 0.1685259446:
66: 153632: loss: 0.1685518829:
66: 156832: loss: 0.1683321925:
66: 160032: loss: 0.1681075162:
66: 163232: loss: 0.1679155712:
66: 166432: loss: 0.1680522602:
Dev-Acc: 66: Accuracy: 0.9349301457: precision: 0.4358293839: recall: 0.3909199116: f1: 0.4121548942
Train-Acc: 66: Accuracy: 0.9371567369: precision: 0.7544981574: recall: 0.4576293472: f1: 0.5697098662
67: 3232: loss: 0.1689481574:
67: 6432: loss: 0.1726299188:
67: 9632: loss: 0.1683434573:
67: 12832: loss: 0.1682775019:
67: 16032: loss: 0.1693128264:
67: 19232: loss: 0.1670362401:
67: 22432: loss: 0.1695560905:
67: 25632: loss: 0.1697136995:
67: 28832: loss: 0.1681539228:
67: 32032: loss: 0.1665966123:
67: 35232: loss: 0.1673253978:
67: 38432: loss: 0.1672415908:
67: 41632: loss: 0.1687376857:
67: 44832: loss: 0.1686061644:
67: 48032: loss: 0.1680432580:
67: 51232: loss: 0.1678793466:
67: 54432: loss: 0.1679146671:
67: 57632: loss: 0.1666348197:
67: 60832: loss: 0.1668994382:
67: 64032: loss: 0.1663427561:
67: 67232: loss: 0.1659362887:
67: 70432: loss: 0.1656139793:
67: 73632: loss: 0.1654625556:
67: 76832: loss: 0.1658885377:
67: 80032: loss: 0.1658398228:
67: 83232: loss: 0.1662496866:
67: 86432: loss: 0.1669115712:
67: 89632: loss: 0.1673106311:
67: 92832: loss: 0.1673059307:
67: 96032: loss: 0.1671227620:
67: 99232: loss: 0.1668042702:
67: 102432: loss: 0.1665104391:
67: 105632: loss: 0.1664871587:
67: 108832: loss: 0.1664367645:
67: 112032: loss: 0.1662193531:
67: 115232: loss: 0.1663517046:
67: 118432: loss: 0.1663509533:
67: 121632: loss: 0.1664125894:
67: 124832: loss: 0.1665587485:
67: 128032: loss: 0.1666764546:
67: 131232: loss: 0.1668203043:
67: 134432: loss: 0.1665790434:
67: 137632: loss: 0.1667458678:
67: 140832: loss: 0.1668219777:
67: 144032: loss: 0.1668638459:
67: 147232: loss: 0.1669760864:
67: 150432: loss: 0.1672036365:
67: 153632: loss: 0.1673104945:
67: 156832: loss: 0.1671816614:
67: 160032: loss: 0.1674500947:
67: 163232: loss: 0.1673025718:
67: 166432: loss: 0.1674103008:
Dev-Acc: 67: Accuracy: 0.9348011613: precision: 0.4349302150: recall: 0.3921101853: f1: 0.4124116963
Train-Acc: 67: Accuracy: 0.9373300672: precision: 0.7551571444: recall: 0.4596673460: f1: 0.5714752758
68: 3232: loss: 0.1662635983:
68: 6432: loss: 0.1715790159:
68: 9632: loss: 0.1739099203:
68: 12832: loss: 0.1704779187:
68: 16032: loss: 0.1677068521:
68: 19232: loss: 0.1694906185:
68: 22432: loss: 0.1687194502:
68: 25632: loss: 0.1667835576:
68: 28832: loss: 0.1687145514:
68: 32032: loss: 0.1695825842:
68: 35232: loss: 0.1682633123:
68: 38432: loss: 0.1678733179:
68: 41632: loss: 0.1681615351:
68: 44832: loss: 0.1694207061:
68: 48032: loss: 0.1697189398:
68: 51232: loss: 0.1691854926:
68: 54432: loss: 0.1692400259:
68: 57632: loss: 0.1689072650:
68: 60832: loss: 0.1689771775:
68: 64032: loss: 0.1686228888:
68: 67232: loss: 0.1680688275:
68: 70432: loss: 0.1679412944:
68: 73632: loss: 0.1678765917:
68: 76832: loss: 0.1679012457:
68: 80032: loss: 0.1678427005:
68: 83232: loss: 0.1677820332:
68: 86432: loss: 0.1670908100:
68: 89632: loss: 0.1671315650:
68: 92832: loss: 0.1668592778:
68: 96032: loss: 0.1669763675:
68: 99232: loss: 0.1669968689:
68: 102432: loss: 0.1670875231:
68: 105632: loss: 0.1672838890:
68: 108832: loss: 0.1673271275:
68: 112032: loss: 0.1675392476:
68: 115232: loss: 0.1675824265:
68: 118432: loss: 0.1674082492:
68: 121632: loss: 0.1672770524:
68: 124832: loss: 0.1673940052:
68: 128032: loss: 0.1672526467:
68: 131232: loss: 0.1674705929:
68: 134432: loss: 0.1672958880:
68: 137632: loss: 0.1673141246:
68: 140832: loss: 0.1673831849:
68: 144032: loss: 0.1672381721:
68: 147232: loss: 0.1668260743:
68: 150432: loss: 0.1670291617:
68: 153632: loss: 0.1668854759:
68: 156832: loss: 0.1668289069:
68: 160032: loss: 0.1670378605:
68: 163232: loss: 0.1668728948:
68: 166432: loss: 0.1671184279:
Dev-Acc: 68: Accuracy: 0.9347614646: precision: 0.4347007904: recall: 0.3927903418: f1: 0.4126842340
Train-Acc: 68: Accuracy: 0.9374854565: precision: 0.7555675094: recall: 0.4617053448: f1: 0.5731657553
69: 3232: loss: 0.1758536334:
69: 6432: loss: 0.1717810049:
69: 9632: loss: 0.1701530875:
69: 12832: loss: 0.1682248955:
69: 16032: loss: 0.1655242283:
69: 19232: loss: 0.1653967409:
69: 22432: loss: 0.1665860531:
69: 25632: loss: 0.1669020779:
69: 28832: loss: 0.1674988756:
69: 32032: loss: 0.1663758030:
69: 35232: loss: 0.1647712330:
69: 38432: loss: 0.1646119664:
69: 41632: loss: 0.1647930916:
69: 44832: loss: 0.1644868590:
69: 48032: loss: 0.1640028751:
69: 51232: loss: 0.1642165151:
69: 54432: loss: 0.1634319451:
69: 57632: loss: 0.1639596295:
69: 60832: loss: 0.1636475874:
69: 64032: loss: 0.1633503822:
69: 67232: loss: 0.1636630872:
69: 70432: loss: 0.1634429605:
69: 73632: loss: 0.1643032319:
69: 76832: loss: 0.1640908601:
69: 80032: loss: 0.1642748922:
69: 83232: loss: 0.1643721428:
69: 86432: loss: 0.1643225825:
69: 89632: loss: 0.1648141482:
69: 92832: loss: 0.1650190422:
69: 96032: loss: 0.1652180222:
69: 99232: loss: 0.1652851674:
69: 102432: loss: 0.1652304128:
69: 105632: loss: 0.1653647442:
69: 108832: loss: 0.1655517312:
69: 112032: loss: 0.1658023905:
69: 115232: loss: 0.1655425501:
69: 118432: loss: 0.1656327854:
69: 121632: loss: 0.1659508897:
69: 124832: loss: 0.1661084879:
69: 128032: loss: 0.1660989877:
69: 131232: loss: 0.1659558920:
69: 134432: loss: 0.1663182137:
69: 137632: loss: 0.1662597572:
69: 140832: loss: 0.1659535511:
69: 144032: loss: 0.1659324100:
69: 147232: loss: 0.1659619135:
69: 150432: loss: 0.1658250192:
69: 153632: loss: 0.1662297610:
69: 156832: loss: 0.1662161361:
69: 160032: loss: 0.1663389953:
69: 163232: loss: 0.1663863014:
69: 166432: loss: 0.1665490272:
Dev-Acc: 69: Accuracy: 0.9346622229: precision: 0.4339587242: recall: 0.3933004591: f1: 0.4126304522
Train-Acc: 69: Accuracy: 0.9376288652: precision: 0.7561420448: recall: 0.4633488922: f1: 0.5745964455
70: 3232: loss: 0.1557344016:
70: 6432: loss: 0.1619351371:
70: 9632: loss: 0.1668915485:
70: 12832: loss: 0.1666927696:
70: 16032: loss: 0.1661188900:
70: 19232: loss: 0.1665760457:
70: 22432: loss: 0.1672628564:
70: 25632: loss: 0.1664499669:
70: 28832: loss: 0.1650439481:
70: 32032: loss: 0.1669605027:
70: 35232: loss: 0.1673345561:
70: 38432: loss: 0.1672303236:
70: 41632: loss: 0.1679784986:
70: 44832: loss: 0.1680104471:
70: 48032: loss: 0.1674447133:
70: 51232: loss: 0.1671109054:
70: 54432: loss: 0.1680564464:
70: 57632: loss: 0.1681631466:
70: 60832: loss: 0.1679327100:
70: 64032: loss: 0.1676508625:
70: 67232: loss: 0.1675609068:
70: 70432: loss: 0.1678744926:
70: 73632: loss: 0.1674515525:
70: 76832: loss: 0.1679791714:
70: 80032: loss: 0.1678102728:
70: 83232: loss: 0.1673122482:
70: 86432: loss: 0.1673638643:
70: 89632: loss: 0.1670591672:
70: 92832: loss: 0.1671194024:
70: 96032: loss: 0.1668192070:
70: 99232: loss: 0.1665660707:
70: 102432: loss: 0.1665305005:
70: 105632: loss: 0.1664900901:
70: 108832: loss: 0.1663193107:
70: 112032: loss: 0.1662947416:
70: 115232: loss: 0.1659710092:
70: 118432: loss: 0.1657192905:
70: 121632: loss: 0.1655027461:
70: 124832: loss: 0.1654213622:
70: 128032: loss: 0.1653371418:
70: 131232: loss: 0.1651309965:
70: 134432: loss: 0.1654868354:
70: 137632: loss: 0.1655184735:
70: 140832: loss: 0.1657504792:
70: 144032: loss: 0.1659046558:
70: 147232: loss: 0.1657789784:
70: 150432: loss: 0.1656808896:
70: 153632: loss: 0.1655209323:
70: 156832: loss: 0.1657069382:
70: 160032: loss: 0.1656449355:
70: 163232: loss: 0.1658684947:
70: 166432: loss: 0.1659638427:
Dev-Acc: 70: Accuracy: 0.9346622229: precision: 0.4340329835: recall: 0.3938105764: f1: 0.4129446376
Train-Acc: 70: Accuracy: 0.9377065897: precision: 0.7564267352: recall: 0.4642692788: f1: 0.5753859942
71: 3232: loss: 0.1728636762:
71: 6432: loss: 0.1658399977:
71: 9632: loss: 0.1670975223:
71: 12832: loss: 0.1675925553:
71: 16032: loss: 0.1684235004:
71: 19232: loss: 0.1693566717:
71: 22432: loss: 0.1683709852:
71: 25632: loss: 0.1685607570:
71: 28832: loss: 0.1675754808:
71: 32032: loss: 0.1661994816:
71: 35232: loss: 0.1654674245:
71: 38432: loss: 0.1656669824:
71: 41632: loss: 0.1649132559:
71: 44832: loss: 0.1653160893:
71: 48032: loss: 0.1651433817:
71: 51232: loss: 0.1646630735:
71: 54432: loss: 0.1644227128:
71: 57632: loss: 0.1648860863:
71: 60832: loss: 0.1644879899:
71: 64032: loss: 0.1646012388:
71: 67232: loss: 0.1645324885:
71: 70432: loss: 0.1642019998:
71: 73632: loss: 0.1640160691:
71: 76832: loss: 0.1645367527:
71: 80032: loss: 0.1645280873:
71: 83232: loss: 0.1644238588:
71: 86432: loss: 0.1644203770:
71: 89632: loss: 0.1647267414:
71: 92832: loss: 0.1643260541:
71: 96032: loss: 0.1641337216:
71: 99232: loss: 0.1645536068:
71: 102432: loss: 0.1650216813:
71: 105632: loss: 0.1653648977:
71: 108832: loss: 0.1653738912:
71: 112032: loss: 0.1652678281:
71: 115232: loss: 0.1655568512:
71: 118432: loss: 0.1653428442:
71: 121632: loss: 0.1654952501:
71: 124832: loss: 0.1656070048:
71: 128032: loss: 0.1652657989:
71: 131232: loss: 0.1652996071:
71: 134432: loss: 0.1649374331:
71: 137632: loss: 0.1650931577:
71: 140832: loss: 0.1651251708:
71: 144032: loss: 0.1649989681:
71: 147232: loss: 0.1649626647:
71: 150432: loss: 0.1650830012:
71: 153632: loss: 0.1651966850:
71: 156832: loss: 0.1651446843:
71: 160032: loss: 0.1653544997:
71: 163232: loss: 0.1653233846:
71: 166432: loss: 0.1655159247:
Dev-Acc: 71: Accuracy: 0.9346820712: precision: 0.4343434343: recall: 0.3948308111: f1: 0.4136456756
Train-Acc: 71: Accuracy: 0.9378559589: precision: 0.7568029026: recall: 0.4662415357: f1: 0.5770075665
72: 3232: loss: 0.1736610576:
72: 6432: loss: 0.1726642047:
72: 9632: loss: 0.1728809816:
72: 12832: loss: 0.1760626270:
72: 16032: loss: 0.1741340933:
72: 19232: loss: 0.1714198041:
72: 22432: loss: 0.1707511076:
72: 25632: loss: 0.1691191200:
72: 28832: loss: 0.1682048643:
72: 32032: loss: 0.1690442137:
72: 35232: loss: 0.1682351385:
72: 38432: loss: 0.1680506732:
72: 41632: loss: 0.1677570002:
72: 44832: loss: 0.1666112527:
72: 48032: loss: 0.1669875305:
72: 51232: loss: 0.1673961660:
72: 54432: loss: 0.1669927478:
72: 57632: loss: 0.1673155032:
72: 60832: loss: 0.1668857617:
72: 64032: loss: 0.1675942236:
72: 67232: loss: 0.1676686138:
72: 70432: loss: 0.1683632702:
72: 73632: loss: 0.1681387384:
72: 76832: loss: 0.1679142549:
72: 80032: loss: 0.1679191083:
72: 83232: loss: 0.1680114739:
72: 86432: loss: 0.1680507775:
72: 89632: loss: 0.1677129451:
72: 92832: loss: 0.1674908582:
72: 96032: loss: 0.1672252347:
72: 99232: loss: 0.1668440132:
72: 102432: loss: 0.1667892670:
72: 105632: loss: 0.1669703697:
72: 108832: loss: 0.1667935664:
72: 112032: loss: 0.1665735182:
72: 115232: loss: 0.1663717062:
72: 118432: loss: 0.1665309167:
72: 121632: loss: 0.1663538781:
72: 124832: loss: 0.1658661861:
72: 128032: loss: 0.1656759838:
72: 131232: loss: 0.1656554575:
72: 134432: loss: 0.1656604594:
72: 137632: loss: 0.1656209044:
72: 140832: loss: 0.1660621529:
72: 144032: loss: 0.1657364763:
72: 147232: loss: 0.1655977243:
72: 150432: loss: 0.1658118699:
72: 153632: loss: 0.1654843576:
72: 156832: loss: 0.1653015005:
72: 160032: loss: 0.1653729058:
72: 163232: loss: 0.1653610096:
72: 166432: loss: 0.1652577974:
Dev-Acc: 72: Accuracy: 0.9346721768: precision: 0.4341883542: recall: 0.3943206938: f1: 0.4132953128
Train-Acc: 72: Accuracy: 0.9379277229: precision: 0.7571139294: recall: 0.4670304385: f1: 0.5777018785
73: 3232: loss: 0.1523228383:
73: 6432: loss: 0.1569938484:
73: 9632: loss: 0.1596089898:
73: 12832: loss: 0.1589943586:
73: 16032: loss: 0.1605995537:
73: 19232: loss: 0.1600024245:
73: 22432: loss: 0.1609532768:
73: 25632: loss: 0.1624625036:
73: 28832: loss: 0.1616385674:
73: 32032: loss: 0.1617387858:
73: 35232: loss: 0.1623999250:
73: 38432: loss: 0.1618933213:
73: 41632: loss: 0.1626986795:
73: 44832: loss: 0.1628387424:
73: 48032: loss: 0.1631498476:
73: 51232: loss: 0.1627890133:
73: 54432: loss: 0.1634992095:
73: 57632: loss: 0.1623229619:
73: 60832: loss: 0.1627537343:
73: 64032: loss: 0.1624468150:
73: 67232: loss: 0.1624966504:
73: 70432: loss: 0.1633302945:
73: 73632: loss: 0.1629400083:
73: 76832: loss: 0.1626554328:
73: 80032: loss: 0.1623452111:
73: 83232: loss: 0.1626870415:
73: 86432: loss: 0.1625722110:
73: 89632: loss: 0.1626524803:
73: 92832: loss: 0.1628267762:
73: 96032: loss: 0.1626592084:
73: 99232: loss: 0.1627838362:
73: 102432: loss: 0.1629613642:
73: 105632: loss: 0.1629475325:
73: 108832: loss: 0.1631474673:
73: 112032: loss: 0.1629294760:
73: 115232: loss: 0.1632268488:
73: 118432: loss: 0.1633609542:
73: 121632: loss: 0.1632900868:
73: 124832: loss: 0.1634408788:
73: 128032: loss: 0.1634194407:
73: 131232: loss: 0.1633945626:
73: 134432: loss: 0.1635643416:
73: 137632: loss: 0.1638524945:
73: 140832: loss: 0.1641441934:
73: 144032: loss: 0.1642273895:
73: 147232: loss: 0.1643196778:
73: 150432: loss: 0.1641562874:
73: 153632: loss: 0.1643923637:
73: 156832: loss: 0.1643906177:
73: 160032: loss: 0.1644040637:
73: 163232: loss: 0.1645406487:
73: 166432: loss: 0.1648591701:
Dev-Acc: 73: Accuracy: 0.9345233440: precision: 0.4332217262: recall: 0.3960210848: f1: 0.4137869770
Train-Acc: 73: Accuracy: 0.9381787181: precision: 0.7577587120: recall: 0.4703175334: f1: 0.5803991563
74: 3232: loss: 0.1783085879:
74: 6432: loss: 0.1659072499:
74: 9632: loss: 0.1690361206:
74: 12832: loss: 0.1665907805:
74: 16032: loss: 0.1635840930:
74: 19232: loss: 0.1619736704:
74: 22432: loss: 0.1629503719:
74: 25632: loss: 0.1641491335:
74: 28832: loss: 0.1645887021:
74: 32032: loss: 0.1647810635:
74: 35232: loss: 0.1645147219:
74: 38432: loss: 0.1649002895:
74: 41632: loss: 0.1642456612:
74: 44832: loss: 0.1637389079:
74: 48032: loss: 0.1646616115:
74: 51232: loss: 0.1654064267:
74: 54432: loss: 0.1657600792:
74: 57632: loss: 0.1652855375:
74: 60832: loss: 0.1652189660:
74: 64032: loss: 0.1652189964:
74: 67232: loss: 0.1649589128:
74: 70432: loss: 0.1642944444:
74: 73632: loss: 0.1642774317:
74: 76832: loss: 0.1651561022:
74: 80032: loss: 0.1644024563:
74: 83232: loss: 0.1640641297:
74: 86432: loss: 0.1640448041:
74: 89632: loss: 0.1643779459:
74: 92832: loss: 0.1644223190:
74: 96032: loss: 0.1640608541:
74: 99232: loss: 0.1643405338:
74: 102432: loss: 0.1645605420:
74: 105632: loss: 0.1646401819:
74: 108832: loss: 0.1645560598:
74: 112032: loss: 0.1643866090:
74: 115232: loss: 0.1648177395:
74: 118432: loss: 0.1645339577:
74: 121632: loss: 0.1645854601:
74: 124832: loss: 0.1647065634:
74: 128032: loss: 0.1645564480:
74: 131232: loss: 0.1645077165:
74: 134432: loss: 0.1646706944:
74: 137632: loss: 0.1647513646:
74: 140832: loss: 0.1645344416:
74: 144032: loss: 0.1643139634:
74: 147232: loss: 0.1643773558:
74: 150432: loss: 0.1642527994:
74: 153632: loss: 0.1641509203:
74: 156832: loss: 0.1643811834:
74: 160032: loss: 0.1644929673:
74: 163232: loss: 0.1644479383:
74: 166432: loss: 0.1646021229:
Dev-Acc: 74: Accuracy: 0.9345133901: precision: 0.4331908567: recall: 0.3963611631: f1: 0.4139584443
Train-Acc: 74: Accuracy: 0.9383341074: precision: 0.7586425626: recall: 0.4717638551: f1: 0.5817592217
75: 3232: loss: 0.1679238785:
75: 6432: loss: 0.1666973349:
75: 9632: loss: 0.1710846970:
75: 12832: loss: 0.1743183483:
75: 16032: loss: 0.1748301652:
75: 19232: loss: 0.1702747841:
75: 22432: loss: 0.1682437871:
75: 25632: loss: 0.1661940352:
75: 28832: loss: 0.1651653120:
75: 32032: loss: 0.1656646007:
75: 35232: loss: 0.1643372291:
75: 38432: loss: 0.1639713689:
75: 41632: loss: 0.1644157441:
75: 44832: loss: 0.1633381549:
75: 48032: loss: 0.1639576760:
75: 51232: loss: 0.1640390394:
75: 54432: loss: 0.1652365985:
75: 57632: loss: 0.1654943285:
75: 60832: loss: 0.1655525503:
75: 64032: loss: 0.1656657427:
75: 67232: loss: 0.1658767456:
75: 70432: loss: 0.1656763749:
75: 73632: loss: 0.1647991468:
75: 76832: loss: 0.1645398673:
75: 80032: loss: 0.1642288055:
75: 83232: loss: 0.1634251044:
75: 86432: loss: 0.1642217579:
75: 89632: loss: 0.1640819994:
75: 92832: loss: 0.1645105190:
75: 96032: loss: 0.1646645074:
75: 99232: loss: 0.1645577980:
75: 102432: loss: 0.1647628283:
75: 105632: loss: 0.1651492858:
75: 108832: loss: 0.1648448790:
75: 112032: loss: 0.1645823157:
75: 115232: loss: 0.1642474900:
75: 118432: loss: 0.1643881678:
75: 121632: loss: 0.1647108516:
75: 124832: loss: 0.1642435257:
75: 128032: loss: 0.1644865790:
75: 131232: loss: 0.1641557807:
75: 134432: loss: 0.1642167791:
75: 137632: loss: 0.1639980192:
75: 140832: loss: 0.1639747762:
75: 144032: loss: 0.1638308493:
75: 147232: loss: 0.1641482152:
75: 150432: loss: 0.1643842970:
75: 153632: loss: 0.1642026086:
75: 156832: loss: 0.1643286062:
75: 160032: loss: 0.1641989934:
75: 163232: loss: 0.1640374388:
75: 166432: loss: 0.1639244355:
Dev-Acc: 75: Accuracy: 0.9345332384: precision: 0.4335002782: recall: 0.3973813977: f1: 0.4146557842
Train-Acc: 75: Accuracy: 0.9384177923: precision: 0.7591086704: recall: 0.4725527579: f1: 0.5824959481
76: 3232: loss: 0.1552789488:
76: 6432: loss: 0.1588328086:
76: 9632: loss: 0.1621675715:
76: 12832: loss: 0.1617053920:
76: 16032: loss: 0.1625587626:
76: 19232: loss: 0.1613645805:
76: 22432: loss: 0.1607843136:
76: 25632: loss: 0.1616938374:
76: 28832: loss: 0.1627558950:
76: 32032: loss: 0.1626361905:
76: 35232: loss: 0.1623775168:
76: 38432: loss: 0.1623950538:
76: 41632: loss: 0.1630252256:
76: 44832: loss: 0.1637660471:
76: 48032: loss: 0.1638092503:
76: 51232: loss: 0.1629272625:
76: 54432: loss: 0.1632361338:
76: 57632: loss: 0.1629099768:
76: 60832: loss: 0.1634508059:
76: 64032: loss: 0.1635935657:
76: 67232: loss: 0.1638001261:
76: 70432: loss: 0.1642412893:
76: 73632: loss: 0.1639470775:
76: 76832: loss: 0.1632464142:
76: 80032: loss: 0.1633617628:
76: 83232: loss: 0.1637336254:
76: 86432: loss: 0.1637195467:
76: 89632: loss: 0.1641902774:
76: 92832: loss: 0.1648091136:
76: 96032: loss: 0.1644048227:
76: 99232: loss: 0.1643294785:
76: 102432: loss: 0.1644251112:
76: 105632: loss: 0.1642343846:
76: 108832: loss: 0.1643015937:
76: 112032: loss: 0.1641324787:
76: 115232: loss: 0.1640496930:
76: 118432: loss: 0.1638672925:
76: 121632: loss: 0.1637732729:
76: 124832: loss: 0.1635356716:
76: 128032: loss: 0.1640592192:
76: 131232: loss: 0.1639661163:
76: 134432: loss: 0.1636135117:
76: 137632: loss: 0.1636327870:
76: 140832: loss: 0.1637830630:
76: 144032: loss: 0.1637278619:
76: 147232: loss: 0.1638348964:
76: 150432: loss: 0.1638253267:
76: 153632: loss: 0.1637402148:
76: 156832: loss: 0.1637241533:
76: 160032: loss: 0.1637858466:
76: 163232: loss: 0.1635052057:
76: 166432: loss: 0.1633932344:
Dev-Acc: 76: Accuracy: 0.9345133901: precision: 0.4335121139: recall: 0.3985716715: f1: 0.4153082920
Train-Acc: 76: Accuracy: 0.9385791421: precision: 0.7601771778: recall: 0.4738675958: f1: 0.5838091767
77: 3232: loss: 0.1740146035:
77: 6432: loss: 0.1679133542:
77: 9632: loss: 0.1651251729:
77: 12832: loss: 0.1617061875:
77: 16032: loss: 0.1630555381:
77: 19232: loss: 0.1641958866:
77: 22432: loss: 0.1653309170:
77: 25632: loss: 0.1647537669:
77: 28832: loss: 0.1637757724:
77: 32032: loss: 0.1625756740:
77: 35232: loss: 0.1615208102:
77: 38432: loss: 0.1623607297:
77: 41632: loss: 0.1629159536:
77: 44832: loss: 0.1619067840:
77: 48032: loss: 0.1619723686:
77: 51232: loss: 0.1627655262:
77: 54432: loss: 0.1627105496:
77: 57632: loss: 0.1631163886:
77: 60832: loss: 0.1630563433:
77: 64032: loss: 0.1637857285:
77: 67232: loss: 0.1636586025:
77: 70432: loss: 0.1637457142:
77: 73632: loss: 0.1633736173:
77: 76832: loss: 0.1636197660:
77: 80032: loss: 0.1641634889:
77: 83232: loss: 0.1637631002:
77: 86432: loss: 0.1638644561:
77: 89632: loss: 0.1636728611:
77: 92832: loss: 0.1632247462:
77: 96032: loss: 0.1635033700:
77: 99232: loss: 0.1631616455:
77: 102432: loss: 0.1631067641:
77: 105632: loss: 0.1631193023:
77: 108832: loss: 0.1631443941:
77: 112032: loss: 0.1629576322:
77: 115232: loss: 0.1629867324:
77: 118432: loss: 0.1630625666:
77: 121632: loss: 0.1628143739:
77: 124832: loss: 0.1628095767:
77: 128032: loss: 0.1628806225:
77: 131232: loss: 0.1630440781:
77: 134432: loss: 0.1629623435:
77: 137632: loss: 0.1631170788:
77: 140832: loss: 0.1633802840:
77: 144032: loss: 0.1628766000:
77: 147232: loss: 0.1626453190:
77: 150432: loss: 0.1624901080:
77: 153632: loss: 0.1623973699:
77: 156832: loss: 0.1625834222:
77: 160032: loss: 0.1627204735:
77: 163232: loss: 0.1627470137:
77: 166432: loss: 0.1629498314:
Dev-Acc: 77: Accuracy: 0.9344836473: precision: 0.4335419735: recall: 0.4004421017: f1: 0.4163351896
Train-Acc: 77: Accuracy: 0.9386867285: precision: 0.7601386846: recall: 0.4756426270: f1: 0.5851429496
78: 3232: loss: 0.1657379426:
78: 6432: loss: 0.1599394246:
78: 9632: loss: 0.1624744471:
78: 12832: loss: 0.1608642360:
78: 16032: loss: 0.1615131098:
78: 19232: loss: 0.1614822893:
78: 22432: loss: 0.1624672425:
78: 25632: loss: 0.1624265607:
78: 28832: loss: 0.1622814968:
78: 32032: loss: 0.1643226148:
78: 35232: loss: 0.1647842961:
78: 38432: loss: 0.1649616411:
78: 41632: loss: 0.1660858260:
78: 44832: loss: 0.1656766889:
78: 48032: loss: 0.1667840124:
78: 51232: loss: 0.1662253973:
78: 54432: loss: 0.1658723957:
78: 57632: loss: 0.1652893784:
78: 60832: loss: 0.1650251483:
78: 64032: loss: 0.1654309309:
78: 67232: loss: 0.1650315061:
78: 70432: loss: 0.1647910385:
78: 73632: loss: 0.1648285531:
78: 76832: loss: 0.1641915098:
78: 80032: loss: 0.1637940636:
78: 83232: loss: 0.1637422099:
78: 86432: loss: 0.1635281064:
78: 89632: loss: 0.1634688749:
78: 92832: loss: 0.1633385571:
78: 96032: loss: 0.1630985773:
78: 99232: loss: 0.1629298606:
78: 102432: loss: 0.1630822983:
78: 105632: loss: 0.1632867582:
78: 108832: loss: 0.1628952809:
78: 112032: loss: 0.1627640461:
78: 115232: loss: 0.1632410399:
78: 118432: loss: 0.1629836292:
78: 121632: loss: 0.1631043190:
78: 124832: loss: 0.1631122009:
78: 128032: loss: 0.1634431252:
78: 131232: loss: 0.1636069131:
78: 134432: loss: 0.1633176027:
78: 137632: loss: 0.1633183355:
78: 140832: loss: 0.1630555476:
78: 144032: loss: 0.1630511476:
78: 147232: loss: 0.1631725748:
78: 150432: loss: 0.1632864723:
78: 153632: loss: 0.1630870081:
78: 156832: loss: 0.1631230587:
78: 160032: loss: 0.1629341012:
78: 163232: loss: 0.1629062845:
78: 166432: loss: 0.1625040500:
Dev-Acc: 78: Accuracy: 0.9344339967: precision: 0.4331924279: recall: 0.4007821799: f1: 0.4163575340
Train-Acc: 78: Accuracy: 0.9388002753: precision: 0.7606712113: recall: 0.4768259812: f1: 0.5861957488
79: 3232: loss: 0.1675158314:
79: 6432: loss: 0.1687070172:
79: 9632: loss: 0.1671681338:
79: 12832: loss: 0.1675685958:
79: 16032: loss: 0.1655343767:
79: 19232: loss: 0.1650303593:
79: 22432: loss: 0.1657519076:
79: 25632: loss: 0.1642617683:
79: 28832: loss: 0.1643426867:
79: 32032: loss: 0.1651217409:
79: 35232: loss: 0.1654740648:
79: 38432: loss: 0.1645061348:
79: 41632: loss: 0.1651868935:
79: 44832: loss: 0.1651686868:
79: 48032: loss: 0.1655633117:
79: 51232: loss: 0.1649218387:
79: 54432: loss: 0.1643865737:
79: 57632: loss: 0.1645943907:
79: 60832: loss: 0.1637078249:
79: 64032: loss: 0.1642277563:
79: 67232: loss: 0.1640496185:
79: 70432: loss: 0.1638442907:
79: 73632: loss: 0.1633174155:
79: 76832: loss: 0.1632177638:
79: 80032: loss: 0.1631180028:
79: 83232: loss: 0.1629857820:
79: 86432: loss: 0.1631736330:
79: 89632: loss: 0.1632591314:
79: 92832: loss: 0.1629650942:
79: 96032: loss: 0.1629256284:
79: 99232: loss: 0.1631508205:
79: 102432: loss: 0.1631401293:
79: 105632: loss: 0.1629844307:
79: 108832: loss: 0.1631631238:
79: 112032: loss: 0.1632970698:
79: 115232: loss: 0.1630996959:
79: 118432: loss: 0.1629532184:
79: 121632: loss: 0.1627167033:
79: 124832: loss: 0.1627076484:
79: 128032: loss: 0.1627813273:
79: 131232: loss: 0.1630082690:
79: 134432: loss: 0.1631172939:
79: 137632: loss: 0.1631238826:
79: 140832: loss: 0.1630395925:
79: 144032: loss: 0.1628004979:
79: 147232: loss: 0.1627966215:
79: 150432: loss: 0.1629296641:
79: 153632: loss: 0.1627631749:
79: 156832: loss: 0.1627034700:
79: 160032: loss: 0.1626025942:
79: 163232: loss: 0.1623726943:
79: 166432: loss: 0.1622947627:
Dev-Acc: 79: Accuracy: 0.9343745112: precision: 0.4327399523: recall: 0.4009522190: f1: 0.4162400706
Train-Acc: 79: Accuracy: 0.9389197826: precision: 0.7612815412: recall: 0.4780093353: f1: 0.5872708182
80: 3232: loss: 0.1528301053:
80: 6432: loss: 0.1578080281:
80: 9632: loss: 0.1534099308:
80: 12832: loss: 0.1540953659:
80: 16032: loss: 0.1551995969:
80: 19232: loss: 0.1553422906:
80: 22432: loss: 0.1546489406:
80: 25632: loss: 0.1551095363:
80: 28832: loss: 0.1556941304:
80: 32032: loss: 0.1555894779:
80: 35232: loss: 0.1548792633:
80: 38432: loss: 0.1554390091:
80: 41632: loss: 0.1554340057:
80: 44832: loss: 0.1562453540:
80: 48032: loss: 0.1555548404:
80: 51232: loss: 0.1563330379:
80: 54432: loss: 0.1569982659:
80: 57632: loss: 0.1567752017:
80: 60832: loss: 0.1574875279:
80: 64032: loss: 0.1576472635:
80: 67232: loss: 0.1582721657:
80: 70432: loss: 0.1587181460:
80: 73632: loss: 0.1591572090:
80: 76832: loss: 0.1595402416:
80: 80032: loss: 0.1593251130:
80: 83232: loss: 0.1599124367:
80: 86432: loss: 0.1601881877:
80: 89632: loss: 0.1598139472:
80: 92832: loss: 0.1596585162:
80: 96032: loss: 0.1595708988:
80: 99232: loss: 0.1598437226:
80: 102432: loss: 0.1598322652:
80: 105632: loss: 0.1596658444:
80: 108832: loss: 0.1596451569:
80: 112032: loss: 0.1598247339:
80: 115232: loss: 0.1602556401:
80: 118432: loss: 0.1603827330:
80: 121632: loss: 0.1607066710:
80: 124832: loss: 0.1605792916:
80: 128032: loss: 0.1607458338:
80: 131232: loss: 0.1608019892:
80: 134432: loss: 0.1607430977:
80: 137632: loss: 0.1606144886:
80: 140832: loss: 0.1606879918:
80: 144032: loss: 0.1610540734:
80: 147232: loss: 0.1611696683:
80: 150432: loss: 0.1612134268:
80: 153632: loss: 0.1613421954:
80: 156832: loss: 0.1615158729:
80: 160032: loss: 0.1616349277:
80: 163232: loss: 0.1619403386:
80: 166432: loss: 0.1618157635:
Dev-Acc: 80: Accuracy: 0.9342554212: precision: 0.4319137269: recall: 0.4018024146: f1: 0.4163143058
Train-Acc: 80: Accuracy: 0.9390154481: precision: 0.7611348701: recall: 0.4797186247: f1: 0.5885152028
81: 3232: loss: 0.1634787192:
81: 6432: loss: 0.1544389620:
81: 9632: loss: 0.1526397089:
81: 12832: loss: 0.1546463966:
81: 16032: loss: 0.1584214393:
81: 19232: loss: 0.1611664873:
81: 22432: loss: 0.1608913310:
81: 25632: loss: 0.1594642853:
81: 28832: loss: 0.1595698104:
81: 32032: loss: 0.1602581353:
81: 35232: loss: 0.1601512339:
81: 38432: loss: 0.1590597509:
81: 41632: loss: 0.1600072825:
81: 44832: loss: 0.1607487759:
81: 48032: loss: 0.1610388274:
81: 51232: loss: 0.1612625168:
81: 54432: loss: 0.1612619736:
81: 57632: loss: 0.1604082601:
81: 60832: loss: 0.1593240528:
81: 64032: loss: 0.1589630942:
81: 67232: loss: 0.1595420584:
81: 70432: loss: 0.1602915742:
81: 73632: loss: 0.1601425001:
81: 76832: loss: 0.1606131433:
81: 80032: loss: 0.1607938604:
81: 83232: loss: 0.1606952789:
81: 86432: loss: 0.1607197665:
81: 89632: loss: 0.1606981087:
81: 92832: loss: 0.1609314657:
81: 96032: loss: 0.1609467171:
81: 99232: loss: 0.1610114783:
81: 102432: loss: 0.1608044800:
81: 105632: loss: 0.1613804557:
81: 108832: loss: 0.1612087726:
81: 112032: loss: 0.1612585338:
81: 115232: loss: 0.1615994223:
81: 118432: loss: 0.1614218383:
81: 121632: loss: 0.1615220666:
81: 124832: loss: 0.1613601362:
81: 128032: loss: 0.1612400287:
81: 131232: loss: 0.1609919672:
81: 134432: loss: 0.1611974375:
81: 137632: loss: 0.1611666657:
81: 140832: loss: 0.1609525159:
81: 144032: loss: 0.1610009996:
81: 147232: loss: 0.1611827705:
81: 150432: loss: 0.1613843383:
81: 153632: loss: 0.1616284418:
81: 156832: loss: 0.1615172380:
81: 160032: loss: 0.1614531137:
81: 163232: loss: 0.1615155784:
81: 166432: loss: 0.1614135335:
Dev-Acc: 81: Accuracy: 0.9341462851: precision: 0.4311224490: recall: 0.4023125319: f1: 0.4162195444
Train-Acc: 81: Accuracy: 0.9391708374: precision: 0.7618898949: recall: 0.4812964302: f1: 0.5899274778
82: 3232: loss: 0.1773083502:
82: 6432: loss: 0.1657666571:
82: 9632: loss: 0.1631686716:
82: 12832: loss: 0.1650445902:
82: 16032: loss: 0.1630231979:
82: 19232: loss: 0.1620799592:
82: 22432: loss: 0.1624898398:
82: 25632: loss: 0.1650281961:
82: 28832: loss: 0.1656600942:
82: 32032: loss: 0.1648981039:
82: 35232: loss: 0.1652598081:
82: 38432: loss: 0.1644882776:
82: 41632: loss: 0.1644311434:
82: 44832: loss: 0.1629837305:
82: 48032: loss: 0.1630572335:
82: 51232: loss: 0.1623638810:
82: 54432: loss: 0.1625700292:
82: 57632: loss: 0.1634216417:
82: 60832: loss: 0.1628709833:
82: 64032: loss: 0.1633165560:
82: 67232: loss: 0.1624408080:
82: 70432: loss: 0.1620878923:
82: 73632: loss: 0.1617095838:
82: 76832: loss: 0.1614025961:
82: 80032: loss: 0.1618130434:
82: 83232: loss: 0.1613846842:
82: 86432: loss: 0.1612285633:
82: 89632: loss: 0.1614250574:
82: 92832: loss: 0.1610757230:
82: 96032: loss: 0.1615723010:
82: 99232: loss: 0.1612947078:
82: 102432: loss: 0.1617175663:
82: 105632: loss: 0.1617586004:
82: 108832: loss: 0.1613337409:
82: 112032: loss: 0.1608502818:
82: 115232: loss: 0.1609122983:
82: 118432: loss: 0.1614029496:
82: 121632: loss: 0.1614723247:
82: 124832: loss: 0.1612626157:
82: 128032: loss: 0.1612519160:
82: 131232: loss: 0.1613788808:
82: 134432: loss: 0.1614423451:
82: 137632: loss: 0.1612064728:
82: 140832: loss: 0.1611424997:
82: 144032: loss: 0.1611028505:
82: 147232: loss: 0.1610941358:
82: 150432: loss: 0.1611496961:
82: 153632: loss: 0.1608873237:
82: 156832: loss: 0.1608691097:
82: 160032: loss: 0.1606095414:
82: 163232: loss: 0.1608965026:
82: 166432: loss: 0.1609685717:
Dev-Acc: 82: Accuracy: 0.9340768456: precision: 0.4306741777: recall: 0.4029926883: f1: 0.4163738580
Train-Acc: 82: Accuracy: 0.9392485023: precision: 0.7624297899: recall: 0.4818881073: f1: 0.5905337362
83: 3232: loss: 0.1709721691:
83: 6432: loss: 0.1672510860:
83: 9632: loss: 0.1646281420:
83: 12832: loss: 0.1644900260:
83: 16032: loss: 0.1611972835:
83: 19232: loss: 0.1604332315:
83: 22432: loss: 0.1618059360:
83: 25632: loss: 0.1611833594:
83: 28832: loss: 0.1625089343:
83: 32032: loss: 0.1616925353:
83: 35232: loss: 0.1608627068:
83: 38432: loss: 0.1602406220:
83: 41632: loss: 0.1599703864:
83: 44832: loss: 0.1606850979:
83: 48032: loss: 0.1605424701:
83: 51232: loss: 0.1611413566:
83: 54432: loss: 0.1595272015:
83: 57632: loss: 0.1601085748:
83: 60832: loss: 0.1600831278:
83: 64032: loss: 0.1594784160:
83: 67232: loss: 0.1594801490:
83: 70432: loss: 0.1597144222:
83: 73632: loss: 0.1600359537:
83: 76832: loss: 0.1603194414:
83: 80032: loss: 0.1602727870:
83: 83232: loss: 0.1600005346:
83: 86432: loss: 0.1600385036:
83: 89632: loss: 0.1601080206:
83: 92832: loss: 0.1605180733:
83: 96032: loss: 0.1607739014:
83: 99232: loss: 0.1609404752:
83: 102432: loss: 0.1609296172:
83: 105632: loss: 0.1609838894:
83: 108832: loss: 0.1607100353:
83: 112032: loss: 0.1607478704:
83: 115232: loss: 0.1609921762:
83: 118432: loss: 0.1608697171:
83: 121632: loss: 0.1608672093:
83: 124832: loss: 0.1607578725:
83: 128032: loss: 0.1610493478:
83: 131232: loss: 0.1608236312:
83: 134432: loss: 0.1606576450:
83: 137632: loss: 0.1606287947:
83: 140832: loss: 0.1607718741:
83: 144032: loss: 0.1606886717:
83: 147232: loss: 0.1605084882:
83: 150432: loss: 0.1606726888:
83: 153632: loss: 0.1607155599:
83: 156832: loss: 0.1607033844:
83: 160032: loss: 0.1606692146:
83: 163232: loss: 0.1606233369:
83: 166432: loss: 0.1605671896:
Dev-Acc: 83: Accuracy: 0.9339577556: precision: 0.4297625521: recall: 0.4031627274: f1: 0.4160379014
Train-Acc: 83: Accuracy: 0.9393919706: precision: 0.7630213737: recall: 0.4834659128: f1: 0.5918950461
84: 3232: loss: 0.1571372142:
84: 6432: loss: 0.1567870963:
84: 9632: loss: 0.1588027972:
84: 12832: loss: 0.1598263362:
84: 16032: loss: 0.1607171200:
84: 19232: loss: 0.1609120436:
84: 22432: loss: 0.1617659626:
84: 25632: loss: 0.1628812937:
84: 28832: loss: 0.1611785857:
84: 32032: loss: 0.1614907466:
84: 35232: loss: 0.1609675366:
84: 38432: loss: 0.1609826733:
84: 41632: loss: 0.1612579992:
84: 44832: loss: 0.1612659646:
84: 48032: loss: 0.1624850992:
84: 51232: loss: 0.1616892514:
84: 54432: loss: 0.1621648275:
84: 57632: loss: 0.1620664765:
84: 60832: loss: 0.1624281830:
84: 64032: loss: 0.1624843215:
84: 67232: loss: 0.1620498646:
84: 70432: loss: 0.1617970428:
84: 73632: loss: 0.1609957529:
84: 76832: loss: 0.1605208915:
84: 80032: loss: 0.1604150549:
84: 83232: loss: 0.1604590761:
84: 86432: loss: 0.1605166194:
84: 89632: loss: 0.1607453549:
84: 92832: loss: 0.1608574679:
84: 96032: loss: 0.1607351349:
84: 99232: loss: 0.1612081787:
84: 102432: loss: 0.1608504577:
84: 105632: loss: 0.1611420742:
84: 108832: loss: 0.1610102304:
84: 112032: loss: 0.1611151396:
84: 115232: loss: 0.1613454982:
84: 118432: loss: 0.1611167432:
84: 121632: loss: 0.1614438090:
84: 124832: loss: 0.1612830819:
84: 128032: loss: 0.1613430962:
84: 131232: loss: 0.1609759146:
84: 134432: loss: 0.1613330327:
84: 137632: loss: 0.1611666607:
84: 140832: loss: 0.1612739760:
84: 144032: loss: 0.1612786932:
84: 147232: loss: 0.1613877688:
84: 150432: loss: 0.1610832450:
84: 153632: loss: 0.1611251208:
84: 156832: loss: 0.1610622608:
84: 160032: loss: 0.1609387112:
84: 163232: loss: 0.1607161235:
84: 166432: loss: 0.1603915349:
Dev-Acc: 84: Accuracy: 0.9338486195: precision: 0.4290101156: recall: 0.4038428839: f1: 0.4160462468
Train-Acc: 84: Accuracy: 0.9394516945: precision: 0.7630488815: recall: 0.4843862994: f1: 0.5925925926
85: 3232: loss: 0.1725657129:
85: 6432: loss: 0.1687710314:
85: 9632: loss: 0.1628390117:
85: 12832: loss: 0.1613032127:
85: 16032: loss: 0.1622351299:
85: 19232: loss: 0.1621015240:
85: 22432: loss: 0.1630706961:
85: 25632: loss: 0.1625296366:
85: 28832: loss: 0.1620427895:
85: 32032: loss: 0.1608675026:
85: 35232: loss: 0.1615144340:
85: 38432: loss: 0.1613649791:
85: 41632: loss: 0.1615540718:
85: 44832: loss: 0.1605388976:
85: 48032: loss: 0.1607107466:
85: 51232: loss: 0.1614083866:
85: 54432: loss: 0.1603860882:
85: 57632: loss: 0.1605558574:
85: 60832: loss: 0.1603496507:
85: 64032: loss: 0.1598425396:
85: 67232: loss: 0.1599348965:
85: 70432: loss: 0.1602749877:
85: 73632: loss: 0.1611841454:
85: 76832: loss: 0.1608758114:
85: 80032: loss: 0.1605273600:
85: 83232: loss: 0.1608295603:
85: 86432: loss: 0.1610017411:
85: 89632: loss: 0.1612393816:
85: 92832: loss: 0.1615099544:
85: 96032: loss: 0.1612124216:
85: 99232: loss: 0.1607492199:
85: 102432: loss: 0.1608789510:
85: 105632: loss: 0.1609236559:
85: 108832: loss: 0.1607058446:
85: 112032: loss: 0.1606498771:
85: 115232: loss: 0.1607143345:
85: 118432: loss: 0.1609378341:
85: 121632: loss: 0.1605451080:
85: 124832: loss: 0.1607051115:
85: 128032: loss: 0.1604264119:
85: 131232: loss: 0.1603621346:
85: 134432: loss: 0.1607206042:
85: 137632: loss: 0.1603252285:
85: 140832: loss: 0.1601801990:
85: 144032: loss: 0.1601068393:
85: 147232: loss: 0.1603650172:
85: 150432: loss: 0.1603169517:
85: 153632: loss: 0.1603370756:
85: 156832: loss: 0.1602781595:
85: 160032: loss: 0.1601972420:
85: 163232: loss: 0.1600706497:
85: 166432: loss: 0.1598398978:
Dev-Acc: 85: Accuracy: 0.9337990284: precision: 0.4287258966: recall: 0.4045230403: f1: 0.4162729659
Train-Acc: 85: Accuracy: 0.9395413399: precision: 0.7637436588: recall: 0.4849779765: f1: 0.5932448733
86: 3232: loss: 0.1595053381:
86: 6432: loss: 0.1623367110:
86: 9632: loss: 0.1603876548:
86: 12832: loss: 0.1619932005:
86: 16032: loss: 0.1608602098:
86: 19232: loss: 0.1599745873:
86: 22432: loss: 0.1592346202:
86: 25632: loss: 0.1593005692:
86: 28832: loss: 0.1609648321:
86: 32032: loss: 0.1602661646:
86: 35232: loss: 0.1602910771:
86: 38432: loss: 0.1603939739:
86: 41632: loss: 0.1614776423:
86: 44832: loss: 0.1605807675:
86: 48032: loss: 0.1604077830:
86: 51232: loss: 0.1612392571:
86: 54432: loss: 0.1607742407:
86: 57632: loss: 0.1618495342:
86: 60832: loss: 0.1616947773:
86: 64032: loss: 0.1611851256:
86: 67232: loss: 0.1609616617:
86: 70432: loss: 0.1604629195:
86: 73632: loss: 0.1604641877:
86: 76832: loss: 0.1605090887:
86: 80032: loss: 0.1604614731:
86: 83232: loss: 0.1606745194:
86: 86432: loss: 0.1610235282:
86: 89632: loss: 0.1607639974:
86: 92832: loss: 0.1610198226:
86: 96032: loss: 0.1610891285:
86: 99232: loss: 0.1613709411:
86: 102432: loss: 0.1615392052:
86: 105632: loss: 0.1617303510:
86: 108832: loss: 0.1615180840:
86: 112032: loss: 0.1612481272:
86: 115232: loss: 0.1613884661:
86: 118432: loss: 0.1610959292:
86: 121632: loss: 0.1608265965:
86: 124832: loss: 0.1606228045:
86: 128032: loss: 0.1608108148:
86: 131232: loss: 0.1610817231:
86: 134432: loss: 0.1609360710:
86: 137632: loss: 0.1611060541:
86: 140832: loss: 0.1612131013:
86: 144032: loss: 0.1608225905:
86: 147232: loss: 0.1607222074:
86: 150432: loss: 0.1605014364:
86: 153632: loss: 0.1602055433:
86: 156832: loss: 0.1599906339:
86: 160032: loss: 0.1598174753:
86: 163232: loss: 0.1601643917:
86: 166432: loss: 0.1597852267:
Dev-Acc: 86: Accuracy: 0.9337593317: precision: 0.4284170719: recall: 0.4045230403: f1: 0.4161273395
Train-Acc: 86: Accuracy: 0.9396788478: precision: 0.7648519975: recall: 0.4858326211: f1: 0.5942186306
87: 3232: loss: 0.1534097340:
87: 6432: loss: 0.1626324412:
87: 9632: loss: 0.1606936696:
87: 12832: loss: 0.1586375059:
87: 16032: loss: 0.1573845047:
87: 19232: loss: 0.1595711600:
87: 22432: loss: 0.1580540855:
87: 25632: loss: 0.1588846517:
87: 28832: loss: 0.1587129857:
87: 32032: loss: 0.1605179470:
87: 35232: loss: 0.1601344105:
87: 38432: loss: 0.1605865591:
87: 41632: loss: 0.1593361472:
87: 44832: loss: 0.1593097412:
87: 48032: loss: 0.1596184156:
87: 51232: loss: 0.1596033227:
87: 54432: loss: 0.1595117861:
87: 57632: loss: 0.1594270367:
87: 60832: loss: 0.1591399753:
87: 64032: loss: 0.1596468077:
87: 67232: loss: 0.1596118038:
87: 70432: loss: 0.1599863721:
87: 73632: loss: 0.1597081444:
87: 76832: loss: 0.1599529662:
87: 80032: loss: 0.1597217993:
87: 83232: loss: 0.1602761826:
87: 86432: loss: 0.1599865036:
87: 89632: loss: 0.1594212246:
87: 92832: loss: 0.1590259375:
87: 96032: loss: 0.1587650534:
87: 99232: loss: 0.1585779412:
87: 102432: loss: 0.1585495887:
87: 105632: loss: 0.1586073726:
87: 108832: loss: 0.1585969705:
87: 112032: loss: 0.1589739990:
87: 115232: loss: 0.1590557043:
87: 118432: loss: 0.1592829799:
87: 121632: loss: 0.1593784612:
87: 124832: loss: 0.1592903986:
87: 128032: loss: 0.1594452659:
87: 131232: loss: 0.1588698623:
87: 134432: loss: 0.1589134115:
87: 137632: loss: 0.1593814967:
87: 140832: loss: 0.1594657957:
87: 144032: loss: 0.1594499018:
87: 147232: loss: 0.1593896492:
87: 150432: loss: 0.1594015951:
87: 153632: loss: 0.1595153099:
87: 156832: loss: 0.1596897537:
87: 160032: loss: 0.1597503719:
87: 163232: loss: 0.1593408126:
87: 166432: loss: 0.1592829895:
Dev-Acc: 87: Accuracy: 0.9336402416: precision: 0.4276233184: recall: 0.4053732358: f1: 0.4162011173
Train-Acc: 87: Accuracy: 0.9397983551: precision: 0.7649546205: recall: 0.4876076524: f1: 0.5955755410
88: 3232: loss: 0.1661488606:
88: 6432: loss: 0.1588254307:
88: 9632: loss: 0.1585067475:
88: 12832: loss: 0.1575096916:
88: 16032: loss: 0.1573435446:
88: 19232: loss: 0.1589710277:
88: 22432: loss: 0.1577588136:
88: 25632: loss: 0.1595499633:
88: 28832: loss: 0.1584866619:
88: 32032: loss: 0.1594492975:
88: 35232: loss: 0.1589322097:
88: 38432: loss: 0.1593981996:
88: 41632: loss: 0.1600440299:
88: 44832: loss: 0.1592311207:
88: 48032: loss: 0.1589903823:
88: 51232: loss: 0.1587121500:
88: 54432: loss: 0.1593396091:
88: 57632: loss: 0.1598551765:
88: 60832: loss: 0.1601916432:
88: 64032: loss: 0.1603727545:
88: 67232: loss: 0.1601464013:
88: 70432: loss: 0.1599439687:
88: 73632: loss: 0.1595236546:
88: 76832: loss: 0.1592027916:
88: 80032: loss: 0.1590946503:
88: 83232: loss: 0.1593822601:
88: 86432: loss: 0.1597259645:
88: 89632: loss: 0.1596875871:
88: 92832: loss: 0.1595957539:
88: 96032: loss: 0.1592927118:
88: 99232: loss: 0.1591984727:
88: 102432: loss: 0.1594690057:
88: 105632: loss: 0.1593378840:
88: 108832: loss: 0.1593737652:
88: 112032: loss: 0.1597347466:
88: 115232: loss: 0.1596972467:
88: 118432: loss: 0.1593922083:
88: 121632: loss: 0.1590584009:
88: 124832: loss: 0.1591775751:
88: 128032: loss: 0.1585759712:
88: 131232: loss: 0.1586403103:
88: 134432: loss: 0.1588152544:
88: 137632: loss: 0.1585724116:
88: 140832: loss: 0.1589400002:
88: 144032: loss: 0.1589544666:
88: 147232: loss: 0.1589718025:
88: 150432: loss: 0.1590846190:
88: 153632: loss: 0.1590930753:
88: 156832: loss: 0.1591065298:
88: 160032: loss: 0.1591964891:
88: 163232: loss: 0.1589650491:
88: 166432: loss: 0.1588281677:
Dev-Acc: 88: Accuracy: 0.9336203933: precision: 0.4278065322: recall: 0.4075837443: f1: 0.4174503657
Train-Acc: 88: Accuracy: 0.9399836063: precision: 0.7659224200: recall: 0.4893826836: f1: 0.5971921380
89: 3232: loss: 0.1631649137:
89: 6432: loss: 0.1621268033:
89: 9632: loss: 0.1654507113:
89: 12832: loss: 0.1595866944:
89: 16032: loss: 0.1565999914:
89: 19232: loss: 0.1578063319:
89: 22432: loss: 0.1579113297:
89: 25632: loss: 0.1584303965:
89: 28832: loss: 0.1596065674:
89: 32032: loss: 0.1594697893:
89: 35232: loss: 0.1605435579:
89: 38432: loss: 0.1610268930:
89: 41632: loss: 0.1615223966:
89: 44832: loss: 0.1604167388:
89: 48032: loss: 0.1595781508:
89: 51232: loss: 0.1592212645:
89: 54432: loss: 0.1596024727:
89: 57632: loss: 0.1597299945:
89: 60832: loss: 0.1592701052:
89: 64032: loss: 0.1586084532:
89: 67232: loss: 0.1588634718:
89: 70432: loss: 0.1588069961:
89: 73632: loss: 0.1585965454:
89: 76832: loss: 0.1588911686:
89: 80032: loss: 0.1591705259:
89: 83232: loss: 0.1591946879:
89: 86432: loss: 0.1592955568:
89: 89632: loss: 0.1592299684:
89: 92832: loss: 0.1592385195:
89: 96032: loss: 0.1591001027:
89: 99232: loss: 0.1587410009:
89: 102432: loss: 0.1588291630:
89: 105632: loss: 0.1588739819:
89: 108832: loss: 0.1588729928:
89: 112032: loss: 0.1588917621:
89: 115232: loss: 0.1584908990:
89: 118432: loss: 0.1586934195:
89: 121632: loss: 0.1589993710:
89: 124832: loss: 0.1591034606:
89: 128032: loss: 0.1592391305:
89: 131232: loss: 0.1593683670:
89: 134432: loss: 0.1586506778:
89: 137632: loss: 0.1589767954:
89: 140832: loss: 0.1589832979:
89: 144032: loss: 0.1590082900:
89: 147232: loss: 0.1587988870:
89: 150432: loss: 0.1588755998:
89: 153632: loss: 0.1588454626:
89: 156832: loss: 0.1584276459:
89: 160032: loss: 0.1584023127:
89: 163232: loss: 0.1586920649:
89: 166432: loss: 0.1586436294:
Dev-Acc: 89: Accuracy: 0.9334715605: precision: 0.4267945984: recall: 0.4084339398: f1: 0.4174124598
Train-Acc: 89: Accuracy: 0.9401031733: precision: 0.7664031215: recall: 0.4906975215: f1: 0.5983166333
90: 3232: loss: 0.1700927963:
90: 6432: loss: 0.1676134139:
90: 9632: loss: 0.1622096946:
90: 12832: loss: 0.1581182616:
90: 16032: loss: 0.1575010190:
90: 19232: loss: 0.1574633246:
90: 22432: loss: 0.1583784778:
90: 25632: loss: 0.1573506387:
90: 28832: loss: 0.1572759152:
90: 32032: loss: 0.1574527118:
90: 35232: loss: 0.1588120204:
90: 38432: loss: 0.1585386388:
90: 41632: loss: 0.1590473016:
90: 44832: loss: 0.1587315872:
90: 48032: loss: 0.1581889285:
90: 51232: loss: 0.1580069604:
90: 54432: loss: 0.1577750895:
90: 57632: loss: 0.1571463166:
90: 60832: loss: 0.1568257635:
90: 64032: loss: 0.1571927323:
90: 67232: loss: 0.1579909879:
90: 70432: loss: 0.1577336180:
90: 73632: loss: 0.1586574253:
90: 76832: loss: 0.1587212672:
90: 80032: loss: 0.1589572359:
90: 83232: loss: 0.1590616453:
90: 86432: loss: 0.1589555889:
90: 89632: loss: 0.1590018476:
90: 92832: loss: 0.1589140307:
90: 96032: loss: 0.1588114077:
90: 99232: loss: 0.1588023690:
90: 102432: loss: 0.1587124185:
90: 105632: loss: 0.1585830632:
90: 108832: loss: 0.1583969115:
90: 112032: loss: 0.1582497431:
90: 115232: loss: 0.1582722732:
90: 118432: loss: 0.1584463517:
90: 121632: loss: 0.1588127665:
90: 124832: loss: 0.1588986232:
90: 128032: loss: 0.1586466594:
90: 131232: loss: 0.1588761461:
90: 134432: loss: 0.1587494737:
90: 137632: loss: 0.1585574627:
90: 140832: loss: 0.1584513763:
90: 144032: loss: 0.1583065692:
90: 147232: loss: 0.1580854335:
90: 150432: loss: 0.1582468420:
90: 153632: loss: 0.1582442549:
90: 156832: loss: 0.1581536018:
90: 160032: loss: 0.1580312428:
90: 163232: loss: 0.1578972671:
90: 166432: loss: 0.1580817155:
Dev-Acc: 90: Accuracy: 0.9333822727: precision: 0.4260869565: recall: 0.4082639007: f1: 0.4169850643
Train-Acc: 90: Accuracy: 0.9402107596: precision: 0.7667247208: recall: 0.4920123595: f1: 0.5993913183
91: 3232: loss: 0.1561368368:
91: 6432: loss: 0.1540987913:
91: 9632: loss: 0.1560990697:
91: 12832: loss: 0.1559545048:
91: 16032: loss: 0.1551057147:
91: 19232: loss: 0.1549225445:
91: 22432: loss: 0.1556190165:
91: 25632: loss: 0.1569507522:
91: 28832: loss: 0.1579799709:
91: 32032: loss: 0.1595098117:
91: 35232: loss: 0.1594131746:
91: 38432: loss: 0.1589165198:
91: 41632: loss: 0.1585436366:
91: 44832: loss: 0.1578917326:
91: 48032: loss: 0.1578829258:
91: 51232: loss: 0.1577067640:
91: 54432: loss: 0.1580920458:
91: 57632: loss: 0.1581622868:
91: 60832: loss: 0.1574325784:
91: 64032: loss: 0.1577395249:
91: 67232: loss: 0.1577802532:
91: 70432: loss: 0.1577646062:
91: 73632: loss: 0.1577760877:
91: 76832: loss: 0.1578886078:
91: 80032: loss: 0.1569416639:
91: 83232: loss: 0.1569351432:
91: 86432: loss: 0.1573159761:
91: 89632: loss: 0.1571496983:
91: 92832: loss: 0.1572698713:
91: 96032: loss: 0.1572517276:
91: 99232: loss: 0.1569778059:
91: 102432: loss: 0.1573674838:
91: 105632: loss: 0.1574815131:
91: 108832: loss: 0.1575516383:
91: 112032: loss: 0.1577878172:
91: 115232: loss: 0.1578125786:
91: 118432: loss: 0.1576431813:
91: 121632: loss: 0.1575765939:
91: 124832: loss: 0.1579386166:
91: 128032: loss: 0.1579981327:
91: 131232: loss: 0.1576486855:
91: 134432: loss: 0.1575959697:
91: 137632: loss: 0.1575296064:
91: 140832: loss: 0.1571192732:
91: 144032: loss: 0.1572939025:
91: 147232: loss: 0.1572710257:
91: 150432: loss: 0.1573251992:
91: 153632: loss: 0.1573860061:
91: 156832: loss: 0.1573520315:
91: 160032: loss: 0.1571890463:
91: 163232: loss: 0.1573027985:
91: 166432: loss: 0.1576882366:
Dev-Acc: 91: Accuracy: 0.9333227277: precision: 0.4257127678: recall: 0.4087740180: f1: 0.4170714781
Train-Acc: 91: Accuracy: 0.9402884245: precision: 0.7668166019: recall: 0.4931299717: f1: 0.6002480695
92: 3232: loss: 0.1591060990:
92: 6432: loss: 0.1609350583:
92: 9632: loss: 0.1612749094:
92: 12832: loss: 0.1588486611:
92: 16032: loss: 0.1593068137:
92: 19232: loss: 0.1566657314:
92: 22432: loss: 0.1553034275:
92: 25632: loss: 0.1553612556:
92: 28832: loss: 0.1540531669:
92: 32032: loss: 0.1539401699:
92: 35232: loss: 0.1539366133:
92: 38432: loss: 0.1544836343:
92: 41632: loss: 0.1547056235:
92: 44832: loss: 0.1540780114:
92: 48032: loss: 0.1533045628:
92: 51232: loss: 0.1540392939:
92: 54432: loss: 0.1541837592:
92: 57632: loss: 0.1538525962:
92: 60832: loss: 0.1538911021:
92: 64032: loss: 0.1546681524:
92: 67232: loss: 0.1549236210:
92: 70432: loss: 0.1553343190:
92: 73632: loss: 0.1557731015:
92: 76832: loss: 0.1556811076:
92: 80032: loss: 0.1560188614:
92: 83232: loss: 0.1563056397:
92: 86432: loss: 0.1559624149:
92: 89632: loss: 0.1565022406:
92: 92832: loss: 0.1565968981:
92: 96032: loss: 0.1564420028:
92: 99232: loss: 0.1564952017:
92: 102432: loss: 0.1571255646:
92: 105632: loss: 0.1566283194:
92: 108832: loss: 0.1563320908:
92: 112032: loss: 0.1564246287:
92: 115232: loss: 0.1563456922:
92: 118432: loss: 0.1566228381:
92: 121632: loss: 0.1566393072:
92: 124832: loss: 0.1565977655:
92: 128032: loss: 0.1565502237:
92: 131232: loss: 0.1566941664:
92: 134432: loss: 0.1568729897:
92: 137632: loss: 0.1570101766:
92: 140832: loss: 0.1571148660:
92: 144032: loss: 0.1571729702:
92: 147232: loss: 0.1571597037:
92: 150432: loss: 0.1571905936:
92: 153632: loss: 0.1571440110:
92: 156832: loss: 0.1572579185:
92: 160032: loss: 0.1573217233:
92: 163232: loss: 0.1572744489:
92: 166432: loss: 0.1574173964:
Dev-Acc: 92: Accuracy: 0.9333227277: precision: 0.4257916151: recall: 0.4092841354: f1: 0.4173747182
Train-Acc: 92: Accuracy: 0.9404258728: precision: 0.7680744452: recall: 0.4937873907: f1: 0.6011204482
93: 3232: loss: 0.1687916607:
93: 6432: loss: 0.1682361317:
93: 9632: loss: 0.1617886486:
93: 12832: loss: 0.1598351985:
93: 16032: loss: 0.1582636571:
93: 19232: loss: 0.1555709301:
93: 22432: loss: 0.1559495712:
93: 25632: loss: 0.1555963951:
93: 28832: loss: 0.1545459874:
93: 32032: loss: 0.1552167933:
93: 35232: loss: 0.1557357549:
93: 38432: loss: 0.1562996109:
93: 41632: loss: 0.1560736825:
93: 44832: loss: 0.1559513622:
93: 48032: loss: 0.1565277120:
93: 51232: loss: 0.1567571336:
93: 54432: loss: 0.1566860174:
93: 57632: loss: 0.1557285814:
93: 60832: loss: 0.1560596552:
93: 64032: loss: 0.1563317993:
93: 67232: loss: 0.1570180610:
93: 70432: loss: 0.1566161216:
93: 73632: loss: 0.1566408608:
93: 76832: loss: 0.1566474842:
93: 80032: loss: 0.1565768394:
93: 83232: loss: 0.1568067141:
93: 86432: loss: 0.1565568213:
93: 89632: loss: 0.1566105887:
93: 92832: loss: 0.1563777123:
93: 96032: loss: 0.1566913240:
93: 99232: loss: 0.1566831090:
93: 102432: loss: 0.1570554521:
93: 105632: loss: 0.1568250486:
93: 108832: loss: 0.1573292024:
93: 112032: loss: 0.1567379673:
93: 115232: loss: 0.1569260155:
93: 118432: loss: 0.1566541163:
93: 121632: loss: 0.1568087448:
93: 124832: loss: 0.1568053493:
93: 128032: loss: 0.1567580727:
93: 131232: loss: 0.1564799842:
93: 134432: loss: 0.1563165057:
93: 137632: loss: 0.1565938624:
93: 140832: loss: 0.1567428826:
93: 144032: loss: 0.1566582947:
93: 147232: loss: 0.1566187990:
93: 150432: loss: 0.1563398280:
93: 153632: loss: 0.1566323583:
93: 156832: loss: 0.1569532961:
93: 160032: loss: 0.1568076976:
93: 163232: loss: 0.1567152803:
93: 166432: loss: 0.1569270621:
Dev-Acc: 93: Accuracy: 0.9332830310: precision: 0.4254641910: recall: 0.4091140962: f1: 0.4171289875
Train-Acc: 93: Accuracy: 0.9405035973: precision: 0.7688766114: recall: 0.4940503583: f1: 0.6015609366
94: 3232: loss: 0.1463049102:
94: 6432: loss: 0.1573243024:
94: 9632: loss: 0.1571139505:
94: 12832: loss: 0.1551338306:
94: 16032: loss: 0.1572356642:
94: 19232: loss: 0.1567629417:
94: 22432: loss: 0.1558424569:
94: 25632: loss: 0.1549516914:
94: 28832: loss: 0.1557295742:
94: 32032: loss: 0.1576477193:
94: 35232: loss: 0.1591319657:
94: 38432: loss: 0.1594510761:
94: 41632: loss: 0.1594161738:
94: 44832: loss: 0.1588672768:
94: 48032: loss: 0.1585289983:
94: 51232: loss: 0.1582138726:
94: 54432: loss: 0.1571958164:
94: 57632: loss: 0.1567182187:
94: 60832: loss: 0.1570112425:
94: 64032: loss: 0.1566770186:
94: 67232: loss: 0.1573164165:
94: 70432: loss: 0.1574208284:
94: 73632: loss: 0.1574829018:
94: 76832: loss: 0.1577452561:
94: 80032: loss: 0.1576822599:
94: 83232: loss: 0.1576742992:
94: 86432: loss: 0.1570840138:
94: 89632: loss: 0.1569461948:
94: 92832: loss: 0.1577298730:
94: 96032: loss: 0.1578573145:
94: 99232: loss: 0.1575003760:
94: 102432: loss: 0.1572581084:
94: 105632: loss: 0.1569528994:
94: 108832: loss: 0.1570238998:
94: 112032: loss: 0.1571913855:
94: 115232: loss: 0.1568719064:
94: 118432: loss: 0.1570141340:
94: 121632: loss: 0.1567730950:
94: 124832: loss: 0.1568330772:
94: 128032: loss: 0.1567372474:
94: 131232: loss: 0.1567023431:
94: 134432: loss: 0.1572354882:
94: 137632: loss: 0.1571876497:
94: 140832: loss: 0.1570849302:
94: 144032: loss: 0.1569142784:
94: 147232: loss: 0.1569509139:
94: 150432: loss: 0.1567922027:
94: 153632: loss: 0.1567449867:
94: 156832: loss: 0.1564446564:
94: 160032: loss: 0.1566960678:
94: 163232: loss: 0.1566397464:
94: 166432: loss: 0.1567949726:
Dev-Acc: 94: Accuracy: 0.9332235456: precision: 0.4251454770: recall: 0.4099642918: f1: 0.4174168975
Train-Acc: 94: Accuracy: 0.9406470060: precision: 0.7694977542: recall: 0.4955624219: f1: 0.6028711961
95: 3232: loss: 0.1519329805:
95: 6432: loss: 0.1580430666:
95: 9632: loss: 0.1546648205:
95: 12832: loss: 0.1598287540:
95: 16032: loss: 0.1597186098:
95: 19232: loss: 0.1591267148:
95: 22432: loss: 0.1596097804:
95: 25632: loss: 0.1590398031:
95: 28832: loss: 0.1579287778:
95: 32032: loss: 0.1573262692:
95: 35232: loss: 0.1580610209:
95: 38432: loss: 0.1585799323:
95: 41632: loss: 0.1584331464:
95: 44832: loss: 0.1583584327:
95: 48032: loss: 0.1578920192:
95: 51232: loss: 0.1577681271:
95: 54432: loss: 0.1569450158:
95: 57632: loss: 0.1574096036:
95: 60832: loss: 0.1578842660:
95: 64032: loss: 0.1580574881:
95: 67232: loss: 0.1575141450:
95: 70432: loss: 0.1572365724:
95: 73632: loss: 0.1574379168:
95: 76832: loss: 0.1568027504:
95: 80032: loss: 0.1562084791:
95: 83232: loss: 0.1563735816:
95: 86432: loss: 0.1565394009:
95: 89632: loss: 0.1562760366:
95: 92832: loss: 0.1562559045:
95: 96032: loss: 0.1560554367:
95: 99232: loss: 0.1561008077:
95: 102432: loss: 0.1560850445:
95: 105632: loss: 0.1563756926:
95: 108832: loss: 0.1565989608:
95: 112032: loss: 0.1570820366:
95: 115232: loss: 0.1572949681:
95: 118432: loss: 0.1572083507:
95: 121632: loss: 0.1571425478:
95: 124832: loss: 0.1573702757:
95: 128032: loss: 0.1573908263:
95: 131232: loss: 0.1574112862:
95: 134432: loss: 0.1573925823:
95: 137632: loss: 0.1570237229:
95: 140832: loss: 0.1570723185:
95: 144032: loss: 0.1567198997:
95: 147232: loss: 0.1567310388:
95: 150432: loss: 0.1567518127:
95: 153632: loss: 0.1567209040:
95: 156832: loss: 0.1565366905:
95: 160032: loss: 0.1564067267:
95: 163232: loss: 0.1563999744:
95: 166432: loss: 0.1563572749:
Dev-Acc: 95: Accuracy: 0.9331838489: precision: 0.4248987498: recall: 0.4103043700: f1: 0.4174740484
Train-Acc: 95: Accuracy: 0.9408502579: precision: 0.7710671292: recall: 0.4968772599: f1: 0.6043257506
96: 3232: loss: 0.1592111896:
96: 6432: loss: 0.1569102043:
96: 9632: loss: 0.1555417508:
96: 12832: loss: 0.1539376938:
96: 16032: loss: 0.1562534757:
96: 19232: loss: 0.1582826711:
96: 22432: loss: 0.1603024182:
96: 25632: loss: 0.1578219400:
96: 28832: loss: 0.1575685354:
96: 32032: loss: 0.1569137532:
96: 35232: loss: 0.1566194289:
96: 38432: loss: 0.1562231775:
96: 41632: loss: 0.1557499470:
96: 44832: loss: 0.1561765405:
96: 48032: loss: 0.1557359814:
96: 51232: loss: 0.1566520861:
96: 54432: loss: 0.1567291622:
96: 57632: loss: 0.1566741605:
96: 60832: loss: 0.1572595339:
96: 64032: loss: 0.1571149192:
96: 67232: loss: 0.1569665991:
96: 70432: loss: 0.1565797439:
96: 73632: loss: 0.1570139008:
96: 76832: loss: 0.1566316381:
96: 80032: loss: 0.1563393789:
96: 83232: loss: 0.1558075044:
96: 86432: loss: 0.1560038160:
96: 89632: loss: 0.1559570488:
96: 92832: loss: 0.1561014723:
96: 96032: loss: 0.1560819096:
96: 99232: loss: 0.1559407114:
96: 102432: loss: 0.1559821731:
96: 105632: loss: 0.1557548919:
96: 108832: loss: 0.1558950230:
96: 112032: loss: 0.1557893514:
96: 115232: loss: 0.1558691308:
96: 118432: loss: 0.1555323359:
96: 121632: loss: 0.1555915815:
96: 124832: loss: 0.1557906949:
96: 128032: loss: 0.1556930140:
96: 131232: loss: 0.1555545970:
96: 134432: loss: 0.1555650750:
96: 137632: loss: 0.1556025247:
96: 140832: loss: 0.1557592756:
96: 144032: loss: 0.1559763248:
96: 147232: loss: 0.1561726396:
96: 150432: loss: 0.1562293608:
96: 153632: loss: 0.1564484963:
96: 156832: loss: 0.1562599692:
96: 160032: loss: 0.1563107266:
96: 163232: loss: 0.1563002739:
96: 166432: loss: 0.1560592601:
Dev-Acc: 96: Accuracy: 0.9331540465: precision: 0.4248331577: recall: 0.4113246047: f1: 0.4179697624
Train-Acc: 96: Accuracy: 0.9408681393: precision: 0.7708609272: recall: 0.4974031951: f1: 0.6046511628
97: 3232: loss: 0.1646292339:
97: 6432: loss: 0.1600579776:
97: 9632: loss: 0.1591591493:
97: 12832: loss: 0.1584046520:
97: 16032: loss: 0.1584340464:
97: 19232: loss: 0.1592913705:
97: 22432: loss: 0.1591351808:
97: 25632: loss: 0.1567977406:
97: 28832: loss: 0.1558145958:
97: 32032: loss: 0.1554409989:
97: 35232: loss: 0.1546775009:
97: 38432: loss: 0.1555791196:
97: 41632: loss: 0.1553601193:
97: 44832: loss: 0.1554694613:
97: 48032: loss: 0.1561841148:
97: 51232: loss: 0.1558112478:
97: 54432: loss: 0.1559712283:
97: 57632: loss: 0.1555385814:
97: 60832: loss: 0.1556606104:
97: 64032: loss: 0.1561260556:
97: 67232: loss: 0.1556272446:
97: 70432: loss: 0.1556700807:
97: 73632: loss: 0.1558275229:
97: 76832: loss: 0.1555890313:
97: 80032: loss: 0.1557761131:
97: 83232: loss: 0.1558174906:
97: 86432: loss: 0.1555114311:
97: 89632: loss: 0.1556868032:
97: 92832: loss: 0.1554888232:
97: 96032: loss: 0.1556231954:
97: 99232: loss: 0.1555225865:
97: 102432: loss: 0.1555596966:
97: 105632: loss: 0.1556237383:
97: 108832: loss: 0.1554536796:
97: 112032: loss: 0.1548632834:
97: 115232: loss: 0.1547945070:
97: 118432: loss: 0.1550055782:
97: 121632: loss: 0.1550935237:
97: 124832: loss: 0.1550087651:
97: 128032: loss: 0.1549009903:
97: 131232: loss: 0.1549324001:
97: 134432: loss: 0.1551824370:
97: 137632: loss: 0.1549493558:
97: 140832: loss: 0.1552394878:
97: 144032: loss: 0.1554930283:
97: 147232: loss: 0.1554445871:
97: 150432: loss: 0.1553633058:
97: 153632: loss: 0.1551104715:
97: 156832: loss: 0.1550452912:
97: 160032: loss: 0.1550873666:
97: 163232: loss: 0.1552616949:
97: 166432: loss: 0.1553161696:
Dev-Acc: 97: Accuracy: 0.9330151677: precision: 0.4239510490: recall: 0.4123448393: f1: 0.4180674080
Train-Acc: 97: Accuracy: 0.9411251545: precision: 0.7730792745: recall: 0.4987837749: f1: 0.6063536464
98: 3232: loss: 0.1592627157:
98: 6432: loss: 0.1504651360:
98: 9632: loss: 0.1514128252:
98: 12832: loss: 0.1523558400:
98: 16032: loss: 0.1507358469:
98: 19232: loss: 0.1529822792:
98: 22432: loss: 0.1537843879:
98: 25632: loss: 0.1557943448:
98: 28832: loss: 0.1564719948:
98: 32032: loss: 0.1576935083:
98: 35232: loss: 0.1583412264:
98: 38432: loss: 0.1581200125:
98: 41632: loss: 0.1579807084:
98: 44832: loss: 0.1577214115:
98: 48032: loss: 0.1574086694:
98: 51232: loss: 0.1577292529:
98: 54432: loss: 0.1575457074:
98: 57632: loss: 0.1559770865:
98: 60832: loss: 0.1556986084:
98: 64032: loss: 0.1554193928:
98: 67232: loss: 0.1553063338:
98: 70432: loss: 0.1558415061:
98: 73632: loss: 0.1563195919:
98: 76832: loss: 0.1573224464:
98: 80032: loss: 0.1571274539:
98: 83232: loss: 0.1568189388:
98: 86432: loss: 0.1566345313:
98: 89632: loss: 0.1565677734:
98: 92832: loss: 0.1561347598:
98: 96032: loss: 0.1555722922:
98: 99232: loss: 0.1555650297:
98: 102432: loss: 0.1551618085:
98: 105632: loss: 0.1550177523:
98: 108832: loss: 0.1551828126:
98: 112032: loss: 0.1552376543:
98: 115232: loss: 0.1553971016:
98: 118432: loss: 0.1555619658:
98: 121632: loss: 0.1555300962:
98: 124832: loss: 0.1559391882:
98: 128032: loss: 0.1558931247:
98: 131232: loss: 0.1557299722:
98: 134432: loss: 0.1554903643:
98: 137632: loss: 0.1552514355:
98: 140832: loss: 0.1550640449:
98: 144032: loss: 0.1553080711:
98: 147232: loss: 0.1552225150:
98: 150432: loss: 0.1549908821:
98: 153632: loss: 0.1551688719:
98: 156832: loss: 0.1550442158:
98: 160032: loss: 0.1552925840:
98: 163232: loss: 0.1554840036:
98: 166432: loss: 0.1555352790:
Dev-Acc: 98: Accuracy: 0.9329556227: precision: 0.4235335196: recall: 0.4125148784: f1: 0.4179515893
Train-Acc: 98: Accuracy: 0.9413104057: precision: 0.7748546956: recall: 0.4995726777: f1: 0.6074826125
99: 3232: loss: 0.1441360014:
99: 6432: loss: 0.1528427677:
99: 9632: loss: 0.1523916121:
99: 12832: loss: 0.1500070422:
99: 16032: loss: 0.1524775073:
99: 19232: loss: 0.1500957916:
99: 22432: loss: 0.1514055858:
99: 25632: loss: 0.1521710170:
99: 28832: loss: 0.1540106054:
99: 32032: loss: 0.1551997029:
99: 35232: loss: 0.1552433628:
99: 38432: loss: 0.1565119703:
99: 41632: loss: 0.1554855859:
99: 44832: loss: 0.1545548979:
99: 48032: loss: 0.1542053819:
99: 51232: loss: 0.1548184698:
99: 54432: loss: 0.1549259817:
99: 57632: loss: 0.1550391452:
99: 60832: loss: 0.1551130490:
99: 64032: loss: 0.1553505593:
99: 67232: loss: 0.1551115885:
99: 70432: loss: 0.1548334250:
99: 73632: loss: 0.1545894451:
99: 76832: loss: 0.1543092755:
99: 80032: loss: 0.1543923935:
99: 83232: loss: 0.1549097676:
99: 86432: loss: 0.1543137892:
99: 89632: loss: 0.1546834681:
99: 92832: loss: 0.1547804723:
99: 96032: loss: 0.1550723612:
99: 99232: loss: 0.1550109556:
99: 102432: loss: 0.1548105868:
99: 105632: loss: 0.1547646642:
99: 108832: loss: 0.1547402157:
99: 112032: loss: 0.1547646184:
99: 115232: loss: 0.1548570695:
99: 118432: loss: 0.1549260672:
99: 121632: loss: 0.1550597759:
99: 124832: loss: 0.1550397225:
99: 128032: loss: 0.1547392008:
99: 131232: loss: 0.1544879379:
99: 134432: loss: 0.1545723472:
99: 137632: loss: 0.1546886557:
99: 140832: loss: 0.1547976092:
99: 144032: loss: 0.1547620973:
99: 147232: loss: 0.1547577782:
99: 150432: loss: 0.1548997540:
99: 153632: loss: 0.1548399615:
99: 156832: loss: 0.1549170955:
99: 160032: loss: 0.1551671610:
99: 163232: loss: 0.1550327630:
99: 166432: loss: 0.1550060981:
Dev-Acc: 99: Accuracy: 0.9329556227: precision: 0.4234533380: recall: 0.4120047611: f1: 0.4176506076
Train-Acc: 99: Accuracy: 0.9414777756: precision: 0.7761133191: recall: 0.5006902899: f1: 0.6086956522
100: 3232: loss: 0.1497400799:
100: 6432: loss: 0.1502567952:
100: 9632: loss: 0.1529216360:
100: 12832: loss: 0.1565475945:
100: 16032: loss: 0.1542666762:
100: 19232: loss: 0.1555762712:
100: 22432: loss: 0.1557591118:
100: 25632: loss: 0.1566451260:
100: 28832: loss: 0.1576137331:
100: 32032: loss: 0.1565945143:
100: 35232: loss: 0.1565068292:
100: 38432: loss: 0.1564858248:
100: 41632: loss: 0.1575726542:
100: 44832: loss: 0.1565322691:
100: 48032: loss: 0.1556069666:
100: 51232: loss: 0.1546599827:
100: 54432: loss: 0.1559077493:
100: 57632: loss: 0.1554265864:
100: 60832: loss: 0.1554330098:
100: 64032: loss: 0.1551457852:
100: 67232: loss: 0.1549742730:
100: 70432: loss: 0.1549987504:
100: 73632: loss: 0.1551539893:
100: 76832: loss: 0.1550506197:
100: 80032: loss: 0.1551026578:
100: 83232: loss: 0.1556087610:
100: 86432: loss: 0.1554175262:
100: 89632: loss: 0.1552141833:
100: 92832: loss: 0.1552994723:
100: 96032: loss: 0.1552011321:
100: 99232: loss: 0.1549596256:
100: 102432: loss: 0.1549460050:
100: 105632: loss: 0.1548814603:
100: 108832: loss: 0.1543999608:
100: 112032: loss: 0.1543768176:
100: 115232: loss: 0.1541563819:
100: 118432: loss: 0.1544906073:
100: 121632: loss: 0.1542929865:
100: 124832: loss: 0.1540668849:
100: 128032: loss: 0.1540214950:
100: 131232: loss: 0.1541553014:
100: 134432: loss: 0.1538100485:
100: 137632: loss: 0.1539914515:
100: 140832: loss: 0.1541209467:
100: 144032: loss: 0.1541078096:
100: 147232: loss: 0.1540907096:
100: 150432: loss: 0.1544697577:
100: 153632: loss: 0.1546364974:
100: 156832: loss: 0.1546401295:
100: 160032: loss: 0.1547329652:
100: 163232: loss: 0.1545584207:
100: 166432: loss: 0.1544325375:
Dev-Acc: 100: Accuracy: 0.9329060316: precision: 0.4234046253: recall: 0.4140452304: f1: 0.4186726272
Train-Acc: 100: Accuracy: 0.9415913224: precision: 0.7762088582: recall: 0.5023338374: f1: 0.6099381361
