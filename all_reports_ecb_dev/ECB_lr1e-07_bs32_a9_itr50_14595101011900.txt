1: 3232: loss: 0.7111244577:
1: 6432: loss: 0.7094125569:
1: 9632: loss: 0.7083820947:
1: 12832: loss: 0.7074148309:
1: 16032: loss: 0.7065620910:
1: 19232: loss: 0.7057542194:
1: 22432: loss: 0.7047478926:
1: 25632: loss: 0.7037866612:
1: 28832: loss: 0.7030865491:
1: 32032: loss: 0.7021649307:
1: 35232: loss: 0.7013409737:
1: 38432: loss: 0.7003973394:
1: 41632: loss: 0.6993758652:
1: 44832: loss: 0.6984379728:
1: 48032: loss: 0.6975267936:
1: 51232: loss: 0.6966352424:
1: 54432: loss: 0.6956952740:
1: 57632: loss: 0.6948251848:
1: 60832: loss: 0.6939174083:
1: 64032: loss: 0.6929972117:
1: 67232: loss: 0.6920330500:
1: 70432: loss: 0.6911362050:
1: 73632: loss: 0.6903389028:
1: 76832: loss: 0.6894300652:
1: 80032: loss: 0.6885198372:
1: 83232: loss: 0.6876142357:
1: 86432: loss: 0.6867421676:
1: 89632: loss: 0.6858704972:
1: 92832: loss: 0.6850314956:
1: 96032: loss: 0.6841437631:
1: 99232: loss: 0.6832517824:
1: 102432: loss: 0.6823782315:
1: 105632: loss: 0.6815177701:
1: 108832: loss: 0.6806982373:
1: 112032: loss: 0.6798611891:
1: 115232: loss: 0.6789700724:
1: 118432: loss: 0.6781358388:
1: 121632: loss: 0.6772824614:
1: 124832: loss: 0.6764577491:
1: 128032: loss: 0.6756301627:
1: 131232: loss: 0.6748143931:
1: 134432: loss: 0.6739744677:
1: 137632: loss: 0.6731360874:
1: 140832: loss: 0.6722692980:
1: 144032: loss: 0.6714017851:
1: 147232: loss: 0.6705698666:
1: 150432: loss: 0.6697083469:
Dev-Acc: 1: Accuracy: 0.9352873564: precision: 0.0709504685: recall: 0.0090120728: f1: 0.0159927580
Train-Acc: 1: Accuracy: 0.8952468634: precision: 0.2578700603: recall: 0.0253106305: f1: 0.0460967433
2: 3232: loss: 0.6287059605:
2: 6432: loss: 0.6281528375:
2: 9632: loss: 0.6269738352:
2: 12832: loss: 0.6262593848:
2: 16032: loss: 0.6258687074:
2: 19232: loss: 0.6248125014:
2: 22432: loss: 0.6238944697:
2: 25632: loss: 0.6227080821:
2: 28832: loss: 0.6222386575:
2: 32032: loss: 0.6213797609:
2: 35232: loss: 0.6208388061:
2: 38432: loss: 0.6202462053:
2: 41632: loss: 0.6197291995:
2: 44832: loss: 0.6189679228:
2: 48032: loss: 0.6182607930:
2: 51232: loss: 0.6175890736:
2: 54432: loss: 0.6167038232:
2: 57632: loss: 0.6157203862:
2: 60832: loss: 0.6149861505:
2: 64032: loss: 0.6141333520:
2: 67232: loss: 0.6133992926:
2: 70432: loss: 0.6127012249:
2: 73632: loss: 0.6119510170:
2: 76832: loss: 0.6112364980:
2: 80032: loss: 0.6103247424:
2: 83232: loss: 0.6094769344:
2: 86432: loss: 0.6086784151:
2: 89632: loss: 0.6079074214:
2: 92832: loss: 0.6071721792:
2: 96032: loss: 0.6064485412:
2: 99232: loss: 0.6056695262:
2: 102432: loss: 0.6049351234:
2: 105632: loss: 0.6040841557:
2: 108832: loss: 0.6032260604:
2: 112032: loss: 0.6023864001:
2: 115232: loss: 0.6016858016:
2: 118432: loss: 0.6009249317:
2: 121632: loss: 0.6001332131:
2: 124832: loss: 0.5992123716:
2: 128032: loss: 0.5984274664:
2: 131232: loss: 0.5976618746:
2: 134432: loss: 0.5968648896:
2: 137632: loss: 0.5960876519:
2: 140832: loss: 0.5952446495:
2: 144032: loss: 0.5944984645:
2: 147232: loss: 0.5937823924:
2: 150432: loss: 0.5930545751:
Dev-Acc: 2: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 2: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
3: 3232: loss: 0.5563245171:
3: 6432: loss: 0.5567201337:
3: 9632: loss: 0.5554723357:
3: 12832: loss: 0.5534426728:
3: 16032: loss: 0.5527211322:
3: 19232: loss: 0.5524475500:
3: 22432: loss: 0.5516392888:
3: 25632: loss: 0.5507649744:
3: 28832: loss: 0.5502144073:
3: 32032: loss: 0.5498154551:
3: 35232: loss: 0.5491912520:
3: 38432: loss: 0.5482335689:
3: 41632: loss: 0.5474092528:
3: 44832: loss: 0.5464804520:
3: 48032: loss: 0.5455288795:
3: 51232: loss: 0.5450033374:
3: 54432: loss: 0.5444944425:
3: 57632: loss: 0.5439750802:
3: 60832: loss: 0.5430826939:
3: 64032: loss: 0.5424505071:
3: 67232: loss: 0.5417839313:
3: 70432: loss: 0.5408909609:
3: 73632: loss: 0.5404338519:
3: 76832: loss: 0.5397704324:
3: 80032: loss: 0.5391170383:
3: 83232: loss: 0.5385127973:
3: 86432: loss: 0.5377712782:
3: 89632: loss: 0.5370539374:
3: 92832: loss: 0.5362980399:
3: 96032: loss: 0.5356231990:
3: 99232: loss: 0.5347942539:
3: 102432: loss: 0.5341206611:
3: 105632: loss: 0.5334577984:
3: 108832: loss: 0.5327515791:
3: 112032: loss: 0.5322165992:
3: 115232: loss: 0.5314452467:
3: 118432: loss: 0.5309004305:
3: 121632: loss: 0.5302616488:
3: 124832: loss: 0.5295418924:
3: 128032: loss: 0.5287446705:
3: 131232: loss: 0.5279492861:
3: 134432: loss: 0.5273709668:
3: 137632: loss: 0.5267129966:
3: 140832: loss: 0.5261198953:
3: 144032: loss: 0.5255486537:
3: 147232: loss: 0.5249408234:
3: 150432: loss: 0.5242064102:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.4913266388:
4: 6432: loss: 0.4904819937:
4: 9632: loss: 0.4900741268:
4: 12832: loss: 0.4892859878:
4: 16032: loss: 0.4889517115:
4: 19232: loss: 0.4879994324:
4: 22432: loss: 0.4882463528:
4: 25632: loss: 0.4873012979:
4: 28832: loss: 0.4876868527:
4: 32032: loss: 0.4868017220:
4: 35232: loss: 0.4863861971:
4: 38432: loss: 0.4856660393:
4: 41632: loss: 0.4851257729:
4: 44832: loss: 0.4840123196:
4: 48032: loss: 0.4833452420:
4: 51232: loss: 0.4825029410:
4: 54432: loss: 0.4816464439:
4: 57632: loss: 0.4810352353:
4: 60832: loss: 0.4802902945:
4: 64032: loss: 0.4799512610:
4: 67232: loss: 0.4794841904:
4: 70432: loss: 0.4790989842:
4: 73632: loss: 0.4784338160:
4: 76832: loss: 0.4775538240:
4: 80032: loss: 0.4767078689:
4: 83232: loss: 0.4759508113:
4: 86432: loss: 0.4752266710:
4: 89632: loss: 0.4748427138:
4: 92832: loss: 0.4741563484:
4: 96032: loss: 0.4736528850:
4: 99232: loss: 0.4728215964:
4: 102432: loss: 0.4723767934:
4: 105632: loss: 0.4716076901:
4: 108832: loss: 0.4710934661:
4: 112032: loss: 0.4703090848:
4: 115232: loss: 0.4697288640:
4: 118432: loss: 0.4689530546:
4: 121632: loss: 0.4683378826:
4: 124832: loss: 0.4677239979:
4: 128032: loss: 0.4670731129:
4: 131232: loss: 0.4667640255:
4: 134432: loss: 0.4662299574:
4: 137632: loss: 0.4657622840:
4: 140832: loss: 0.4653106958:
4: 144032: loss: 0.4648016637:
4: 147232: loss: 0.4641017948:
4: 150432: loss: 0.4636714168:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4355729342:
5: 6432: loss: 0.4349409121:
5: 9632: loss: 0.4336597661:
5: 12832: loss: 0.4356011024:
5: 16032: loss: 0.4338588110:
5: 19232: loss: 0.4332160010:
5: 22432: loss: 0.4335530419:
5: 25632: loss: 0.4322853152:
5: 28832: loss: 0.4315384238:
5: 32032: loss: 0.4306270387:
5: 35232: loss: 0.4316708881:
5: 38432: loss: 0.4306295384:
5: 41632: loss: 0.4306276228:
5: 44832: loss: 0.4299086999:
5: 48032: loss: 0.4301309132:
5: 51232: loss: 0.4295664569:
5: 54432: loss: 0.4296146717:
5: 57632: loss: 0.4290034321:
5: 60832: loss: 0.4285042084:
5: 64032: loss: 0.4280729602:
5: 67232: loss: 0.4276458137:
5: 70432: loss: 0.4270683568:
5: 73632: loss: 0.4263049096:
5: 76832: loss: 0.4257095594:
5: 80032: loss: 0.4253398481:
5: 83232: loss: 0.4248018526:
5: 86432: loss: 0.4242744426:
5: 89632: loss: 0.4240041372:
5: 92832: loss: 0.4234627310:
5: 96032: loss: 0.4232178595:
5: 99232: loss: 0.4227228286:
5: 102432: loss: 0.4224543314:
5: 105632: loss: 0.4219554033:
5: 108832: loss: 0.4211990911:
5: 112032: loss: 0.4206348663:
5: 115232: loss: 0.4201463330:
5: 118432: loss: 0.4195248246:
5: 121632: loss: 0.4188151951:
5: 124832: loss: 0.4180038733:
5: 128032: loss: 0.4174258727:
5: 131232: loss: 0.4169535864:
5: 134432: loss: 0.4164034331:
5: 137632: loss: 0.4157973167:
5: 140832: loss: 0.4153193439:
5: 144032: loss: 0.4150163963:
5: 147232: loss: 0.4143594290:
5: 150432: loss: 0.4139559579:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.3870534724:
6: 6432: loss: 0.3858283243:
6: 9632: loss: 0.3880040434:
6: 12832: loss: 0.3911829364:
6: 16032: loss: 0.3904379705:
6: 19232: loss: 0.3904791996:
6: 22432: loss: 0.3912221542:
6: 25632: loss: 0.3906769877:
6: 28832: loss: 0.3893098807:
6: 32032: loss: 0.3887798075:
6: 35232: loss: 0.3888279248:
6: 38432: loss: 0.3882102260:
6: 41632: loss: 0.3874578896:
6: 44832: loss: 0.3873762195:
6: 48032: loss: 0.3877667080:
6: 51232: loss: 0.3875775048:
6: 54432: loss: 0.3875433847:
6: 57632: loss: 0.3875900075:
6: 60832: loss: 0.3868994794:
6: 64032: loss: 0.3863669859:
6: 67232: loss: 0.3856100938:
6: 70432: loss: 0.3846605896:
6: 73632: loss: 0.3841123354:
6: 76832: loss: 0.3839903802:
6: 80032: loss: 0.3837259664:
6: 83232: loss: 0.3833767582:
6: 86432: loss: 0.3831556223:
6: 89632: loss: 0.3826309029:
6: 92832: loss: 0.3820850027:
6: 96032: loss: 0.3818646604:
6: 99232: loss: 0.3813393564:
6: 102432: loss: 0.3803738394:
6: 105632: loss: 0.3802909010:
6: 108832: loss: 0.3797729670:
6: 112032: loss: 0.3794262857:
6: 115232: loss: 0.3790689380:
6: 118432: loss: 0.3789025076:
6: 121632: loss: 0.3784306674:
6: 124832: loss: 0.3782167321:
6: 128032: loss: 0.3776292483:
6: 131232: loss: 0.3773067410:
6: 134432: loss: 0.3766483202:
6: 137632: loss: 0.3762518825:
6: 140832: loss: 0.3758140733:
6: 144032: loss: 0.3754691248:
6: 147232: loss: 0.3751705095:
6: 150432: loss: 0.3748053116:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.3438250795:
7: 6432: loss: 0.3606323738:
7: 9632: loss: 0.3607528919:
7: 12832: loss: 0.3604182174:
7: 16032: loss: 0.3608775521:
7: 19232: loss: 0.3588995270:
7: 22432: loss: 0.3585622026:
7: 25632: loss: 0.3575903645:
7: 28832: loss: 0.3583907807:
7: 32032: loss: 0.3590011830:
7: 35232: loss: 0.3579084848:
7: 38432: loss: 0.3559903879:
7: 41632: loss: 0.3558013327:
7: 44832: loss: 0.3548727339:
7: 48032: loss: 0.3548455405:
7: 51232: loss: 0.3549205866:
7: 54432: loss: 0.3546092441:
7: 57632: loss: 0.3544438936:
7: 60832: loss: 0.3541005430:
7: 64032: loss: 0.3538793912:
7: 67232: loss: 0.3532390817:
7: 70432: loss: 0.3529138730:
7: 73632: loss: 0.3528403012:
7: 76832: loss: 0.3519573421:
7: 80032: loss: 0.3520395543:
7: 83232: loss: 0.3519882879:
7: 86432: loss: 0.3521106657:
7: 89632: loss: 0.3513621932:
7: 92832: loss: 0.3512728999:
7: 96032: loss: 0.3507967698:
7: 99232: loss: 0.3502152769:
7: 102432: loss: 0.3499930207:
7: 105632: loss: 0.3495499822:
7: 108832: loss: 0.3492310659:
7: 112032: loss: 0.3492743715:
7: 115232: loss: 0.3489429471:
7: 118432: loss: 0.3487784253:
7: 121632: loss: 0.3485139410:
7: 124832: loss: 0.3486293694:
7: 128032: loss: 0.3482315072:
7: 131232: loss: 0.3479570371:
7: 134432: loss: 0.3474480155:
7: 137632: loss: 0.3470585133:
7: 140832: loss: 0.3469573817:
7: 144032: loss: 0.3465027452:
7: 147232: loss: 0.3464266725:
7: 150432: loss: 0.3463399001:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3402914742:
8: 6432: loss: 0.3408429311:
8: 9632: loss: 0.3381149892:
8: 12832: loss: 0.3368816010:
8: 16032: loss: 0.3382071004:
8: 19232: loss: 0.3354449291:
8: 22432: loss: 0.3348411033:
8: 25632: loss: 0.3341500592:
8: 28832: loss: 0.3345893618:
8: 32032: loss: 0.3348601366:
8: 35232: loss: 0.3356442361:
8: 38432: loss: 0.3356340561:
8: 41632: loss: 0.3360277096:
8: 44832: loss: 0.3359685918:
8: 48032: loss: 0.3356729834:
8: 51232: loss: 0.3348315252:
8: 54432: loss: 0.3335688860:
8: 57632: loss: 0.3328852866:
8: 60832: loss: 0.3324579335:
8: 64032: loss: 0.3321752055:
8: 67232: loss: 0.3315366790:
8: 70432: loss: 0.3305003279:
8: 73632: loss: 0.3303691184:
8: 76832: loss: 0.3302712013:
8: 80032: loss: 0.3300826797:
8: 83232: loss: 0.3298316133:
8: 86432: loss: 0.3295049081:
8: 89632: loss: 0.3287838688:
8: 92832: loss: 0.3285679857:
8: 96032: loss: 0.3283303474:
8: 99232: loss: 0.3278768979:
8: 102432: loss: 0.3272710729:
8: 105632: loss: 0.3274599965:
8: 108832: loss: 0.3275729912:
8: 112032: loss: 0.3272896419:
8: 115232: loss: 0.3272543317:
8: 118432: loss: 0.3269494619:
8: 121632: loss: 0.3270012538:
8: 124832: loss: 0.3265824091:
8: 128032: loss: 0.3261817861:
8: 131232: loss: 0.3259275464:
8: 134432: loss: 0.3257951654:
8: 137632: loss: 0.3251410086:
8: 140832: loss: 0.3253900287:
8: 144032: loss: 0.3254039521:
8: 147232: loss: 0.3255554894:
8: 150432: loss: 0.3251731733:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3124183159:
9: 6432: loss: 0.3144356333:
9: 9632: loss: 0.3176171371:
9: 12832: loss: 0.3157209527:
9: 16032: loss: 0.3164894571:
9: 19232: loss: 0.3131324935:
9: 22432: loss: 0.3132864449:
9: 25632: loss: 0.3128764573:
9: 28832: loss: 0.3115157354:
9: 32032: loss: 0.3119829738:
9: 35232: loss: 0.3113781272:
9: 38432: loss: 0.3102617054:
9: 41632: loss: 0.3098571982:
9: 44832: loss: 0.3101988715:
9: 48032: loss: 0.3104433051:
9: 51232: loss: 0.3100427888:
9: 54432: loss: 0.3105441177:
9: 57632: loss: 0.3103633581:
9: 60832: loss: 0.3101933297:
9: 64032: loss: 0.3104872546:
9: 67232: loss: 0.3111016038:
9: 70432: loss: 0.3108378686:
9: 73632: loss: 0.3112826637:
9: 76832: loss: 0.3118280027:
9: 80032: loss: 0.3112811057:
9: 83232: loss: 0.3114561357:
9: 86432: loss: 0.3111984405:
9: 89632: loss: 0.3109063735:
9: 92832: loss: 0.3106059638:
9: 96032: loss: 0.3107021359:
9: 99232: loss: 0.3113881483:
9: 102432: loss: 0.3114812552:
9: 105632: loss: 0.3113340013:
9: 108832: loss: 0.3114450738:
9: 112032: loss: 0.3111627518:
9: 115232: loss: 0.3114373798:
9: 118432: loss: 0.3114046421:
9: 121632: loss: 0.3111898903:
9: 124832: loss: 0.3111552517:
9: 128032: loss: 0.3110230071:
9: 131232: loss: 0.3107881522:
9: 134432: loss: 0.3107739929:
9: 137632: loss: 0.3101921454:
9: 140832: loss: 0.3100451589:
9: 144032: loss: 0.3096932708:
9: 147232: loss: 0.3096000831:
9: 150432: loss: 0.3094847892:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.2917311358:
10: 6432: loss: 0.3051430292:
10: 9632: loss: 0.3029045116:
10: 12832: loss: 0.2988844875:
10: 16032: loss: 0.2956851780:
10: 19232: loss: 0.2985717031:
10: 22432: loss: 0.3000133289:
10: 25632: loss: 0.3012894201:
10: 28832: loss: 0.3013947346:
10: 32032: loss: 0.3022518711:
10: 35232: loss: 0.3008248667:
10: 38432: loss: 0.2995748415:
10: 41632: loss: 0.2983818144:
10: 44832: loss: 0.2988742240:
10: 48032: loss: 0.2987071459:
10: 51232: loss: 0.2989301325:
10: 54432: loss: 0.2990327109:
10: 57632: loss: 0.2990271806:
10: 60832: loss: 0.2990651213:
10: 64032: loss: 0.2989332847:
10: 67232: loss: 0.2989874891:
10: 70432: loss: 0.2990688933:
10: 73632: loss: 0.2983475343:
10: 76832: loss: 0.2976803058:
10: 80032: loss: 0.2975186271:
10: 83232: loss: 0.2982874227:
10: 86432: loss: 0.2986150682:
10: 89632: loss: 0.2983210550:
10: 92832: loss: 0.2987906282:
10: 96032: loss: 0.2986786515:
10: 99232: loss: 0.2983758808:
10: 102432: loss: 0.2979320936:
10: 105632: loss: 0.2980471565:
10: 108832: loss: 0.2978801825:
10: 112032: loss: 0.2973965167:
10: 115232: loss: 0.2975021831:
10: 118432: loss: 0.2972448163:
10: 121632: loss: 0.2972783582:
10: 124832: loss: 0.2971802640:
10: 128032: loss: 0.2970843292:
10: 131232: loss: 0.2969066683:
10: 134432: loss: 0.2969188223:
10: 137632: loss: 0.2967435673:
10: 140832: loss: 0.2963233908:
10: 144032: loss: 0.2964463405:
10: 147232: loss: 0.2965499415:
10: 150432: loss: 0.2966580023:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.2922160549:
11: 6432: loss: 0.2978521585:
11: 9632: loss: 0.2941921922:
11: 12832: loss: 0.2929243619:
11: 16032: loss: 0.2946414612:
11: 19232: loss: 0.2929871072:
11: 22432: loss: 0.2914759812:
11: 25632: loss: 0.2910312415:
11: 28832: loss: 0.2905834883:
11: 32032: loss: 0.2901246857:
11: 35232: loss: 0.2890931998:
11: 38432: loss: 0.2889766393:
11: 41632: loss: 0.2882968954:
11: 44832: loss: 0.2872743818:
11: 48032: loss: 0.2877662631:
11: 51232: loss: 0.2877825084:
11: 54432: loss: 0.2869865765:
11: 57632: loss: 0.2867231728:
11: 60832: loss: 0.2877226775:
11: 64032: loss: 0.2865390585:
11: 67232: loss: 0.2865505718:
11: 70432: loss: 0.2870612016:
11: 73632: loss: 0.2875339773:
11: 76832: loss: 0.2878269403:
11: 80032: loss: 0.2873952661:
11: 83232: loss: 0.2878718765:
11: 86432: loss: 0.2875314206:
11: 89632: loss: 0.2871905246:
11: 92832: loss: 0.2868564941:
11: 96032: loss: 0.2871262832:
11: 99232: loss: 0.2869141660:
11: 102432: loss: 0.2868509102:
11: 105632: loss: 0.2865797515:
11: 108832: loss: 0.2871461891:
11: 112032: loss: 0.2871668354:
11: 115232: loss: 0.2871721850:
11: 118432: loss: 0.2873976743:
11: 121632: loss: 0.2867607239:
11: 124832: loss: 0.2864696809:
11: 128032: loss: 0.2867967029:
11: 131232: loss: 0.2867141461:
11: 134432: loss: 0.2863841391:
11: 137632: loss: 0.2862115715:
11: 140832: loss: 0.2859508022:
11: 144032: loss: 0.2860572803:
11: 147232: loss: 0.2864137045:
11: 150432: loss: 0.2863606606:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.2812590869:
12: 6432: loss: 0.2895238905:
12: 9632: loss: 0.2829718986:
12: 12832: loss: 0.2821504930:
12: 16032: loss: 0.2818073618:
12: 19232: loss: 0.2826074572:
12: 22432: loss: 0.2827553565:
12: 25632: loss: 0.2823754240:
12: 28832: loss: 0.2825083807:
12: 32032: loss: 0.2821871806:
12: 35232: loss: 0.2828284562:
12: 38432: loss: 0.2822660418:
12: 41632: loss: 0.2819120634:
12: 44832: loss: 0.2820847602:
12: 48032: loss: 0.2816012771:
12: 51232: loss: 0.2806598491:
12: 54432: loss: 0.2805626340:
12: 57632: loss: 0.2804288840:
12: 60832: loss: 0.2807847059:
12: 64032: loss: 0.2806010726:
12: 67232: loss: 0.2797511138:
12: 70432: loss: 0.2783706083:
12: 73632: loss: 0.2786692158:
12: 76832: loss: 0.2792389571:
12: 80032: loss: 0.2794075184:
12: 83232: loss: 0.2796942107:
12: 86432: loss: 0.2794067545:
12: 89632: loss: 0.2790451973:
12: 92832: loss: 0.2785094886:
12: 96032: loss: 0.2788597408:
12: 99232: loss: 0.2791206823:
12: 102432: loss: 0.2789817606:
12: 105632: loss: 0.2788084020:
12: 108832: loss: 0.2788361523:
12: 112032: loss: 0.2782023850:
12: 115232: loss: 0.2782025643:
12: 118432: loss: 0.2785239330:
12: 121632: loss: 0.2783665680:
12: 124832: loss: 0.2782860792:
12: 128032: loss: 0.2782535201:
12: 131232: loss: 0.2781060658:
12: 134432: loss: 0.2779388142:
12: 137632: loss: 0.2779476300:
12: 140832: loss: 0.2781899295:
12: 144032: loss: 0.2779295231:
12: 147232: loss: 0.2778389598:
12: 150432: loss: 0.2776198754:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2909319723:
13: 6432: loss: 0.2832344667:
13: 9632: loss: 0.2725583443:
13: 12832: loss: 0.2763910281:
13: 16032: loss: 0.2778423490:
13: 19232: loss: 0.2774065713:
13: 22432: loss: 0.2787256910:
13: 25632: loss: 0.2770683816:
13: 28832: loss: 0.2743068188:
13: 32032: loss: 0.2740117814:
13: 35232: loss: 0.2732500665:
13: 38432: loss: 0.2731575733:
13: 41632: loss: 0.2740110856:
13: 44832: loss: 0.2730424206:
13: 48032: loss: 0.2733571729:
13: 51232: loss: 0.2728871004:
13: 54432: loss: 0.2728543167:
13: 57632: loss: 0.2720750458:
13: 60832: loss: 0.2720237409:
13: 64032: loss: 0.2716620059:
13: 67232: loss: 0.2715882533:
13: 70432: loss: 0.2721655395:
13: 73632: loss: 0.2728692305:
13: 76832: loss: 0.2724331759:
13: 80032: loss: 0.2724994168:
13: 83232: loss: 0.2723037643:
13: 86432: loss: 0.2716219535:
13: 89632: loss: 0.2715420941:
13: 92832: loss: 0.2716445954:
13: 96032: loss: 0.2718004201:
13: 99232: loss: 0.2718710832:
13: 102432: loss: 0.2717380231:
13: 105632: loss: 0.2716678897:
13: 108832: loss: 0.2717803086:
13: 112032: loss: 0.2714744500:
13: 115232: loss: 0.2715163305:
13: 118432: loss: 0.2712046785:
13: 121632: loss: 0.2709387110:
13: 124832: loss: 0.2710109989:
13: 128032: loss: 0.2711268447:
13: 131232: loss: 0.2712587748:
13: 134432: loss: 0.2710896598:
13: 137632: loss: 0.2710006970:
13: 140832: loss: 0.2708458445:
13: 144032: loss: 0.2703629536:
13: 147232: loss: 0.2700486588:
13: 150432: loss: 0.2696808907:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.9003484249: precision: 1.0000000000: recall: 0.0034843206: f1: 0.0069444444
14: 3232: loss: 0.2697176825:
14: 6432: loss: 0.2739817545:
14: 9632: loss: 0.2705094926:
14: 12832: loss: 0.2704290625:
14: 16032: loss: 0.2664338295:
14: 19232: loss: 0.2677733593:
14: 22432: loss: 0.2670274546:
14: 25632: loss: 0.2667878072:
14: 28832: loss: 0.2667911119:
14: 32032: loss: 0.2680442902:
14: 35232: loss: 0.2673563329:
14: 38432: loss: 0.2672762421:
14: 41632: loss: 0.2669564128:
14: 44832: loss: 0.2670316639:
14: 48032: loss: 0.2660517046:
14: 51232: loss: 0.2670853156:
14: 54432: loss: 0.2671736895:
14: 57632: loss: 0.2657793370:
14: 60832: loss: 0.2665261244:
14: 64032: loss: 0.2664557096:
14: 67232: loss: 0.2661879096:
14: 70432: loss: 0.2654436451:
14: 73632: loss: 0.2659946834:
14: 76832: loss: 0.2650793081:
14: 80032: loss: 0.2650875855:
14: 83232: loss: 0.2656816511:
14: 86432: loss: 0.2656412710:
14: 89632: loss: 0.2653627500:
14: 92832: loss: 0.2654077407:
14: 96032: loss: 0.2655167488:
14: 99232: loss: 0.2658105388:
14: 102432: loss: 0.2649934244:
14: 105632: loss: 0.2646921381:
14: 108832: loss: 0.2648121517:
14: 112032: loss: 0.2647695847:
14: 115232: loss: 0.2648145519:
14: 118432: loss: 0.2643823522:
14: 121632: loss: 0.2641784138:
14: 124832: loss: 0.2637773205:
14: 128032: loss: 0.2636116885:
14: 131232: loss: 0.2633464267:
14: 134432: loss: 0.2634513824:
14: 137632: loss: 0.2634355675:
14: 140832: loss: 0.2631667561:
14: 144032: loss: 0.2629625356:
14: 147232: loss: 0.2625727073:
14: 150432: loss: 0.2623206675:
Dev-Acc: 14: Accuracy: 0.9418558478: precision: 0.6567164179: recall: 0.0074817208: f1: 0.0147948890
Train-Acc: 14: Accuracy: 0.9020182490: precision: 0.8139059305: recall: 0.0261652751: f1: 0.0507006369
15: 3232: loss: 0.2594810870:
15: 6432: loss: 0.2510575066:
15: 9632: loss: 0.2530396612:
15: 12832: loss: 0.2552531694:
15: 16032: loss: 0.2545790986:
15: 19232: loss: 0.2544009738:
15: 22432: loss: 0.2561039498:
15: 25632: loss: 0.2551279851:
15: 28832: loss: 0.2575299777:
15: 32032: loss: 0.2582708917:
15: 35232: loss: 0.2596429226:
15: 38432: loss: 0.2586580187:
15: 41632: loss: 0.2583548555:
15: 44832: loss: 0.2589657326:
15: 48032: loss: 0.2589357654:
15: 51232: loss: 0.2579927937:
15: 54432: loss: 0.2572903228:
15: 57632: loss: 0.2578773790:
15: 60832: loss: 0.2579754520:
15: 64032: loss: 0.2575893012:
15: 67232: loss: 0.2580820408:
15: 70432: loss: 0.2580382390:
15: 73632: loss: 0.2590609183:
15: 76832: loss: 0.2592365622:
15: 80032: loss: 0.2588956299:
15: 83232: loss: 0.2591817931:
15: 86432: loss: 0.2586780882:
15: 89632: loss: 0.2588391223:
15: 92832: loss: 0.2589744520:
15: 96032: loss: 0.2591239099:
15: 99232: loss: 0.2589316152:
15: 102432: loss: 0.2585411014:
15: 105632: loss: 0.2581902920:
15: 108832: loss: 0.2582112697:
15: 112032: loss: 0.2582262077:
15: 115232: loss: 0.2583135128:
15: 118432: loss: 0.2580276846:
15: 121632: loss: 0.2578004889:
15: 124832: loss: 0.2573052674:
15: 128032: loss: 0.2569088003:
15: 131232: loss: 0.2570705500:
15: 134432: loss: 0.2569800381:
15: 137632: loss: 0.2567535461:
15: 140832: loss: 0.2564772693:
15: 144032: loss: 0.2566966482:
15: 147232: loss: 0.2565856385:
15: 150432: loss: 0.2565672802:
Dev-Acc: 15: Accuracy: 0.9420741200: precision: 0.7009345794: recall: 0.0127529332: f1: 0.0250501002
Train-Acc: 15: Accuracy: 0.9028203487: precision: 0.8206278027: recall: 0.0360923016: f1: 0.0691435768
16: 3232: loss: 0.2523232310:
16: 6432: loss: 0.2522386058:
16: 9632: loss: 0.2486077096:
16: 12832: loss: 0.2491137001:
16: 16032: loss: 0.2490023393:
16: 19232: loss: 0.2479461659:
16: 22432: loss: 0.2492817701:
16: 25632: loss: 0.2484754295:
16: 28832: loss: 0.2507528292:
16: 32032: loss: 0.2513178902:
16: 35232: loss: 0.2505156306:
16: 38432: loss: 0.2513670192:
16: 41632: loss: 0.2521956561:
16: 44832: loss: 0.2525751979:
16: 48032: loss: 0.2517845232:
16: 51232: loss: 0.2514789244:
16: 54432: loss: 0.2510242340:
16: 57632: loss: 0.2506247085:
16: 60832: loss: 0.2499596029:
16: 64032: loss: 0.2508140456:
16: 67232: loss: 0.2507086190:
16: 70432: loss: 0.2508920058:
16: 73632: loss: 0.2516105115:
16: 76832: loss: 0.2514538363:
16: 80032: loss: 0.2516343612:
16: 83232: loss: 0.2519279603:
16: 86432: loss: 0.2521578342:
16: 89632: loss: 0.2528449661:
16: 92832: loss: 0.2526993234:
16: 96032: loss: 0.2526160915:
16: 99232: loss: 0.2522835991:
16: 102432: loss: 0.2519301801:
16: 105632: loss: 0.2519105758:
16: 108832: loss: 0.2519718049:
16: 112032: loss: 0.2517259055:
16: 115232: loss: 0.2515460238:
16: 118432: loss: 0.2515918528:
16: 121632: loss: 0.2514174878:
16: 124832: loss: 0.2514412645:
16: 128032: loss: 0.2514906759:
16: 131232: loss: 0.2509563351:
16: 134432: loss: 0.2509592686:
16: 137632: loss: 0.2508930373:
16: 140832: loss: 0.2504399951:
16: 144032: loss: 0.2505601649:
16: 147232: loss: 0.2506794846:
16: 150432: loss: 0.2506071977:
Dev-Acc: 16: Accuracy: 0.9422923923: precision: 0.5970149254: recall: 0.0340078218: f1: 0.0643500644
Train-Acc: 16: Accuracy: 0.9045953751: precision: 0.8569969356: recall: 0.0551574518: f1: 0.1036442248
17: 3232: loss: 0.2518353560:
17: 6432: loss: 0.2524212846:
17: 9632: loss: 0.2508920569:
17: 12832: loss: 0.2484515785:
17: 16032: loss: 0.2499036130:
17: 19232: loss: 0.2487451746:
17: 22432: loss: 0.2496983988:
17: 25632: loss: 0.2479685703:
17: 28832: loss: 0.2471754253:
17: 32032: loss: 0.2484202965:
17: 35232: loss: 0.2468294185:
17: 38432: loss: 0.2457813538:
17: 41632: loss: 0.2461675572:
17: 44832: loss: 0.2455284523:
17: 48032: loss: 0.2454770509:
17: 51232: loss: 0.2462221212:
17: 54432: loss: 0.2468618605:
17: 57632: loss: 0.2474507519:
17: 60832: loss: 0.2478288778:
17: 64032: loss: 0.2482047310:
17: 67232: loss: 0.2484852635:
17: 70432: loss: 0.2481756819:
17: 73632: loss: 0.2476629306:
17: 76832: loss: 0.2475460264:
17: 80032: loss: 0.2461989290:
17: 83232: loss: 0.2464599544:
17: 86432: loss: 0.2467847385:
17: 89632: loss: 0.2474100672:
17: 92832: loss: 0.2470671075:
17: 96032: loss: 0.2473674220:
17: 99232: loss: 0.2479342819:
17: 102432: loss: 0.2481103216:
17: 105632: loss: 0.2480761395:
17: 108832: loss: 0.2477330156:
17: 112032: loss: 0.2479787717:
17: 115232: loss: 0.2477434036:
17: 118432: loss: 0.2474929478:
17: 121632: loss: 0.2471311101:
17: 124832: loss: 0.2470343641:
17: 128032: loss: 0.2469932622:
17: 131232: loss: 0.2469379872:
17: 134432: loss: 0.2468033833:
17: 137632: loss: 0.2468747073:
17: 140832: loss: 0.2468230925:
17: 144032: loss: 0.2466577175:
17: 147232: loss: 0.2465293279:
17: 150432: loss: 0.2461082627:
Dev-Acc: 17: Accuracy: 0.9419848323: precision: 0.5371179039: recall: 0.0418296208: f1: 0.0776147657
Train-Acc: 17: Accuracy: 0.9059101939: precision: 0.8514464425: recall: 0.0715929262: f1: 0.1320800485
18: 3232: loss: 0.2556764055:
18: 6432: loss: 0.2430247768:
18: 9632: loss: 0.2396186005:
18: 12832: loss: 0.2352557714:
18: 16032: loss: 0.2401929872:
18: 19232: loss: 0.2381765901:
18: 22432: loss: 0.2400578482:
18: 25632: loss: 0.2405303601:
18: 28832: loss: 0.2403014672:
18: 32032: loss: 0.2411397774:
18: 35232: loss: 0.2412607509:
18: 38432: loss: 0.2407487559:
18: 41632: loss: 0.2413086864:
18: 44832: loss: 0.2412965959:
18: 48032: loss: 0.2416851109:
18: 51232: loss: 0.2411335787:
18: 54432: loss: 0.2418917503:
18: 57632: loss: 0.2417414208:
18: 60832: loss: 0.2423974004:
18: 64032: loss: 0.2417145046:
18: 67232: loss: 0.2410927887:
18: 70432: loss: 0.2419087831:
18: 73632: loss: 0.2418711420:
18: 76832: loss: 0.2421712684:
18: 80032: loss: 0.2419380983:
18: 83232: loss: 0.2421546735:
18: 86432: loss: 0.2426706665:
18: 89632: loss: 0.2424697876:
18: 92832: loss: 0.2429126461:
18: 96032: loss: 0.2431033647:
18: 99232: loss: 0.2428401479:
18: 102432: loss: 0.2427251213:
18: 105632: loss: 0.2422817656:
18: 108832: loss: 0.2423992179:
18: 112032: loss: 0.2420694744:
18: 115232: loss: 0.2416635099:
18: 118432: loss: 0.2418145469:
18: 121632: loss: 0.2419424525:
18: 124832: loss: 0.2419931901:
18: 128032: loss: 0.2418233935:
18: 131232: loss: 0.2418308215:
18: 134432: loss: 0.2415402450:
18: 137632: loss: 0.2413283495:
18: 140832: loss: 0.2409586606:
18: 144032: loss: 0.2410324361:
18: 147232: loss: 0.2413944574:
18: 150432: loss: 0.2415013423:
Dev-Acc: 18: Accuracy: 0.9430862069: precision: 0.5898389095: recall: 0.0809386159: f1: 0.1423444976
Train-Acc: 18: Accuracy: 0.9087173939: precision: 0.8583783784: recall: 0.1043981329: f1: 0.1861555595
19: 3232: loss: 0.2443658295:
19: 6432: loss: 0.2379428275:
19: 9632: loss: 0.2365239518:
19: 12832: loss: 0.2377484675:
19: 16032: loss: 0.2418329543:
19: 19232: loss: 0.2419608575:
19: 22432: loss: 0.2413305771:
19: 25632: loss: 0.2424351204:
19: 28832: loss: 0.2413084421:
19: 32032: loss: 0.2414234180:
19: 35232: loss: 0.2401091357:
19: 38432: loss: 0.2388867005:
19: 41632: loss: 0.2385704071:
19: 44832: loss: 0.2389484771:
19: 48032: loss: 0.2388330087:
19: 51232: loss: 0.2374571939:
19: 54432: loss: 0.2371117725:
19: 57632: loss: 0.2365431027:
19: 60832: loss: 0.2359350691:
19: 64032: loss: 0.2361636182:
19: 67232: loss: 0.2363567098:
19: 70432: loss: 0.2366809310:
19: 73632: loss: 0.2368930860:
19: 76832: loss: 0.2365998347:
19: 80032: loss: 0.2358980570:
19: 83232: loss: 0.2355716429:
19: 86432: loss: 0.2350489998:
19: 89632: loss: 0.2359366425:
19: 92832: loss: 0.2361904230:
19: 96032: loss: 0.2366994740:
19: 99232: loss: 0.2370055208:
19: 102432: loss: 0.2370133404:
19: 105632: loss: 0.2373117386:
19: 108832: loss: 0.2372716539:
19: 112032: loss: 0.2370222655:
19: 115232: loss: 0.2367729879:
19: 118432: loss: 0.2370251445:
19: 121632: loss: 0.2371466317:
19: 124832: loss: 0.2371660220:
19: 128032: loss: 0.2373325485:
19: 131232: loss: 0.2369716098:
19: 134432: loss: 0.2369426646:
19: 137632: loss: 0.2370964875:
19: 140832: loss: 0.2371597150:
19: 144032: loss: 0.2372268943:
19: 147232: loss: 0.2375020901:
19: 150432: loss: 0.2374059909:
Dev-Acc: 19: Accuracy: 0.9430762529: precision: 0.5637168142: recall: 0.1083149124: f1: 0.1817144487
Train-Acc: 19: Accuracy: 0.9115048051: precision: 0.8574346405: recall: 0.1379922425: f1: 0.2377258055
20: 3232: loss: 0.2360446853:
20: 6432: loss: 0.2356314659:
20: 9632: loss: 0.2317123758:
20: 12832: loss: 0.2334427912:
20: 16032: loss: 0.2327332486:
20: 19232: loss: 0.2340714235:
20: 22432: loss: 0.2335612131:
20: 25632: loss: 0.2323141309:
20: 28832: loss: 0.2327255907:
20: 32032: loss: 0.2329321877:
20: 35232: loss: 0.2331036033:
20: 38432: loss: 0.2328951588:
20: 41632: loss: 0.2339588816:
20: 44832: loss: 0.2326705435:
20: 48032: loss: 0.2336924844:
20: 51232: loss: 0.2340369934:
20: 54432: loss: 0.2344456003:
20: 57632: loss: 0.2350512889:
20: 60832: loss: 0.2347388039:
20: 64032: loss: 0.2345511156:
20: 67232: loss: 0.2342578355:
20: 70432: loss: 0.2352101915:
20: 73632: loss: 0.2355708827:
20: 76832: loss: 0.2353801104:
20: 80032: loss: 0.2351529284:
20: 83232: loss: 0.2350463314:
20: 86432: loss: 0.2345985378:
20: 89632: loss: 0.2348766422:
20: 92832: loss: 0.2348452570:
20: 96032: loss: 0.2346791057:
20: 99232: loss: 0.2343021296:
20: 102432: loss: 0.2343178464:
20: 105632: loss: 0.2348689141:
20: 108832: loss: 0.2350326315:
20: 112032: loss: 0.2344650692:
20: 115232: loss: 0.2343818508:
20: 118432: loss: 0.2338998561:
20: 121632: loss: 0.2335588371:
20: 124832: loss: 0.2334396631:
20: 128032: loss: 0.2332142760:
20: 131232: loss: 0.2331228605:
20: 134432: loss: 0.2331898331:
20: 137632: loss: 0.2332851036:
20: 140832: loss: 0.2327303476:
20: 144032: loss: 0.2325601405:
20: 147232: loss: 0.2329575371:
20: 150432: loss: 0.2329301085:
Dev-Acc: 20: Accuracy: 0.9433640242: precision: 0.5657794677: recall: 0.1265090971: f1: 0.2067815453
Train-Acc: 20: Accuracy: 0.9138189554: precision: 0.8401294498: recall: 0.1706659654: f1: 0.2837003442
21: 3232: loss: 0.2337836141:
21: 6432: loss: 0.2386746920:
21: 9632: loss: 0.2377593328:
21: 12832: loss: 0.2353358879:
21: 16032: loss: 0.2343451087:
21: 19232: loss: 0.2340132290:
21: 22432: loss: 0.2328468905:
21: 25632: loss: 0.2317755451:
21: 28832: loss: 0.2321173285:
21: 32032: loss: 0.2324474341:
21: 35232: loss: 0.2315308961:
21: 38432: loss: 0.2320670552:
21: 41632: loss: 0.2314572785:
21: 44832: loss: 0.2315393372:
21: 48032: loss: 0.2321349969:
21: 51232: loss: 0.2322809165:
21: 54432: loss: 0.2321895291:
21: 57632: loss: 0.2321971187:
21: 60832: loss: 0.2327853469:
21: 64032: loss: 0.2323477011:
21: 67232: loss: 0.2318420617:
21: 70432: loss: 0.2311274967:
21: 73632: loss: 0.2310699847:
21: 76832: loss: 0.2304490245:
21: 80032: loss: 0.2303516796:
21: 83232: loss: 0.2304939047:
21: 86432: loss: 0.2310108997:
21: 89632: loss: 0.2308060297:
21: 92832: loss: 0.2305352532:
21: 96032: loss: 0.2304385457:
21: 99232: loss: 0.2299283235:
21: 102432: loss: 0.2295643555:
21: 105632: loss: 0.2298401391:
21: 108832: loss: 0.2299286334:
21: 112032: loss: 0.2297865793:
21: 115232: loss: 0.2301007900:
21: 118432: loss: 0.2302970948:
21: 121632: loss: 0.2304719588:
21: 124832: loss: 0.2304591704:
21: 128032: loss: 0.2302266078:
21: 131232: loss: 0.2299141292:
21: 134432: loss: 0.2298402388:
21: 137632: loss: 0.2294847039:
21: 140832: loss: 0.2296586214:
21: 144032: loss: 0.2293896760:
21: 147232: loss: 0.2295363217:
21: 150432: loss: 0.2298602086:
Dev-Acc: 21: Accuracy: 0.9437906742: precision: 0.5661764706: recall: 0.1571161367: f1: 0.2459736457
Train-Acc: 21: Accuracy: 0.9158700705: precision: 0.8136694387: recall: 0.2058378805: f1: 0.3285586862
22: 3232: loss: 0.2233457481:
22: 6432: loss: 0.2216406908:
22: 9632: loss: 0.2236730734:
22: 12832: loss: 0.2209929599:
22: 16032: loss: 0.2257479863:
22: 19232: loss: 0.2253401735:
22: 22432: loss: 0.2260293515:
22: 25632: loss: 0.2248705652:
22: 28832: loss: 0.2264388156:
22: 32032: loss: 0.2264923263:
22: 35232: loss: 0.2267475805:
22: 38432: loss: 0.2270101828:
22: 41632: loss: 0.2273609858:
22: 44832: loss: 0.2280445918:
22: 48032: loss: 0.2274135853:
22: 51232: loss: 0.2270254821:
22: 54432: loss: 0.2273301298:
22: 57632: loss: 0.2269227043:
22: 60832: loss: 0.2273889739:
22: 64032: loss: 0.2273895854:
22: 67232: loss: 0.2268854530:
22: 70432: loss: 0.2268914964:
22: 73632: loss: 0.2264772987:
22: 76832: loss: 0.2261774592:
22: 80032: loss: 0.2261116705:
22: 83232: loss: 0.2254534208:
22: 86432: loss: 0.2258206177:
22: 89632: loss: 0.2257488836:
22: 92832: loss: 0.2253996099:
22: 96032: loss: 0.2256235307:
22: 99232: loss: 0.2260236559:
22: 102432: loss: 0.2261524145:
22: 105632: loss: 0.2264324827:
22: 108832: loss: 0.2257365713:
22: 112032: loss: 0.2258744367:
22: 115232: loss: 0.2253422073:
22: 118432: loss: 0.2254472377:
22: 121632: loss: 0.2260519858:
22: 124832: loss: 0.2256653425:
22: 128032: loss: 0.2257105148:
22: 131232: loss: 0.2257676648:
22: 134432: loss: 0.2255672001:
22: 137632: loss: 0.2262363127:
22: 140832: loss: 0.2262500564:
22: 144032: loss: 0.2262785291:
22: 147232: loss: 0.2263548607:
22: 150432: loss: 0.2263491705:
Dev-Acc: 22: Accuracy: 0.9424908757: precision: 0.5189816883: recall: 0.1975854447: f1: 0.2862068966
Train-Acc: 22: Accuracy: 0.9171192050: precision: 0.7937725632: recall: 0.2312799947: f1: 0.3581937586
23: 3232: loss: 0.2231176108:
23: 6432: loss: 0.2199385388:
23: 9632: loss: 0.2252349412:
23: 12832: loss: 0.2236154380:
23: 16032: loss: 0.2302138073:
23: 19232: loss: 0.2276900165:
23: 22432: loss: 0.2263576574:
23: 25632: loss: 0.2273506556:
23: 28832: loss: 0.2267418255:
23: 32032: loss: 0.2263929635:
23: 35232: loss: 0.2256034372:
23: 38432: loss: 0.2258061648:
23: 41632: loss: 0.2248737934:
23: 44832: loss: 0.2234791886:
23: 48032: loss: 0.2240987027:
23: 51232: loss: 0.2232433027:
23: 54432: loss: 0.2237095271:
23: 57632: loss: 0.2235094069:
23: 60832: loss: 0.2238203325:
23: 64032: loss: 0.2240185017:
23: 67232: loss: 0.2235598285:
23: 70432: loss: 0.2238681622:
23: 73632: loss: 0.2240478102:
23: 76832: loss: 0.2243466324:
23: 80032: loss: 0.2241726258:
23: 83232: loss: 0.2243991597:
23: 86432: loss: 0.2242306523:
23: 89632: loss: 0.2247666908:
23: 92832: loss: 0.2245192498:
23: 96032: loss: 0.2247039139:
23: 99232: loss: 0.2240453251:
23: 102432: loss: 0.2239375694:
23: 105632: loss: 0.2244758356:
23: 108832: loss: 0.2244360286:
23: 112032: loss: 0.2242068469:
23: 115232: loss: 0.2246162039:
23: 118432: loss: 0.2250067027:
23: 121632: loss: 0.2247898186:
23: 124832: loss: 0.2244508538:
23: 128032: loss: 0.2240698494:
23: 131232: loss: 0.2239555876:
23: 134432: loss: 0.2239748887:
23: 137632: loss: 0.2241041504:
23: 140832: loss: 0.2236702519:
23: 144032: loss: 0.2234630058:
23: 147232: loss: 0.2235651078:
23: 150432: loss: 0.2235186493:
Dev-Acc: 23: Accuracy: 0.9412902594: precision: 0.4939106901: recall: 0.2482570991: f1: 0.3304288786
Train-Acc: 23: Accuracy: 0.9187890291: precision: 0.7903291345: recall: 0.2557359805: f1: 0.3864302389
24: 3232: loss: 0.2242110323:
24: 6432: loss: 0.2233418079:
24: 9632: loss: 0.2281669070:
24: 12832: loss: 0.2255444998:
24: 16032: loss: 0.2240412159:
24: 19232: loss: 0.2215679478:
24: 22432: loss: 0.2193335355:
24: 25632: loss: 0.2195827497:
24: 28832: loss: 0.2191604240:
24: 32032: loss: 0.2188391445:
24: 35232: loss: 0.2185951378:
24: 38432: loss: 0.2194796974:
24: 41632: loss: 0.2189127701:
24: 44832: loss: 0.2198641649:
24: 48032: loss: 0.2193833806:
24: 51232: loss: 0.2207105413:
24: 54432: loss: 0.2217364879:
24: 57632: loss: 0.2213516215:
24: 60832: loss: 0.2216293012:
24: 64032: loss: 0.2210138405:
24: 67232: loss: 0.2207317675:
24: 70432: loss: 0.2219301272:
24: 73632: loss: 0.2218224193:
24: 76832: loss: 0.2214307814:
24: 80032: loss: 0.2219861011:
24: 83232: loss: 0.2216401883:
24: 86432: loss: 0.2218796335:
24: 89632: loss: 0.2217614519:
24: 92832: loss: 0.2216163445:
24: 96032: loss: 0.2209154370:
24: 99232: loss: 0.2210213527:
24: 102432: loss: 0.2210414763:
24: 105632: loss: 0.2214117919:
24: 108832: loss: 0.2209481612:
24: 112032: loss: 0.2205376625:
24: 115232: loss: 0.2202537985:
24: 118432: loss: 0.2205500625:
24: 121632: loss: 0.2205227857:
24: 124832: loss: 0.2205503272:
24: 128032: loss: 0.2207321854:
24: 131232: loss: 0.2207348402:
24: 134432: loss: 0.2206116707:
24: 137632: loss: 0.2206932491:
24: 140832: loss: 0.2204781264:
24: 144032: loss: 0.2204289913:
24: 147232: loss: 0.2205785492:
24: 150432: loss: 0.2204880115:
Dev-Acc: 24: Accuracy: 0.9407941699: precision: 0.4870793269: recall: 0.2756333957: f1: 0.3520469106
Train-Acc: 24: Accuracy: 0.9202287793: precision: 0.7939996178: recall: 0.2731575833: f1: 0.4064762277
25: 3232: loss: 0.2158234953:
25: 6432: loss: 0.2122583912:
25: 9632: loss: 0.2181054443:
25: 12832: loss: 0.2147078731:
25: 16032: loss: 0.2176981762:
25: 19232: loss: 0.2180585091:
25: 22432: loss: 0.2191713631:
25: 25632: loss: 0.2177482700:
25: 28832: loss: 0.2169236861:
25: 32032: loss: 0.2186927671:
25: 35232: loss: 0.2185297744:
25: 38432: loss: 0.2173117169:
25: 41632: loss: 0.2187347519:
25: 44832: loss: 0.2184358169:
25: 48032: loss: 0.2187273146:
25: 51232: loss: 0.2198895349:
25: 54432: loss: 0.2199552529:
25: 57632: loss: 0.2198299324:
25: 60832: loss: 0.2204758975:
25: 64032: loss: 0.2197384165:
25: 67232: loss: 0.2190302542:
25: 70432: loss: 0.2190783084:
25: 73632: loss: 0.2185276301:
25: 76832: loss: 0.2178730273:
25: 80032: loss: 0.2181470838:
25: 83232: loss: 0.2179533779:
25: 86432: loss: 0.2179606774:
25: 89632: loss: 0.2183056386:
25: 92832: loss: 0.2185480984:
25: 96032: loss: 0.2180014215:
25: 99232: loss: 0.2178095247:
25: 102432: loss: 0.2177958863:
25: 105632: loss: 0.2175186621:
25: 108832: loss: 0.2174932314:
25: 112032: loss: 0.2175513944:
25: 115232: loss: 0.2174545041:
25: 118432: loss: 0.2175907374:
25: 121632: loss: 0.2172526976:
25: 124832: loss: 0.2173402498:
25: 128032: loss: 0.2169712286:
25: 131232: loss: 0.2169791645:
25: 134432: loss: 0.2173947503:
25: 137632: loss: 0.2173625842:
25: 140832: loss: 0.2173384915:
25: 144032: loss: 0.2172620758:
25: 147232: loss: 0.2174762663:
25: 150432: loss: 0.2174187041:
Dev-Acc: 25: Accuracy: 0.9400698543: precision: 0.4777746715: recall: 0.2905968373: f1: 0.3613871855
Train-Acc: 25: Accuracy: 0.9211885929: precision: 0.7940156906: recall: 0.2861087371: f1: 0.4206456602
26: 3232: loss: 0.2086712680:
26: 6432: loss: 0.2150471903:
26: 9632: loss: 0.2180584651:
26: 12832: loss: 0.2152525196:
26: 16032: loss: 0.2166624555:
26: 19232: loss: 0.2145291297:
26: 22432: loss: 0.2142554290:
26: 25632: loss: 0.2163493359:
26: 28832: loss: 0.2173026544:
26: 32032: loss: 0.2175764432:
26: 35232: loss: 0.2176686133:
26: 38432: loss: 0.2179480318:
26: 41632: loss: 0.2176609167:
26: 44832: loss: 0.2175152198:
26: 48032: loss: 0.2170494000:
26: 51232: loss: 0.2168023069:
26: 54432: loss: 0.2165450319:
26: 57632: loss: 0.2161644618:
26: 60832: loss: 0.2159472059:
26: 64032: loss: 0.2161273125:
26: 67232: loss: 0.2162568253:
26: 70432: loss: 0.2160667723:
26: 73632: loss: 0.2164411424:
26: 76832: loss: 0.2161252543:
26: 80032: loss: 0.2161365920:
26: 83232: loss: 0.2161044855:
26: 86432: loss: 0.2159593265:
26: 89632: loss: 0.2161232158:
26: 92832: loss: 0.2159400788:
26: 96032: loss: 0.2156740826:
26: 99232: loss: 0.2161063627:
26: 102432: loss: 0.2162052073:
26: 105632: loss: 0.2162045725:
26: 108832: loss: 0.2162080174:
26: 112032: loss: 0.2161083443:
26: 115232: loss: 0.2161966057:
26: 118432: loss: 0.2160314512:
26: 121632: loss: 0.2162834477:
26: 124832: loss: 0.2162405094:
26: 128032: loss: 0.2162157468:
26: 131232: loss: 0.2160164943:
26: 134432: loss: 0.2157387373:
26: 137632: loss: 0.2156729553:
26: 140832: loss: 0.2155119641:
26: 144032: loss: 0.2155524805:
26: 147232: loss: 0.2155804711:
26: 150432: loss: 0.2150680300:
Dev-Acc: 26: Accuracy: 0.9396927953: precision: 0.4737263270: recall: 0.3019894576: f1: 0.3688473520
Train-Acc: 26: Accuracy: 0.9220038056: precision: 0.7909928708: recall: 0.2990598909: f1: 0.4340234710
27: 3232: loss: 0.2190107996:
27: 6432: loss: 0.2299556990:
27: 9632: loss: 0.2217115014:
27: 12832: loss: 0.2153061342:
27: 16032: loss: 0.2156175889:
27: 19232: loss: 0.2144947644:
27: 22432: loss: 0.2155848773:
27: 25632: loss: 0.2128982994:
27: 28832: loss: 0.2118986158:
27: 32032: loss: 0.2128378049:
27: 35232: loss: 0.2125495328:
27: 38432: loss: 0.2127155190:
27: 41632: loss: 0.2126225026:
27: 44832: loss: 0.2137857133:
27: 48032: loss: 0.2138486081:
27: 51232: loss: 0.2138138443:
27: 54432: loss: 0.2138829837:
27: 57632: loss: 0.2140105922:
27: 60832: loss: 0.2148112693:
27: 64032: loss: 0.2143596865:
27: 67232: loss: 0.2141688850:
27: 70432: loss: 0.2142224101:
27: 73632: loss: 0.2137448968:
27: 76832: loss: 0.2131800790:
27: 80032: loss: 0.2133194421:
27: 83232: loss: 0.2136944269:
27: 86432: loss: 0.2137956553:
27: 89632: loss: 0.2136534060:
27: 92832: loss: 0.2135475444:
27: 96032: loss: 0.2131453826:
27: 99232: loss: 0.2130491206:
27: 102432: loss: 0.2132170576:
27: 105632: loss: 0.2131926496:
27: 108832: loss: 0.2127402703:
27: 112032: loss: 0.2128206473:
27: 115232: loss: 0.2131717168:
27: 118432: loss: 0.2127638154:
27: 121632: loss: 0.2125350221:
27: 124832: loss: 0.2123565526:
27: 128032: loss: 0.2126251824:
27: 131232: loss: 0.2126838904:
27: 134432: loss: 0.2126409561:
27: 137632: loss: 0.2125981409:
27: 140832: loss: 0.2127682824:
27: 144032: loss: 0.2127752766:
27: 147232: loss: 0.2127903225:
27: 150432: loss: 0.2126571329:
Dev-Acc: 27: Accuracy: 0.9393058419: precision: 0.4696189495: recall: 0.3101513348: f1: 0.3735791091
Train-Acc: 27: Accuracy: 0.9229702353: precision: 0.7919451872: recall: 0.3115508514: f1: 0.4471809389
28: 3232: loss: 0.2064735034:
28: 6432: loss: 0.2180269147:
28: 9632: loss: 0.2200338703:
28: 12832: loss: 0.2156198309:
28: 16032: loss: 0.2162779139:
28: 19232: loss: 0.2143467267:
28: 22432: loss: 0.2133169538:
28: 25632: loss: 0.2123551139:
28: 28832: loss: 0.2117689268:
28: 32032: loss: 0.2125247502:
28: 35232: loss: 0.2129194674:
28: 38432: loss: 0.2126418687:
28: 41632: loss: 0.2125212239:
28: 44832: loss: 0.2118499495:
28: 48032: loss: 0.2121324815:
28: 51232: loss: 0.2126625513:
28: 54432: loss: 0.2124654179:
28: 57632: loss: 0.2127035635:
28: 60832: loss: 0.2123388817:
28: 64032: loss: 0.2120949527:
28: 67232: loss: 0.2124222189:
28: 70432: loss: 0.2119275703:
28: 73632: loss: 0.2116029493:
28: 76832: loss: 0.2111547370:
28: 80032: loss: 0.2106341414:
28: 83232: loss: 0.2109832265:
28: 86432: loss: 0.2106226324:
28: 89632: loss: 0.2109510960:
28: 92832: loss: 0.2104878500:
28: 96032: loss: 0.2104836279:
28: 99232: loss: 0.2106823950:
28: 102432: loss: 0.2105669724:
28: 105632: loss: 0.2103569293:
28: 108832: loss: 0.2104140132:
28: 112032: loss: 0.2104814132:
28: 115232: loss: 0.2104367836:
28: 118432: loss: 0.2105882878:
28: 121632: loss: 0.2102115101:
28: 124832: loss: 0.2104358415:
28: 128032: loss: 0.2106889527:
28: 131232: loss: 0.2107613874:
28: 134432: loss: 0.2109251511:
28: 137632: loss: 0.2107248172:
28: 140832: loss: 0.2107471404:
28: 144032: loss: 0.2108113103:
28: 147232: loss: 0.2106188079:
28: 150432: loss: 0.2104235755:
Dev-Acc: 28: Accuracy: 0.9390478730: precision: 0.4670688788: recall: 0.3159326645: f1: 0.3769144944
Train-Acc: 28: Accuracy: 0.9236999750: precision: 0.7894652321: recall: 0.3231871672: f1: 0.4586248717
29: 3232: loss: 0.2097356471:
29: 6432: loss: 0.2135244634:
29: 9632: loss: 0.2079803140:
29: 12832: loss: 0.2082102360:
29: 16032: loss: 0.2085751809:
29: 19232: loss: 0.2100642058:
29: 22432: loss: 0.2107831118:
29: 25632: loss: 0.2103365746:
29: 28832: loss: 0.2086349062:
29: 32032: loss: 0.2084519695:
29: 35232: loss: 0.2091938534:
29: 38432: loss: 0.2102823147:
29: 41632: loss: 0.2108922906:
29: 44832: loss: 0.2110930976:
29: 48032: loss: 0.2112188510:
29: 51232: loss: 0.2102942513:
29: 54432: loss: 0.2107021656:
29: 57632: loss: 0.2100971997:
29: 60832: loss: 0.2099324295:
29: 64032: loss: 0.2100322874:
29: 67232: loss: 0.2098607016:
29: 70432: loss: 0.2102945506:
29: 73632: loss: 0.2100535036:
29: 76832: loss: 0.2100466449:
29: 80032: loss: 0.2098129580:
29: 83232: loss: 0.2101650926:
29: 86432: loss: 0.2100009119:
29: 89632: loss: 0.2099096725:
29: 92832: loss: 0.2097490841:
29: 96032: loss: 0.2094794202:
29: 99232: loss: 0.2096893846:
29: 102432: loss: 0.2095353414:
29: 105632: loss: 0.2094520089:
29: 108832: loss: 0.2100794129:
29: 112032: loss: 0.2103241122:
29: 115232: loss: 0.2103853670:
29: 118432: loss: 0.2106628211:
29: 121632: loss: 0.2107470443:
29: 124832: loss: 0.2104530753:
29: 128032: loss: 0.2100937024:
29: 131232: loss: 0.2100405040:
29: 134432: loss: 0.2099511380:
29: 137632: loss: 0.2097066475:
29: 140832: loss: 0.2095609578:
29: 144032: loss: 0.2089374507:
29: 147232: loss: 0.2087882439:
29: 150432: loss: 0.2086842121:
Dev-Acc: 29: Accuracy: 0.9389287829: precision: 0.4664215686: recall: 0.3235844244: f1: 0.3820901516
Train-Acc: 29: Accuracy: 0.9244296551: precision: 0.7882407695: recall: 0.3340345802: f1: 0.4692247310
30: 3232: loss: 0.2013839352:
30: 6432: loss: 0.2019742136:
30: 9632: loss: 0.2075294150:
30: 12832: loss: 0.2066527970:
30: 16032: loss: 0.2068091050:
30: 19232: loss: 0.2047374053:
30: 22432: loss: 0.2040447184:
30: 25632: loss: 0.2046436905:
30: 28832: loss: 0.2043053527:
30: 32032: loss: 0.2053517842:
30: 35232: loss: 0.2064296765:
30: 38432: loss: 0.2065223380:
30: 41632: loss: 0.2074799260:
30: 44832: loss: 0.2076078515:
30: 48032: loss: 0.2084067556:
30: 51232: loss: 0.2077228969:
30: 54432: loss: 0.2071676639:
30: 57632: loss: 0.2071821950:
30: 60832: loss: 0.2062925395:
30: 64032: loss: 0.2066747976:
30: 67232: loss: 0.2063672691:
30: 70432: loss: 0.2060203730:
30: 73632: loss: 0.2058348830:
30: 76832: loss: 0.2061369124:
30: 80032: loss: 0.2061373020:
30: 83232: loss: 0.2063735700:
30: 86432: loss: 0.2065591545:
30: 89632: loss: 0.2065088698:
30: 92832: loss: 0.2064411494:
30: 96032: loss: 0.2063290682:
30: 99232: loss: 0.2064696341:
30: 102432: loss: 0.2066713753:
30: 105632: loss: 0.2069582799:
30: 108832: loss: 0.2073315443:
30: 112032: loss: 0.2078496016:
30: 115232: loss: 0.2074376705:
30: 118432: loss: 0.2074551560:
30: 121632: loss: 0.2073047197:
30: 124832: loss: 0.2072026240:
30: 128032: loss: 0.2070718256:
30: 131232: loss: 0.2067447595:
30: 134432: loss: 0.2067734025:
30: 137632: loss: 0.2069590142:
30: 140832: loss: 0.2069671557:
30: 144032: loss: 0.2069726614:
30: 147232: loss: 0.2068689892:
30: 150432: loss: 0.2067823180:
Dev-Acc: 30: Accuracy: 0.9385616779: precision: 0.4628257232: recall: 0.3291957150: f1: 0.3847376789
Train-Acc: 30: Accuracy: 0.9251593947: precision: 0.7870106495: recall: 0.3449477352: f1: 0.4796599324
31: 3232: loss: 0.1983412641:
31: 6432: loss: 0.2013952271:
31: 9632: loss: 0.2044347205:
31: 12832: loss: 0.2059273146:
31: 16032: loss: 0.2055846938:
31: 19232: loss: 0.2079555204:
31: 22432: loss: 0.2060195100:
31: 25632: loss: 0.2056370448:
31: 28832: loss: 0.2040019580:
31: 32032: loss: 0.2026751018:
31: 35232: loss: 0.2027615137:
31: 38432: loss: 0.2020604960:
31: 41632: loss: 0.2022901475:
31: 44832: loss: 0.2023769690:
31: 48032: loss: 0.2029093853:
31: 51232: loss: 0.2030337226:
31: 54432: loss: 0.2038891257:
31: 57632: loss: 0.2038209227:
31: 60832: loss: 0.2043370122:
31: 64032: loss: 0.2045655414:
31: 67232: loss: 0.2049986803:
31: 70432: loss: 0.2044257755:
31: 73632: loss: 0.2038078377:
31: 76832: loss: 0.2037208073:
31: 80032: loss: 0.2038124594:
31: 83232: loss: 0.2033122168:
31: 86432: loss: 0.2035757493:
31: 89632: loss: 0.2036888338:
31: 92832: loss: 0.2039603005:
31: 96032: loss: 0.2038315412:
31: 99232: loss: 0.2038765427:
31: 102432: loss: 0.2037669790:
31: 105632: loss: 0.2039455293:
31: 108832: loss: 0.2036354410:
31: 112032: loss: 0.2035941347:
31: 115232: loss: 0.2035856102:
31: 118432: loss: 0.2035124736:
31: 121632: loss: 0.2036112594:
31: 124832: loss: 0.2040858755:
31: 128032: loss: 0.2045312582:
31: 131232: loss: 0.2044485466:
31: 134432: loss: 0.2043947317:
31: 137632: loss: 0.2043244994:
31: 140832: loss: 0.2044208713:
31: 144032: loss: 0.2046679176:
31: 147232: loss: 0.2047194436:
31: 150432: loss: 0.2042565867:
Dev-Acc: 31: Accuracy: 0.9379960895: precision: 0.4571694600: recall: 0.3339568101: f1: 0.3859683600
Train-Acc: 31: Accuracy: 0.9258497357: precision: 0.7865889213: recall: 0.3547432779: f1: 0.4889674233
32: 3232: loss: 0.1976180552:
32: 6432: loss: 0.1980074168:
32: 9632: loss: 0.2042539403:
32: 12832: loss: 0.2042623606:
32: 16032: loss: 0.2076101527:
32: 19232: loss: 0.2092250392:
32: 22432: loss: 0.2067302648:
32: 25632: loss: 0.2066017632:
32: 28832: loss: 0.2046861289:
32: 32032: loss: 0.2053088528:
32: 35232: loss: 0.2041354975:
32: 38432: loss: 0.2035213080:
32: 41632: loss: 0.2036953997:
32: 44832: loss: 0.2027958043:
32: 48032: loss: 0.2031750456:
32: 51232: loss: 0.2026809673:
32: 54432: loss: 0.2034874583:
32: 57632: loss: 0.2047386802:
32: 60832: loss: 0.2051477458:
32: 64032: loss: 0.2050984801:
32: 67232: loss: 0.2049510414:
32: 70432: loss: 0.2047527789:
32: 73632: loss: 0.2048860114:
32: 76832: loss: 0.2045156436:
32: 80032: loss: 0.2045478905:
32: 83232: loss: 0.2046237595:
32: 86432: loss: 0.2045989593:
32: 89632: loss: 0.2050208245:
32: 92832: loss: 0.2049851790:
32: 96032: loss: 0.2046331961:
32: 99232: loss: 0.2046339457:
32: 102432: loss: 0.2049061532:
32: 105632: loss: 0.2049477672:
32: 108832: loss: 0.2043432634:
32: 112032: loss: 0.2042897103:
32: 115232: loss: 0.2040232742:
32: 118432: loss: 0.2039828346:
32: 121632: loss: 0.2034664534:
32: 124832: loss: 0.2032587111:
32: 128032: loss: 0.2032456278:
32: 131232: loss: 0.2032436575:
32: 134432: loss: 0.2032680754:
32: 137632: loss: 0.2031835467:
32: 140832: loss: 0.2029803268:
32: 144032: loss: 0.2027219012:
32: 147232: loss: 0.2029167249:
32: 150432: loss: 0.2029957688:
Dev-Acc: 32: Accuracy: 0.9374603033: precision: 0.4523486902: recall: 0.3405883353: f1: 0.3885924920
Train-Acc: 32: Accuracy: 0.9262375832: precision: 0.7847053788: recall: 0.3615804352: f1: 0.4950495050
33: 3232: loss: 0.1995846470:
33: 6432: loss: 0.2003884291:
33: 9632: loss: 0.2030352076:
33: 12832: loss: 0.2020781370:
33: 16032: loss: 0.2016239900:
33: 19232: loss: 0.2016904718:
33: 22432: loss: 0.2027147948:
33: 25632: loss: 0.2011254562:
33: 28832: loss: 0.2016856587:
33: 32032: loss: 0.2004966219:
33: 35232: loss: 0.1999716342:
33: 38432: loss: 0.2003833116:
33: 41632: loss: 0.2011296133:
33: 44832: loss: 0.2010562176:
33: 48032: loss: 0.2009321900:
33: 51232: loss: 0.2007866521:
33: 54432: loss: 0.2015229467:
33: 57632: loss: 0.2015963131:
33: 60832: loss: 0.2019652126:
33: 64032: loss: 0.2015296781:
33: 67232: loss: 0.2025062087:
33: 70432: loss: 0.2027635636:
33: 73632: loss: 0.2024158188:
33: 76832: loss: 0.2030675741:
33: 80032: loss: 0.2031479640:
33: 83232: loss: 0.2033368474:
33: 86432: loss: 0.2031182458:
33: 89632: loss: 0.2028955906:
33: 92832: loss: 0.2028486891:
33: 96032: loss: 0.2026260468:
33: 99232: loss: 0.2025291036:
33: 102432: loss: 0.2027769832:
33: 105632: loss: 0.2023780183:
33: 108832: loss: 0.2024742103:
33: 112032: loss: 0.2021439021:
33: 115232: loss: 0.2022194160:
33: 118432: loss: 0.2017870524:
33: 121632: loss: 0.2011978257:
33: 124832: loss: 0.2012809817:
33: 128032: loss: 0.2016990065:
33: 131232: loss: 0.2016740370:
33: 134432: loss: 0.2015888629:
33: 137632: loss: 0.2012843317:
33: 140832: loss: 0.2010573095:
33: 144032: loss: 0.2011479448:
33: 147232: loss: 0.2008574610:
33: 150432: loss: 0.2010893963:
Dev-Acc: 33: Accuracy: 0.9370336533: precision: 0.4485505643: recall: 0.3446692739: f1: 0.3898076923
Train-Acc: 33: Accuracy: 0.9267175198: precision: 0.7848331932: recall: 0.3680888830: f1: 0.5011411949
34: 3232: loss: 0.2048141437:
34: 6432: loss: 0.1994218520:
34: 9632: loss: 0.2008719228:
34: 12832: loss: 0.1989569019:
34: 16032: loss: 0.1984376270:
34: 19232: loss: 0.2004043342:
34: 22432: loss: 0.2007025213:
34: 25632: loss: 0.2007551928:
34: 28832: loss: 0.2028443077:
34: 32032: loss: 0.2026395656:
34: 35232: loss: 0.2026487329:
34: 38432: loss: 0.2021205017:
34: 41632: loss: 0.2020698466:
34: 44832: loss: 0.2011753873:
34: 48032: loss: 0.2018857503:
34: 51232: loss: 0.2008239501:
34: 54432: loss: 0.2000604280:
34: 57632: loss: 0.1995264993:
34: 60832: loss: 0.1994573248:
34: 64032: loss: 0.1995948303:
34: 67232: loss: 0.2004197604:
34: 70432: loss: 0.2005601078:
34: 73632: loss: 0.2005698469:
34: 76832: loss: 0.2010657430:
34: 80032: loss: 0.2010093711:
34: 83232: loss: 0.2004788251:
34: 86432: loss: 0.2005125118:
34: 89632: loss: 0.2001706545:
34: 92832: loss: 0.2004880750:
34: 96032: loss: 0.2004358495:
34: 99232: loss: 0.2003402730:
34: 102432: loss: 0.2002480933:
34: 105632: loss: 0.1995370473:
34: 108832: loss: 0.1993813045:
34: 112032: loss: 0.1996572550:
34: 115232: loss: 0.1991128751:
34: 118432: loss: 0.1998739977:
34: 121632: loss: 0.1998565865:
34: 124832: loss: 0.1999388880:
34: 128032: loss: 0.1999861990:
34: 131232: loss: 0.1998225164:
34: 134432: loss: 0.1997659241:
34: 137632: loss: 0.1998999531:
34: 140832: loss: 0.1997894002:
34: 144032: loss: 0.1998853026:
34: 147232: loss: 0.1999058341:
34: 150432: loss: 0.1997353840:
Dev-Acc: 34: Accuracy: 0.9367955327: precision: 0.4467436288: recall: 0.3487502125: f1: 0.3917112299
Train-Acc: 34: Accuracy: 0.9271908402: precision: 0.7835207019: recall: 0.3757149431: f1: 0.5078871362
35: 3232: loss: 0.1955801238:
35: 6432: loss: 0.1965591965:
35: 9632: loss: 0.1899372298:
35: 12832: loss: 0.1917954646:
35: 16032: loss: 0.1909494990:
35: 19232: loss: 0.1939442644:
35: 22432: loss: 0.1960384585:
35: 25632: loss: 0.1966945306:
35: 28832: loss: 0.1963920658:
35: 32032: loss: 0.1953098217:
35: 35232: loss: 0.1973274817:
35: 38432: loss: 0.1981909905:
35: 41632: loss: 0.1980362164:
35: 44832: loss: 0.1978204775:
35: 48032: loss: 0.1970781471:
35: 51232: loss: 0.1971442391:
35: 54432: loss: 0.1972778705:
35: 57632: loss: 0.1965389894:
35: 60832: loss: 0.1965040941:
35: 64032: loss: 0.1965401676:
35: 67232: loss: 0.1964162922:
35: 70432: loss: 0.1965556253:
35: 73632: loss: 0.1966312086:
35: 76832: loss: 0.1968776559:
35: 80032: loss: 0.1967738193:
35: 83232: loss: 0.1963469646:
35: 86432: loss: 0.1961025941:
35: 89632: loss: 0.1960019891:
35: 92832: loss: 0.1966872659:
35: 96032: loss: 0.1968658839:
35: 99232: loss: 0.1970620857:
35: 102432: loss: 0.1969807196:
35: 105632: loss: 0.1972297364:
35: 108832: loss: 0.1971254425:
35: 112032: loss: 0.1975616816:
35: 115232: loss: 0.1975243279:
35: 118432: loss: 0.1974830262:
35: 121632: loss: 0.1977118621:
35: 124832: loss: 0.1978634453:
35: 128032: loss: 0.1982779597:
35: 131232: loss: 0.1980908471:
35: 134432: loss: 0.1983329923:
35: 137632: loss: 0.1979724752:
35: 140832: loss: 0.1979047646:
35: 144032: loss: 0.1980048913:
35: 147232: loss: 0.1978338495:
35: 150432: loss: 0.1980154967:
Dev-Acc: 35: Accuracy: 0.9362002015: precision: 0.4414587332: recall: 0.3519809556: f1: 0.3916745506
Train-Acc: 35: Accuracy: 0.9276378751: precision: 0.7835176693: recall: 0.3818946815: f1: 0.5135027624
36: 3232: loss: 0.1895884220:
36: 6432: loss: 0.1918377475:
36: 9632: loss: 0.2001544366:
36: 12832: loss: 0.2004719572:
36: 16032: loss: 0.1963741121:
36: 19232: loss: 0.1973174042:
36: 22432: loss: 0.1971222045:
36: 25632: loss: 0.1938426108:
36: 28832: loss: 0.1939575702:
36: 32032: loss: 0.1939308321:
36: 35232: loss: 0.1932214458:
36: 38432: loss: 0.1938574683:
36: 41632: loss: 0.1955980412:
36: 44832: loss: 0.1956480553:
36: 48032: loss: 0.1955427750:
36: 51232: loss: 0.1957644118:
36: 54432: loss: 0.1966416000:
36: 57632: loss: 0.1958937961:
36: 60832: loss: 0.1956496373:
36: 64032: loss: 0.1956279361:
36: 67232: loss: 0.1954987891:
36: 70432: loss: 0.1956380487:
36: 73632: loss: 0.1959158199:
36: 76832: loss: 0.1957321508:
36: 80032: loss: 0.1957480277:
36: 83232: loss: 0.1953871373:
36: 86432: loss: 0.1953683549:
36: 89632: loss: 0.1955220071:
36: 92832: loss: 0.1955407412:
36: 96032: loss: 0.1957572746:
36: 99232: loss: 0.1961143331:
36: 102432: loss: 0.1963028278:
36: 105632: loss: 0.1961921340:
36: 108832: loss: 0.1962180421:
36: 112032: loss: 0.1965117286:
36: 115232: loss: 0.1963080089:
36: 118432: loss: 0.1961053197:
36: 121632: loss: 0.1960214490:
36: 124832: loss: 0.1961424367:
36: 128032: loss: 0.1961163532:
36: 131232: loss: 0.1961483194:
36: 134432: loss: 0.1965697958:
36: 137632: loss: 0.1963366179:
36: 140832: loss: 0.1964128317:
36: 144032: loss: 0.1967160907:
36: 147232: loss: 0.1967856763:
36: 150432: loss: 0.1967262578:
Dev-Acc: 36: Accuracy: 0.9359025359: precision: 0.4391167192: recall: 0.3550416596: f1: 0.3926288078
Train-Acc: 36: Accuracy: 0.9280126095: precision: 0.7816259088: recall: 0.3887318388: f1: 0.5192307692
37: 3232: loss: 0.1895314709:
37: 6432: loss: 0.1889792383:
37: 9632: loss: 0.1939111861:
37: 12832: loss: 0.1948891345:
37: 16032: loss: 0.1965723759:
37: 19232: loss: 0.1975325093:
37: 22432: loss: 0.1985444909:
37: 25632: loss: 0.1976669680:
37: 28832: loss: 0.1972171219:
37: 32032: loss: 0.1976913308:
37: 35232: loss: 0.1973181212:
37: 38432: loss: 0.1964876756:
37: 41632: loss: 0.1974199331:
37: 44832: loss: 0.1969667390:
37: 48032: loss: 0.1968804820:
37: 51232: loss: 0.1961153380:
37: 54432: loss: 0.1956233959:
37: 57632: loss: 0.1950090618:
37: 60832: loss: 0.1948584763:
37: 64032: loss: 0.1949929906:
37: 67232: loss: 0.1954926253:
37: 70432: loss: 0.1956174306:
37: 73632: loss: 0.1959623368:
37: 76832: loss: 0.1961123589:
37: 80032: loss: 0.1959743976:
37: 83232: loss: 0.1960594912:
37: 86432: loss: 0.1958999067:
37: 89632: loss: 0.1960741417:
37: 92832: loss: 0.1958894092:
37: 96032: loss: 0.1961455559:
37: 99232: loss: 0.1965328216:
37: 102432: loss: 0.1961077623:
37: 105632: loss: 0.1961584341:
37: 108832: loss: 0.1961960280:
37: 112032: loss: 0.1963718800:
37: 115232: loss: 0.1963146734:
37: 118432: loss: 0.1963699016:
37: 121632: loss: 0.1962204063:
37: 124832: loss: 0.1963222050:
37: 128032: loss: 0.1960684924:
37: 131232: loss: 0.1959497629:
37: 134432: loss: 0.1958173429:
37: 137632: loss: 0.1958182459:
37: 140832: loss: 0.1959199006:
37: 144032: loss: 0.1957548652:
37: 147232: loss: 0.1958335078:
37: 150432: loss: 0.1956939423:
Dev-Acc: 37: Accuracy: 0.9355850220: precision: 0.4366576819: recall: 0.3581023635: f1: 0.3934977578
Train-Acc: 37: Accuracy: 0.9283347726: precision: 0.7823637317: recall: 0.3925448688: f1: 0.5227859738
38: 3232: loss: 0.1825073593:
38: 6432: loss: 0.1845205593:
38: 9632: loss: 0.1909691681:
38: 12832: loss: 0.1950241197:
38: 16032: loss: 0.1953553569:
38: 19232: loss: 0.1963002423:
38: 22432: loss: 0.1954992224:
38: 25632: loss: 0.1959939025:
38: 28832: loss: 0.1976524295:
38: 32032: loss: 0.1973225217:
38: 35232: loss: 0.1971896219:
38: 38432: loss: 0.1970104443:
38: 41632: loss: 0.1971886999:
38: 44832: loss: 0.1970738821:
38: 48032: loss: 0.1968842303:
38: 51232: loss: 0.1964532836:
38: 54432: loss: 0.1964987248:
38: 57632: loss: 0.1953456421:
38: 60832: loss: 0.1959532034:
38: 64032: loss: 0.1962009912:
38: 67232: loss: 0.1962424277:
38: 70432: loss: 0.1962166119:
38: 73632: loss: 0.1958814106:
38: 76832: loss: 0.1963345871:
38: 80032: loss: 0.1958524730:
38: 83232: loss: 0.1962702263:
38: 86432: loss: 0.1961597454:
38: 89632: loss: 0.1958402243:
38: 92832: loss: 0.1954967477:
38: 96032: loss: 0.1950824538:
38: 99232: loss: 0.1954261928:
38: 102432: loss: 0.1955237847:
38: 105632: loss: 0.1955721744:
38: 108832: loss: 0.1958163640:
38: 112032: loss: 0.1957791176:
38: 115232: loss: 0.1954761207:
38: 118432: loss: 0.1953701466:
38: 121632: loss: 0.1953291978:
38: 124832: loss: 0.1947671128:
38: 128032: loss: 0.1945139181:
38: 131232: loss: 0.1946045384:
38: 134432: loss: 0.1942975553:
38: 137632: loss: 0.1942549677:
38: 140832: loss: 0.1942287788:
38: 144032: loss: 0.1943185827:
38: 147232: loss: 0.1939191965:
38: 150432: loss: 0.1941982000:
Dev-Acc: 38: Accuracy: 0.9354758859: precision: 0.4361658456: recall: 0.3613331066: f1: 0.3952385381
Train-Acc: 38: Accuracy: 0.9286174178: precision: 0.7816746473: recall: 0.3970810598: f1: 0.5266370215
39: 3232: loss: 0.2044032901:
39: 6432: loss: 0.1959825953:
39: 9632: loss: 0.1902540114:
39: 12832: loss: 0.1896461925:
39: 16032: loss: 0.1876802717:
39: 19232: loss: 0.1884311675:
39: 22432: loss: 0.1887241498:
39: 25632: loss: 0.1901862920:
39: 28832: loss: 0.1921478858:
39: 32032: loss: 0.1932088040:
39: 35232: loss: 0.1928793877:
39: 38432: loss: 0.1927385011:
39: 41632: loss: 0.1926741388:
39: 44832: loss: 0.1928990265:
39: 48032: loss: 0.1931957586:
39: 51232: loss: 0.1926889259:
39: 54432: loss: 0.1932421051:
39: 57632: loss: 0.1936307730:
39: 60832: loss: 0.1933673979:
39: 64032: loss: 0.1934168611:
39: 67232: loss: 0.1934937497:
39: 70432: loss: 0.1936001897:
39: 73632: loss: 0.1938990032:
39: 76832: loss: 0.1939064978:
39: 80032: loss: 0.1938232713:
39: 83232: loss: 0.1930387607:
39: 86432: loss: 0.1930974858:
39: 89632: loss: 0.1929393200:
39: 92832: loss: 0.1931773194:
39: 96032: loss: 0.1929608464:
39: 99232: loss: 0.1932597298:
39: 102432: loss: 0.1927533976:
39: 105632: loss: 0.1928578878:
39: 108832: loss: 0.1928620048:
39: 112032: loss: 0.1928567488:
39: 115232: loss: 0.1929732367:
39: 118432: loss: 0.1930626230:
39: 121632: loss: 0.1935882799:
39: 124832: loss: 0.1936755010:
39: 128032: loss: 0.1938303380:
39: 131232: loss: 0.1939641179:
39: 134432: loss: 0.1939120816:
39: 137632: loss: 0.1938829837:
39: 140832: loss: 0.1934815603:
39: 144032: loss: 0.1934647556:
39: 147232: loss: 0.1932543677:
39: 150432: loss: 0.1933823612:
Dev-Acc: 39: Accuracy: 0.9352575541: precision: 0.4350282486: recall: 0.3666043190: f1: 0.3978960967
Train-Acc: 39: Accuracy: 0.9290513396: precision: 0.7827255278: recall: 0.4021431859: f1: 0.5313124294
40: 3232: loss: 0.1975000756:
40: 6432: loss: 0.1881299182:
40: 9632: loss: 0.1857202582:
40: 12832: loss: 0.1845706817:
40: 16032: loss: 0.1868319271:
40: 19232: loss: 0.1866802893:
40: 22432: loss: 0.1862948525:
40: 25632: loss: 0.1878332792:
40: 28832: loss: 0.1889022007:
40: 32032: loss: 0.1888395493:
40: 35232: loss: 0.1887971110:
40: 38432: loss: 0.1891356086:
40: 41632: loss: 0.1896076831:
40: 44832: loss: 0.1894420578:
40: 48032: loss: 0.1901085567:
40: 51232: loss: 0.1907181149:
40: 54432: loss: 0.1923149084:
40: 57632: loss: 0.1919761850:
40: 60832: loss: 0.1927985837:
40: 64032: loss: 0.1932145115:
40: 67232: loss: 0.1934832255:
40: 70432: loss: 0.1928866610:
40: 73632: loss: 0.1928186305:
40: 76832: loss: 0.1923639906:
40: 80032: loss: 0.1926213155:
40: 83232: loss: 0.1923625378:
40: 86432: loss: 0.1921994951:
40: 89632: loss: 0.1922020518:
40: 92832: loss: 0.1925884873:
40: 96032: loss: 0.1925709530:
40: 99232: loss: 0.1928439433:
40: 102432: loss: 0.1927172751:
40: 105632: loss: 0.1925213036:
40: 108832: loss: 0.1922286349:
40: 112032: loss: 0.1918725586:
40: 115232: loss: 0.1921321238:
40: 118432: loss: 0.1920377018:
40: 121632: loss: 0.1921530413:
40: 124832: loss: 0.1918991193:
40: 128032: loss: 0.1920991061:
40: 131232: loss: 0.1919621227:
40: 134432: loss: 0.1918178299:
40: 137632: loss: 0.1917995393:
40: 140832: loss: 0.1918306728:
40: 144032: loss: 0.1921621018:
40: 147232: loss: 0.1920203541:
40: 150432: loss: 0.1918502441:
Dev-Acc: 40: Accuracy: 0.9348309040: precision: 0.4318857823: recall: 0.3703451794: f1: 0.3987550348
Train-Acc: 40: Accuracy: 0.9293997884: precision: 0.7825372757: recall: 0.4071395700: f1: 0.5356108108
41: 3232: loss: 0.1959706905:
41: 6432: loss: 0.1916873040:
41: 9632: loss: 0.1948816161:
41: 12832: loss: 0.1929587388:
41: 16032: loss: 0.1949758224:
41: 19232: loss: 0.1959186435:
41: 22432: loss: 0.1948768967:
41: 25632: loss: 0.1946545257:
41: 28832: loss: 0.1935992873:
41: 32032: loss: 0.1938499636:
41: 35232: loss: 0.1933199847:
41: 38432: loss: 0.1922075473:
41: 41632: loss: 0.1926546416:
41: 44832: loss: 0.1920056269:
41: 48032: loss: 0.1919208903:
41: 51232: loss: 0.1926483059:
41: 54432: loss: 0.1928146577:
41: 57632: loss: 0.1924393357:
41: 60832: loss: 0.1929398665:
41: 64032: loss: 0.1927811638:
41: 67232: loss: 0.1928366829:
41: 70432: loss: 0.1927394721:
41: 73632: loss: 0.1923133146:
41: 76832: loss: 0.1919201513:
41: 80032: loss: 0.1916423971:
41: 83232: loss: 0.1913242383:
41: 86432: loss: 0.1908365938:
41: 89632: loss: 0.1906737944:
41: 92832: loss: 0.1905475062:
41: 96032: loss: 0.1905650849:
41: 99232: loss: 0.1901829803:
41: 102432: loss: 0.1900435729:
41: 105632: loss: 0.1903084929:
41: 108832: loss: 0.1900627762:
41: 112032: loss: 0.1898947079:
41: 115232: loss: 0.1898978162:
41: 118432: loss: 0.1906090544:
41: 121632: loss: 0.1905088955:
41: 124832: loss: 0.1905333047:
41: 128032: loss: 0.1908096305:
41: 131232: loss: 0.1906144513:
41: 134432: loss: 0.1907552360:
41: 137632: loss: 0.1908002113:
41: 140832: loss: 0.1908941711:
41: 144032: loss: 0.1907577199:
41: 147232: loss: 0.1907180298:
41: 150432: loss: 0.1908072694:
Dev-Acc: 41: Accuracy: 0.9346126318: precision: 0.4304492839: recall: 0.3730658051: f1: 0.3997085079
Train-Acc: 41: Accuracy: 0.9295772910: precision: 0.7813633521: recall: 0.4106896325: f1: 0.5383952426
42: 3232: loss: 0.1945567998:
42: 6432: loss: 0.1898256886:
42: 9632: loss: 0.1945505055:
42: 12832: loss: 0.1952865464:
42: 16032: loss: 0.1951497152:
42: 19232: loss: 0.1931338669:
42: 22432: loss: 0.1925955740:
42: 25632: loss: 0.1910457528:
42: 28832: loss: 0.1912774695:
42: 32032: loss: 0.1917637108:
42: 35232: loss: 0.1930934566:
42: 38432: loss: 0.1917033702:
42: 41632: loss: 0.1923848200:
42: 44832: loss: 0.1921949883:
42: 48032: loss: 0.1917081937:
42: 51232: loss: 0.1910299391:
42: 54432: loss: 0.1906890132:
42: 57632: loss: 0.1898777953:
42: 60832: loss: 0.1897816341:
42: 64032: loss: 0.1896939401:
42: 67232: loss: 0.1894687941:
42: 70432: loss: 0.1895471341:
42: 73632: loss: 0.1892676124:
42: 76832: loss: 0.1896643097:
42: 80032: loss: 0.1897114848:
42: 83232: loss: 0.1898708238:
42: 86432: loss: 0.1896144002:
42: 89632: loss: 0.1894527435:
42: 92832: loss: 0.1899785927:
42: 96032: loss: 0.1900254198:
42: 99232: loss: 0.1904697693:
42: 102432: loss: 0.1905303004:
42: 105632: loss: 0.1904054786:
42: 108832: loss: 0.1902085427:
42: 112032: loss: 0.1901876350:
42: 115232: loss: 0.1902724152:
42: 118432: loss: 0.1899336017:
42: 121632: loss: 0.1898754492:
42: 124832: loss: 0.1896253223:
42: 128032: loss: 0.1898246496:
42: 131232: loss: 0.1898727263:
42: 134432: loss: 0.1897646110:
42: 137632: loss: 0.1899020364:
42: 140832: loss: 0.1899208597:
42: 144032: loss: 0.1897037979:
42: 147232: loss: 0.1896353519:
42: 150432: loss: 0.1894881742:
Dev-Acc: 42: Accuracy: 0.9344042540: precision: 0.4290711232: recall: 0.3754463527: f1: 0.4004715698
Train-Acc: 42: Accuracy: 0.9298468232: precision: 0.7814980159: recall: 0.4143054369: f1: 0.5415252417
43: 3232: loss: 0.1972610676:
43: 6432: loss: 0.1946690408:
43: 9632: loss: 0.1865555857:
43: 12832: loss: 0.1887266830:
43: 16032: loss: 0.1891932308:
43: 19232: loss: 0.1907308799:
43: 22432: loss: 0.1923960016:
43: 25632: loss: 0.1916006924:
43: 28832: loss: 0.1908978558:
43: 32032: loss: 0.1897086349:
43: 35232: loss: 0.1893023281:
43: 38432: loss: 0.1898283600:
43: 41632: loss: 0.1894850763:
43: 44832: loss: 0.1887745466:
43: 48032: loss: 0.1879200867:
43: 51232: loss: 0.1878900628:
43: 54432: loss: 0.1889966264:
43: 57632: loss: 0.1891876112:
43: 60832: loss: 0.1891417503:
43: 64032: loss: 0.1888793758:
43: 67232: loss: 0.1888378228:
43: 70432: loss: 0.1888205570:
43: 73632: loss: 0.1886259442:
43: 76832: loss: 0.1889226264:
43: 80032: loss: 0.1894870573:
43: 83232: loss: 0.1892260727:
43: 86432: loss: 0.1888446148:
43: 89632: loss: 0.1888469467:
43: 92832: loss: 0.1889712511:
43: 96032: loss: 0.1888129317:
43: 99232: loss: 0.1886536962:
43: 102432: loss: 0.1891831989:
43: 105632: loss: 0.1889661091:
43: 108832: loss: 0.1886926485:
43: 112032: loss: 0.1884241337:
43: 115232: loss: 0.1890262184:
43: 118432: loss: 0.1887961667:
43: 121632: loss: 0.1889334501:
43: 124832: loss: 0.1889037440:
43: 128032: loss: 0.1887879113:
43: 131232: loss: 0.1885950180:
43: 134432: loss: 0.1886380012:
43: 137632: loss: 0.1885290500:
43: 140832: loss: 0.1888722142:
43: 144032: loss: 0.1890200431:
43: 147232: loss: 0.1893836895:
43: 150432: loss: 0.1890958677:
Dev-Acc: 43: Accuracy: 0.9342355728: precision: 0.4280762565: recall: 0.3779969393: f1: 0.4014809464
Train-Acc: 43: Accuracy: 0.9300835133: precision: 0.7817040138: recall: 0.4173953060: f1: 0.5442077744
44: 3232: loss: 0.1839842739:
44: 6432: loss: 0.1890185209:
44: 9632: loss: 0.1913652107:
44: 12832: loss: 0.1914742860:
44: 16032: loss: 0.1905852065:
44: 19232: loss: 0.1918533745:
44: 22432: loss: 0.1917466218:
44: 25632: loss: 0.1899937438:
44: 28832: loss: 0.1889275548:
44: 32032: loss: 0.1885407766:
44: 35232: loss: 0.1888289044:
44: 38432: loss: 0.1876396305:
44: 41632: loss: 0.1871590429:
44: 44832: loss: 0.1880357054:
44: 48032: loss: 0.1882600273:
44: 51232: loss: 0.1890825319:
44: 54432: loss: 0.1896256608:
44: 57632: loss: 0.1893471871:
44: 60832: loss: 0.1891026442:
44: 64032: loss: 0.1885891499:
44: 67232: loss: 0.1887502203:
44: 70432: loss: 0.1887309270:
44: 73632: loss: 0.1888921456:
44: 76832: loss: 0.1888465512:
44: 80032: loss: 0.1885686007:
44: 83232: loss: 0.1884927282:
44: 86432: loss: 0.1882072857:
44: 89632: loss: 0.1880487370:
44: 92832: loss: 0.1883174401:
44: 96032: loss: 0.1884242517:
44: 99232: loss: 0.1887052091:
44: 102432: loss: 0.1887537710:
44: 105632: loss: 0.1888422272:
44: 108832: loss: 0.1884757727:
44: 112032: loss: 0.1880917688:
44: 115232: loss: 0.1881761607:
44: 118432: loss: 0.1877762121:
44: 121632: loss: 0.1874929844:
44: 124832: loss: 0.1873710847:
44: 128032: loss: 0.1875760313:
44: 131232: loss: 0.1874907682:
44: 134432: loss: 0.1873076632:
44: 137632: loss: 0.1875170566:
44: 140832: loss: 0.1874681070:
44: 144032: loss: 0.1874871167:
44: 147232: loss: 0.1879051832:
44: 150432: loss: 0.1878921043:
Dev-Acc: 44: Accuracy: 0.9341165423: precision: 0.4275348482: recall: 0.3807175650: f1: 0.4027702824
Train-Acc: 44: Accuracy: 0.9302873015: precision: 0.7812232939: recall: 0.4206824009: f1: 0.5468763354
45: 3232: loss: 0.1986803373:
45: 6432: loss: 0.1866147411:
45: 9632: loss: 0.1853182464:
45: 12832: loss: 0.1831168562:
45: 16032: loss: 0.1817214817:
45: 19232: loss: 0.1840497952:
45: 22432: loss: 0.1848393707:
45: 25632: loss: 0.1833164695:
45: 28832: loss: 0.1833733010:
45: 32032: loss: 0.1846760901:
45: 35232: loss: 0.1840328217:
45: 38432: loss: 0.1847776487:
45: 41632: loss: 0.1854860435:
45: 44832: loss: 0.1854445492:
45: 48032: loss: 0.1857493698:
45: 51232: loss: 0.1858230415:
45: 54432: loss: 0.1855538639:
45: 57632: loss: 0.1867270111:
45: 60832: loss: 0.1867603072:
45: 64032: loss: 0.1870850294:
45: 67232: loss: 0.1867171707:
45: 70432: loss: 0.1872451475:
45: 73632: loss: 0.1872433942:
45: 76832: loss: 0.1870783996:
45: 80032: loss: 0.1869469749:
45: 83232: loss: 0.1873684856:
45: 86432: loss: 0.1874964418:
45: 89632: loss: 0.1877496314:
45: 92832: loss: 0.1876473049:
45: 96032: loss: 0.1871859906:
45: 99232: loss: 0.1871554798:
45: 102432: loss: 0.1872601002:
45: 105632: loss: 0.1873708929:
45: 108832: loss: 0.1872527170:
45: 112032: loss: 0.1872547167:
45: 115232: loss: 0.1874292838:
45: 118432: loss: 0.1873693237:
45: 121632: loss: 0.1874188301:
45: 124832: loss: 0.1877643110:
45: 128032: loss: 0.1876271028:
45: 131232: loss: 0.1874615280:
45: 134432: loss: 0.1876485507:
45: 137632: loss: 0.1877192849:
45: 140832: loss: 0.1874587887:
45: 144032: loss: 0.1871965454:
45: 147232: loss: 0.1871459208:
45: 150432: loss: 0.1871497959:
Dev-Acc: 45: Accuracy: 0.9339776039: precision: 0.4271167264: recall: 0.3851385819: f1: 0.4050429185
Train-Acc: 45: Accuracy: 0.9304121733: precision: 0.7803636364: recall: 0.4232463349: f1: 0.5488257108
46: 3232: loss: 0.1977352859:
46: 6432: loss: 0.1905583683:
46: 9632: loss: 0.1915433888:
46: 12832: loss: 0.1865269294:
46: 16032: loss: 0.1841002389:
46: 19232: loss: 0.1875155036:
46: 22432: loss: 0.1868549492:
46: 25632: loss: 0.1850467295:
46: 28832: loss: 0.1865639024:
46: 32032: loss: 0.1860356636:
46: 35232: loss: 0.1859253280:
46: 38432: loss: 0.1851159272:
46: 41632: loss: 0.1851278733:
46: 44832: loss: 0.1858283050:
46: 48032: loss: 0.1868329156:
46: 51232: loss: 0.1868670296:
46: 54432: loss: 0.1878645984:
46: 57632: loss: 0.1880162490:
46: 60832: loss: 0.1878583312:
46: 64032: loss: 0.1880336560:
46: 67232: loss: 0.1883640776:
46: 70432: loss: 0.1880849101:
46: 73632: loss: 0.1883946912:
46: 76832: loss: 0.1878778913:
46: 80032: loss: 0.1870507682:
46: 83232: loss: 0.1870980410:
46: 86432: loss: 0.1870027142:
46: 89632: loss: 0.1865549833:
46: 92832: loss: 0.1861496798:
46: 96032: loss: 0.1856765140:
46: 99232: loss: 0.1855886350:
46: 102432: loss: 0.1856638456:
46: 105632: loss: 0.1850730023:
46: 108832: loss: 0.1849482397:
46: 112032: loss: 0.1846282390:
46: 115232: loss: 0.1847653755:
46: 118432: loss: 0.1847363504:
46: 121632: loss: 0.1846129526:
46: 124832: loss: 0.1852432959:
46: 128032: loss: 0.1854614141:
46: 131232: loss: 0.1857736325:
46: 134432: loss: 0.1857724204:
46: 137632: loss: 0.1857544093:
46: 140832: loss: 0.1860131924:
46: 144032: loss: 0.1859892772:
46: 147232: loss: 0.1857979777:
46: 150432: loss: 0.1856954459:
Dev-Acc: 46: Accuracy: 0.9339577556: precision: 0.4272573681: recall: 0.3870090121: f1: 0.4061384725
Train-Acc: 46: Accuracy: 0.9306488633: precision: 0.7806405008: recall: 0.4262704622: f1: 0.5514308798
47: 3232: loss: 0.1962563246:
47: 6432: loss: 0.1896139863:
47: 9632: loss: 0.1879081974:
47: 12832: loss: 0.1869171337:
47: 16032: loss: 0.1884250520:
47: 19232: loss: 0.1883176377:
47: 22432: loss: 0.1888328165:
47: 25632: loss: 0.1877808598:
47: 28832: loss: 0.1874723425:
47: 32032: loss: 0.1872073373:
47: 35232: loss: 0.1863135281:
47: 38432: loss: 0.1865572802:
47: 41632: loss: 0.1858332539:
47: 44832: loss: 0.1856494395:
47: 48032: loss: 0.1846310573:
47: 51232: loss: 0.1843499132:
47: 54432: loss: 0.1842292645:
47: 57632: loss: 0.1847682936:
47: 60832: loss: 0.1848573266:
47: 64032: loss: 0.1847437828:
47: 67232: loss: 0.1852215514:
47: 70432: loss: 0.1851126884:
47: 73632: loss: 0.1852040633:
47: 76832: loss: 0.1853171633:
47: 80032: loss: 0.1852969124:
47: 83232: loss: 0.1855941311:
47: 86432: loss: 0.1858673911:
47: 89632: loss: 0.1858731799:
47: 92832: loss: 0.1856843158:
47: 96032: loss: 0.1856261214:
47: 99232: loss: 0.1854141918:
47: 102432: loss: 0.1856906675:
47: 105632: loss: 0.1855020501:
47: 108832: loss: 0.1858083990:
47: 112032: loss: 0.1854520785:
47: 115232: loss: 0.1854263022:
47: 118432: loss: 0.1851300308:
47: 121632: loss: 0.1853440411:
47: 124832: loss: 0.1855178944:
47: 128032: loss: 0.1856453849:
47: 131232: loss: 0.1858956371:
47: 134432: loss: 0.1854901089:
47: 137632: loss: 0.1854228782:
47: 140832: loss: 0.1852220487:
47: 144032: loss: 0.1852692661:
47: 147232: loss: 0.1852581600:
47: 150432: loss: 0.1853484936:
Dev-Acc: 47: Accuracy: 0.9338188767: precision: 0.4263580362: recall: 0.3883693249: f1: 0.4064780210
Train-Acc: 47: Accuracy: 0.9309118390: precision: 0.7802813543: recall: 0.4302807179: f1: 0.5546845205
48: 3232: loss: 0.1797756241:
48: 6432: loss: 0.1927477129:
48: 9632: loss: 0.1912553225:
48: 12832: loss: 0.1900732231:
48: 16032: loss: 0.1896201621:
48: 19232: loss: 0.1888705104:
48: 22432: loss: 0.1879352759:
48: 25632: loss: 0.1870147281:
48: 28832: loss: 0.1890270603:
48: 32032: loss: 0.1887236603:
48: 35232: loss: 0.1889619210:
48: 38432: loss: 0.1888610839:
48: 41632: loss: 0.1895290399:
48: 44832: loss: 0.1891085657:
48: 48032: loss: 0.1878189987:
48: 51232: loss: 0.1880745086:
48: 54432: loss: 0.1881360889:
48: 57632: loss: 0.1875802377:
48: 60832: loss: 0.1871739782:
48: 64032: loss: 0.1870704496:
48: 67232: loss: 0.1864884158:
48: 70432: loss: 0.1865878146:
48: 73632: loss: 0.1860013827:
48: 76832: loss: 0.1861633501:
48: 80032: loss: 0.1867451041:
48: 83232: loss: 0.1873237067:
48: 86432: loss: 0.1868708534:
48: 89632: loss: 0.1867466109:
48: 92832: loss: 0.1866590859:
48: 96032: loss: 0.1867818835:
48: 99232: loss: 0.1864827013:
48: 102432: loss: 0.1866485290:
48: 105632: loss: 0.1867077493:
48: 108832: loss: 0.1862913720:
48: 112032: loss: 0.1860510575:
48: 115232: loss: 0.1861583043:
48: 118432: loss: 0.1861552960:
48: 121632: loss: 0.1861023348:
48: 124832: loss: 0.1858585884:
48: 128032: loss: 0.1854969521:
48: 131232: loss: 0.1857166750:
48: 134432: loss: 0.1853361934:
48: 137632: loss: 0.1851302174:
48: 140832: loss: 0.1848471001:
48: 144032: loss: 0.1847688339:
48: 147232: loss: 0.1847867397:
48: 150432: loss: 0.1848756126:
Dev-Acc: 48: Accuracy: 0.9336898327: precision: 0.4255200594: recall: 0.3895595987: f1: 0.4067465601
Train-Acc: 48: Accuracy: 0.9311156273: precision: 0.7805572021: recall: 0.4328446519: f1: 0.5568806563
49: 3232: loss: 0.1894543964:
49: 6432: loss: 0.1882509759:
49: 9632: loss: 0.1874548958:
49: 12832: loss: 0.1876359887:
49: 16032: loss: 0.1869355410:
49: 19232: loss: 0.1883866059:
49: 22432: loss: 0.1889629122:
49: 25632: loss: 0.1887813525:
49: 28832: loss: 0.1869910693:
49: 32032: loss: 0.1859690130:
49: 35232: loss: 0.1845876593:
49: 38432: loss: 0.1854886832:
49: 41632: loss: 0.1843186093:
49: 44832: loss: 0.1839285721:
49: 48032: loss: 0.1844671271:
49: 51232: loss: 0.1847070887:
49: 54432: loss: 0.1839351692:
49: 57632: loss: 0.1842918722:
49: 60832: loss: 0.1842187685:
49: 64032: loss: 0.1838193820:
49: 67232: loss: 0.1840682309:
49: 70432: loss: 0.1835195047:
49: 73632: loss: 0.1834005762:
49: 76832: loss: 0.1831588291:
49: 80032: loss: 0.1835722076:
49: 83232: loss: 0.1837918003:
49: 86432: loss: 0.1838263180:
49: 89632: loss: 0.1838151593:
49: 92832: loss: 0.1839067416:
49: 96032: loss: 0.1837378691:
49: 99232: loss: 0.1833265665:
49: 102432: loss: 0.1833268139:
49: 105632: loss: 0.1826207497:
49: 108832: loss: 0.1827111897:
49: 112032: loss: 0.1824371873:
49: 115232: loss: 0.1825087681:
49: 118432: loss: 0.1825032857:
49: 121632: loss: 0.1825811548:
49: 124832: loss: 0.1827119206:
49: 128032: loss: 0.1828420719:
49: 131232: loss: 0.1832078523:
49: 134432: loss: 0.1835367028:
49: 137632: loss: 0.1835283707:
49: 140832: loss: 0.1832658286:
49: 144032: loss: 0.1832863709:
49: 147232: loss: 0.1834745916:
49: 150432: loss: 0.1836067615:
Dev-Acc: 49: Accuracy: 0.9336799383: precision: 0.4257992977: recall: 0.3917701071: f1: 0.4080765143
Train-Acc: 49: Accuracy: 0.9314114451: precision: 0.7808605690: recall: 0.4366576819: f1: 0.5601045663
50: 3232: loss: 0.1778781995:
50: 6432: loss: 0.1786616529:
50: 9632: loss: 0.1782201944:
50: 12832: loss: 0.1779093089:
50: 16032: loss: 0.1761310828:
50: 19232: loss: 0.1791476399:
50: 22432: loss: 0.1804595060:
50: 25632: loss: 0.1807819371:
50: 28832: loss: 0.1800594747:
50: 32032: loss: 0.1794146177:
50: 35232: loss: 0.1793628920:
50: 38432: loss: 0.1803873942:
50: 41632: loss: 0.1806428017:
50: 44832: loss: 0.1812413332:
50: 48032: loss: 0.1813255363:
50: 51232: loss: 0.1825031098:
50: 54432: loss: 0.1824411364:
50: 57632: loss: 0.1818024030:
50: 60832: loss: 0.1824809856:
50: 64032: loss: 0.1831312106:
50: 67232: loss: 0.1831140584:
50: 70432: loss: 0.1831919324:
50: 73632: loss: 0.1829900521:
50: 76832: loss: 0.1834551079:
50: 80032: loss: 0.1827730558:
50: 83232: loss: 0.1830375562:
50: 86432: loss: 0.1830423984:
50: 89632: loss: 0.1827969104:
50: 92832: loss: 0.1830271443:
50: 96032: loss: 0.1835204757:
50: 99232: loss: 0.1832739074:
50: 102432: loss: 0.1831958534:
50: 105632: loss: 0.1829579073:
50: 108832: loss: 0.1831716610:
50: 112032: loss: 0.1829115448:
50: 115232: loss: 0.1831086671:
50: 118432: loss: 0.1829540778:
50: 121632: loss: 0.1829020150:
50: 124832: loss: 0.1828794207:
50: 128032: loss: 0.1828362588:
50: 131232: loss: 0.1831922416:
50: 134432: loss: 0.1830349715:
50: 137632: loss: 0.1833149376:
50: 140832: loss: 0.1834701696:
50: 144032: loss: 0.1833480550:
50: 147232: loss: 0.1832797591:
50: 150432: loss: 0.1827858349:
Dev-Acc: 50: Accuracy: 0.9336203933: precision: 0.4254377880: recall: 0.3924502636: f1: 0.4082787900
Train-Acc: 50: Accuracy: 0.9316481352: precision: 0.7809946299: recall: 0.4398132930: f1: 0.5627286874
