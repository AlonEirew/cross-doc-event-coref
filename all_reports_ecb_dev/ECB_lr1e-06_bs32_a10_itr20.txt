1: 3232: loss: 0.7021460646:
1: 6432: loss: 0.6922341475:
1: 9632: loss: 0.6829572688:
1: 12832: loss: 0.6741420944:
1: 16032: loss: 0.6651136414:
1: 19232: loss: 0.6564364578:
1: 22432: loss: 0.6478844404:
1: 25632: loss: 0.6401123224:
1: 28832: loss: 0.6320903969:
1: 32032: loss: 0.6241046999:
1: 35232: loss: 0.6162115765:
1: 38432: loss: 0.6084740863:
1: 41632: loss: 0.6006302061:
1: 44832: loss: 0.5929501959:
1: 48032: loss: 0.5851717709:
1: 51232: loss: 0.5773751532:
1: 54432: loss: 0.5697789003:
1: 57632: loss: 0.5625698026:
1: 60832: loss: 0.5554722269:
1: 64032: loss: 0.5484435542:
1: 67232: loss: 0.5415659538:
1: 70432: loss: 0.5350361250:
1: 73632: loss: 0.5285964519:
1: 76832: loss: 0.5221565500:
1: 80032: loss: 0.5162078332:
1: 83232: loss: 0.5103719230:
1: 86432: loss: 0.5049456256:
1: 89632: loss: 0.4998259176:
1: 92832: loss: 0.4939995273:
1: 96032: loss: 0.4889438147:
1: 99232: loss: 0.4837939495:
1: 102432: loss: 0.4789223457:
1: 105632: loss: 0.4743065341:
1: 108832: loss: 0.4696198291:
1: 112032: loss: 0.4655882713:
1: 115232: loss: 0.4611483986:
1: 118432: loss: 0.4568454064:
1: 121632: loss: 0.4528054674:
1: 124832: loss: 0.4486589013:
1: 128032: loss: 0.4448976388:
1: 131232: loss: 0.4414396817:
1: 134432: loss: 0.4379949760:
1: 137632: loss: 0.4346376542:
1: 140832: loss: 0.4308900388:
1: 144032: loss: 0.4273032450:
1: 147232: loss: 0.4241926093:
1: 150432: loss: 0.4210383038:
1: 153632: loss: 0.4180682366:
1: 156832: loss: 0.4152141482:
1: 160032: loss: 0.4122628668:
1: 163232: loss: 0.4092873607:
1: 166432: loss: 0.4067879050:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 3232: loss: 0.2653732442:
2: 6432: loss: 0.2750884660:
2: 9632: loss: 0.2689958932:
2: 12832: loss: 0.2691998307:
2: 16032: loss: 0.2659272462:
2: 19232: loss: 0.2675474520:
2: 22432: loss: 0.2684975919:
2: 25632: loss: 0.2671990689:
2: 28832: loss: 0.2650689516:
2: 32032: loss: 0.2639058638:
2: 35232: loss: 0.2629501773:
2: 38432: loss: 0.2622589545:
2: 41632: loss: 0.2615876876:
2: 44832: loss: 0.2600919684:
2: 48032: loss: 0.2589018197:
2: 51232: loss: 0.2585109997:
2: 54432: loss: 0.2579191239:
2: 57632: loss: 0.2577860063:
2: 60832: loss: 0.2580988719:
2: 64032: loss: 0.2571841273:
2: 67232: loss: 0.2570534758:
2: 70432: loss: 0.2565697017:
2: 73632: loss: 0.2553950537:
2: 76832: loss: 0.2555353966:
2: 80032: loss: 0.2543371194:
2: 83232: loss: 0.2536542179:
2: 86432: loss: 0.2529306190:
2: 89632: loss: 0.2522746577:
2: 92832: loss: 0.2518058391:
2: 96032: loss: 0.2510526106:
2: 99232: loss: 0.2503557182:
2: 102432: loss: 0.2496561241:
2: 105632: loss: 0.2488430872:
2: 108832: loss: 0.2485823395:
2: 112032: loss: 0.2475307183:
2: 115232: loss: 0.2472682839:
2: 118432: loss: 0.2463269965:
2: 121632: loss: 0.2460308009:
2: 124832: loss: 0.2453708058:
2: 128032: loss: 0.2449462645:
2: 131232: loss: 0.2447019169:
2: 134432: loss: 0.2439342923:
2: 137632: loss: 0.2434865051:
2: 140832: loss: 0.2427702593:
2: 144032: loss: 0.2419873413:
2: 147232: loss: 0.2413021446:
2: 150432: loss: 0.2408436395:
2: 153632: loss: 0.2405358651:
2: 156832: loss: 0.2399370631:
2: 160032: loss: 0.2396581359:
2: 163232: loss: 0.2391159137:
2: 166432: loss: 0.2386298383:
Dev-Acc: 2: Accuracy: 0.9439890981: precision: 0.5763260026: recall: 0.1515048461: f1: 0.2399353709
Train-Acc: 2: Accuracy: 0.9218089581: precision: 0.7897603486: recall: 0.1906515022: f1: 0.3071545835
3: 3232: loss: 0.2168794478:
3: 6432: loss: 0.2188809800:
3: 9632: loss: 0.2218045039:
3: 12832: loss: 0.2204930065:
3: 16032: loss: 0.2180409422:
3: 19232: loss: 0.2171633869:
3: 22432: loss: 0.2167198354:
3: 25632: loss: 0.2172647828:
3: 28832: loss: 0.2160482464:
3: 32032: loss: 0.2164006865:
3: 35232: loss: 0.2152055852:
3: 38432: loss: 0.2129141799:
3: 41632: loss: 0.2133288893:
3: 44832: loss: 0.2126875240:
3: 48032: loss: 0.2128158643:
3: 51232: loss: 0.2117386152:
3: 54432: loss: 0.2114392198:
3: 57632: loss: 0.2110993444:
3: 60832: loss: 0.2114011743:
3: 64032: loss: 0.2106009502:
3: 67232: loss: 0.2109581656:
3: 70432: loss: 0.2110154541:
3: 73632: loss: 0.2105031891:
3: 76832: loss: 0.2101543242:
3: 80032: loss: 0.2097569225:
3: 83232: loss: 0.2096012933:
3: 86432: loss: 0.2095894160:
3: 89632: loss: 0.2094192641:
3: 92832: loss: 0.2090443190:
3: 96032: loss: 0.2089638424:
3: 99232: loss: 0.2087434332:
3: 102432: loss: 0.2086685249:
3: 105632: loss: 0.2087397148:
3: 108832: loss: 0.2087758635:
3: 112032: loss: 0.2081843912:
3: 115232: loss: 0.2080936224:
3: 118432: loss: 0.2081093114:
3: 121632: loss: 0.2080873737:
3: 124832: loss: 0.2079283573:
3: 128032: loss: 0.2075650381:
3: 131232: loss: 0.2067910109:
3: 134432: loss: 0.2068781429:
3: 137632: loss: 0.2067956591:
3: 140832: loss: 0.2065955797:
3: 144032: loss: 0.2065121563:
3: 147232: loss: 0.2066914581:
3: 150432: loss: 0.2065426760:
3: 153632: loss: 0.2064598212:
3: 156832: loss: 0.2058880959:
3: 160032: loss: 0.2057104626:
3: 163232: loss: 0.2055490975:
3: 166432: loss: 0.2049700880:
Dev-Acc: 3: Accuracy: 0.9389089346: precision: 0.4668428640: recall: 0.3303859888: f1: 0.3869361744
Train-Acc: 3: Accuracy: 0.9296262860: precision: 0.7474078341: recall: 0.3412004470: f1: 0.4685172647
4: 3232: loss: 0.1858797486:
4: 6432: loss: 0.1889887429:
4: 9632: loss: 0.1912460211:
4: 12832: loss: 0.1923077803:
4: 16032: loss: 0.1902823579:
4: 19232: loss: 0.1898161479:
4: 22432: loss: 0.1891873628:
4: 25632: loss: 0.1896493366:
4: 28832: loss: 0.1913728568:
4: 32032: loss: 0.1913942948:
4: 35232: loss: 0.1926428965:
4: 38432: loss: 0.1916996518:
4: 41632: loss: 0.1911808911:
4: 44832: loss: 0.1912754775:
4: 48032: loss: 0.1904979612:
4: 51232: loss: 0.1907934358:
4: 54432: loss: 0.1914381630:
4: 57632: loss: 0.1911391334:
4: 60832: loss: 0.1913607177:
4: 64032: loss: 0.1909234845:
4: 67232: loss: 0.1914732092:
4: 70432: loss: 0.1913366029:
4: 73632: loss: 0.1913747689:
4: 76832: loss: 0.1913694748:
4: 80032: loss: 0.1922253998:
4: 83232: loss: 0.1924050151:
4: 86432: loss: 0.1918649488:
4: 89632: loss: 0.1915651436:
4: 92832: loss: 0.1911217601:
4: 96032: loss: 0.1910261704:
4: 99232: loss: 0.1904682550:
4: 102432: loss: 0.1900310301:
4: 105632: loss: 0.1901122290:
4: 108832: loss: 0.1905507478:
4: 112032: loss: 0.1904745750:
4: 115232: loss: 0.1906838329:
4: 118432: loss: 0.1909735396:
4: 121632: loss: 0.1908255836:
4: 124832: loss: 0.1908790294:
4: 128032: loss: 0.1905872792:
4: 131232: loss: 0.1905269080:
4: 134432: loss: 0.1903288568:
4: 137632: loss: 0.1901744280:
4: 140832: loss: 0.1901465188:
4: 144032: loss: 0.1904799149:
4: 147232: loss: 0.1902590601:
4: 150432: loss: 0.1902967948:
4: 153632: loss: 0.1898922499:
4: 156832: loss: 0.1896533007:
4: 160032: loss: 0.1894241019:
4: 163232: loss: 0.1892024183:
4: 166432: loss: 0.1891069429:
Dev-Acc: 4: Accuracy: 0.9367061853: precision: 0.4471112999: recall: 0.3579323244: f1: 0.3975823968
Train-Acc: 4: Accuracy: 0.9322978258: precision: 0.7457906064: recall: 0.3872855171: f1: 0.5098225876
5: 3232: loss: 0.1682370706:
5: 6432: loss: 0.1828897622:
5: 9632: loss: 0.1876170424:
5: 12832: loss: 0.1843669022:
5: 16032: loss: 0.1839917339:
5: 19232: loss: 0.1828068099:
5: 22432: loss: 0.1829148768:
5: 25632: loss: 0.1821388478:
5: 28832: loss: 0.1828873272:
5: 32032: loss: 0.1820890972:
5: 35232: loss: 0.1814473383:
5: 38432: loss: 0.1818018026:
5: 41632: loss: 0.1807064799:
5: 44832: loss: 0.1802356182:
5: 48032: loss: 0.1812097773:
5: 51232: loss: 0.1814393155:
5: 54432: loss: 0.1814383828:
5: 57632: loss: 0.1814840352:
5: 60832: loss: 0.1811998954:
5: 64032: loss: 0.1811760309:
5: 67232: loss: 0.1806750236:
5: 70432: loss: 0.1809014214:
5: 73632: loss: 0.1810243277:
5: 76832: loss: 0.1810766972:
5: 80032: loss: 0.1813748189:
5: 83232: loss: 0.1809090302:
5: 86432: loss: 0.1806713930:
5: 89632: loss: 0.1809217369:
5: 92832: loss: 0.1804301525:
5: 96032: loss: 0.1807184126:
5: 99232: loss: 0.1805692080:
5: 102432: loss: 0.1805871460:
5: 105632: loss: 0.1810965227:
5: 108832: loss: 0.1812729332:
5: 112032: loss: 0.1814980314:
5: 115232: loss: 0.1811878331:
5: 118432: loss: 0.1809227092:
5: 121632: loss: 0.1805869748:
5: 124832: loss: 0.1806082037:
5: 128032: loss: 0.1806501216:
5: 131232: loss: 0.1808527094:
5: 134432: loss: 0.1810247626:
5: 137632: loss: 0.1808253794:
5: 140832: loss: 0.1805981577:
5: 144032: loss: 0.1802446230:
5: 147232: loss: 0.1802221276:
5: 150432: loss: 0.1802059508:
5: 153632: loss: 0.1800071436:
5: 156832: loss: 0.1800366571:
5: 160032: loss: 0.1799799569:
5: 163232: loss: 0.1799207163:
5: 166432: loss: 0.1801473024:
Dev-Acc: 5: Accuracy: 0.9359025359: precision: 0.4427073026: recall: 0.3803774868: f1: 0.4091823669
Train-Acc: 5: Accuracy: 0.9347362518: precision: 0.7493317838: recall: 0.4239037539: f1: 0.5414847162
6: 3232: loss: 0.1734116594:
6: 6432: loss: 0.1803486034:
6: 9632: loss: 0.1843193336:
6: 12832: loss: 0.1859465124:
6: 16032: loss: 0.1863204459:
6: 19232: loss: 0.1855474243:
6: 22432: loss: 0.1847230200:
6: 25632: loss: 0.1830450486:
6: 28832: loss: 0.1829273226:
6: 32032: loss: 0.1816840623:
6: 35232: loss: 0.1809381386:
6: 38432: loss: 0.1802948687:
6: 41632: loss: 0.1797133321:
6: 44832: loss: 0.1798290380:
6: 48032: loss: 0.1804643437:
6: 51232: loss: 0.1800730081:
6: 54432: loss: 0.1797168649:
6: 57632: loss: 0.1796222422:
6: 60832: loss: 0.1790521820:
6: 64032: loss: 0.1788003190:
6: 67232: loss: 0.1780668664:
6: 70432: loss: 0.1773653190:
6: 73632: loss: 0.1767031983:
6: 76832: loss: 0.1763997933:
6: 80032: loss: 0.1769525602:
6: 83232: loss: 0.1771215157:
6: 86432: loss: 0.1774667316:
6: 89632: loss: 0.1771111785:
6: 92832: loss: 0.1769357226:
6: 96032: loss: 0.1766465159:
6: 99232: loss: 0.1766606462:
6: 102432: loss: 0.1761132144:
6: 105632: loss: 0.1756701718:
6: 108832: loss: 0.1760089399:
6: 112032: loss: 0.1760695326:
6: 115232: loss: 0.1754404380:
6: 118432: loss: 0.1752733851:
6: 121632: loss: 0.1751842021:
6: 124832: loss: 0.1747609252:
6: 128032: loss: 0.1744596297:
6: 131232: loss: 0.1742153770:
6: 134432: loss: 0.1740660273:
6: 137632: loss: 0.1741027696:
6: 140832: loss: 0.1741752545:
6: 144032: loss: 0.1739872410:
6: 147232: loss: 0.1738353774:
6: 150432: loss: 0.1734588030:
6: 153632: loss: 0.1735893008:
6: 156832: loss: 0.1736588643:
6: 160032: loss: 0.1735203201:
6: 163232: loss: 0.1734909124:
6: 166432: loss: 0.1732283786:
Dev-Acc: 6: Accuracy: 0.9351583719: precision: 0.4370427416: recall: 0.3859887774: f1: 0.4099322799
Train-Acc: 6: Accuracy: 0.9363738298: precision: 0.7540345019: recall: 0.4454013543: f1: 0.5600099190
7: 3232: loss: 0.1729675668:
7: 6432: loss: 0.1687615894:
7: 9632: loss: 0.1658587592:
7: 12832: loss: 0.1640100666:
7: 16032: loss: 0.1656760021:
7: 19232: loss: 0.1716160449:
7: 22432: loss: 0.1709927877:
7: 25632: loss: 0.1709387559:
7: 28832: loss: 0.1706103784:
7: 32032: loss: 0.1684283468:
7: 35232: loss: 0.1694884199:
7: 38432: loss: 0.1696313152:
7: 41632: loss: 0.1691731294:
7: 44832: loss: 0.1683125335:
7: 48032: loss: 0.1687132715:
7: 51232: loss: 0.1693232371:
7: 54432: loss: 0.1698071315:
7: 57632: loss: 0.1702452098:
7: 60832: loss: 0.1698172684:
7: 64032: loss: 0.1698830734:
7: 67232: loss: 0.1694727364:
7: 70432: loss: 0.1692767625:
7: 73632: loss: 0.1691626966:
7: 76832: loss: 0.1688556589:
7: 80032: loss: 0.1689428280:
7: 83232: loss: 0.1688763318:
7: 86432: loss: 0.1688513674:
7: 89632: loss: 0.1691126517:
7: 92832: loss: 0.1696181380:
7: 96032: loss: 0.1694269294:
7: 99232: loss: 0.1694175572:
7: 102432: loss: 0.1696238927:
7: 105632: loss: 0.1691264203:
7: 108832: loss: 0.1696114926:
7: 112032: loss: 0.1694131498:
7: 115232: loss: 0.1695286296:
7: 118432: loss: 0.1692894653:
7: 121632: loss: 0.1688870328:
7: 124832: loss: 0.1690183682:
7: 128032: loss: 0.1685938476:
7: 131232: loss: 0.1685897651:
7: 134432: loss: 0.1683819063:
7: 137632: loss: 0.1681922170:
7: 140832: loss: 0.1684311141:
7: 144032: loss: 0.1681167657:
7: 147232: loss: 0.1680672003:
7: 150432: loss: 0.1679237776:
7: 153632: loss: 0.1679695533:
7: 156832: loss: 0.1679589809:
7: 160032: loss: 0.1680270940:
7: 163232: loss: 0.1677699962:
7: 166432: loss: 0.1678123533:
Dev-Acc: 7: Accuracy: 0.9347614646: precision: 0.4347499060: recall: 0.3931304200: f1: 0.4128940084
Train-Acc: 7: Accuracy: 0.9379814863: precision: 0.7602282515: recall: 0.4642035369: f1: 0.5764316911
8: 3232: loss: 0.1670640685:
8: 6432: loss: 0.1689871683:
8: 9632: loss: 0.1701565120:
8: 12832: loss: 0.1672674144:
8: 16032: loss: 0.1675788543:
8: 19232: loss: 0.1671584127:
8: 22432: loss: 0.1669712731:
8: 25632: loss: 0.1658252458:
8: 28832: loss: 0.1680133060:
8: 32032: loss: 0.1670060740:
8: 35232: loss: 0.1663378822:
8: 38432: loss: 0.1658596724:
8: 41632: loss: 0.1662224635:
8: 44832: loss: 0.1654504747:
8: 48032: loss: 0.1653751409:
8: 51232: loss: 0.1657890989:
8: 54432: loss: 0.1652728270:
8: 57632: loss: 0.1655205371:
8: 60832: loss: 0.1657931462:
8: 64032: loss: 0.1657729124:
8: 67232: loss: 0.1652445371:
8: 70432: loss: 0.1653710841:
8: 73632: loss: 0.1652810083:
8: 76832: loss: 0.1653175829:
8: 80032: loss: 0.1658642035:
8: 83232: loss: 0.1659243153:
8: 86432: loss: 0.1660894628:
8: 89632: loss: 0.1659580910:
8: 92832: loss: 0.1654940665:
8: 96032: loss: 0.1651164821:
8: 99232: loss: 0.1650877625:
8: 102432: loss: 0.1647682738:
8: 105632: loss: 0.1650547197:
8: 108832: loss: 0.1646271177:
8: 112032: loss: 0.1643932335:
8: 115232: loss: 0.1642012327:
8: 118432: loss: 0.1644991622:
8: 121632: loss: 0.1644827321:
8: 124832: loss: 0.1642606186:
8: 128032: loss: 0.1642609853:
8: 131232: loss: 0.1643117276:
8: 134432: loss: 0.1640278698:
8: 137632: loss: 0.1637319295:
8: 140832: loss: 0.1637204992:
8: 144032: loss: 0.1635792602:
8: 147232: loss: 0.1637823616:
8: 150432: loss: 0.1636970831:
8: 153632: loss: 0.1636686595:
8: 156832: loss: 0.1632916596:
8: 160032: loss: 0.1632901088:
8: 163232: loss: 0.1633261913:
8: 166432: loss: 0.1635274793:
Dev-Acc: 8: Accuracy: 0.9335806966: precision: 0.4274236743: recall: 0.4070736269: f1: 0.4170005226
Train-Acc: 8: Accuracy: 0.9392903447: precision: 0.7586242195: recall: 0.4872132010: f1: 0.5933546837
9: 3232: loss: 0.1435118572:
9: 6432: loss: 0.1542321517:
9: 9632: loss: 0.1550923184:
9: 12832: loss: 0.1602597366:
9: 16032: loss: 0.1584094554:
9: 19232: loss: 0.1569976307:
9: 22432: loss: 0.1584133498:
9: 25632: loss: 0.1572366738:
9: 28832: loss: 0.1571463543:
9: 32032: loss: 0.1573831928:
9: 35232: loss: 0.1583419940:
9: 38432: loss: 0.1582151691:
9: 41632: loss: 0.1593633010:
9: 44832: loss: 0.1595649933:
9: 48032: loss: 0.1597086651:
9: 51232: loss: 0.1602011216:
9: 54432: loss: 0.1601079097:
9: 57632: loss: 0.1614336483:
9: 60832: loss: 0.1614186358:
9: 64032: loss: 0.1609643707:
9: 67232: loss: 0.1609889729:
9: 70432: loss: 0.1607018894:
9: 73632: loss: 0.1608967696:
9: 76832: loss: 0.1601790100:
9: 80032: loss: 0.1600360453:
9: 83232: loss: 0.1596870975:
9: 86432: loss: 0.1597181496:
9: 89632: loss: 0.1597126373:
9: 92832: loss: 0.1598395718:
9: 96032: loss: 0.1596304947:
9: 99232: loss: 0.1591528719:
9: 102432: loss: 0.1590657835:
9: 105632: loss: 0.1592078578:
9: 108832: loss: 0.1591501217:
9: 112032: loss: 0.1591199799:
9: 115232: loss: 0.1590149611:
9: 118432: loss: 0.1592436970:
9: 121632: loss: 0.1598018629:
9: 124832: loss: 0.1597466502:
9: 128032: loss: 0.1600490883:
9: 131232: loss: 0.1599087440:
9: 134432: loss: 0.1596616455:
9: 137632: loss: 0.1597306571:
9: 140832: loss: 0.1595593433:
9: 144032: loss: 0.1594335630:
9: 147232: loss: 0.1595666100:
9: 150432: loss: 0.1595087857:
9: 153632: loss: 0.1594821463:
9: 156832: loss: 0.1593369127:
9: 160032: loss: 0.1593486535:
9: 163232: loss: 0.1595039091:
9: 166432: loss: 0.1594930763:
Dev-Acc: 9: Accuracy: 0.9333425760: precision: 0.4259160913: recall: 0.4091140962: f1: 0.4173460538
Train-Acc: 9: Accuracy: 0.9407426715: precision: 0.7733842660: recall: 0.4924725528: f1: 0.6017592481
10: 3232: loss: 0.1341260745:
10: 6432: loss: 0.1505789882:
10: 9632: loss: 0.1520852821:
10: 12832: loss: 0.1522904601:
10: 16032: loss: 0.1538208227:
10: 19232: loss: 0.1546517574:
10: 22432: loss: 0.1530188394:
10: 25632: loss: 0.1524223702:
10: 28832: loss: 0.1537519516:
10: 32032: loss: 0.1553775079:
10: 35232: loss: 0.1555518886:
10: 38432: loss: 0.1557494861:
10: 41632: loss: 0.1565461887:
10: 44832: loss: 0.1568720648:
10: 48032: loss: 0.1568990986:
10: 51232: loss: 0.1573284594:
10: 54432: loss: 0.1568066425:
10: 57632: loss: 0.1566565333:
10: 60832: loss: 0.1570598072:
10: 64032: loss: 0.1571078122:
10: 67232: loss: 0.1567661459:
10: 70432: loss: 0.1560402912:
10: 73632: loss: 0.1555611506:
10: 76832: loss: 0.1554260498:
10: 80032: loss: 0.1555465154:
10: 83232: loss: 0.1558052616:
10: 86432: loss: 0.1554860176:
10: 89632: loss: 0.1552822640:
10: 92832: loss: 0.1551389112:
10: 96032: loss: 0.1549664641:
10: 99232: loss: 0.1552609381:
10: 102432: loss: 0.1556244376:
10: 105632: loss: 0.1563241594:
10: 108832: loss: 0.1563537491:
10: 112032: loss: 0.1566001405:
10: 115232: loss: 0.1567704639:
10: 118432: loss: 0.1564780934:
10: 121632: loss: 0.1565014535:
10: 124832: loss: 0.1568687612:
10: 128032: loss: 0.1567277916:
10: 131232: loss: 0.1568425254:
10: 134432: loss: 0.1567991099:
10: 137632: loss: 0.1567578326:
10: 140832: loss: 0.1565523295:
10: 144032: loss: 0.1564831519:
10: 147232: loss: 0.1562073780:
10: 150432: loss: 0.1561876213:
10: 153632: loss: 0.1562303085:
10: 156832: loss: 0.1561526898:
10: 160032: loss: 0.1562393661:
10: 163232: loss: 0.1563894220:
10: 166432: loss: 0.1564928316:
Dev-Acc: 10: Accuracy: 0.9327075481: precision: 0.4219913420: recall: 0.4143853086: f1: 0.4181537406
Train-Acc: 10: Accuracy: 0.9418722391: precision: 0.7780030411: recall: 0.5045690619: f1: 0.6121390971
11: 3232: loss: 0.1535919441:
11: 6432: loss: 0.1495777012:
11: 9632: loss: 0.1564674321:
11: 12832: loss: 0.1579253158:
11: 16032: loss: 0.1579295187:
11: 19232: loss: 0.1553957935:
11: 22432: loss: 0.1561521719:
11: 25632: loss: 0.1551183828:
11: 28832: loss: 0.1552232374:
11: 32032: loss: 0.1544718818:
11: 35232: loss: 0.1555716538:
11: 38432: loss: 0.1543643444:
11: 41632: loss: 0.1538222880:
11: 44832: loss: 0.1542038838:
11: 48032: loss: 0.1537284213:
11: 51232: loss: 0.1531458928:
11: 54432: loss: 0.1523212845:
11: 57632: loss: 0.1524777012:
11: 60832: loss: 0.1531240203:
11: 64032: loss: 0.1531871483:
11: 67232: loss: 0.1527213138:
11: 70432: loss: 0.1530160012:
11: 73632: loss: 0.1527916958:
11: 76832: loss: 0.1530908606:
11: 80032: loss: 0.1530186950:
11: 83232: loss: 0.1529511705:
11: 86432: loss: 0.1530922955:
11: 89632: loss: 0.1529632887:
11: 92832: loss: 0.1526315722:
11: 96032: loss: 0.1528261853:
11: 99232: loss: 0.1531843832:
11: 102432: loss: 0.1535193861:
11: 105632: loss: 0.1538654944:
11: 108832: loss: 0.1544278416:
11: 112032: loss: 0.1541479034:
11: 115232: loss: 0.1538029701:
11: 118432: loss: 0.1540680925:
11: 121632: loss: 0.1536340888:
11: 124832: loss: 0.1537151905:
11: 128032: loss: 0.1534881214:
11: 131232: loss: 0.1536504012:
11: 134432: loss: 0.1538922573:
11: 137632: loss: 0.1536919285:
11: 140832: loss: 0.1535658797:
11: 144032: loss: 0.1534117913:
11: 147232: loss: 0.1536060449:
11: 150432: loss: 0.1536770592:
11: 153632: loss: 0.1534715264:
11: 156832: loss: 0.1533591458:
11: 160032: loss: 0.1533627677:
11: 163232: loss: 0.1533362121:
11: 166432: loss: 0.1531971315:
Dev-Acc: 11: Accuracy: 0.9327373505: precision: 0.4217224547: recall: 0.4113246047: f1: 0.4164586382
Train-Acc: 11: Accuracy: 0.9429240823: precision: 0.7896244756: recall: 0.5073302216: f1: 0.6177553634
12: 3232: loss: 0.1600478199:
12: 6432: loss: 0.1512077995:
12: 9632: loss: 0.1542141123:
12: 12832: loss: 0.1535859092:
12: 16032: loss: 0.1531950234:
12: 19232: loss: 0.1515852403:
12: 22432: loss: 0.1529376985:
12: 25632: loss: 0.1505674604:
12: 28832: loss: 0.1499340187:
12: 32032: loss: 0.1491303445:
12: 35232: loss: 0.1497088170:
12: 38432: loss: 0.1494970022:
12: 41632: loss: 0.1504706702:
12: 44832: loss: 0.1494726417:
12: 48032: loss: 0.1503021267:
12: 51232: loss: 0.1498854654:
12: 54432: loss: 0.1495427574:
12: 57632: loss: 0.1497995322:
12: 60832: loss: 0.1503535492:
12: 64032: loss: 0.1501504126:
12: 67232: loss: 0.1499996122:
12: 70432: loss: 0.1503748205:
12: 73632: loss: 0.1500577706:
12: 76832: loss: 0.1503060053:
12: 80032: loss: 0.1498767783:
12: 83232: loss: 0.1499066747:
12: 86432: loss: 0.1501513934:
12: 89632: loss: 0.1497957637:
12: 92832: loss: 0.1496132205:
12: 96032: loss: 0.1494525458:
12: 99232: loss: 0.1496775935:
12: 102432: loss: 0.1494874940:
12: 105632: loss: 0.1496957833:
12: 108832: loss: 0.1497925189:
12: 112032: loss: 0.1503386627:
12: 115232: loss: 0.1507237112:
12: 118432: loss: 0.1503857761:
12: 121632: loss: 0.1500442537:
12: 124832: loss: 0.1500203342:
12: 128032: loss: 0.1500397129:
12: 131232: loss: 0.1502681426:
12: 134432: loss: 0.1499946431:
12: 137632: loss: 0.1501728026:
12: 140832: loss: 0.1503514815:
12: 144032: loss: 0.1500763929:
12: 147232: loss: 0.1498985023:
12: 150432: loss: 0.1498916281:
12: 153632: loss: 0.1499160110:
12: 156832: loss: 0.1498098233:
12: 160032: loss: 0.1497924020:
12: 163232: loss: 0.1496149672:
12: 166432: loss: 0.1497959824:
Dev-Acc: 12: Accuracy: 0.9313581586: precision: 0.4137414740: recall: 0.4228872641: f1: 0.4182643794
Train-Acc: 12: Accuracy: 0.9443106651: precision: 0.7883354536: recall: 0.5296167247: f1: 0.6335823830
13: 3232: loss: 0.1496852170:
13: 6432: loss: 0.1510963183:
13: 9632: loss: 0.1507788174:
13: 12832: loss: 0.1472534507:
13: 16032: loss: 0.1455159795:
13: 19232: loss: 0.1452601508:
13: 22432: loss: 0.1468769302:
13: 25632: loss: 0.1470102364:
13: 28832: loss: 0.1474691462:
13: 32032: loss: 0.1478509394:
13: 35232: loss: 0.1490070253:
13: 38432: loss: 0.1489930623:
13: 41632: loss: 0.1492179423:
13: 44832: loss: 0.1484916084:
13: 48032: loss: 0.1482050464:
13: 51232: loss: 0.1477958998:
13: 54432: loss: 0.1480159243:
13: 57632: loss: 0.1476763497:
13: 60832: loss: 0.1476840935:
13: 64032: loss: 0.1474010791:
13: 67232: loss: 0.1474240406:
13: 70432: loss: 0.1475690866:
13: 73632: loss: 0.1475248532:
13: 76832: loss: 0.1472759784:
13: 80032: loss: 0.1471538016:
13: 83232: loss: 0.1466023292:
13: 86432: loss: 0.1467340307:
13: 89632: loss: 0.1466308433:
13: 92832: loss: 0.1469050424:
13: 96032: loss: 0.1473275104:
13: 99232: loss: 0.1474285969:
13: 102432: loss: 0.1477319096:
13: 105632: loss: 0.1476356528:
13: 108832: loss: 0.1475684811:
13: 112032: loss: 0.1474437587:
13: 115232: loss: 0.1475161353:
13: 118432: loss: 0.1474389700:
13: 121632: loss: 0.1473892450:
13: 124832: loss: 0.1472789352:
13: 128032: loss: 0.1472053598:
13: 131232: loss: 0.1473777884:
13: 134432: loss: 0.1475991331:
13: 137632: loss: 0.1476736431:
13: 140832: loss: 0.1477465912:
13: 144032: loss: 0.1474070392:
13: 147232: loss: 0.1472765569:
13: 150432: loss: 0.1473831317:
13: 153632: loss: 0.1474091519:
13: 156832: loss: 0.1474735401:
13: 160032: loss: 0.1474252487:
13: 163232: loss: 0.1473401062:
13: 166432: loss: 0.1473688333:
Dev-Acc: 13: Accuracy: 0.9314374924: precision: 0.4139775957: recall: 0.4210168339: f1: 0.4174675434
Train-Acc: 13: Accuracy: 0.9452967644: precision: 0.7980125935: recall: 0.5332325291: f1: 0.6392906404
14: 3232: loss: 0.1459669248:
14: 6432: loss: 0.1427636521:
14: 9632: loss: 0.1439986439:
14: 12832: loss: 0.1408441916:
14: 16032: loss: 0.1403341985:
14: 19232: loss: 0.1409801236:
14: 22432: loss: 0.1424785024:
14: 25632: loss: 0.1432304021:
14: 28832: loss: 0.1436024211:
14: 32032: loss: 0.1441247545:
14: 35232: loss: 0.1444854483:
14: 38432: loss: 0.1435759790:
14: 41632: loss: 0.1443018094:
14: 44832: loss: 0.1450078824:
14: 48032: loss: 0.1454508377:
14: 51232: loss: 0.1451081077:
14: 54432: loss: 0.1448450343:
14: 57632: loss: 0.1446009201:
14: 60832: loss: 0.1445166174:
14: 64032: loss: 0.1440006650:
14: 67232: loss: 0.1438224088:
14: 70432: loss: 0.1438339468:
14: 73632: loss: 0.1439821024:
14: 76832: loss: 0.1438670444:
14: 80032: loss: 0.1436282615:
14: 83232: loss: 0.1444061812:
14: 86432: loss: 0.1448463656:
14: 89632: loss: 0.1452625921:
14: 92832: loss: 0.1453704104:
14: 96032: loss: 0.1449800270:
14: 99232: loss: 0.1450117742:
14: 102432: loss: 0.1450983262:
14: 105632: loss: 0.1448888276:
14: 108832: loss: 0.1446079785:
14: 112032: loss: 0.1445175383:
14: 115232: loss: 0.1449476642:
14: 118432: loss: 0.1448860161:
14: 121632: loss: 0.1448846512:
14: 124832: loss: 0.1451234773:
14: 128032: loss: 0.1452126435:
14: 131232: loss: 0.1452402955:
14: 134432: loss: 0.1452406090:
14: 137632: loss: 0.1452984631:
14: 140832: loss: 0.1451944773:
14: 144032: loss: 0.1451191350:
14: 147232: loss: 0.1454343562:
14: 150432: loss: 0.1454182092:
14: 153632: loss: 0.1453931916:
14: 156832: loss: 0.1453524524:
14: 160032: loss: 0.1450755529:
14: 163232: loss: 0.1449684560:
14: 166432: loss: 0.1450190054:
Dev-Acc: 14: Accuracy: 0.9306437373: precision: 0.4082712986: recall: 0.4196565210: f1: 0.4138856280
Train-Acc: 14: Accuracy: 0.9461753368: precision: 0.8009506257: recall: 0.5428308461: f1: 0.6471003135
15: 3232: loss: 0.1387868305:
15: 6432: loss: 0.1373216644:
15: 9632: loss: 0.1371295716:
15: 12832: loss: 0.1378609745:
15: 16032: loss: 0.1371587314:
15: 19232: loss: 0.1383412963:
15: 22432: loss: 0.1402970505:
15: 25632: loss: 0.1411390733:
15: 28832: loss: 0.1404276592:
15: 32032: loss: 0.1403025492:
15: 35232: loss: 0.1403913178:
15: 38432: loss: 0.1404420837:
15: 41632: loss: 0.1406458404:
15: 44832: loss: 0.1410082642:
15: 48032: loss: 0.1408502739:
15: 51232: loss: 0.1409536257:
15: 54432: loss: 0.1413234573:
15: 57632: loss: 0.1415354270:
15: 60832: loss: 0.1415102365:
15: 64032: loss: 0.1415763775:
15: 67232: loss: 0.1414213255:
15: 70432: loss: 0.1412198647:
15: 73632: loss: 0.1415635671:
15: 76832: loss: 0.1415333788:
15: 80032: loss: 0.1418064343:
15: 83232: loss: 0.1422694243:
15: 86432: loss: 0.1422547553:
15: 89632: loss: 0.1425328493:
15: 92832: loss: 0.1430819523:
15: 96032: loss: 0.1428925987:
15: 99232: loss: 0.1430041635:
15: 102432: loss: 0.1432247546:
15: 105632: loss: 0.1429114391:
15: 108832: loss: 0.1426734916:
15: 112032: loss: 0.1425088989:
15: 115232: loss: 0.1422686193:
15: 118432: loss: 0.1425595720:
15: 121632: loss: 0.1425699324:
15: 124832: loss: 0.1424043641:
15: 128032: loss: 0.1423828863:
15: 131232: loss: 0.1425069127:
15: 134432: loss: 0.1422811962:
15: 137632: loss: 0.1423339047:
15: 140832: loss: 0.1423957824:
15: 144032: loss: 0.1424954654:
15: 147232: loss: 0.1425625547:
15: 150432: loss: 0.1425777000:
15: 153632: loss: 0.1425440035:
15: 156832: loss: 0.1424553436:
15: 160032: loss: 0.1425760748:
15: 163232: loss: 0.1424300826:
15: 166432: loss: 0.1423520102:
Dev-Acc: 15: Accuracy: 0.9295622110: precision: 0.4013924870: recall: 0.4215269512: f1: 0.4112134030
Train-Acc: 15: Accuracy: 0.9470359683: precision: 0.8004732608: recall: 0.5559792256: f1: 0.6561918063
16: 3232: loss: 0.1534264394:
16: 6432: loss: 0.1480453442:
16: 9632: loss: 0.1483304908:
16: 12832: loss: 0.1478286028:
16: 16032: loss: 0.1469734347:
16: 19232: loss: 0.1459403605:
16: 22432: loss: 0.1449346699:
16: 25632: loss: 0.1453106375:
16: 28832: loss: 0.1448024262:
16: 32032: loss: 0.1452768947:
16: 35232: loss: 0.1444177575:
16: 38432: loss: 0.1430372420:
16: 41632: loss: 0.1439169765:
16: 44832: loss: 0.1437887104:
16: 48032: loss: 0.1425216985:
16: 51232: loss: 0.1424539202:
16: 54432: loss: 0.1423270578:
16: 57632: loss: 0.1414351333:
16: 60832: loss: 0.1411167476:
16: 64032: loss: 0.1414206270:
16: 67232: loss: 0.1414981844:
16: 70432: loss: 0.1414472923:
16: 73632: loss: 0.1410943867:
16: 76832: loss: 0.1408172500:
16: 80032: loss: 0.1406790538:
16: 83232: loss: 0.1405223823:
16: 86432: loss: 0.1404391650:
16: 89632: loss: 0.1403510821:
16: 92832: loss: 0.1404134546:
16: 96032: loss: 0.1403123187:
16: 99232: loss: 0.1404393102:
16: 102432: loss: 0.1407714278:
16: 105632: loss: 0.1399968303:
16: 108832: loss: 0.1400467309:
16: 112032: loss: 0.1397835019:
16: 115232: loss: 0.1396876636:
16: 118432: loss: 0.1397881049:
16: 121632: loss: 0.1396918535:
16: 124832: loss: 0.1395434213:
16: 128032: loss: 0.1398302502:
16: 131232: loss: 0.1397114674:
16: 134432: loss: 0.1396728401:
16: 137632: loss: 0.1397138034:
16: 140832: loss: 0.1397957992:
16: 144032: loss: 0.1397372276:
16: 147232: loss: 0.1397132451:
16: 150432: loss: 0.1396993401:
16: 153632: loss: 0.1398557403:
16: 156832: loss: 0.1398179730:
16: 160032: loss: 0.1401585113:
16: 163232: loss: 0.1404250849:
16: 166432: loss: 0.1402521684:
Dev-Acc: 16: Accuracy: 0.9287981987: precision: 0.3956150250: recall: 0.4172759735: f1: 0.4061569017
Train-Acc: 16: Accuracy: 0.9478965402: precision: 0.8064759747: recall: 0.5616330287: f1: 0.6621454038
17: 3232: loss: 0.1253605300:
17: 6432: loss: 0.1447546871:
17: 9632: loss: 0.1449551266:
17: 12832: loss: 0.1414119427:
17: 16032: loss: 0.1435978127:
17: 19232: loss: 0.1440974941:
17: 22432: loss: 0.1438994991:
17: 25632: loss: 0.1425808920:
17: 28832: loss: 0.1427275905:
17: 32032: loss: 0.1432609974:
17: 35232: loss: 0.1427335016:
17: 38432: loss: 0.1422360910:
17: 41632: loss: 0.1427912648:
17: 44832: loss: 0.1428212273:
17: 48032: loss: 0.1424281966:
17: 51232: loss: 0.1426519709:
17: 54432: loss: 0.1415672866:
17: 57632: loss: 0.1407028368:
17: 60832: loss: 0.1407108484:
17: 64032: loss: 0.1403171412:
17: 67232: loss: 0.1402877525:
17: 70432: loss: 0.1400142900:
17: 73632: loss: 0.1400382928:
17: 76832: loss: 0.1398878313:
17: 80032: loss: 0.1399883435:
17: 83232: loss: 0.1395549296:
17: 86432: loss: 0.1393069264:
17: 89632: loss: 0.1390445951:
17: 92832: loss: 0.1388937607:
17: 96032: loss: 0.1385093254:
17: 99232: loss: 0.1389242976:
17: 102432: loss: 0.1386476877:
17: 105632: loss: 0.1389262799:
17: 108832: loss: 0.1389821800:
17: 112032: loss: 0.1388470197:
17: 115232: loss: 0.1388052819:
17: 118432: loss: 0.1388859033:
17: 121632: loss: 0.1392534775:
17: 124832: loss: 0.1393240273:
17: 128032: loss: 0.1391707259:
17: 131232: loss: 0.1390363159:
17: 134432: loss: 0.1388341444:
17: 137632: loss: 0.1388274994:
17: 140832: loss: 0.1386833454:
17: 144032: loss: 0.1382990986:
17: 147232: loss: 0.1381569266:
17: 150432: loss: 0.1382733976:
17: 153632: loss: 0.1380870213:
17: 156832: loss: 0.1378449571:
17: 160032: loss: 0.1379126879:
17: 163232: loss: 0.1380213474:
17: 166432: loss: 0.1379692625:
Dev-Acc: 17: Accuracy: 0.9281830192: precision: 0.3906526994: recall: 0.4121748002: f1: 0.4011252689
Train-Acc: 17: Accuracy: 0.9485360384: precision: 0.8153669725: recall: 0.5609098679: f1: 0.6646153846
18: 3232: loss: 0.1356967762:
18: 6432: loss: 0.1398216150:
18: 9632: loss: 0.1377616409:
18: 12832: loss: 0.1381948745:
18: 16032: loss: 0.1368705460:
18: 19232: loss: 0.1370435134:
18: 22432: loss: 0.1364622028:
18: 25632: loss: 0.1371693443:
18: 28832: loss: 0.1371127699:
18: 32032: loss: 0.1366097325:
18: 35232: loss: 0.1362053496:
18: 38432: loss: 0.1366833676:
18: 41632: loss: 0.1358639474:
18: 44832: loss: 0.1359379808:
18: 48032: loss: 0.1354753206:
18: 51232: loss: 0.1354539765:
18: 54432: loss: 0.1362982164:
18: 57632: loss: 0.1367752351:
18: 60832: loss: 0.1362966236:
18: 64032: loss: 0.1363631548:
18: 67232: loss: 0.1363742427:
18: 70432: loss: 0.1364174173:
18: 73632: loss: 0.1357798847:
18: 76832: loss: 0.1360323572:
18: 80032: loss: 0.1357910677:
18: 83232: loss: 0.1362180788:
18: 86432: loss: 0.1358171577:
18: 89632: loss: 0.1358510810:
18: 92832: loss: 0.1361516940:
18: 96032: loss: 0.1362164716:
18: 99232: loss: 0.1362865430:
18: 102432: loss: 0.1361224799:
18: 105632: loss: 0.1359461716:
18: 108832: loss: 0.1359229511:
18: 112032: loss: 0.1359305414:
18: 115232: loss: 0.1358077260:
18: 118432: loss: 0.1359135698:
18: 121632: loss: 0.1357763179:
18: 124832: loss: 0.1356529479:
18: 128032: loss: 0.1355178058:
18: 131232: loss: 0.1355768960:
18: 134432: loss: 0.1356530210:
18: 137632: loss: 0.1353738654:
18: 140832: loss: 0.1354042440:
18: 144032: loss: 0.1353227436:
18: 147232: loss: 0.1350392171:
18: 150432: loss: 0.1351377372:
18: 153632: loss: 0.1354085414:
18: 156832: loss: 0.1357373954:
18: 160032: loss: 0.1358068614:
18: 163232: loss: 0.1359123867:
18: 166432: loss: 0.1359778917:
Dev-Acc: 18: Accuracy: 0.9266054034: precision: 0.3815995002: recall: 0.4154055433: f1: 0.3977855573
Train-Acc: 18: Accuracy: 0.9492293596: precision: 0.8120237874: recall: 0.5745184406: f1: 0.6729295807
19: 3232: loss: 0.1363515183:
19: 6432: loss: 0.1346261455:
19: 9632: loss: 0.1336954106:
19: 12832: loss: 0.1333276240:
19: 16032: loss: 0.1341874211:
19: 19232: loss: 0.1344413521:
19: 22432: loss: 0.1345068878:
19: 25632: loss: 0.1342047419:
19: 28832: loss: 0.1335900449:
19: 32032: loss: 0.1338764911:
19: 35232: loss: 0.1333690952:
19: 38432: loss: 0.1332000898:
19: 41632: loss: 0.1330449114:
19: 44832: loss: 0.1334472291:
19: 48032: loss: 0.1334086666:
19: 51232: loss: 0.1329764145:
19: 54432: loss: 0.1331153154:
19: 57632: loss: 0.1335254808:
19: 60832: loss: 0.1332302544:
19: 64032: loss: 0.1326920355:
19: 67232: loss: 0.1326221181:
19: 70432: loss: 0.1325510001:
19: 73632: loss: 0.1323946624:
19: 76832: loss: 0.1327888699:
19: 80032: loss: 0.1331981156:
19: 83232: loss: 0.1329870914:
19: 86432: loss: 0.1331507291:
19: 89632: loss: 0.1332981966:
19: 92832: loss: 0.1329187862:
19: 96032: loss: 0.1330128076:
19: 99232: loss: 0.1330472299:
19: 102432: loss: 0.1331473271:
19: 105632: loss: 0.1329831237:
19: 108832: loss: 0.1332284649:
19: 112032: loss: 0.1330976099:
19: 115232: loss: 0.1333562651:
19: 118432: loss: 0.1332131601:
19: 121632: loss: 0.1334612591:
19: 124832: loss: 0.1333320983:
19: 128032: loss: 0.1338394173:
19: 131232: loss: 0.1340102572:
19: 134432: loss: 0.1342688735:
19: 137632: loss: 0.1345772081:
19: 140832: loss: 0.1343369064:
19: 144032: loss: 0.1344496770:
19: 147232: loss: 0.1344687606:
19: 150432: loss: 0.1344818368:
19: 153632: loss: 0.1344746152:
19: 156832: loss: 0.1344664762:
19: 160032: loss: 0.1342903331:
19: 163232: loss: 0.1343108744:
19: 166432: loss: 0.1341940586:
Dev-Acc: 19: Accuracy: 0.9263672829: precision: 0.3778553299: recall: 0.4050331576: f1: 0.3909725072
Train-Acc: 19: Accuracy: 0.9497253895: precision: 0.8208588957: recall: 0.5717572809: f1: 0.6740292955
20: 3232: loss: 0.1462452741:
20: 6432: loss: 0.1414254961:
20: 9632: loss: 0.1395572253:
20: 12832: loss: 0.1365792076:
20: 16032: loss: 0.1347402411:
20: 19232: loss: 0.1306716776:
20: 22432: loss: 0.1293712172:
20: 25632: loss: 0.1308389260:
20: 28832: loss: 0.1317083940:
20: 32032: loss: 0.1311490000:
20: 35232: loss: 0.1316998825:
20: 38432: loss: 0.1321793460:
20: 41632: loss: 0.1317128538:
20: 44832: loss: 0.1320673168:
20: 48032: loss: 0.1321550372:
20: 51232: loss: 0.1316323620:
20: 54432: loss: 0.1323125810:
20: 57632: loss: 0.1326554060:
20: 60832: loss: 0.1329105114:
20: 64032: loss: 0.1327997509:
20: 67232: loss: 0.1326347319:
20: 70432: loss: 0.1325551581:
20: 73632: loss: 0.1326132733:
20: 76832: loss: 0.1327503198:
20: 80032: loss: 0.1324579090:
20: 83232: loss: 0.1321647416:
20: 86432: loss: 0.1317461247:
20: 89632: loss: 0.1314248624:
20: 92832: loss: 0.1315744570:
20: 96032: loss: 0.1317019255:
20: 99232: loss: 0.1318294463:
20: 102432: loss: 0.1317601923:
20: 105632: loss: 0.1317114941:
20: 108832: loss: 0.1320015728:
20: 112032: loss: 0.1323841009:
20: 115232: loss: 0.1323265717:
20: 118432: loss: 0.1326773593:
20: 121632: loss: 0.1327450706:
20: 124832: loss: 0.1327752421:
20: 128032: loss: 0.1325034077:
20: 131232: loss: 0.1322590117:
20: 134432: loss: 0.1322739612:
20: 137632: loss: 0.1323321970:
20: 140832: loss: 0.1323238820:
20: 144032: loss: 0.1324178707:
20: 147232: loss: 0.1325517150:
20: 150432: loss: 0.1327055145:
20: 153632: loss: 0.1325481739:
20: 156832: loss: 0.1325293010:
20: 160032: loss: 0.1322288876:
20: 163232: loss: 0.1322228691:
20: 166432: loss: 0.1323834210:
Dev-Acc: 20: Accuracy: 0.9251766205: precision: 0.3715170279: recall: 0.4080938616: f1: 0.3889474111
Train-Acc: 20: Accuracy: 0.9501676559: precision: 0.8182239096: recall: 0.5808954046: f1: 0.6794309881
