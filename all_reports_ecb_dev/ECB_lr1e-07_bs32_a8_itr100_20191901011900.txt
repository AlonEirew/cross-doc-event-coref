1: 3232: loss: 0.7096930283:
1: 6432: loss: 0.7078628597:
1: 9632: loss: 0.7069279067:
1: 12832: loss: 0.7062362909:
1: 16032: loss: 0.7052760290:
1: 19232: loss: 0.7045255835:
1: 22432: loss: 0.7036874715:
1: 25632: loss: 0.7027131055:
1: 28832: loss: 0.7017498285:
1: 32032: loss: 0.7008981225:
1: 35232: loss: 0.7002181942:
1: 38432: loss: 0.6994278524:
1: 41632: loss: 0.6984843575:
1: 44832: loss: 0.6976051316:
1: 48032: loss: 0.6967114698:
1: 51232: loss: 0.6958339429:
1: 54432: loss: 0.6949826915:
1: 57632: loss: 0.6941746781:
1: 60832: loss: 0.6933586607:
1: 64032: loss: 0.6924716136:
1: 67232: loss: 0.6916188921:
1: 70432: loss: 0.6907760879:
1: 73632: loss: 0.6899620375:
1: 76832: loss: 0.6891404515:
1: 80032: loss: 0.6882947471:
1: 83232: loss: 0.6875416143:
1: 86432: loss: 0.6866985631:
1: 89632: loss: 0.6858446520:
1: 92832: loss: 0.6850062666:
1: 96032: loss: 0.6841868732:
1: 99232: loss: 0.6833186960:
1: 102432: loss: 0.6824609211:
1: 105632: loss: 0.6815928219:
1: 108832: loss: 0.6807794347:
1: 112032: loss: 0.6799604088:
1: 115232: loss: 0.6791194355:
1: 118432: loss: 0.6783471288:
1: 121632: loss: 0.6775537842:
1: 124832: loss: 0.6767594661:
1: 128032: loss: 0.6759561829:
1: 131232: loss: 0.6751875513:
1: 134432: loss: 0.6743764614:
Dev-Acc: 1: Accuracy: 0.9261489511: precision: 0.0732240437: recall: 0.0227852406: f1: 0.0347555440
Train-Acc: 1: Accuracy: 0.8804958463: precision: 0.2794625720: recall: 0.0478601012: f1: 0.0817242928
2: 3232: loss: 0.6390621430:
2: 6432: loss: 0.6386736158:
2: 9632: loss: 0.6374958318:
2: 12832: loss: 0.6369918601:
2: 16032: loss: 0.6363665794:
2: 19232: loss: 0.6355602546:
2: 22432: loss: 0.6347459753:
2: 25632: loss: 0.6339892333:
2: 28832: loss: 0.6331794568:
2: 32032: loss: 0.6323603097:
2: 35232: loss: 0.6317440234:
2: 38432: loss: 0.6309006256:
2: 41632: loss: 0.6300367703:
2: 44832: loss: 0.6291673681:
2: 48032: loss: 0.6284449434:
2: 51232: loss: 0.6275349003:
2: 54432: loss: 0.6269300078:
2: 57632: loss: 0.6262906156:
2: 60832: loss: 0.6254587282:
2: 64032: loss: 0.6245692623:
2: 67232: loss: 0.6238249668:
2: 70432: loss: 0.6230462943:
2: 73632: loss: 0.6221465456:
2: 76832: loss: 0.6213943606:
2: 80032: loss: 0.6207044325:
2: 83232: loss: 0.6200044564:
2: 86432: loss: 0.6192618045:
2: 89632: loss: 0.6185784915:
2: 92832: loss: 0.6178772105:
2: 96032: loss: 0.6171444389:
2: 99232: loss: 0.6164326098:
2: 102432: loss: 0.6156602170:
2: 105632: loss: 0.6148855207:
2: 108832: loss: 0.6141852817:
2: 112032: loss: 0.6134690967:
2: 115232: loss: 0.6127552092:
2: 118432: loss: 0.6120462090:
2: 121632: loss: 0.6113757065:
2: 124832: loss: 0.6105948366:
2: 128032: loss: 0.6098899366:
2: 131232: loss: 0.6091698900:
2: 134432: loss: 0.6084814271:
Dev-Acc: 2: Accuracy: 0.9416574240: precision: 1.0000000000: recall: 0.0001700391: f1: 0.0003400204
Train-Acc: 2: Accuracy: 0.8889254332: precision: 1.0000000000: recall: 0.0003287095: f1: 0.0006572029
3: 3232: loss: 0.5769263446:
3: 6432: loss: 0.5763994446:
3: 9632: loss: 0.5768054658:
3: 12832: loss: 0.5758752549:
3: 16032: loss: 0.5749626234:
3: 19232: loss: 0.5743066004:
3: 22432: loss: 0.5729382902:
3: 25632: loss: 0.5721619212:
3: 28832: loss: 0.5714162088:
3: 32032: loss: 0.5706531818:
3: 35232: loss: 0.5700815767:
3: 38432: loss: 0.5691067080:
3: 41632: loss: 0.5681047982:
3: 44832: loss: 0.5671895605:
3: 48032: loss: 0.5667579549:
3: 51232: loss: 0.5663800164:
3: 54432: loss: 0.5656145104:
3: 57632: loss: 0.5650436662:
3: 60832: loss: 0.5642246154:
3: 64032: loss: 0.5636060400:
3: 67232: loss: 0.5629878085:
3: 70432: loss: 0.5624485094:
3: 73632: loss: 0.5617158353:
3: 76832: loss: 0.5608602242:
3: 80032: loss: 0.5600802123:
3: 83232: loss: 0.5593794117:
3: 86432: loss: 0.5586478697:
3: 89632: loss: 0.5579599473:
3: 92832: loss: 0.5572659789:
3: 96032: loss: 0.5563205374:
3: 99232: loss: 0.5556812028:
3: 102432: loss: 0.5551092768:
3: 105632: loss: 0.5544517773:
3: 108832: loss: 0.5538487857:
3: 112032: loss: 0.5531461269:
3: 115232: loss: 0.5524846982:
3: 118432: loss: 0.5518039084:
3: 121632: loss: 0.5511079196:
3: 124832: loss: 0.5504718941:
3: 128032: loss: 0.5500130976:
3: 131232: loss: 0.5494708357:
3: 134432: loss: 0.5489077067:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.5212054276:
4: 6432: loss: 0.5163943148:
4: 9632: loss: 0.5170160579:
4: 12832: loss: 0.5176221374:
4: 16032: loss: 0.5160315592:
4: 19232: loss: 0.5150181082:
4: 22432: loss: 0.5141108670:
4: 25632: loss: 0.5143524878:
4: 28832: loss: 0.5142224475:
4: 32032: loss: 0.5138601762:
4: 35232: loss: 0.5128337133:
4: 38432: loss: 0.5122690112:
4: 41632: loss: 0.5117764894:
4: 44832: loss: 0.5112927606:
4: 48032: loss: 0.5108120409:
4: 51232: loss: 0.5101411369:
4: 54432: loss: 0.5096297689:
4: 57632: loss: 0.5088078601:
4: 60832: loss: 0.5085088208:
4: 64032: loss: 0.5078648579:
4: 67232: loss: 0.5074501137:
4: 70432: loss: 0.5069983401:
4: 73632: loss: 0.5063965923:
4: 76832: loss: 0.5059845165:
4: 80032: loss: 0.5056697088:
4: 83232: loss: 0.5051050111:
4: 86432: loss: 0.5045581393:
4: 89632: loss: 0.5040665390:
4: 92832: loss: 0.5033170463:
4: 96032: loss: 0.5028334227:
4: 99232: loss: 0.5022211682:
4: 102432: loss: 0.5014365906:
4: 105632: loss: 0.5007396439:
4: 108832: loss: 0.5003201196:
4: 112032: loss: 0.4994318633:
4: 115232: loss: 0.4988949712:
4: 118432: loss: 0.4982731933:
4: 121632: loss: 0.4975784065:
4: 124832: loss: 0.4969155378:
4: 128032: loss: 0.4963803221:
4: 131232: loss: 0.4956737832:
4: 134432: loss: 0.4953624544:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4789079276:
5: 6432: loss: 0.4747523233:
5: 9632: loss: 0.4725700838:
5: 12832: loss: 0.4716378047:
5: 16032: loss: 0.4697150254:
5: 19232: loss: 0.4687345765:
5: 22432: loss: 0.4681843971:
5: 25632: loss: 0.4687271750:
5: 28832: loss: 0.4674717393:
5: 32032: loss: 0.4675737520:
5: 35232: loss: 0.4659673002:
5: 38432: loss: 0.4649952428:
5: 41632: loss: 0.4641635543:
5: 44832: loss: 0.4632999139:
5: 48032: loss: 0.4627981151:
5: 51232: loss: 0.4620683211:
5: 54432: loss: 0.4617070440:
5: 57632: loss: 0.4609316978:
5: 60832: loss: 0.4601801060:
5: 64032: loss: 0.4599679393:
5: 67232: loss: 0.4593015381:
5: 70432: loss: 0.4588752664:
5: 73632: loss: 0.4581597254:
5: 76832: loss: 0.4572134343:
5: 80032: loss: 0.4570101961:
5: 83232: loss: 0.4563842744:
5: 86432: loss: 0.4558783766:
5: 89632: loss: 0.4554996382:
5: 92832: loss: 0.4550045245:
5: 96032: loss: 0.4546661279:
5: 99232: loss: 0.4539516878:
5: 102432: loss: 0.4535772150:
5: 105632: loss: 0.4527251727:
5: 108832: loss: 0.4523581437:
5: 112032: loss: 0.4516052954:
5: 115232: loss: 0.4510900293:
5: 118432: loss: 0.4507870288:
5: 121632: loss: 0.4502923135:
5: 124832: loss: 0.4498918969:
5: 128032: loss: 0.4497543789:
5: 131232: loss: 0.4494351149:
5: 134432: loss: 0.4489215843:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.4379089352:
6: 6432: loss: 0.4299750403:
6: 9632: loss: 0.4250047163:
6: 12832: loss: 0.4244258198:
6: 16032: loss: 0.4262945448:
6: 19232: loss: 0.4253626161:
6: 22432: loss: 0.4243018157:
6: 25632: loss: 0.4240138066:
6: 28832: loss: 0.4232830271:
6: 32032: loss: 0.4225387489:
6: 35232: loss: 0.4221151354:
6: 38432: loss: 0.4213061915:
6: 41632: loss: 0.4212479456:
6: 44832: loss: 0.4209983254:
6: 48032: loss: 0.4199085501:
6: 51232: loss: 0.4198797299:
6: 54432: loss: 0.4196151129:
6: 57632: loss: 0.4194735270:
6: 60832: loss: 0.4186528075:
6: 64032: loss: 0.4176808494:
6: 67232: loss: 0.4175199366:
6: 70432: loss: 0.4174469362:
6: 73632: loss: 0.4175114677:
6: 76832: loss: 0.4171940715:
6: 80032: loss: 0.4171516970:
6: 83232: loss: 0.4168632997:
6: 86432: loss: 0.4167273199:
6: 89632: loss: 0.4163060060:
6: 92832: loss: 0.4158663472:
6: 96032: loss: 0.4153899037:
6: 99232: loss: 0.4149590774:
6: 102432: loss: 0.4144089562:
6: 105632: loss: 0.4139634939:
6: 108832: loss: 0.4135060168:
6: 112032: loss: 0.4131370403:
6: 115232: loss: 0.4127680471:
6: 118432: loss: 0.4123406730:
6: 121632: loss: 0.4120570953:
6: 124832: loss: 0.4115252915:
6: 128032: loss: 0.4112925955:
6: 131232: loss: 0.4109492850:
6: 134432: loss: 0.4105521289:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.4000214601:
7: 6432: loss: 0.3969339809:
7: 9632: loss: 0.3941231573:
7: 12832: loss: 0.3925554966:
7: 16032: loss: 0.3907344448:
7: 19232: loss: 0.3910211843:
7: 22432: loss: 0.3904143899:
7: 25632: loss: 0.3903668718:
7: 28832: loss: 0.3901731663:
7: 32032: loss: 0.3907122966:
7: 35232: loss: 0.3900517260:
7: 38432: loss: 0.3901455287:
7: 41632: loss: 0.3897592869:
7: 44832: loss: 0.3889447200:
7: 48032: loss: 0.3888920538:
7: 51232: loss: 0.3880680078:
7: 54432: loss: 0.3880470272:
7: 57632: loss: 0.3878377761:
7: 60832: loss: 0.3873768365:
7: 64032: loss: 0.3870378271:
7: 67232: loss: 0.3874610174:
7: 70432: loss: 0.3876285812:
7: 73632: loss: 0.3876518394:
7: 76832: loss: 0.3870325019:
7: 80032: loss: 0.3865828741:
7: 83232: loss: 0.3858018400:
7: 86432: loss: 0.3854353025:
7: 89632: loss: 0.3852551357:
7: 92832: loss: 0.3850831052:
7: 96032: loss: 0.3840896467:
7: 99232: loss: 0.3837358306:
7: 102432: loss: 0.3837302961:
7: 105632: loss: 0.3833461302:
7: 108832: loss: 0.3832945965:
7: 112032: loss: 0.3826839591:
7: 115232: loss: 0.3822147879:
7: 118432: loss: 0.3820430776:
7: 121632: loss: 0.3817910132:
7: 124832: loss: 0.3814651541:
7: 128032: loss: 0.3809160822:
7: 131232: loss: 0.3806107309:
7: 134432: loss: 0.3804303625:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3635316381:
8: 6432: loss: 0.3721704490:
8: 9632: loss: 0.3686372123:
8: 12832: loss: 0.3690531770:
8: 16032: loss: 0.3682251856:
8: 19232: loss: 0.3661326795:
8: 22432: loss: 0.3668606788:
8: 25632: loss: 0.3661362049:
8: 28832: loss: 0.3663751284:
8: 32032: loss: 0.3653124530:
8: 35232: loss: 0.3648682830:
8: 38432: loss: 0.3643624539:
8: 41632: loss: 0.3640638093:
8: 44832: loss: 0.3637317033:
8: 48032: loss: 0.3633730156:
8: 51232: loss: 0.3633258418:
8: 54432: loss: 0.3633609921:
8: 57632: loss: 0.3632124894:
8: 60832: loss: 0.3629474613:
8: 64032: loss: 0.3627165798:
8: 67232: loss: 0.3625664968:
8: 70432: loss: 0.3626300175:
8: 73632: loss: 0.3622170203:
8: 76832: loss: 0.3619222472:
8: 80032: loss: 0.3616902035:
8: 83232: loss: 0.3608571176:
8: 86432: loss: 0.3609141146:
8: 89632: loss: 0.3604640868:
8: 92832: loss: 0.3602330655:
8: 96032: loss: 0.3598387010:
8: 99232: loss: 0.3594937616:
8: 102432: loss: 0.3593259530:
8: 105632: loss: 0.3588655790:
8: 108832: loss: 0.3586150675:
8: 112032: loss: 0.3584895948:
8: 115232: loss: 0.3581625658:
8: 118432: loss: 0.3577619036:
8: 121632: loss: 0.3571548155:
8: 124832: loss: 0.3567818406:
8: 128032: loss: 0.3564353096:
8: 131232: loss: 0.3563265381:
8: 134432: loss: 0.3563817778:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3446514875:
9: 6432: loss: 0.3463664839:
9: 9632: loss: 0.3448845216:
9: 12832: loss: 0.3425830401:
9: 16032: loss: 0.3423963949:
9: 19232: loss: 0.3424303767:
9: 22432: loss: 0.3436105946:
9: 25632: loss: 0.3443954876:
9: 28832: loss: 0.3426983670:
9: 32032: loss: 0.3419429564:
9: 35232: loss: 0.3421149364:
9: 38432: loss: 0.3432015869:
9: 41632: loss: 0.3428589453:
9: 44832: loss: 0.3431214524:
9: 48032: loss: 0.3434921286:
9: 51232: loss: 0.3444705915:
9: 54432: loss: 0.3446208040:
9: 57632: loss: 0.3447941989:
9: 60832: loss: 0.3447374951:
9: 64032: loss: 0.3447431000:
9: 67232: loss: 0.3440817244:
9: 70432: loss: 0.3437676569:
9: 73632: loss: 0.3443307015:
9: 76832: loss: 0.3436665792:
9: 80032: loss: 0.3430435898:
9: 83232: loss: 0.3425977217:
9: 86432: loss: 0.3415934742:
9: 89632: loss: 0.3413555326:
9: 92832: loss: 0.3412506811:
9: 96032: loss: 0.3409489444:
9: 99232: loss: 0.3409665871:
9: 102432: loss: 0.3405070838:
9: 105632: loss: 0.3402864354:
9: 108832: loss: 0.3401750786:
9: 112032: loss: 0.3400843566:
9: 115232: loss: 0.3400763123:
9: 118432: loss: 0.3401103324:
9: 121632: loss: 0.3399286645:
9: 124832: loss: 0.3396242748:
9: 128032: loss: 0.3394087371:
9: 131232: loss: 0.3390357059:
9: 134432: loss: 0.3387963303:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.3394562635:
10: 6432: loss: 0.3399997818:
10: 9632: loss: 0.3360729469:
10: 12832: loss: 0.3314115148:
10: 16032: loss: 0.3301592436:
10: 19232: loss: 0.3291013955:
10: 22432: loss: 0.3265528291:
10: 25632: loss: 0.3244947599:
10: 28832: loss: 0.3259730046:
10: 32032: loss: 0.3252279285:
10: 35232: loss: 0.3243219388:
10: 38432: loss: 0.3241330864:
10: 41632: loss: 0.3236223748:
10: 44832: loss: 0.3239123917:
10: 48032: loss: 0.3243119031:
10: 51232: loss: 0.3252536669:
10: 54432: loss: 0.3250050327:
10: 57632: loss: 0.3250917678:
10: 60832: loss: 0.3256004834:
10: 64032: loss: 0.3254467757:
10: 67232: loss: 0.3257777824:
10: 70432: loss: 0.3253397685:
10: 73632: loss: 0.3254355478:
10: 76832: loss: 0.3250670910:
10: 80032: loss: 0.3246772003:
10: 83232: loss: 0.3242648009:
10: 86432: loss: 0.3242775161:
10: 89632: loss: 0.3239657160:
10: 92832: loss: 0.3238495249:
10: 96032: loss: 0.3231193563:
10: 99232: loss: 0.3230032154:
10: 102432: loss: 0.3228599014:
10: 105632: loss: 0.3228847727:
10: 108832: loss: 0.3227614919:
10: 112032: loss: 0.3228382086:
10: 115232: loss: 0.3232647599:
10: 118432: loss: 0.3233184019:
10: 121632: loss: 0.3231325880:
10: 124832: loss: 0.3234550374:
10: 128032: loss: 0.3233074215:
10: 131232: loss: 0.3237152214:
10: 134432: loss: 0.3239867982:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.3197711286:
11: 6432: loss: 0.3185363071:
11: 9632: loss: 0.3147252452:
11: 12832: loss: 0.3170658660:
11: 16032: loss: 0.3173668551:
11: 19232: loss: 0.3155893014:
11: 22432: loss: 0.3169975292:
11: 25632: loss: 0.3163351540:
11: 28832: loss: 0.3156608581:
11: 32032: loss: 0.3146349393:
11: 35232: loss: 0.3162392548:
11: 38432: loss: 0.3154094576:
11: 41632: loss: 0.3147674382:
11: 44832: loss: 0.3159726032:
11: 48032: loss: 0.3159624193:
11: 51232: loss: 0.3152028901:
11: 54432: loss: 0.3154986740:
11: 57632: loss: 0.3152483749:
11: 60832: loss: 0.3155083912:
11: 64032: loss: 0.3156052438:
11: 67232: loss: 0.3157649949:
11: 70432: loss: 0.3154832324:
11: 73632: loss: 0.3151087906:
11: 76832: loss: 0.3147290075:
11: 80032: loss: 0.3146938590:
11: 83232: loss: 0.3152406526:
11: 86432: loss: 0.3150193915:
11: 89632: loss: 0.3150322930:
11: 92832: loss: 0.3147090249:
11: 96032: loss: 0.3148748702:
11: 99232: loss: 0.3146578994:
11: 102432: loss: 0.3142307340:
11: 105632: loss: 0.3138375056:
11: 108832: loss: 0.3135416663:
11: 112032: loss: 0.3130873778:
11: 115232: loss: 0.3133993199:
11: 118432: loss: 0.3132242718:
11: 121632: loss: 0.3131528753:
11: 124832: loss: 0.3126349611:
11: 128032: loss: 0.3123169255:
11: 131232: loss: 0.3122462041:
11: 134432: loss: 0.3119522080:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.3090593450:
12: 6432: loss: 0.3117762492:
12: 9632: loss: 0.3129316822:
12: 12832: loss: 0.3118967273:
12: 16032: loss: 0.3116264709:
12: 19232: loss: 0.3075871713:
12: 22432: loss: 0.3059215939:
12: 25632: loss: 0.3060833276:
12: 28832: loss: 0.3059539562:
12: 32032: loss: 0.3058484950:
12: 35232: loss: 0.3045376159:
12: 38432: loss: 0.3044667419:
12: 41632: loss: 0.3046843248:
12: 44832: loss: 0.3045464278:
12: 48032: loss: 0.3043007734:
12: 51232: loss: 0.3040927323:
12: 54432: loss: 0.3040913352:
12: 57632: loss: 0.3039123106:
12: 60832: loss: 0.3042712354:
12: 64032: loss: 0.3032885311:
12: 67232: loss: 0.3031355759:
12: 70432: loss: 0.3035367029:
12: 73632: loss: 0.3032627992:
12: 76832: loss: 0.3030260557:
12: 80032: loss: 0.3024228272:
12: 83232: loss: 0.3024830556:
12: 86432: loss: 0.3021391603:
12: 89632: loss: 0.3019898961:
12: 92832: loss: 0.3022800647:
12: 96032: loss: 0.3019424380:
12: 99232: loss: 0.3015778456:
12: 102432: loss: 0.3013582485:
12: 105632: loss: 0.3015459116:
12: 108832: loss: 0.3015859058:
12: 112032: loss: 0.3017450623:
12: 115232: loss: 0.3020257605:
12: 118432: loss: 0.3019360333:
12: 121632: loss: 0.3019117535:
12: 124832: loss: 0.3017799509:
12: 128032: loss: 0.3015916278:
12: 131232: loss: 0.3017964313:
12: 134432: loss: 0.3016380232:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2940458944:
13: 6432: loss: 0.2944374941:
13: 9632: loss: 0.2962472975:
13: 12832: loss: 0.2981727892:
13: 16032: loss: 0.2994268970:
13: 19232: loss: 0.2994323084:
13: 22432: loss: 0.2976682745:
13: 25632: loss: 0.2962311165:
13: 28832: loss: 0.2971885605:
13: 32032: loss: 0.2967963166:
13: 35232: loss: 0.2939538192:
13: 38432: loss: 0.2935500767:
13: 41632: loss: 0.2938456719:
13: 44832: loss: 0.2938659031:
13: 48032: loss: 0.2941120696:
13: 51232: loss: 0.2932447847:
13: 54432: loss: 0.2941071936:
13: 57632: loss: 0.2939028139:
13: 60832: loss: 0.2937998471:
13: 64032: loss: 0.2935918969:
13: 67232: loss: 0.2934006943:
13: 70432: loss: 0.2933111152:
13: 73632: loss: 0.2939278967:
13: 76832: loss: 0.2937161608:
13: 80032: loss: 0.2939760350:
13: 83232: loss: 0.2940849809:
13: 86432: loss: 0.2937459768:
13: 89632: loss: 0.2931376475:
13: 92832: loss: 0.2937867560:
13: 96032: loss: 0.2940551653:
13: 99232: loss: 0.2939935109:
13: 102432: loss: 0.2938960699:
13: 105632: loss: 0.2942359436:
13: 108832: loss: 0.2935981477:
13: 112032: loss: 0.2933139135:
13: 115232: loss: 0.2933957811:
13: 118432: loss: 0.2928786790:
13: 121632: loss: 0.2928263182:
13: 124832: loss: 0.2926520540:
13: 128032: loss: 0.2928264652:
13: 131232: loss: 0.2929148460:
13: 134432: loss: 0.2926802560:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.8892102838: precision: 1.0000000000: recall: 0.0028926435: f1: 0.0057686005
14: 3232: loss: 0.2931587488:
14: 6432: loss: 0.3013044717:
14: 9632: loss: 0.2966512394:
14: 12832: loss: 0.2918143024:
14: 16032: loss: 0.2901179874:
14: 19232: loss: 0.2925304252:
14: 22432: loss: 0.2920866781:
14: 25632: loss: 0.2904382074:
14: 28832: loss: 0.2897275966:
14: 32032: loss: 0.2899487618:
14: 35232: loss: 0.2880703455:
14: 38432: loss: 0.2889528954:
14: 41632: loss: 0.2887610392:
14: 44832: loss: 0.2884577157:
14: 48032: loss: 0.2885022954:
14: 51232: loss: 0.2874488822:
14: 54432: loss: 0.2871216536:
14: 57632: loss: 0.2865723201:
14: 60832: loss: 0.2866915493:
14: 64032: loss: 0.2861343727:
14: 67232: loss: 0.2861997291:
14: 70432: loss: 0.2864548840:
14: 73632: loss: 0.2859364946:
14: 76832: loss: 0.2859214498:
14: 80032: loss: 0.2858361479:
14: 83232: loss: 0.2858671695:
14: 86432: loss: 0.2862929772:
14: 89632: loss: 0.2858105834:
14: 92832: loss: 0.2856861654:
14: 96032: loss: 0.2855418821:
14: 99232: loss: 0.2860129690:
14: 102432: loss: 0.2861284978:
14: 105632: loss: 0.2858085112:
14: 108832: loss: 0.2859937642:
14: 112032: loss: 0.2857164456:
14: 115232: loss: 0.2855413401:
14: 118432: loss: 0.2855203569:
14: 121632: loss: 0.2856445290:
14: 124832: loss: 0.2858305603:
14: 128032: loss: 0.2855585509:
14: 131232: loss: 0.2852609571:
14: 134432: loss: 0.2849642593:
Dev-Acc: 14: Accuracy: 0.9416574240: precision: 0.5200000000: recall: 0.0022105084: f1: 0.0044023027
Train-Acc: 14: Accuracy: 0.8905689716: precision: 0.8506097561: recall: 0.0183419893: f1: 0.0359096467
15: 3232: loss: 0.2849934416:
15: 6432: loss: 0.2823551174:
15: 9632: loss: 0.2860842915:
15: 12832: loss: 0.2865023580:
15: 16032: loss: 0.2860474414:
15: 19232: loss: 0.2872862669:
15: 22432: loss: 0.2858781622:
15: 25632: loss: 0.2890552754:
15: 28832: loss: 0.2893908783:
15: 32032: loss: 0.2899844620:
15: 35232: loss: 0.2883348008:
15: 38432: loss: 0.2861091997:
15: 41632: loss: 0.2851549603:
15: 44832: loss: 0.2841388916:
15: 48032: loss: 0.2839134338:
15: 51232: loss: 0.2824402548:
15: 54432: loss: 0.2825120370:
15: 57632: loss: 0.2814658238:
15: 60832: loss: 0.2818704128:
15: 64032: loss: 0.2811021539:
15: 67232: loss: 0.2813631373:
15: 70432: loss: 0.2810609389:
15: 73632: loss: 0.2815056322:
15: 76832: loss: 0.2808563939:
15: 80032: loss: 0.2814200103:
15: 83232: loss: 0.2813687417:
15: 86432: loss: 0.2804731546:
15: 89632: loss: 0.2801408053:
15: 92832: loss: 0.2804203710:
15: 96032: loss: 0.2795918374:
15: 99232: loss: 0.2791073552:
15: 102432: loss: 0.2791191668:
15: 105632: loss: 0.2787149933:
15: 108832: loss: 0.2786482801:
15: 112032: loss: 0.2784668534:
15: 115232: loss: 0.2784961875:
15: 118432: loss: 0.2781478246:
15: 121632: loss: 0.2779777257:
15: 124832: loss: 0.2777926651:
15: 128032: loss: 0.2777462753:
15: 131232: loss: 0.2778649720:
15: 134432: loss: 0.2779649120:
Dev-Acc: 15: Accuracy: 0.9421138167: precision: 0.7117117117: recall: 0.0134330896: f1: 0.0263684913
Train-Acc: 15: Accuracy: 0.8918983936: precision: 0.8159509202: recall: 0.0349746894: f1: 0.0670743239
16: 3232: loss: 0.2659997483:
16: 6432: loss: 0.2692243816:
16: 9632: loss: 0.2725569406:
16: 12832: loss: 0.2709818121:
16: 16032: loss: 0.2709203175:
16: 19232: loss: 0.2745033663:
16: 22432: loss: 0.2763733033:
16: 25632: loss: 0.2754143880:
16: 28832: loss: 0.2745138295:
16: 32032: loss: 0.2743047053:
16: 35232: loss: 0.2749840269:
16: 38432: loss: 0.2754760648:
16: 41632: loss: 0.2748642412:
16: 44832: loss: 0.2747195542:
16: 48032: loss: 0.2748001718:
16: 51232: loss: 0.2736752365:
16: 54432: loss: 0.2735078702:
16: 57632: loss: 0.2742115841:
16: 60832: loss: 0.2740065027:
16: 64032: loss: 0.2738513172:
16: 67232: loss: 0.2735964638:
16: 70432: loss: 0.2738059277:
16: 73632: loss: 0.2739727257:
16: 76832: loss: 0.2739853641:
16: 80032: loss: 0.2740420804:
16: 83232: loss: 0.2739527327:
16: 86432: loss: 0.2740064886:
16: 89632: loss: 0.2746333592:
16: 92832: loss: 0.2741999109:
16: 96032: loss: 0.2740579997:
16: 99232: loss: 0.2741711617:
16: 102432: loss: 0.2744388849:
16: 105632: loss: 0.2740549229:
16: 108832: loss: 0.2738084236:
16: 112032: loss: 0.2735116474:
16: 115232: loss: 0.2734090367:
16: 118432: loss: 0.2729841040:
16: 121632: loss: 0.2726251624:
16: 124832: loss: 0.2726843372:
16: 128032: loss: 0.2726626426:
16: 131232: loss: 0.2722078167:
16: 134432: loss: 0.2717531922:
Dev-Acc: 16: Accuracy: 0.9424015880: precision: 0.6187500000: recall: 0.0336677436: f1: 0.0638606676
Train-Acc: 16: Accuracy: 0.8936807513: precision: 0.8588621444: recall: 0.0516073894: f1: 0.0973643411
17: 3232: loss: 0.2568382264:
17: 6432: loss: 0.2601134314:
17: 9632: loss: 0.2650369607:
17: 12832: loss: 0.2613631846:
17: 16032: loss: 0.2597989820:
17: 19232: loss: 0.2641706257:
17: 22432: loss: 0.2635395826:
17: 25632: loss: 0.2656649681:
17: 28832: loss: 0.2657640681:
17: 32032: loss: 0.2665345109:
17: 35232: loss: 0.2658400421:
17: 38432: loss: 0.2666700153:
17: 41632: loss: 0.2669614900:
17: 44832: loss: 0.2665046925:
17: 48032: loss: 0.2671524108:
17: 51232: loss: 0.2673273488:
17: 54432: loss: 0.2667104143:
17: 57632: loss: 0.2665900488:
17: 60832: loss: 0.2669117412:
17: 64032: loss: 0.2667977250:
17: 67232: loss: 0.2666436940:
17: 70432: loss: 0.2664726931:
17: 73632: loss: 0.2668825499:
17: 76832: loss: 0.2667902413:
17: 80032: loss: 0.2668436770:
17: 83232: loss: 0.2668508018:
17: 86432: loss: 0.2669942862:
17: 89632: loss: 0.2664289524:
17: 92832: loss: 0.2669543020:
17: 96032: loss: 0.2669950967:
17: 99232: loss: 0.2662580085:
17: 102432: loss: 0.2661293974:
17: 105632: loss: 0.2659308820:
17: 108832: loss: 0.2658425251:
17: 112032: loss: 0.2655062909:
17: 115232: loss: 0.2653073556:
17: 118432: loss: 0.2649293618:
17: 121632: loss: 0.2652094362:
17: 124832: loss: 0.2650618154:
17: 128032: loss: 0.2650494025:
17: 131232: loss: 0.2649183056:
17: 134432: loss: 0.2651026496:
Dev-Acc: 17: Accuracy: 0.9420542717: precision: 0.5448577681: recall: 0.0423397381: f1: 0.0785736825
Train-Acc: 17: Accuracy: 0.8958356380: precision: 0.8666152660: recall: 0.0738938926: f1: 0.1361763993
18: 3232: loss: 0.2565662131:
18: 6432: loss: 0.2515549009:
18: 9632: loss: 0.2545225963:
18: 12832: loss: 0.2583683324:
18: 16032: loss: 0.2612956274:
18: 19232: loss: 0.2623720191:
18: 22432: loss: 0.2607745142:
18: 25632: loss: 0.2609411064:
18: 28832: loss: 0.2614242855:
18: 32032: loss: 0.2620764438:
18: 35232: loss: 0.2620369292:
18: 38432: loss: 0.2603923055:
18: 41632: loss: 0.2597806123:
18: 44832: loss: 0.2587941802:
18: 48032: loss: 0.2599050764:
18: 51232: loss: 0.2598370958:
18: 54432: loss: 0.2596453650:
18: 57632: loss: 0.2593715138:
18: 60832: loss: 0.2601926442:
18: 64032: loss: 0.2601153597:
18: 67232: loss: 0.2607241305:
18: 70432: loss: 0.2608635024:
18: 73632: loss: 0.2611387429:
18: 76832: loss: 0.2606175040:
18: 80032: loss: 0.2599480894:
18: 83232: loss: 0.2601467237:
18: 86432: loss: 0.2600071592:
18: 89632: loss: 0.2599482693:
18: 92832: loss: 0.2604419932:
18: 96032: loss: 0.2607401621:
18: 99232: loss: 0.2610572820:
18: 102432: loss: 0.2608337431:
18: 105632: loss: 0.2609475958:
18: 108832: loss: 0.2607168368:
18: 112032: loss: 0.2604109291:
18: 115232: loss: 0.2605824056:
18: 118432: loss: 0.2605115802:
18: 121632: loss: 0.2604281623:
18: 124832: loss: 0.2604054366:
18: 128032: loss: 0.2600909608:
18: 131232: loss: 0.2601250929:
18: 134432: loss: 0.2600210765:
Dev-Acc: 18: Accuracy: 0.9425107241: precision: 0.5568627451: recall: 0.0724366604: f1: 0.1281974120
Train-Acc: 18: Accuracy: 0.8985748887: precision: 0.8622950820: recall: 0.1037407140: f1: 0.1852003990
19: 3232: loss: 0.2717392243:
19: 6432: loss: 0.2686866765:
19: 9632: loss: 0.2688135324:
19: 12832: loss: 0.2669736604:
19: 16032: loss: 0.2682579708:
19: 19232: loss: 0.2662059549:
19: 22432: loss: 0.2651909486:
19: 25632: loss: 0.2647781434:
19: 28832: loss: 0.2624707319:
19: 32032: loss: 0.2614617598:
19: 35232: loss: 0.2606742983:
19: 38432: loss: 0.2599836866:
19: 41632: loss: 0.2601447773:
19: 44832: loss: 0.2596149001:
19: 48032: loss: 0.2589088539:
19: 51232: loss: 0.2588771874:
19: 54432: loss: 0.2580636328:
19: 57632: loss: 0.2573553115:
19: 60832: loss: 0.2576534387:
19: 64032: loss: 0.2577324338:
19: 67232: loss: 0.2574987026:
19: 70432: loss: 0.2576669485:
19: 73632: loss: 0.2575439181:
19: 76832: loss: 0.2572673947:
19: 80032: loss: 0.2577366318:
19: 83232: loss: 0.2578969251:
19: 86432: loss: 0.2578133006:
19: 89632: loss: 0.2575834419:
19: 92832: loss: 0.2570243283:
19: 96032: loss: 0.2567028819:
19: 99232: loss: 0.2564667718:
19: 102432: loss: 0.2562377344:
19: 105632: loss: 0.2558852806:
19: 108832: loss: 0.2555771673:
19: 112032: loss: 0.2558702418:
19: 115232: loss: 0.2555510431:
19: 118432: loss: 0.2556816210:
19: 121632: loss: 0.2557377643:
19: 124832: loss: 0.2555182472:
19: 128032: loss: 0.2553945995:
19: 131232: loss: 0.2559984372:
19: 134432: loss: 0.2557018113:
Dev-Acc: 19: Accuracy: 0.9436616898: precision: 0.5834018077: recall: 0.1207277674: f1: 0.2000563539
Train-Acc: 19: Accuracy: 0.9019861221: precision: 0.8712215321: recall: 0.1383209519: f1: 0.2387382276
20: 3232: loss: 0.2550795195:
20: 6432: loss: 0.2537631021:
20: 9632: loss: 0.2565130097:
20: 12832: loss: 0.2510244006:
20: 16032: loss: 0.2507806347:
20: 19232: loss: 0.2492968253:
20: 22432: loss: 0.2514435294:
20: 25632: loss: 0.2498674174:
20: 28832: loss: 0.2505403849:
20: 32032: loss: 0.2495483844:
20: 35232: loss: 0.2487237903:
20: 38432: loss: 0.2481542194:
20: 41632: loss: 0.2487829336:
20: 44832: loss: 0.2498610257:
20: 48032: loss: 0.2499170974:
20: 51232: loss: 0.2489790758:
20: 54432: loss: 0.2496745058:
20: 57632: loss: 0.2500993389:
20: 60832: loss: 0.2501206650:
20: 64032: loss: 0.2495785838:
20: 67232: loss: 0.2508063623:
20: 70432: loss: 0.2509380258:
20: 73632: loss: 0.2514145118:
20: 76832: loss: 0.2511528394:
20: 80032: loss: 0.2509196768:
20: 83232: loss: 0.2503980431:
20: 86432: loss: 0.2502947732:
20: 89632: loss: 0.2508494510:
20: 92832: loss: 0.2513783905:
20: 96032: loss: 0.2512496282:
20: 99232: loss: 0.2514291500:
20: 102432: loss: 0.2511946984:
20: 105632: loss: 0.2513778056:
20: 108832: loss: 0.2517185495:
20: 112032: loss: 0.2519345537:
20: 115232: loss: 0.2521342129:
20: 118432: loss: 0.2522341828:
20: 121632: loss: 0.2519845968:
20: 124832: loss: 0.2514402399:
20: 128032: loss: 0.2511622693:
20: 131232: loss: 0.2514968656:
20: 134432: loss: 0.2511867649:
Dev-Acc: 20: Accuracy: 0.9437609315: precision: 0.5760171306: recall: 0.1372215610: f1: 0.2216424059
Train-Acc: 20: Accuracy: 0.9052367210: precision: 0.8644951140: recall: 0.1744789955: f1: 0.2903561074
21: 3232: loss: 0.2400305362:
21: 6432: loss: 0.2395796851:
21: 9632: loss: 0.2414975500:
21: 12832: loss: 0.2439969688:
21: 16032: loss: 0.2456268598:
21: 19232: loss: 0.2433940009:
21: 22432: loss: 0.2437216672:
21: 25632: loss: 0.2465876938:
21: 28832: loss: 0.2469019107:
21: 32032: loss: 0.2467491545:
21: 35232: loss: 0.2461015921:
21: 38432: loss: 0.2452509091:
21: 41632: loss: 0.2460006939:
21: 44832: loss: 0.2450245791:
21: 48032: loss: 0.2459250556:
21: 51232: loss: 0.2462077201:
21: 54432: loss: 0.2460263810:
21: 57632: loss: 0.2455599544:
21: 60832: loss: 0.2453493022:
21: 64032: loss: 0.2464926913:
21: 67232: loss: 0.2467469327:
21: 70432: loss: 0.2468282419:
21: 73632: loss: 0.2470057232:
21: 76832: loss: 0.2467592802:
21: 80032: loss: 0.2467750199:
21: 83232: loss: 0.2468113067:
21: 86432: loss: 0.2468108291:
21: 89632: loss: 0.2472125641:
21: 92832: loss: 0.2470924562:
21: 96032: loss: 0.2472520700:
21: 99232: loss: 0.2470293613:
21: 102432: loss: 0.2470579874:
21: 105632: loss: 0.2472109035:
21: 108832: loss: 0.2470380001:
21: 112032: loss: 0.2474790714:
21: 115232: loss: 0.2472389430:
21: 118432: loss: 0.2472522158:
21: 121632: loss: 0.2472692550:
21: 124832: loss: 0.2471111873:
21: 128032: loss: 0.2472989080:
21: 131232: loss: 0.2471764950:
21: 134432: loss: 0.2469488262:
Dev-Acc: 21: Accuracy: 0.9431656003: precision: 0.5365155131: recall: 0.1911239585: f1: 0.2818455366
Train-Acc: 21: Accuracy: 0.9073988795: precision: 0.8357180710: recall: 0.2073499441: f1: 0.3322623124
22: 3232: loss: 0.2433128108:
22: 6432: loss: 0.2505348771:
22: 9632: loss: 0.2495612887:
22: 12832: loss: 0.2481414793:
22: 16032: loss: 0.2475067260:
22: 19232: loss: 0.2475962230:
22: 22432: loss: 0.2449325738:
22: 25632: loss: 0.2457232699:
22: 28832: loss: 0.2445399072:
22: 32032: loss: 0.2442628027:
22: 35232: loss: 0.2443575866:
22: 38432: loss: 0.2442679566:
22: 41632: loss: 0.2444016370:
22: 44832: loss: 0.2430738591:
22: 48032: loss: 0.2430390915:
22: 51232: loss: 0.2423273814:
22: 54432: loss: 0.2430057860:
22: 57632: loss: 0.2423263566:
22: 60832: loss: 0.2424052172:
22: 64032: loss: 0.2427846225:
22: 67232: loss: 0.2429987219:
22: 70432: loss: 0.2428353488:
22: 73632: loss: 0.2428497124:
22: 76832: loss: 0.2429127612:
22: 80032: loss: 0.2430529761:
22: 83232: loss: 0.2430315242:
22: 86432: loss: 0.2430386337:
22: 89632: loss: 0.2429085895:
22: 92832: loss: 0.2426508002:
22: 96032: loss: 0.2427613562:
22: 99232: loss: 0.2425891485:
22: 102432: loss: 0.2424149834:
22: 105632: loss: 0.2430564801:
22: 108832: loss: 0.2431099303:
22: 112032: loss: 0.2430862276:
22: 115232: loss: 0.2434612951:
22: 118432: loss: 0.2442181986:
22: 121632: loss: 0.2438022900:
22: 124832: loss: 0.2440079672:
22: 128032: loss: 0.2439642983:
22: 131232: loss: 0.2437391907:
22: 134432: loss: 0.2435026725:
Dev-Acc: 22: Accuracy: 0.9419352412: precision: 0.5053723601: recall: 0.2319333447: f1: 0.3179487179
Train-Acc: 22: Accuracy: 0.9093127251: precision: 0.8171506352: recall: 0.2368023141: f1: 0.3671950660
23: 3232: loss: 0.2263022549:
23: 6432: loss: 0.2326796892:
23: 9632: loss: 0.2367544180:
23: 12832: loss: 0.2394381000:
23: 16032: loss: 0.2395855570:
23: 19232: loss: 0.2412948359:
23: 22432: loss: 0.2425137017:
23: 25632: loss: 0.2411042215:
23: 28832: loss: 0.2404624477:
23: 32032: loss: 0.2415169883:
23: 35232: loss: 0.2422110183:
23: 38432: loss: 0.2436937506:
23: 41632: loss: 0.2433522266:
23: 44832: loss: 0.2429308489:
23: 48032: loss: 0.2433830027:
23: 51232: loss: 0.2428695828:
23: 54432: loss: 0.2420385657:
23: 57632: loss: 0.2402062342:
23: 60832: loss: 0.2401392195:
23: 64032: loss: 0.2399302307:
23: 67232: loss: 0.2400479059:
23: 70432: loss: 0.2399526770:
23: 73632: loss: 0.2396104698:
23: 76832: loss: 0.2399875044:
23: 80032: loss: 0.2408608108:
23: 83232: loss: 0.2409276708:
23: 86432: loss: 0.2407243375:
23: 89632: loss: 0.2406448890:
23: 92832: loss: 0.2404891838:
23: 96032: loss: 0.2405890968:
23: 99232: loss: 0.2403305914:
23: 102432: loss: 0.2399624291:
23: 105632: loss: 0.2402396641:
23: 108832: loss: 0.2395811585:
23: 112032: loss: 0.2391457275:
23: 115232: loss: 0.2390413574:
23: 118432: loss: 0.2390803360:
23: 121632: loss: 0.2392542820:
23: 124832: loss: 0.2395658950:
23: 128032: loss: 0.2397138757:
23: 131232: loss: 0.2400718234:
23: 134432: loss: 0.2398554640:
Dev-Acc: 23: Accuracy: 0.9408537149: precision: 0.4870967742: recall: 0.2567590546: f1: 0.3362654493
Train-Acc: 23: Accuracy: 0.9113141894: precision: 0.8084807074: recall: 0.2644796529: f1: 0.3985733393
24: 3232: loss: 0.2530676587:
24: 6432: loss: 0.2475747374:
24: 9632: loss: 0.2406097081:
24: 12832: loss: 0.2417223884:
24: 16032: loss: 0.2409988593:
24: 19232: loss: 0.2418837843:
24: 22432: loss: 0.2395757637:
24: 25632: loss: 0.2376985123:
24: 28832: loss: 0.2380314360:
24: 32032: loss: 0.2371053907:
24: 35232: loss: 0.2375085946:
24: 38432: loss: 0.2385097652:
24: 41632: loss: 0.2382731163:
24: 44832: loss: 0.2377924221:
24: 48032: loss: 0.2363990910:
24: 51232: loss: 0.2366084876:
24: 54432: loss: 0.2356001106:
24: 57632: loss: 0.2349975313:
24: 60832: loss: 0.2358734423:
24: 64032: loss: 0.2361375548:
24: 67232: loss: 0.2353592637:
24: 70432: loss: 0.2355553589:
24: 73632: loss: 0.2356175250:
24: 76832: loss: 0.2357010577:
24: 80032: loss: 0.2358905065:
24: 83232: loss: 0.2364608272:
24: 86432: loss: 0.2372992837:
24: 89632: loss: 0.2374025675:
24: 92832: loss: 0.2371651128:
24: 96032: loss: 0.2370510526:
24: 99232: loss: 0.2368377407:
24: 102432: loss: 0.2368181638:
24: 105632: loss: 0.2364854248:
24: 108832: loss: 0.2365844099:
24: 112032: loss: 0.2363848040:
24: 115232: loss: 0.2366424917:
24: 118432: loss: 0.2365326021:
24: 121632: loss: 0.2363210484:
24: 124832: loss: 0.2360519300:
24: 128032: loss: 0.2360987259:
24: 131232: loss: 0.2360337012:
24: 134432: loss: 0.2363579665:
Dev-Acc: 24: Accuracy: 0.9401492476: precision: 0.4780968959: recall: 0.2802244516: f1: 0.3533447684
Train-Acc: 24: Accuracy: 0.9130015969: precision: 0.8058180471: recall: 0.2859115114: f1: 0.4220690994
25: 3232: loss: 0.2348058191:
25: 6432: loss: 0.2313792652:
25: 9632: loss: 0.2316070566:
25: 12832: loss: 0.2338592621:
25: 16032: loss: 0.2353364740:
25: 19232: loss: 0.2344199310:
25: 22432: loss: 0.2351673768:
25: 25632: loss: 0.2338790751:
25: 28832: loss: 0.2342331179:
25: 32032: loss: 0.2339305150:
25: 35232: loss: 0.2347096839:
25: 38432: loss: 0.2343025223:
25: 41632: loss: 0.2351571446:
25: 44832: loss: 0.2342794715:
25: 48032: loss: 0.2354180541:
25: 51232: loss: 0.2346692981:
25: 54432: loss: 0.2341536024:
25: 57632: loss: 0.2345065814:
25: 60832: loss: 0.2343202078:
25: 64032: loss: 0.2346023363:
25: 67232: loss: 0.2351713617:
25: 70432: loss: 0.2347149751:
25: 73632: loss: 0.2345888425:
25: 76832: loss: 0.2342925474:
25: 80032: loss: 0.2345314225:
25: 83232: loss: 0.2350202751:
25: 86432: loss: 0.2346431719:
25: 89632: loss: 0.2333230763:
25: 92832: loss: 0.2335136283:
25: 96032: loss: 0.2332564179:
25: 99232: loss: 0.2336552831:
25: 102432: loss: 0.2333460163:
25: 105632: loss: 0.2333929717:
25: 108832: loss: 0.2334290926:
25: 112032: loss: 0.2337885103:
25: 115232: loss: 0.2341673881:
25: 118432: loss: 0.2338674137:
25: 121632: loss: 0.2339376458:
25: 124832: loss: 0.2336692327:
25: 128032: loss: 0.2337035565:
25: 131232: loss: 0.2337796046:
25: 134432: loss: 0.2336538863:
Dev-Acc: 25: Accuracy: 0.9394149780: precision: 0.4701828783: recall: 0.3016493794: f1: 0.3675160555
Train-Acc: 25: Accuracy: 0.9142360687: precision: 0.8043859649: recall: 0.3014265992: f1: 0.4385251781
26: 3232: loss: 0.2424643070:
26: 6432: loss: 0.2316645246:
26: 9632: loss: 0.2290040650:
26: 12832: loss: 0.2298672759:
26: 16032: loss: 0.2296627963:
26: 19232: loss: 0.2294211674:
26: 22432: loss: 0.2315727148:
26: 25632: loss: 0.2321851069:
26: 28832: loss: 0.2317517783:
26: 32032: loss: 0.2314158189:
26: 35232: loss: 0.2317812476:
26: 38432: loss: 0.2331253133:
26: 41632: loss: 0.2324389187:
26: 44832: loss: 0.2320188336:
26: 48032: loss: 0.2321313740:
26: 51232: loss: 0.2313738100:
26: 54432: loss: 0.2309694274:
26: 57632: loss: 0.2296322763:
26: 60832: loss: 0.2303073305:
26: 64032: loss: 0.2299146390:
26: 67232: loss: 0.2299653702:
26: 70432: loss: 0.2299267521:
26: 73632: loss: 0.2292455464:
26: 76832: loss: 0.2302410897:
26: 80032: loss: 0.2305341472:
26: 83232: loss: 0.2298064143:
26: 86432: loss: 0.2302368636:
26: 89632: loss: 0.2299027172:
26: 92832: loss: 0.2298452446:
26: 96032: loss: 0.2297997083:
26: 99232: loss: 0.2299441584:
26: 102432: loss: 0.2300445110:
26: 105632: loss: 0.2301737530:
26: 108832: loss: 0.2299719396:
26: 112032: loss: 0.2303960685:
26: 115232: loss: 0.2299468905:
26: 118432: loss: 0.2301966548:
26: 121632: loss: 0.2304532879:
26: 124832: loss: 0.2303268023:
26: 128032: loss: 0.2305784715:
26: 131232: loss: 0.2304217114:
26: 134432: loss: 0.2303489279:
Dev-Acc: 26: Accuracy: 0.9384425879: precision: 0.4598358617: recall: 0.3144023125: f1: 0.3734599071
Train-Acc: 26: Accuracy: 0.9151052833: precision: 0.7996326599: recall: 0.3148379462: f1: 0.4517924528
27: 3232: loss: 0.2285873876:
27: 6432: loss: 0.2374202204:
27: 9632: loss: 0.2351723704:
27: 12832: loss: 0.2322506076:
27: 16032: loss: 0.2327144749:
27: 19232: loss: 0.2347245433:
27: 22432: loss: 0.2327569983:
27: 25632: loss: 0.2334673348:
27: 28832: loss: 0.2318425460:
27: 32032: loss: 0.2320622602:
27: 35232: loss: 0.2319048253:
27: 38432: loss: 0.2320686038:
27: 41632: loss: 0.2323134005:
27: 44832: loss: 0.2318437359:
27: 48032: loss: 0.2313518375:
27: 51232: loss: 0.2319051850:
27: 54432: loss: 0.2312204826:
27: 57632: loss: 0.2306016120:
27: 60832: loss: 0.2307153144:
27: 64032: loss: 0.2314929122:
27: 67232: loss: 0.2320717232:
27: 70432: loss: 0.2320219089:
27: 73632: loss: 0.2314490243:
27: 76832: loss: 0.2314408253:
27: 80032: loss: 0.2309174832:
27: 83232: loss: 0.2307261048:
27: 86432: loss: 0.2302400490:
27: 89632: loss: 0.2298298934:
27: 92832: loss: 0.2292645673:
27: 96032: loss: 0.2288536576:
27: 99232: loss: 0.2285885762:
27: 102432: loss: 0.2280722691:
27: 105632: loss: 0.2278400296:
27: 108832: loss: 0.2281425279:
27: 112032: loss: 0.2280630382:
27: 115232: loss: 0.2281801978:
27: 118432: loss: 0.2284448990:
27: 121632: loss: 0.2284139685:
27: 124832: loss: 0.2283382833:
27: 128032: loss: 0.2281771545:
27: 131232: loss: 0.2281683068:
27: 134432: loss: 0.2279774580:
Dev-Acc: 27: Accuracy: 0.9379960895: precision: 0.4558752998: recall: 0.3232443462: f1: 0.3782708188
Train-Acc: 27: Accuracy: 0.9160110950: precision: 0.7985206625: recall: 0.3264742620: f1: 0.4634624358
28: 3232: loss: 0.2285085698:
28: 6432: loss: 0.2321045038:
28: 9632: loss: 0.2342973288:
28: 12832: loss: 0.2322686219:
28: 16032: loss: 0.2324505932:
28: 19232: loss: 0.2292812158:
28: 22432: loss: 0.2277254888:
28: 25632: loss: 0.2260981265:
28: 28832: loss: 0.2279179186:
28: 32032: loss: 0.2281619036:
28: 35232: loss: 0.2273607777:
28: 38432: loss: 0.2262278384:
28: 41632: loss: 0.2267468688:
28: 44832: loss: 0.2273814078:
28: 48032: loss: 0.2275411863:
28: 51232: loss: 0.2272803707:
28: 54432: loss: 0.2269074189:
28: 57632: loss: 0.2276830692:
28: 60832: loss: 0.2269862034:
28: 64032: loss: 0.2269831571:
28: 67232: loss: 0.2271899895:
28: 70432: loss: 0.2273943503:
28: 73632: loss: 0.2273468434:
28: 76832: loss: 0.2266872528:
28: 80032: loss: 0.2261635722:
28: 83232: loss: 0.2264571635:
28: 86432: loss: 0.2262201091:
28: 89632: loss: 0.2259430080:
28: 92832: loss: 0.2255759157:
28: 96032: loss: 0.2256810314:
28: 99232: loss: 0.2254184633:
28: 102432: loss: 0.2259050280:
28: 105632: loss: 0.2259267959:
28: 108832: loss: 0.2255464685:
28: 112032: loss: 0.2254755153:
28: 115232: loss: 0.2256438520:
28: 118432: loss: 0.2259238179:
28: 121632: loss: 0.2259249834:
28: 124832: loss: 0.2258025492:
28: 128032: loss: 0.2257205449:
28: 131232: loss: 0.2258092684:
28: 134432: loss: 0.2255195838:
Dev-Acc: 28: Accuracy: 0.9375396967: precision: 0.4513399154: recall: 0.3264750893: f1: 0.3788850518
Train-Acc: 28: Accuracy: 0.9170629382: precision: 0.7986681121: recall: 0.3390309644: f1: 0.4760014768
29: 3232: loss: 0.2168129270:
29: 6432: loss: 0.2217763747:
29: 9632: loss: 0.2273454118:
29: 12832: loss: 0.2245201113:
29: 16032: loss: 0.2240167598:
29: 19232: loss: 0.2239695021:
29: 22432: loss: 0.2248529843:
29: 25632: loss: 0.2253324257:
29: 28832: loss: 0.2249076339:
29: 32032: loss: 0.2242750758:
29: 35232: loss: 0.2238750861:
29: 38432: loss: 0.2246026432:
29: 41632: loss: 0.2248358276:
29: 44832: loss: 0.2245751201:
29: 48032: loss: 0.2256419638:
29: 51232: loss: 0.2262692770:
29: 54432: loss: 0.2254543873:
29: 57632: loss: 0.2255121095:
29: 60832: loss: 0.2261847835:
29: 64032: loss: 0.2259461467:
29: 67232: loss: 0.2256455869:
29: 70432: loss: 0.2252130580:
29: 73632: loss: 0.2248750300:
29: 76832: loss: 0.2241196894:
29: 80032: loss: 0.2236970108:
29: 83232: loss: 0.2239804516:
29: 86432: loss: 0.2237547365:
29: 89632: loss: 0.2234971321:
29: 92832: loss: 0.2239123493:
29: 96032: loss: 0.2240886776:
29: 99232: loss: 0.2244028674:
29: 102432: loss: 0.2237970814:
29: 105632: loss: 0.2237874350:
29: 108832: loss: 0.2235965304:
29: 112032: loss: 0.2233496516:
29: 115232: loss: 0.2231097849:
29: 118432: loss: 0.2223724959:
29: 121632: loss: 0.2223660583:
29: 124832: loss: 0.2224253828:
29: 128032: loss: 0.2226935446:
29: 131232: loss: 0.2229273711:
29: 134432: loss: 0.2229681578:
Dev-Acc: 29: Accuracy: 0.9374701977: precision: 0.4515535098: recall: 0.3336167318: f1: 0.3837277528
Train-Acc: 29: Accuracy: 0.9178153276: precision: 0.7974759615: recall: 0.3489579909: f1: 0.4854804043
30: 3232: loss: 0.2119038353:
30: 6432: loss: 0.2194455973:
30: 9632: loss: 0.2231347088:
30: 12832: loss: 0.2240958113:
30: 16032: loss: 0.2242503695:
30: 19232: loss: 0.2234661353:
30: 22432: loss: 0.2217817444:
30: 25632: loss: 0.2215156361:
30: 28832: loss: 0.2208981363:
30: 32032: loss: 0.2207232583:
30: 35232: loss: 0.2201801800:
30: 38432: loss: 0.2191834784:
30: 41632: loss: 0.2197641458:
30: 44832: loss: 0.2191615891:
30: 48032: loss: 0.2186529468:
30: 51232: loss: 0.2188699145:
30: 54432: loss: 0.2204816144:
30: 57632: loss: 0.2203702938:
30: 60832: loss: 0.2206834935:
30: 64032: loss: 0.2201363377:
30: 67232: loss: 0.2202060463:
30: 70432: loss: 0.2202978452:
30: 73632: loss: 0.2200886687:
30: 76832: loss: 0.2200012068:
30: 80032: loss: 0.2197356274:
30: 83232: loss: 0.2195188982:
30: 86432: loss: 0.2197144414:
30: 89632: loss: 0.2193550077:
30: 92832: loss: 0.2188626513:
30: 96032: loss: 0.2189236197:
30: 99232: loss: 0.2195793482:
30: 102432: loss: 0.2199632878:
30: 105632: loss: 0.2198718998:
30: 108832: loss: 0.2197092679:
30: 112032: loss: 0.2199265653:
30: 115232: loss: 0.2201838021:
30: 118432: loss: 0.2203273657:
30: 121632: loss: 0.2208717428:
30: 124832: loss: 0.2208248511:
30: 128032: loss: 0.2206139850:
30: 131232: loss: 0.2206965549:
30: 134432: loss: 0.2205679740:
Dev-Acc: 30: Accuracy: 0.9374007583: precision: 0.4520609319: recall: 0.3431389220: f1: 0.3901401643
Train-Acc: 30: Accuracy: 0.9188160896: precision: 0.7976173180: recall: 0.3609230162: f1: 0.4969675025
31: 3232: loss: 0.2246587920:
31: 6432: loss: 0.2217106117:
31: 9632: loss: 0.2195518933:
31: 12832: loss: 0.2203617775:
31: 16032: loss: 0.2213607015:
31: 19232: loss: 0.2216025671:
31: 22432: loss: 0.2209744956:
31: 25632: loss: 0.2219879314:
31: 28832: loss: 0.2224528121:
31: 32032: loss: 0.2219965326:
31: 35232: loss: 0.2211991221:
31: 38432: loss: 0.2225694395:
31: 41632: loss: 0.2216882250:
31: 44832: loss: 0.2215455511:
31: 48032: loss: 0.2213551426:
31: 51232: loss: 0.2213190722:
31: 54432: loss: 0.2219317453:
31: 57632: loss: 0.2219644149:
31: 60832: loss: 0.2230164721:
31: 64032: loss: 0.2233810521:
31: 67232: loss: 0.2231944564:
31: 70432: loss: 0.2226201846:
31: 73632: loss: 0.2222406789:
31: 76832: loss: 0.2223570266:
31: 80032: loss: 0.2222077090:
31: 83232: loss: 0.2219125361:
31: 86432: loss: 0.2213794537:
31: 89632: loss: 0.2209866180:
31: 92832: loss: 0.2206235257:
31: 96032: loss: 0.2206709886:
31: 99232: loss: 0.2208515788:
31: 102432: loss: 0.2203277001:
31: 105632: loss: 0.2201224976:
31: 108832: loss: 0.2201615592:
31: 112032: loss: 0.2199265282:
31: 115232: loss: 0.2197919001:
31: 118432: loss: 0.2197702007:
31: 121632: loss: 0.2195528149:
31: 124832: loss: 0.2192127869:
31: 128032: loss: 0.2191902589:
31: 131232: loss: 0.2195581276:
31: 134432: loss: 0.2190675852:
Dev-Acc: 31: Accuracy: 0.9367061853: precision: 0.4456331878: recall: 0.3470498215: f1: 0.3902112609
Train-Acc: 31: Accuracy: 0.9197218418: precision: 0.7994891443: recall: 0.3703898495: f1: 0.5062449456
32: 3232: loss: 0.2136610480:
32: 6432: loss: 0.2106399878:
32: 9632: loss: 0.2072519881:
32: 12832: loss: 0.2081172668:
32: 16032: loss: 0.2105452065:
32: 19232: loss: 0.2122565483:
32: 22432: loss: 0.2129156666:
32: 25632: loss: 0.2142737856:
32: 28832: loss: 0.2150645592:
32: 32032: loss: 0.2150186800:
32: 35232: loss: 0.2136622465:
32: 38432: loss: 0.2128492860:
32: 41632: loss: 0.2125455945:
32: 44832: loss: 0.2116162257:
32: 48032: loss: 0.2120568217:
32: 51232: loss: 0.2123952423:
32: 54432: loss: 0.2131486893:
32: 57632: loss: 0.2139575447:
32: 60832: loss: 0.2146031842:
32: 64032: loss: 0.2149630941:
32: 67232: loss: 0.2150640741:
32: 70432: loss: 0.2147682077:
32: 73632: loss: 0.2146668375:
32: 76832: loss: 0.2147318521:
32: 80032: loss: 0.2147137925:
32: 83232: loss: 0.2146601677:
32: 86432: loss: 0.2144776279:
32: 89632: loss: 0.2146397899:
32: 92832: loss: 0.2144217735:
32: 96032: loss: 0.2150224235:
32: 99232: loss: 0.2150903870:
32: 102432: loss: 0.2154618287:
32: 105632: loss: 0.2153795655:
32: 108832: loss: 0.2153681608:
32: 112032: loss: 0.2161348457:
32: 115232: loss: 0.2163580926:
32: 118432: loss: 0.2163778324:
32: 121632: loss: 0.2162714683:
32: 124832: loss: 0.2162499317:
32: 128032: loss: 0.2163375982:
32: 131232: loss: 0.2164640417:
32: 134432: loss: 0.2167321908:
Dev-Acc: 32: Accuracy: 0.9362696409: precision: 0.4422174840: recall: 0.3526611121: f1: 0.3923942863
Train-Acc: 32: Accuracy: 0.9202112556: precision: 0.7972823073: recall: 0.3780159095: f1: 0.5128662534
33: 3232: loss: 0.2147225253:
33: 6432: loss: 0.2207913079:
33: 9632: loss: 0.2171691755:
33: 12832: loss: 0.2170627029:
33: 16032: loss: 0.2182693375:
33: 19232: loss: 0.2179462237:
33: 22432: loss: 0.2181192953:
33: 25632: loss: 0.2184343712:
33: 28832: loss: 0.2170382302:
33: 32032: loss: 0.2178657808:
33: 35232: loss: 0.2179867732:
33: 38432: loss: 0.2173041174:
33: 41632: loss: 0.2175335396:
33: 44832: loss: 0.2174271552:
33: 48032: loss: 0.2176948122:
33: 51232: loss: 0.2178610337:
33: 54432: loss: 0.2176515011:
33: 57632: loss: 0.2173118883:
33: 60832: loss: 0.2169218904:
33: 64032: loss: 0.2169529889:
33: 67232: loss: 0.2168575348:
33: 70432: loss: 0.2165855794:
33: 73632: loss: 0.2168443894:
33: 76832: loss: 0.2167909317:
33: 80032: loss: 0.2167902787:
33: 83232: loss: 0.2169239009:
33: 86432: loss: 0.2163058254:
33: 89632: loss: 0.2159793044:
33: 92832: loss: 0.2157793279:
33: 96032: loss: 0.2158399961:
33: 99232: loss: 0.2156259454:
33: 102432: loss: 0.2157630504:
33: 105632: loss: 0.2157388858:
33: 108832: loss: 0.2157667335:
33: 112032: loss: 0.2155822184:
33: 115232: loss: 0.2157161309:
33: 118432: loss: 0.2153943097:
33: 121632: loss: 0.2149910995:
33: 124832: loss: 0.2146228042:
33: 128032: loss: 0.2149051502:
33: 131232: loss: 0.2146573970:
33: 134432: loss: 0.2146538844:
Dev-Acc: 33: Accuracy: 0.9359322786: precision: 0.4398496241: recall: 0.3581023635: f1: 0.3947886400
Train-Acc: 33: Accuracy: 0.9207518101: precision: 0.7972199509: recall: 0.3845900993: f1: 0.5188700164
34: 3232: loss: 0.2100080434:
34: 6432: loss: 0.2111283339:
34: 9632: loss: 0.2074692155:
34: 12832: loss: 0.2119918669:
34: 16032: loss: 0.2112599402:
34: 19232: loss: 0.2122272624:
34: 22432: loss: 0.2139336492:
34: 25632: loss: 0.2110124364:
34: 28832: loss: 0.2099760209:
34: 32032: loss: 0.2104350803:
34: 35232: loss: 0.2122231354:
34: 38432: loss: 0.2131621055:
34: 41632: loss: 0.2126146555:
34: 44832: loss: 0.2122709321:
34: 48032: loss: 0.2122269930:
34: 51232: loss: 0.2131225980:
34: 54432: loss: 0.2138465355:
34: 57632: loss: 0.2141712821:
34: 60832: loss: 0.2149126342:
34: 64032: loss: 0.2146780821:
34: 67232: loss: 0.2139545867:
34: 70432: loss: 0.2132660692:
34: 73632: loss: 0.2134101855:
34: 76832: loss: 0.2127684357:
34: 80032: loss: 0.2131074541:
34: 83232: loss: 0.2128849667:
34: 86432: loss: 0.2124714812:
34: 89632: loss: 0.2129027236:
34: 92832: loss: 0.2128675525:
34: 96032: loss: 0.2129681906:
34: 99232: loss: 0.2126854609:
34: 102432: loss: 0.2128209054:
34: 105632: loss: 0.2127588211:
34: 108832: loss: 0.2130029764:
34: 112032: loss: 0.2125900570:
34: 115232: loss: 0.2126978292:
34: 118432: loss: 0.2124775403:
34: 121632: loss: 0.2127202452:
34: 124832: loss: 0.2127565500:
34: 128032: loss: 0.2129462893:
34: 131232: loss: 0.2131052502:
34: 134432: loss: 0.2131183671:
Dev-Acc: 34: Accuracy: 0.9355750680: precision: 0.4369851730: recall: 0.3608229893: f1: 0.3952686970
Train-Acc: 34: Accuracy: 0.9212558270: precision: 0.7963879599: recall: 0.3913615147: f1: 0.5248170678
35: 3232: loss: 0.2186760454:
35: 6432: loss: 0.2132923397:
35: 9632: loss: 0.2104198110:
35: 12832: loss: 0.2083328646:
35: 16032: loss: 0.2067931580:
35: 19232: loss: 0.2084754772:
35: 22432: loss: 0.2083796706:
35: 25632: loss: 0.2102099972:
35: 28832: loss: 0.2109066385:
35: 32032: loss: 0.2098136669:
35: 35232: loss: 0.2095469438:
35: 38432: loss: 0.2103528101:
35: 41632: loss: 0.2109748576:
35: 44832: loss: 0.2115198843:
35: 48032: loss: 0.2113219904:
35: 51232: loss: 0.2117159910:
35: 54432: loss: 0.2124905533:
35: 57632: loss: 0.2127154044:
35: 60832: loss: 0.2121825815:
35: 64032: loss: 0.2121358391:
35: 67232: loss: 0.2121500173:
35: 70432: loss: 0.2124240150:
35: 73632: loss: 0.2121148106:
35: 76832: loss: 0.2118268293:
35: 80032: loss: 0.2119958282:
35: 83232: loss: 0.2122845009:
35: 86432: loss: 0.2120197622:
35: 89632: loss: 0.2121473963:
35: 92832: loss: 0.2118823196:
35: 96032: loss: 0.2117539255:
35: 99232: loss: 0.2118798113:
35: 102432: loss: 0.2121026226:
35: 105632: loss: 0.2122985425:
35: 108832: loss: 0.2126572172:
35: 112032: loss: 0.2122944571:
35: 115232: loss: 0.2120584905:
35: 118432: loss: 0.2121139290:
35: 121632: loss: 0.2120088718:
35: 124832: loss: 0.2118900854:
35: 128032: loss: 0.2116527154:
35: 131232: loss: 0.2115168555:
35: 134432: loss: 0.2112932945:
Dev-Acc: 35: Accuracy: 0.9352178574: precision: 0.4344660194: recall: 0.3652440061: f1: 0.3968591224
Train-Acc: 35: Accuracy: 0.9215188026: precision: 0.7957103138: recall: 0.3951088028: f1: 0.5280267088
36: 3232: loss: 0.2017091558:
36: 6432: loss: 0.2137028451:
36: 9632: loss: 0.2140774479:
36: 12832: loss: 0.2126293788:
36: 16032: loss: 0.2153191244:
36: 19232: loss: 0.2128115289:
36: 22432: loss: 0.2148519460:
36: 25632: loss: 0.2148168630:
36: 28832: loss: 0.2139431612:
36: 32032: loss: 0.2135833100:
36: 35232: loss: 0.2137156585:
36: 38432: loss: 0.2131580655:
36: 41632: loss: 0.2140115769:
36: 44832: loss: 0.2130794513:
36: 48032: loss: 0.2126469069:
36: 51232: loss: 0.2125211593:
36: 54432: loss: 0.2124225421:
36: 57632: loss: 0.2118067949:
36: 60832: loss: 0.2111367336:
36: 64032: loss: 0.2113165763:
36: 67232: loss: 0.2112375372:
36: 70432: loss: 0.2108516265:
36: 73632: loss: 0.2113114061:
36: 76832: loss: 0.2109332013:
36: 80032: loss: 0.2111082574:
36: 83232: loss: 0.2112164638:
36: 86432: loss: 0.2112322578:
36: 89632: loss: 0.2105855668:
36: 92832: loss: 0.2101519997:
36: 96032: loss: 0.2099370569:
36: 99232: loss: 0.2102937051:
36: 102432: loss: 0.2102528506:
36: 105632: loss: 0.2103677303:
36: 108832: loss: 0.2102671885:
36: 112032: loss: 0.2100064715:
36: 115232: loss: 0.2101351084:
36: 118432: loss: 0.2101621082:
36: 121632: loss: 0.2102440734:
36: 124832: loss: 0.2099321449:
36: 128032: loss: 0.2098717037:
36: 131232: loss: 0.2095915350:
36: 134432: loss: 0.2096793451:
Dev-Acc: 36: Accuracy: 0.9350491762: precision: 0.4337253339: recall: 0.3700051012: f1: 0.3993393283
Train-Acc: 36: Accuracy: 0.9219644070: precision: 0.7955613577: recall: 0.4006311222: f1: 0.5329019282
37: 3232: loss: 0.2096318029:
37: 6432: loss: 0.2068332081:
37: 9632: loss: 0.2054209798:
37: 12832: loss: 0.2037679005:
37: 16032: loss: 0.2044033305:
37: 19232: loss: 0.2072770565:
37: 22432: loss: 0.2077681717:
37: 25632: loss: 0.2069834977:
37: 28832: loss: 0.2082423930:
37: 32032: loss: 0.2083801043:
37: 35232: loss: 0.2084247390:
37: 38432: loss: 0.2084253571:
37: 41632: loss: 0.2079884951:
37: 44832: loss: 0.2078928447:
37: 48032: loss: 0.2090893298:
37: 51232: loss: 0.2088765471:
37: 54432: loss: 0.2094311358:
37: 57632: loss: 0.2095121854:
37: 60832: loss: 0.2092093104:
37: 64032: loss: 0.2087014607:
37: 67232: loss: 0.2087529169:
37: 70432: loss: 0.2087578852:
37: 73632: loss: 0.2086934668:
37: 76832: loss: 0.2083669891:
37: 80032: loss: 0.2086148056:
37: 83232: loss: 0.2084105878:
37: 86432: loss: 0.2082783059:
37: 89632: loss: 0.2076583153:
37: 92832: loss: 0.2074555088:
37: 96032: loss: 0.2073353173:
37: 99232: loss: 0.2069585159:
37: 102432: loss: 0.2067962385:
37: 105632: loss: 0.2068326885:
37: 108832: loss: 0.2071535591:
37: 112032: loss: 0.2073495975:
37: 115232: loss: 0.2071977646:
37: 118432: loss: 0.2076550532:
37: 121632: loss: 0.2079398835:
37: 124832: loss: 0.2079006922:
37: 128032: loss: 0.2077988294:
37: 131232: loss: 0.2081657620:
37: 134432: loss: 0.2081862773:
Dev-Acc: 37: Accuracy: 0.9346721768: precision: 0.4314146341: recall: 0.3759564700: f1: 0.4017808468
Train-Acc: 37: Accuracy: 0.9225122333: precision: 0.7966997551: recall: 0.4062849254: f1: 0.5381400209
38: 3232: loss: 0.2065719005:
38: 6432: loss: 0.2117328955:
38: 9632: loss: 0.2141122013:
38: 12832: loss: 0.2103878847:
38: 16032: loss: 0.2119048388:
38: 19232: loss: 0.2095467041:
38: 22432: loss: 0.2090756868:
38: 25632: loss: 0.2087077350:
38: 28832: loss: 0.2075983617:
38: 32032: loss: 0.2065535477:
38: 35232: loss: 0.2075456144:
38: 38432: loss: 0.2076987504:
38: 41632: loss: 0.2086146867:
38: 44832: loss: 0.2076654113:
38: 48032: loss: 0.2082144701:
38: 51232: loss: 0.2082503082:
38: 54432: loss: 0.2085734501:
38: 57632: loss: 0.2083341983:
38: 60832: loss: 0.2084974652:
38: 64032: loss: 0.2080219635:
38: 67232: loss: 0.2083062497:
38: 70432: loss: 0.2086576887:
38: 73632: loss: 0.2075948943:
38: 76832: loss: 0.2077476388:
38: 80032: loss: 0.2078024554:
38: 83232: loss: 0.2077265494:
38: 86432: loss: 0.2079193016:
38: 89632: loss: 0.2074865322:
38: 92832: loss: 0.2076420421:
38: 96032: loss: 0.2079160516:
38: 99232: loss: 0.2075472677:
38: 102432: loss: 0.2081012507:
38: 105632: loss: 0.2082844401:
38: 108832: loss: 0.2081693997:
38: 112032: loss: 0.2079938024:
38: 115232: loss: 0.2079760708:
38: 118432: loss: 0.2077313830:
38: 121632: loss: 0.2075657981:
38: 124832: loss: 0.2076299308:
38: 128032: loss: 0.2070147636:
38: 131232: loss: 0.2069871624:
38: 134432: loss: 0.2067027341:
Dev-Acc: 38: Accuracy: 0.9345233440: precision: 0.4309083911: recall: 0.3807175650: f1: 0.4042610815
Train-Acc: 38: Accuracy: 0.9229212999: precision: 0.7969407266: recall: 0.4110183420: f1: 0.5423317141
39: 3232: loss: 0.2019720681:
39: 6432: loss: 0.1966277113:
39: 9632: loss: 0.2022936508:
39: 12832: loss: 0.2011387137:
39: 16032: loss: 0.2016305780:
39: 19232: loss: 0.2022185246:
39: 22432: loss: 0.2020207850:
39: 25632: loss: 0.2023840999:
39: 28832: loss: 0.2025510176:
39: 32032: loss: 0.2019432722:
39: 35232: loss: 0.2029399025:
39: 38432: loss: 0.2036277809:
39: 41632: loss: 0.2043289860:
39: 44832: loss: 0.2048811413:
39: 48032: loss: 0.2033672035:
39: 51232: loss: 0.2034400649:
39: 54432: loss: 0.2029467930:
39: 57632: loss: 0.2032195518:
39: 60832: loss: 0.2037713850:
39: 64032: loss: 0.2036721673:
39: 67232: loss: 0.2039296049:
39: 70432: loss: 0.2036258279:
39: 73632: loss: 0.2039140344:
39: 76832: loss: 0.2044816321:
39: 80032: loss: 0.2044202943:
39: 83232: loss: 0.2050236255:
39: 86432: loss: 0.2045936272:
39: 89632: loss: 0.2044635200:
39: 92832: loss: 0.2039736453:
39: 96032: loss: 0.2040911032:
39: 99232: loss: 0.2038884928:
39: 102432: loss: 0.2038883531:
39: 105632: loss: 0.2039700680:
39: 108832: loss: 0.2041175083:
39: 112032: loss: 0.2041431457:
39: 115232: loss: 0.2043195218:
39: 118432: loss: 0.2044240929:
39: 121632: loss: 0.2044437917:
39: 124832: loss: 0.2047300812:
39: 128032: loss: 0.2048675105:
39: 131232: loss: 0.2052171857:
39: 134432: loss: 0.2054072994:
Dev-Acc: 39: Accuracy: 0.9342355728: precision: 0.4290058924: recall: 0.3837782690: f1: 0.4051337282
Train-Acc: 39: Accuracy: 0.9233961105: precision: 0.7972564812: recall: 0.4164749195: f1: 0.5471347757
40: 3232: loss: 0.2118481441:
40: 6432: loss: 0.2073329842:
40: 9632: loss: 0.2064239970:
40: 12832: loss: 0.2066150051:
40: 16032: loss: 0.2064917824:
40: 19232: loss: 0.2058041877:
40: 22432: loss: 0.2058209293:
40: 25632: loss: 0.2051662941:
40: 28832: loss: 0.2043493165:
40: 32032: loss: 0.2039141451:
40: 35232: loss: 0.2031316303:
40: 38432: loss: 0.2039692530:
40: 41632: loss: 0.2027110232:
40: 44832: loss: 0.2030418864:
40: 48032: loss: 0.2042273574:
40: 51232: loss: 0.2044234800:
40: 54432: loss: 0.2044778387:
40: 57632: loss: 0.2052516405:
40: 60832: loss: 0.2054120014:
40: 64032: loss: 0.2056289794:
40: 67232: loss: 0.2055144885:
40: 70432: loss: 0.2053176483:
40: 73632: loss: 0.2050679856:
40: 76832: loss: 0.2048433370:
40: 80032: loss: 0.2051268338:
40: 83232: loss: 0.2051068892:
40: 86432: loss: 0.2051113190:
40: 89632: loss: 0.2048401244:
40: 92832: loss: 0.2046710854:
40: 96032: loss: 0.2048177018:
40: 99232: loss: 0.2043726565:
40: 102432: loss: 0.2041501035:
40: 105632: loss: 0.2041906683:
40: 108832: loss: 0.2044437251:
40: 112032: loss: 0.2042932153:
40: 115232: loss: 0.2045147852:
40: 118432: loss: 0.2045489238:
40: 121632: loss: 0.2046298938:
40: 124832: loss: 0.2043689560:
40: 128032: loss: 0.2041574965:
40: 131232: loss: 0.2041480731:
40: 134432: loss: 0.2041361249:
Dev-Acc: 40: Accuracy: 0.9341462851: precision: 0.4291869614: recall: 0.3895595987: f1: 0.4084142972
Train-Acc: 40: Accuracy: 0.9237540364: precision: 0.7976799301: recall: 0.4204194333: f1: 0.5506285517
41: 3232: loss: 0.2220995017:
41: 6432: loss: 0.2100122175:
41: 9632: loss: 0.2055775391:
41: 12832: loss: 0.2059637995:
41: 16032: loss: 0.2062126651:
41: 19232: loss: 0.2054210807:
41: 22432: loss: 0.2056281863:
41: 25632: loss: 0.2067269888:
41: 28832: loss: 0.2054616160:
41: 32032: loss: 0.2057036353:
41: 35232: loss: 0.2045601870:
41: 38432: loss: 0.2032437249:
41: 41632: loss: 0.2042577532:
41: 44832: loss: 0.2048846170:
41: 48032: loss: 0.2047445467:
41: 51232: loss: 0.2037272166:
41: 54432: loss: 0.2035178496:
41: 57632: loss: 0.2032714073:
41: 60832: loss: 0.2031736979:
41: 64032: loss: 0.2032563173:
41: 67232: loss: 0.2034826661:
41: 70432: loss: 0.2037353463:
41: 73632: loss: 0.2041164216:
41: 76832: loss: 0.2048389927:
41: 80032: loss: 0.2042816845:
41: 83232: loss: 0.2040056279:
41: 86432: loss: 0.2037812612:
41: 89632: loss: 0.2038327484:
41: 92832: loss: 0.2035644634:
41: 96032: loss: 0.2034351727:
41: 99232: loss: 0.2034866897:
41: 102432: loss: 0.2033355502:
41: 105632: loss: 0.2030876672:
41: 108832: loss: 0.2029948202:
41: 112032: loss: 0.2032372422:
41: 115232: loss: 0.2032334154:
41: 118432: loss: 0.2030451427:
41: 121632: loss: 0.2027979690:
41: 124832: loss: 0.2027680337:
41: 128032: loss: 0.2024167182:
41: 131232: loss: 0.2028113328:
41: 134432: loss: 0.2028568435:
Dev-Acc: 41: Accuracy: 0.9339180589: precision: 0.4277767476: recall: 0.3922802245: f1: 0.4092602448
Train-Acc: 41: Accuracy: 0.9242069125: precision: 0.7979051140: recall: 0.4256787851: f1: 0.5551744834
42: 3232: loss: 0.1948450437:
42: 6432: loss: 0.1923248058:
42: 9632: loss: 0.2002795591:
42: 12832: loss: 0.2019358564:
42: 16032: loss: 0.2000928312:
42: 19232: loss: 0.2003707368:
42: 22432: loss: 0.2008009752:
42: 25632: loss: 0.2000759468:
42: 28832: loss: 0.2028725832:
42: 32032: loss: 0.2036427888:
42: 35232: loss: 0.2032252738:
42: 38432: loss: 0.2026457083:
42: 41632: loss: 0.2028078668:
42: 44832: loss: 0.2026942750:
42: 48032: loss: 0.2030542276:
42: 51232: loss: 0.2040001929:
42: 54432: loss: 0.2037505893:
42: 57632: loss: 0.2040462826:
42: 60832: loss: 0.2041537807:
42: 64032: loss: 0.2044684473:
42: 67232: loss: 0.2041853950:
42: 70432: loss: 0.2037234566:
42: 73632: loss: 0.2036088481:
42: 76832: loss: 0.2037218453:
42: 80032: loss: 0.2034823952:
42: 83232: loss: 0.2027935354:
42: 86432: loss: 0.2023536729:
42: 89632: loss: 0.2025755991:
42: 92832: loss: 0.2021050607:
42: 96032: loss: 0.2020381399:
42: 99232: loss: 0.2019004626:
42: 102432: loss: 0.2018184175:
42: 105632: loss: 0.2015987428:
42: 108832: loss: 0.2014465293:
42: 112032: loss: 0.2015262159:
42: 115232: loss: 0.2013636386:
42: 118432: loss: 0.2015156382:
42: 121632: loss: 0.2015811985:
42: 124832: loss: 0.2016845798:
42: 128032: loss: 0.2017309778:
42: 131232: loss: 0.2015781038:
42: 134432: loss: 0.2017249830:
Dev-Acc: 42: Accuracy: 0.9337791800: precision: 0.4272610530: recall: 0.3960210848: f1: 0.4110483586
Train-Acc: 42: Accuracy: 0.9245867729: precision: 0.7987529038: recall: 0.4294918151: f1: 0.5586147926
43: 3232: loss: 0.2081873593:
43: 6432: loss: 0.2122483615:
43: 9632: loss: 0.2102443572:
43: 12832: loss: 0.2061579446:
43: 16032: loss: 0.2051744090:
43: 19232: loss: 0.2040204571:
43: 22432: loss: 0.2015731930:
43: 25632: loss: 0.2023270206:
43: 28832: loss: 0.2025992373:
43: 32032: loss: 0.2024610385:
43: 35232: loss: 0.2022156858:
43: 38432: loss: 0.2022286119:
43: 41632: loss: 0.2012505281:
43: 44832: loss: 0.2018259017:
43: 48032: loss: 0.2011742636:
43: 51232: loss: 0.2009416679:
43: 54432: loss: 0.2004486236:
43: 57632: loss: 0.2002862543:
43: 60832: loss: 0.2006756055:
43: 64032: loss: 0.2002372401:
43: 67232: loss: 0.1999743071:
43: 70432: loss: 0.2003239675:
43: 73632: loss: 0.2000842156:
43: 76832: loss: 0.2005849216:
43: 80032: loss: 0.1999871677:
43: 83232: loss: 0.1997137337:
43: 86432: loss: 0.1996635946:
43: 89632: loss: 0.1998559227:
43: 92832: loss: 0.1993252642:
43: 96032: loss: 0.1993261483:
43: 99232: loss: 0.1995414770:
43: 102432: loss: 0.1999971002:
43: 105632: loss: 0.2003111551:
43: 108832: loss: 0.2002657981:
43: 112032: loss: 0.1997195719:
43: 115232: loss: 0.1998983862:
43: 118432: loss: 0.1999958742:
43: 121632: loss: 0.2001787514:
43: 124832: loss: 0.2002208569:
43: 128032: loss: 0.2002430010:
43: 131232: loss: 0.2002735446:
43: 134432: loss: 0.2002820869:
Dev-Acc: 43: Accuracy: 0.9335806966: precision: 0.4261312012: recall: 0.3987417106: f1: 0.4119817287
Train-Acc: 43: Accuracy: 0.9248350859: precision: 0.7990035241: recall: 0.4322529748: f1: 0.5610068259
44: 3232: loss: 0.1892024083:
44: 6432: loss: 0.1956920531:
44: 9632: loss: 0.1997805719:
44: 12832: loss: 0.2005750957:
44: 16032: loss: 0.2018293586:
44: 19232: loss: 0.2023180073:
44: 22432: loss: 0.2011258444:
44: 25632: loss: 0.2028356023:
44: 28832: loss: 0.2028543565:
44: 32032: loss: 0.2022709432:
44: 35232: loss: 0.2024156818:
44: 38432: loss: 0.2002660238:
44: 41632: loss: 0.1994767184:
44: 44832: loss: 0.1994886185:
44: 48032: loss: 0.2000541594:
44: 51232: loss: 0.1992768801:
44: 54432: loss: 0.1998482790:
44: 57632: loss: 0.1996034063:
44: 60832: loss: 0.1997845492:
44: 64032: loss: 0.1995810255:
44: 67232: loss: 0.1990873845:
44: 70432: loss: 0.1992330524:
44: 73632: loss: 0.1990916102:
44: 76832: loss: 0.1999824260:
44: 80032: loss: 0.1996218761:
44: 83232: loss: 0.1989739511:
44: 86432: loss: 0.1993063595:
44: 89632: loss: 0.1993636238:
44: 92832: loss: 0.1995514780:
44: 96032: loss: 0.1992232398:
44: 99232: loss: 0.1993186575:
44: 102432: loss: 0.1997932132:
44: 105632: loss: 0.1999228887:
44: 108832: loss: 0.1996792608:
44: 112032: loss: 0.1997346178:
44: 115232: loss: 0.1998480388:
44: 118432: loss: 0.1998960553:
44: 121632: loss: 0.1996881562:
44: 124832: loss: 0.1997358815:
44: 128032: loss: 0.1995034551:
44: 131232: loss: 0.1994839454:
44: 134432: loss: 0.1994057630:
Dev-Acc: 44: Accuracy: 0.9336005449: precision: 0.4271338724: recall: 0.4041829621: f1: 0.4153416041
Train-Acc: 44: Accuracy: 0.9251199961: precision: 0.7990834539: recall: 0.4356058116: f1: 0.5638429137
45: 3232: loss: 0.1867883100:
45: 6432: loss: 0.1913935146:
45: 9632: loss: 0.1961321798:
45: 12832: loss: 0.1998142107:
45: 16032: loss: 0.2006199393:
45: 19232: loss: 0.2013556036:
45: 22432: loss: 0.1994011135:
45: 25632: loss: 0.1994536035:
45: 28832: loss: 0.2005922160:
45: 32032: loss: 0.2004162890:
45: 35232: loss: 0.1993236096:
45: 38432: loss: 0.1993611508:
45: 41632: loss: 0.1993331668:
45: 44832: loss: 0.1983483448:
45: 48032: loss: 0.1978532184:
45: 51232: loss: 0.1975545092:
45: 54432: loss: 0.1976056438:
45: 57632: loss: 0.1981661679:
45: 60832: loss: 0.1982759273:
45: 64032: loss: 0.1984422867:
45: 67232: loss: 0.1990238940:
45: 70432: loss: 0.1992089663:
45: 73632: loss: 0.1992815207:
45: 76832: loss: 0.1988274763:
45: 80032: loss: 0.1996573907:
45: 83232: loss: 0.1994653203:
45: 86432: loss: 0.1993053391:
45: 89632: loss: 0.1995409071:
45: 92832: loss: 0.1992553463:
45: 96032: loss: 0.1990693529:
45: 99232: loss: 0.1992133665:
45: 102432: loss: 0.1993428779:
45: 105632: loss: 0.1996115433:
45: 108832: loss: 0.1995403927:
45: 112032: loss: 0.1992740350:
45: 115232: loss: 0.1988831267:
45: 118432: loss: 0.1988124739:
45: 121632: loss: 0.1989223867:
45: 124832: loss: 0.1989011599:
45: 128032: loss: 0.1988687909:
45: 131232: loss: 0.1986445404:
45: 134432: loss: 0.1983743781:
Dev-Acc: 45: Accuracy: 0.9334418178: precision: 0.4264627423: recall: 0.4077537834: f1: 0.4168984701
Train-Acc: 45: Accuracy: 0.9252952933: precision: 0.7987293215: recall: 0.4380382618: f1: 0.5657877977
46: 3232: loss: 0.2015713139:
46: 6432: loss: 0.2009974218:
46: 9632: loss: 0.1982060405:
46: 12832: loss: 0.2013412067:
46: 16032: loss: 0.1989560647:
46: 19232: loss: 0.1991971646:
46: 22432: loss: 0.1995492549:
46: 25632: loss: 0.1997133476:
46: 28832: loss: 0.1983890850:
46: 32032: loss: 0.1970732174:
46: 35232: loss: 0.1976755726:
46: 38432: loss: 0.1970572583:
46: 41632: loss: 0.1966412247:
46: 44832: loss: 0.1969286291:
46: 48032: loss: 0.1969404859:
46: 51232: loss: 0.1968883631:
46: 54432: loss: 0.1974826333:
46: 57632: loss: 0.1981168257:
46: 60832: loss: 0.1982733127:
46: 64032: loss: 0.1976929324:
46: 67232: loss: 0.1973773513:
46: 70432: loss: 0.1971579294:
46: 73632: loss: 0.1977264718:
46: 76832: loss: 0.1970355351:
46: 80032: loss: 0.1967314603:
46: 83232: loss: 0.1967722097:
46: 86432: loss: 0.1973582176:
46: 89632: loss: 0.1980084890:
46: 92832: loss: 0.1977153959:
46: 96032: loss: 0.1979555608:
46: 99232: loss: 0.1981935641:
46: 102432: loss: 0.1982110717:
46: 105632: loss: 0.1981063252:
46: 108832: loss: 0.1975172387:
46: 112032: loss: 0.1971843574:
46: 115232: loss: 0.1974743060:
46: 118432: loss: 0.1970638334:
46: 121632: loss: 0.1972001434:
46: 124832: loss: 0.1971602579:
46: 128032: loss: 0.1972148558:
46: 131232: loss: 0.1970525687:
46: 134432: loss: 0.1970199822:
Dev-Acc: 46: Accuracy: 0.9330449104: precision: 0.4238003164: recall: 0.4099642918: f1: 0.4167675022
Train-Acc: 46: Accuracy: 0.9255217314: precision: 0.7987608722: recall: 0.4407336796: f1: 0.5680393154
47: 3232: loss: 0.1795675121:
47: 6432: loss: 0.1875343446:
47: 9632: loss: 0.1918507610:
47: 12832: loss: 0.1921089469:
47: 16032: loss: 0.1925660748:
47: 19232: loss: 0.1934533678:
47: 22432: loss: 0.1933061997:
47: 25632: loss: 0.1928897608:
47: 28832: loss: 0.1942788129:
47: 32032: loss: 0.1932809508:
47: 35232: loss: 0.1933027088:
47: 38432: loss: 0.1921890713:
47: 41632: loss: 0.1927682623:
47: 44832: loss: 0.1924804839:
47: 48032: loss: 0.1930532379:
47: 51232: loss: 0.1935527519:
47: 54432: loss: 0.1932736742:
47: 57632: loss: 0.1937960244:
47: 60832: loss: 0.1941283199:
47: 64032: loss: 0.1945852492:
47: 67232: loss: 0.1942056635:
47: 70432: loss: 0.1953228906:
47: 73632: loss: 0.1954370631:
47: 76832: loss: 0.1959422366:
47: 80032: loss: 0.1957947423:
47: 83232: loss: 0.1953344944:
47: 86432: loss: 0.1957442681:
47: 89632: loss: 0.1958414412:
47: 92832: loss: 0.1957068187:
47: 96032: loss: 0.1957851686:
47: 99232: loss: 0.1954974095:
47: 102432: loss: 0.1953901981:
47: 105632: loss: 0.1951827985:
47: 108832: loss: 0.1955025422:
47: 112032: loss: 0.1955281850:
47: 115232: loss: 0.1959567423:
47: 118432: loss: 0.1957682892:
47: 121632: loss: 0.1963075065:
47: 124832: loss: 0.1963650104:
47: 128032: loss: 0.1961745277:
47: 131232: loss: 0.1960843549:
47: 134432: loss: 0.1959436878:
Dev-Acc: 47: Accuracy: 0.9330250621: precision: 0.4243426780: recall: 0.4143853086: f1: 0.4193048864
Train-Acc: 47: Accuracy: 0.9256824255: precision: 0.7983651226: recall: 0.4430346460: f1: 0.5698461018
48: 3232: loss: 0.1954350098:
48: 6432: loss: 0.1914874044:
48: 9632: loss: 0.1941995830:
48: 12832: loss: 0.1928006260:
48: 16032: loss: 0.1921001088:
48: 19232: loss: 0.1921190795:
48: 22432: loss: 0.1918075685:
48: 25632: loss: 0.1915336088:
48: 28832: loss: 0.1918517329:
48: 32032: loss: 0.1919063491:
48: 35232: loss: 0.1919684143:
48: 38432: loss: 0.1925083033:
48: 41632: loss: 0.1924131438:
48: 44832: loss: 0.1928663003:
48: 48032: loss: 0.1928767410:
48: 51232: loss: 0.1944582168:
48: 54432: loss: 0.1945224615:
48: 57632: loss: 0.1946333794:
48: 60832: loss: 0.1948828846:
48: 64032: loss: 0.1950726896:
48: 67232: loss: 0.1957027454:
48: 70432: loss: 0.1947287316:
48: 73632: loss: 0.1949459755:
48: 76832: loss: 0.1951364573:
48: 80032: loss: 0.1950085039:
48: 83232: loss: 0.1951520421:
48: 86432: loss: 0.1947013772:
48: 89632: loss: 0.1948618658:
48: 92832: loss: 0.1951179872:
48: 96032: loss: 0.1957104137:
48: 99232: loss: 0.1959130802:
48: 102432: loss: 0.1957528713:
48: 105632: loss: 0.1954850897:
48: 108832: loss: 0.1959559219:
48: 112032: loss: 0.1957395927:
48: 115232: loss: 0.1956829130:
48: 118432: loss: 0.1954067649:
48: 121632: loss: 0.1954285114:
48: 124832: loss: 0.1950470771:
48: 128032: loss: 0.1949147323:
48: 131232: loss: 0.1951011731:
48: 134432: loss: 0.1951821792:
Dev-Acc: 48: Accuracy: 0.9328960776: precision: 0.4238341969: recall: 0.4172759735: f1: 0.4205295176
Train-Acc: 48: Accuracy: 0.9260330796: precision: 0.7988715176: recall: 0.4467819341: f1: 0.5730668690
49: 3232: loss: 0.1891687087:
49: 6432: loss: 0.1912099263:
49: 9632: loss: 0.1969810339:
49: 12832: loss: 0.1962353132:
49: 16032: loss: 0.1961846910:
49: 19232: loss: 0.1948818996:
49: 22432: loss: 0.1948917289:
49: 25632: loss: 0.1947909532:
49: 28832: loss: 0.1945397700:
49: 32032: loss: 0.1943982088:
49: 35232: loss: 0.1937411845:
49: 38432: loss: 0.1944980619:
49: 41632: loss: 0.1928012467:
49: 44832: loss: 0.1927772982:
49: 48032: loss: 0.1936879092:
49: 51232: loss: 0.1937988932:
49: 54432: loss: 0.1936239677:
49: 57632: loss: 0.1936557830:
49: 60832: loss: 0.1932564729:
49: 64032: loss: 0.1933237232:
49: 67232: loss: 0.1936704519:
49: 70432: loss: 0.1933829180:
49: 73632: loss: 0.1941565188:
49: 76832: loss: 0.1939536033:
49: 80032: loss: 0.1941357336:
49: 83232: loss: 0.1944198855:
49: 86432: loss: 0.1937043216:
49: 89632: loss: 0.1936125850:
49: 92832: loss: 0.1939907854:
49: 96032: loss: 0.1938257512:
49: 99232: loss: 0.1939116640:
49: 102432: loss: 0.1943164096:
49: 105632: loss: 0.1945513927:
49: 108832: loss: 0.1944804853:
49: 112032: loss: 0.1948715230:
49: 115232: loss: 0.1949238562:
49: 118432: loss: 0.1946635294:
49: 121632: loss: 0.1944684576:
49: 124832: loss: 0.1947435353:
49: 128032: loss: 0.1945545965:
49: 131232: loss: 0.1944367247:
49: 134432: loss: 0.1943840520:
Dev-Acc: 49: Accuracy: 0.9326381087: precision: 0.4221269297: recall: 0.4184662472: f1: 0.4202886175
Train-Acc: 49: Accuracy: 0.9264786839: precision: 0.7997437092: recall: 0.4513181250: f1: 0.5770119773
50: 3232: loss: 0.1957834001:
50: 6432: loss: 0.1922337350:
50: 9632: loss: 0.1908088011:
50: 12832: loss: 0.1911214649:
50: 16032: loss: 0.1926074560:
50: 19232: loss: 0.1948128907:
50: 22432: loss: 0.1934836213:
50: 25632: loss: 0.1915250140:
50: 28832: loss: 0.1921423567:
50: 32032: loss: 0.1927235174:
50: 35232: loss: 0.1931262818:
50: 38432: loss: 0.1925745792:
50: 41632: loss: 0.1931882714:
50: 44832: loss: 0.1936692229:
50: 48032: loss: 0.1930412433:
50: 51232: loss: 0.1936637246:
50: 54432: loss: 0.1936389663:
50: 57632: loss: 0.1934572019:
50: 60832: loss: 0.1929486054:
50: 64032: loss: 0.1933163375:
50: 67232: loss: 0.1933032929:
50: 70432: loss: 0.1928131484:
50: 73632: loss: 0.1919072381:
50: 76832: loss: 0.1921283532:
50: 80032: loss: 0.1924723022:
50: 83232: loss: 0.1919656246:
50: 86432: loss: 0.1919647789:
50: 89632: loss: 0.1918718461:
50: 92832: loss: 0.1914473714:
50: 96032: loss: 0.1916609058:
50: 99232: loss: 0.1917022584:
50: 102432: loss: 0.1919765403:
50: 105632: loss: 0.1916750236:
50: 108832: loss: 0.1921391240:
50: 112032: loss: 0.1927080289:
50: 115232: loss: 0.1932119610:
50: 118432: loss: 0.1933060387:
50: 121632: loss: 0.1932074888:
50: 124832: loss: 0.1931919637:
50: 128032: loss: 0.1935706127:
50: 131232: loss: 0.1935165655:
50: 134432: loss: 0.1934735745:
Dev-Acc: 50: Accuracy: 0.9324991703: precision: 0.4214918256: recall: 0.4208467948: f1: 0.4211690632
Train-Acc: 50: Accuracy: 0.9267854691: precision: 0.8000231321: recall: 0.4547367037: f1: 0.5798717358
51: 3232: loss: 0.1993802976:
51: 6432: loss: 0.1982975131:
51: 9632: loss: 0.1989283656:
51: 12832: loss: 0.1967283123:
51: 16032: loss: 0.1961738583:
51: 19232: loss: 0.1967817247:
51: 22432: loss: 0.1979656207:
51: 25632: loss: 0.1970038187:
51: 28832: loss: 0.1986479382:
51: 32032: loss: 0.1965181983:
51: 35232: loss: 0.1965061592:
51: 38432: loss: 0.1949064259:
51: 41632: loss: 0.1949833373:
51: 44832: loss: 0.1944518139:
51: 48032: loss: 0.1945819832:
51: 51232: loss: 0.1939758464:
51: 54432: loss: 0.1930850945:
51: 57632: loss: 0.1936022791:
51: 60832: loss: 0.1924499189:
51: 64032: loss: 0.1916851601:
51: 67232: loss: 0.1917814845:
51: 70432: loss: 0.1914302728:
51: 73632: loss: 0.1918009327:
51: 76832: loss: 0.1919025367:
51: 80032: loss: 0.1919149571:
51: 83232: loss: 0.1913788330:
51: 86432: loss: 0.1912961481:
51: 89632: loss: 0.1912239538:
51: 92832: loss: 0.1914027266:
51: 96032: loss: 0.1919194690:
51: 99232: loss: 0.1916255879:
51: 102432: loss: 0.1914309455:
51: 105632: loss: 0.1916089958:
51: 108832: loss: 0.1919383163:
51: 112032: loss: 0.1920290326:
51: 115232: loss: 0.1924275952:
51: 118432: loss: 0.1928142063:
51: 121632: loss: 0.1925802859:
51: 124832: loss: 0.1925476451:
51: 128032: loss: 0.1925467712:
51: 131232: loss: 0.1926429708:
51: 134432: loss: 0.1927481651:
Dev-Acc: 51: Accuracy: 0.9321023226: precision: 0.4187774401: recall: 0.4216969903: f1: 0.4202321444
Train-Acc: 51: Accuracy: 0.9270119071: precision: 0.7994950075: recall: 0.4579580567: f1: 0.5823440896
52: 3232: loss: 0.1912111453:
52: 6432: loss: 0.1898956034:
52: 9632: loss: 0.1886244220:
52: 12832: loss: 0.1870121039:
52: 16032: loss: 0.1896922657:
52: 19232: loss: 0.1896877859:
52: 22432: loss: 0.1881051015:
52: 25632: loss: 0.1904114756:
52: 28832: loss: 0.1891744831:
52: 32032: loss: 0.1893528229:
52: 35232: loss: 0.1911884891:
52: 38432: loss: 0.1916100899:
52: 41632: loss: 0.1930137692:
52: 44832: loss: 0.1932598640:
52: 48032: loss: 0.1929998656:
52: 51232: loss: 0.1922928463:
52: 54432: loss: 0.1916669483:
52: 57632: loss: 0.1917846994:
52: 60832: loss: 0.1921106701:
52: 64032: loss: 0.1923613809:
52: 67232: loss: 0.1928141830:
52: 70432: loss: 0.1934784824:
52: 73632: loss: 0.1934474749:
52: 76832: loss: 0.1927035980:
52: 80032: loss: 0.1923505182:
52: 83232: loss: 0.1926682173:
52: 86432: loss: 0.1927798668:
52: 89632: loss: 0.1924300620:
52: 92832: loss: 0.1927533353:
52: 96032: loss: 0.1927098787:
52: 99232: loss: 0.1923800928:
52: 102432: loss: 0.1919724029:
52: 105632: loss: 0.1919655674:
52: 108832: loss: 0.1921708122:
52: 112032: loss: 0.1923486157:
52: 115232: loss: 0.1919553511:
52: 118432: loss: 0.1919385361:
52: 121632: loss: 0.1916922330:
52: 124832: loss: 0.1918358519:
52: 128032: loss: 0.1918199494:
52: 131232: loss: 0.1919902712:
52: 134432: loss: 0.1921543711:
Dev-Acc: 52: Accuracy: 0.9320130348: precision: 0.4181695601: recall: 0.4218670294: f1: 0.4200101574
Train-Acc: 52: Accuracy: 0.9272091389: precision: 0.7993608765: recall: 0.4604562488: f1: 0.5843240312
53: 3232: loss: 0.1821550620:
53: 6432: loss: 0.1940807944:
53: 9632: loss: 0.1921641710:
53: 12832: loss: 0.1941626700:
53: 16032: loss: 0.1925774225:
53: 19232: loss: 0.1914171689:
53: 22432: loss: 0.1913276367:
53: 25632: loss: 0.1914742319:
53: 28832: loss: 0.1919276791:
53: 32032: loss: 0.1918360079:
53: 35232: loss: 0.1915982453:
53: 38432: loss: 0.1920402159:
53: 41632: loss: 0.1933949598:
53: 44832: loss: 0.1929726401:
53: 48032: loss: 0.1922956211:
53: 51232: loss: 0.1917651642:
53: 54432: loss: 0.1924247695:
53: 57632: loss: 0.1927211091:
53: 60832: loss: 0.1927221271:
53: 64032: loss: 0.1920236812:
53: 67232: loss: 0.1930445005:
53: 70432: loss: 0.1932245947:
53: 73632: loss: 0.1930564293:
53: 76832: loss: 0.1926271581:
53: 80032: loss: 0.1928526445:
53: 83232: loss: 0.1925087672:
53: 86432: loss: 0.1922548546:
53: 89632: loss: 0.1924508754:
53: 92832: loss: 0.1925401056:
53: 96032: loss: 0.1925650676:
53: 99232: loss: 0.1926525438:
53: 102432: loss: 0.1925512786:
53: 105632: loss: 0.1924725438:
53: 108832: loss: 0.1921713325:
53: 112032: loss: 0.1921163796:
53: 115232: loss: 0.1917018558:
53: 118432: loss: 0.1915805483:
53: 121632: loss: 0.1914335007:
53: 124832: loss: 0.1915370536:
53: 128032: loss: 0.1914262148:
53: 131232: loss: 0.1914104864:
53: 134432: loss: 0.1913232329:
Dev-Acc: 53: Accuracy: 0.9320526719: precision: 0.4192686592: recall: 0.4269682027: f1: 0.4230834035
Train-Acc: 53: Accuracy: 0.9274209738: precision: 0.7992738001: recall: 0.4630859247: f1: 0.5864135864
54: 3232: loss: 0.1871269072:
54: 6432: loss: 0.1873223456:
54: 9632: loss: 0.1893327602:
54: 12832: loss: 0.1909890109:
54: 16032: loss: 0.1903052455:
54: 19232: loss: 0.1902710736:
54: 22432: loss: 0.1905629377:
54: 25632: loss: 0.1891440682:
54: 28832: loss: 0.1880968085:
54: 32032: loss: 0.1873341224:
54: 35232: loss: 0.1892247827:
54: 38432: loss: 0.1898096450:
54: 41632: loss: 0.1900839463:
54: 44832: loss: 0.1903051520:
54: 48032: loss: 0.1905649096:
54: 51232: loss: 0.1904277131:
54: 54432: loss: 0.1914842758:
54: 57632: loss: 0.1910672641:
54: 60832: loss: 0.1903484671:
54: 64032: loss: 0.1894650833:
54: 67232: loss: 0.1897148593:
54: 70432: loss: 0.1899643469:
54: 73632: loss: 0.1901313512:
54: 76832: loss: 0.1901519420:
54: 80032: loss: 0.1902977644:
54: 83232: loss: 0.1909523552:
54: 86432: loss: 0.1909378324:
54: 89632: loss: 0.1907966335:
54: 92832: loss: 0.1901410393:
54: 96032: loss: 0.1897682496:
54: 99232: loss: 0.1897810076:
54: 102432: loss: 0.1896913612:
54: 105632: loss: 0.1900533401:
54: 108832: loss: 0.1899934853:
54: 112032: loss: 0.1899147289:
54: 115232: loss: 0.1901812712:
54: 118432: loss: 0.1902466653:
54: 121632: loss: 0.1903530000:
54: 124832: loss: 0.1904706885:
54: 128032: loss: 0.1906506717:
54: 131232: loss: 0.1905098641:
54: 134432: loss: 0.1903237196:
Dev-Acc: 54: Accuracy: 0.9318939447: precision: 0.4182332391: recall: 0.4274783200: f1: 0.4228052472
Train-Acc: 54: Accuracy: 0.9275013208: precision: 0.7987115732: recall: 0.4645979883: f1: 0.5874724635
55: 3232: loss: 0.1854417073:
55: 6432: loss: 0.1817643672:
55: 9632: loss: 0.1877051218:
55: 12832: loss: 0.1900521208:
55: 16032: loss: 0.1919575206:
55: 19232: loss: 0.1899874735:
55: 22432: loss: 0.1903701869:
55: 25632: loss: 0.1910797868:
55: 28832: loss: 0.1911229063:
55: 32032: loss: 0.1906436177:
55: 35232: loss: 0.1912034691:
55: 38432: loss: 0.1906719087:
55: 41632: loss: 0.1899309362:
55: 44832: loss: 0.1890639370:
55: 48032: loss: 0.1896704326:
55: 51232: loss: 0.1888181631:
55: 54432: loss: 0.1887332627:
55: 57632: loss: 0.1889571996:
55: 60832: loss: 0.1892685381:
55: 64032: loss: 0.1891629340:
55: 67232: loss: 0.1893244583:
55: 70432: loss: 0.1901016326:
55: 73632: loss: 0.1903027910:
55: 76832: loss: 0.1901409546:
55: 80032: loss: 0.1894035017:
55: 83232: loss: 0.1896534105:
55: 86432: loss: 0.1892520482:
55: 89632: loss: 0.1892446960:
55: 92832: loss: 0.1891430138:
55: 96032: loss: 0.1892951250:
55: 99232: loss: 0.1891279252:
55: 102432: loss: 0.1892009882:
55: 105632: loss: 0.1891885064:
55: 108832: loss: 0.1893123026:
55: 112032: loss: 0.1891336148:
55: 115232: loss: 0.1892836025:
55: 118432: loss: 0.1893576622:
55: 121632: loss: 0.1895313241:
55: 124832: loss: 0.1897516697:
55: 128032: loss: 0.1896119713:
55: 131232: loss: 0.1896842579:
55: 134432: loss: 0.1896341979:
Dev-Acc: 55: Accuracy: 0.9318145514: precision: 0.4180314309: recall: 0.4296888284: f1: 0.4237799765
Train-Acc: 55: Accuracy: 0.9277204275: precision: 0.7981157470: recall: 0.4678193413: f1: 0.5898785593
56: 3232: loss: 0.2025535541:
56: 6432: loss: 0.1992707141:
56: 9632: loss: 0.1967416253:
56: 12832: loss: 0.1946914554:
56: 16032: loss: 0.1947247085:
56: 19232: loss: 0.1916491880:
56: 22432: loss: 0.1910240955:
56: 25632: loss: 0.1914240126:
56: 28832: loss: 0.1909400307:
56: 32032: loss: 0.1909874725:
56: 35232: loss: 0.1907901759:
56: 38432: loss: 0.1921412981:
56: 41632: loss: 0.1910420887:
56: 44832: loss: 0.1912528799:
56: 48032: loss: 0.1903697193:
56: 51232: loss: 0.1908427488:
56: 54432: loss: 0.1903204693:
56: 57632: loss: 0.1901783515:
56: 60832: loss: 0.1912168950:
56: 64032: loss: 0.1907456958:
56: 67232: loss: 0.1905194976:
56: 70432: loss: 0.1904601346:
56: 73632: loss: 0.1900040372:
56: 76832: loss: 0.1903999611:
56: 80032: loss: 0.1906045048:
56: 83232: loss: 0.1906056524:
56: 86432: loss: 0.1900649698:
56: 89632: loss: 0.1898439363:
56: 92832: loss: 0.1896739890:
56: 96032: loss: 0.1895673783:
56: 99232: loss: 0.1897098305:
56: 102432: loss: 0.1898540587:
56: 105632: loss: 0.1890282296:
56: 108832: loss: 0.1894017826:
56: 112032: loss: 0.1895707381:
56: 115232: loss: 0.1894323292:
56: 118432: loss: 0.1892872955:
56: 121632: loss: 0.1890287627:
56: 124832: loss: 0.1888952208:
56: 128032: loss: 0.1888893724:
56: 131232: loss: 0.1888692673:
56: 134432: loss: 0.1888983596:
Dev-Acc: 56: Accuracy: 0.9316558242: precision: 0.4170374032: recall: 0.4303689849: f1: 0.4235983264
Train-Acc: 56: Accuracy: 0.9278081059: precision: 0.7977201609: recall: 0.4692656630: f1: 0.5909184983
57: 3232: loss: 0.1741595809:
57: 6432: loss: 0.1806289311:
57: 9632: loss: 0.1845696304:
57: 12832: loss: 0.1868506432:
57: 16032: loss: 0.1852820117:
57: 19232: loss: 0.1850487350:
57: 22432: loss: 0.1853077233:
57: 25632: loss: 0.1854415475:
57: 28832: loss: 0.1862680606:
57: 32032: loss: 0.1858813454:
57: 35232: loss: 0.1862264491:
57: 38432: loss: 0.1863767678:
57: 41632: loss: 0.1852868327:
57: 44832: loss: 0.1854569658:
57: 48032: loss: 0.1859760797:
57: 51232: loss: 0.1865553002:
57: 54432: loss: 0.1870976996:
57: 57632: loss: 0.1866972196:
57: 60832: loss: 0.1868853398:
57: 64032: loss: 0.1862680376:
57: 67232: loss: 0.1862642865:
57: 70432: loss: 0.1853017429:
57: 73632: loss: 0.1852099025:
57: 76832: loss: 0.1855718591:
57: 80032: loss: 0.1856983295:
57: 83232: loss: 0.1861648713:
57: 86432: loss: 0.1862032790:
57: 89632: loss: 0.1861145767:
57: 92832: loss: 0.1863494768:
57: 96032: loss: 0.1866859615:
57: 99232: loss: 0.1869596993:
57: 102432: loss: 0.1874186315:
57: 105632: loss: 0.1873047231:
57: 108832: loss: 0.1871736260:
57: 112032: loss: 0.1873183627:
57: 115232: loss: 0.1876912019:
57: 118432: loss: 0.1880118814:
57: 121632: loss: 0.1876381715:
57: 124832: loss: 0.1878293756:
57: 128032: loss: 0.1877891889:
57: 131232: loss: 0.1878838063:
57: 134432: loss: 0.1879162832:
Dev-Acc: 57: Accuracy: 0.9314672947: precision: 0.4158188382: recall: 0.4308791022: f1: 0.4232150313
Train-Acc: 57: Accuracy: 0.9281294942: precision: 0.7981132075: recall: 0.4727499836: f1: 0.5937822551
58: 3232: loss: 0.1848650933:
58: 6432: loss: 0.1895311164:
58: 9632: loss: 0.1897859242:
58: 12832: loss: 0.1884943228:
58: 16032: loss: 0.1903196222:
58: 19232: loss: 0.1886599579:
58: 22432: loss: 0.1906957825:
58: 25632: loss: 0.1895704777:
58: 28832: loss: 0.1885774629:
58: 32032: loss: 0.1862621739:
58: 35232: loss: 0.1857344715:
58: 38432: loss: 0.1859690849:
58: 41632: loss: 0.1863980612:
58: 44832: loss: 0.1867634026:
58: 48032: loss: 0.1875595073:
58: 51232: loss: 0.1873002076:
58: 54432: loss: 0.1873418966:
58: 57632: loss: 0.1870937929:
58: 60832: loss: 0.1866048144:
58: 64032: loss: 0.1875130178:
58: 67232: loss: 0.1875170811:
58: 70432: loss: 0.1876792995:
58: 73632: loss: 0.1879082347:
58: 76832: loss: 0.1878097079:
58: 80032: loss: 0.1876765341:
58: 83232: loss: 0.1872510880:
58: 86432: loss: 0.1870254207:
58: 89632: loss: 0.1874306890:
58: 92832: loss: 0.1871324808:
58: 96032: loss: 0.1875208914:
58: 99232: loss: 0.1878060692:
58: 102432: loss: 0.1875060525:
58: 105632: loss: 0.1873859548:
58: 108832: loss: 0.1873671961:
58: 112032: loss: 0.1872982298:
58: 115232: loss: 0.1869235684:
58: 118432: loss: 0.1866855661:
58: 121632: loss: 0.1866417447:
58: 124832: loss: 0.1866050642:
58: 128032: loss: 0.1868594358:
58: 131232: loss: 0.1870008168:
58: 134432: loss: 0.1869925968:
Dev-Acc: 58: Accuracy: 0.9312787652: precision: 0.4145264191: recall: 0.4308791022: f1: 0.4225446056
Train-Acc: 58: Accuracy: 0.9283559322: precision: 0.7984753066: recall: 0.4751166919: f1: 0.5957464348
59: 3232: loss: 0.1963824969:
59: 6432: loss: 0.1929103137:
59: 9632: loss: 0.1920331673:
59: 12832: loss: 0.1875481668:
59: 16032: loss: 0.1861563334:
59: 19232: loss: 0.1859415263:
59: 22432: loss: 0.1879143457:
59: 25632: loss: 0.1870086711:
59: 28832: loss: 0.1855838767:
59: 32032: loss: 0.1853813021:
59: 35232: loss: 0.1850116648:
59: 38432: loss: 0.1859111975:
59: 41632: loss: 0.1857283613:
59: 44832: loss: 0.1853697025:
59: 48032: loss: 0.1846143762:
59: 51232: loss: 0.1852493273:
59: 54432: loss: 0.1849657556:
59: 57632: loss: 0.1846347049:
59: 60832: loss: 0.1845570160:
59: 64032: loss: 0.1844352451:
59: 67232: loss: 0.1841317678:
59: 70432: loss: 0.1843777232:
59: 73632: loss: 0.1850682979:
59: 76832: loss: 0.1852950774:
59: 80032: loss: 0.1855599222:
59: 83232: loss: 0.1855858267:
59: 86432: loss: 0.1856047783:
59: 89632: loss: 0.1857823305:
59: 92832: loss: 0.1856381093:
59: 96032: loss: 0.1854569765:
59: 99232: loss: 0.1855566611:
59: 102432: loss: 0.1855498098:
59: 105632: loss: 0.1859027523:
59: 108832: loss: 0.1861056719:
59: 112032: loss: 0.1857356952:
59: 115232: loss: 0.1859057840:
59: 118432: loss: 0.1861095255:
59: 121632: loss: 0.1863179121:
59: 124832: loss: 0.1864510210:
59: 128032: loss: 0.1867198557:
59: 131232: loss: 0.1865976436:
59: 134432: loss: 0.1864341384:
Dev-Acc: 59: Accuracy: 0.9311993718: precision: 0.4140969163: recall: 0.4315592586: f1: 0.4226477935
Train-Acc: 59: Accuracy: 0.9284362793: precision: 0.7980621009: recall: 0.4764972717: f1: 0.5967151031
60: 3232: loss: 0.2151643833:
60: 6432: loss: 0.1939288099:
60: 9632: loss: 0.1914147130:
60: 12832: loss: 0.1904960071:
60: 16032: loss: 0.1919654844:
60: 19232: loss: 0.1925396762:
60: 22432: loss: 0.1907410299:
60: 25632: loss: 0.1888088285:
60: 28832: loss: 0.1881873399:
60: 32032: loss: 0.1880038537:
60: 35232: loss: 0.1862372504:
60: 38432: loss: 0.1875313594:
60: 41632: loss: 0.1872589539:
60: 44832: loss: 0.1861916014:
60: 48032: loss: 0.1850509504:
60: 51232: loss: 0.1854610066:
60: 54432: loss: 0.1855323845:
60: 57632: loss: 0.1853354485:
60: 60832: loss: 0.1847211660:
60: 64032: loss: 0.1848334711:
60: 67232: loss: 0.1858509644:
60: 70432: loss: 0.1859058581:
60: 73632: loss: 0.1855192555:
60: 76832: loss: 0.1858331477:
60: 80032: loss: 0.1856554875:
60: 83232: loss: 0.1851941287:
60: 86432: loss: 0.1856230257:
60: 89632: loss: 0.1857527668:
60: 92832: loss: 0.1855874845:
60: 96032: loss: 0.1856412411:
60: 99232: loss: 0.1857515898:
60: 102432: loss: 0.1861560015:
60: 105632: loss: 0.1861673964:
60: 108832: loss: 0.1859727780:
60: 112032: loss: 0.1859695264:
60: 115232: loss: 0.1858808755:
60: 118432: loss: 0.1861182274:
60: 121632: loss: 0.1860643170:
60: 124832: loss: 0.1860337803:
60: 128032: loss: 0.1862357907:
60: 131232: loss: 0.1862252601:
60: 134432: loss: 0.1862200377:
Dev-Acc: 60: Accuracy: 0.9311993718: precision: 0.4144875751: recall: 0.4339398062: f1: 0.4239906961
Train-Acc: 60: Accuracy: 0.9286846519: precision: 0.7982264068: recall: 0.4793241733: f1: 0.5989730951
61: 3232: loss: 0.1754437527:
61: 6432: loss: 0.1843261532:
61: 9632: loss: 0.1801573710:
61: 12832: loss: 0.1804295815:
61: 16032: loss: 0.1817171806:
61: 19232: loss: 0.1824249425:
61: 22432: loss: 0.1816426560:
61: 25632: loss: 0.1816696877:
61: 28832: loss: 0.1829818406:
61: 32032: loss: 0.1816375814:
61: 35232: loss: 0.1824358699:
61: 38432: loss: 0.1832182360:
61: 41632: loss: 0.1842883182:
61: 44832: loss: 0.1853139045:
61: 48032: loss: 0.1851495155:
61: 51232: loss: 0.1848014051:
61: 54432: loss: 0.1853950117:
61: 57632: loss: 0.1858440479:
61: 60832: loss: 0.1856731501:
61: 64032: loss: 0.1854501189:
61: 67232: loss: 0.1857003024:
61: 70432: loss: 0.1855481897:
61: 73632: loss: 0.1855208433:
61: 76832: loss: 0.1854128911:
61: 80032: loss: 0.1844664340:
61: 83232: loss: 0.1846036535:
61: 86432: loss: 0.1844355885:
61: 89632: loss: 0.1843618754:
61: 92832: loss: 0.1846969765:
61: 96032: loss: 0.1846123347:
61: 99232: loss: 0.1849597674:
61: 102432: loss: 0.1849084101:
61: 105632: loss: 0.1852645502:
61: 108832: loss: 0.1848117985:
61: 112032: loss: 0.1849414882:
61: 115232: loss: 0.1855015240:
61: 118432: loss: 0.1857161638:
61: 121632: loss: 0.1854793766:
61: 124832: loss: 0.1854762735:
61: 128032: loss: 0.1850649680:
61: 131232: loss: 0.1850659857:
61: 134432: loss: 0.1852063860:
Dev-Acc: 61: Accuracy: 0.9311497808: precision: 0.4143735837: recall: 0.4353001190: f1: 0.4245791525
Train-Acc: 61: Accuracy: 0.9289110899: precision: 0.7983229881: recall: 0.4819538492: f1: 0.6010494384
62: 3232: loss: 0.1936238409:
62: 6432: loss: 0.1832919401:
62: 9632: loss: 0.1817034878:
62: 12832: loss: 0.1788966195:
62: 16032: loss: 0.1796622501:
62: 19232: loss: 0.1797426694:
62: 22432: loss: 0.1811865917:
62: 25632: loss: 0.1821867531:
62: 28832: loss: 0.1832098636:
62: 32032: loss: 0.1830175479:
62: 35232: loss: 0.1818647313:
62: 38432: loss: 0.1836881280:
62: 41632: loss: 0.1847014248:
62: 44832: loss: 0.1853610171:
62: 48032: loss: 0.1842917652:
62: 51232: loss: 0.1848827503:
62: 54432: loss: 0.1844323699:
62: 57632: loss: 0.1848998084:
62: 60832: loss: 0.1848504325:
62: 64032: loss: 0.1846756183:
62: 67232: loss: 0.1845789012:
62: 70432: loss: 0.1845472319:
62: 73632: loss: 0.1850355675:
62: 76832: loss: 0.1844713716:
62: 80032: loss: 0.1838591738:
62: 83232: loss: 0.1833112315:
62: 86432: loss: 0.1838898718:
62: 89632: loss: 0.1842562525:
62: 92832: loss: 0.1844458857:
62: 96032: loss: 0.1840890441:
62: 99232: loss: 0.1837662978:
62: 102432: loss: 0.1839748136:
62: 105632: loss: 0.1843222386:
62: 108832: loss: 0.1842995122:
62: 112032: loss: 0.1839951590:
62: 115232: loss: 0.1839245497:
62: 118432: loss: 0.1844177379:
62: 121632: loss: 0.1843308937:
62: 124832: loss: 0.1842056216:
62: 128032: loss: 0.1844363068:
62: 131232: loss: 0.1843614164:
62: 134432: loss: 0.1844000314:
Dev-Acc: 62: Accuracy: 0.9308719635: precision: 0.4126728852: recall: 0.4363203537: f1: 0.4241672866
Train-Acc: 62: Accuracy: 0.9290717840: precision: 0.7982218367: recall: 0.4839918480: f1: 0.6026029303
63: 3232: loss: 0.1896775907:
63: 6432: loss: 0.1874431428:
63: 9632: loss: 0.1874534244:
63: 12832: loss: 0.1857133333:
63: 16032: loss: 0.1869829145:
63: 19232: loss: 0.1850408374:
63: 22432: loss: 0.1839683297:
63: 25632: loss: 0.1835057662:
63: 28832: loss: 0.1825910122:
63: 32032: loss: 0.1824429994:
63: 35232: loss: 0.1839369933:
63: 38432: loss: 0.1837066009:
63: 41632: loss: 0.1842524107:
63: 44832: loss: 0.1838731635:
63: 48032: loss: 0.1842383046:
63: 51232: loss: 0.1848509521:
63: 54432: loss: 0.1853704815:
63: 57632: loss: 0.1850698855:
63: 60832: loss: 0.1850340316:
63: 64032: loss: 0.1850833145:
63: 67232: loss: 0.1847657981:
63: 70432: loss: 0.1850019489:
63: 73632: loss: 0.1852562202:
63: 76832: loss: 0.1851711007:
63: 80032: loss: 0.1853446185:
63: 83232: loss: 0.1848769285:
63: 86432: loss: 0.1850854766:
63: 89632: loss: 0.1847881670:
63: 92832: loss: 0.1847438059:
63: 96032: loss: 0.1846420046:
63: 99232: loss: 0.1846640040:
63: 102432: loss: 0.1849577510:
63: 105632: loss: 0.1851172156:
63: 108832: loss: 0.1849064305:
63: 112032: loss: 0.1846899425:
63: 115232: loss: 0.1846611343:
63: 118432: loss: 0.1845599257:
63: 121632: loss: 0.1846779691:
63: 124832: loss: 0.1845692783:
63: 128032: loss: 0.1845257153:
63: 131232: loss: 0.1844050446:
63: 134432: loss: 0.1842056312:
Dev-Acc: 63: Accuracy: 0.9307131767: precision: 0.4117552851: recall: 0.4371705492: f1: 0.4240824742
Train-Acc: 63: Accuracy: 0.9293420911: precision: 0.7989635068: recall: 0.4864900401: f1: 0.6047480897
64: 3232: loss: 0.1790928738:
64: 6432: loss: 0.1747104029:
64: 9632: loss: 0.1758044531:
64: 12832: loss: 0.1789948648:
64: 16032: loss: 0.1832247977:
64: 19232: loss: 0.1815109611:
64: 22432: loss: 0.1811691409:
64: 25632: loss: 0.1809885777:
64: 28832: loss: 0.1813302590:
64: 32032: loss: 0.1819661597:
64: 35232: loss: 0.1815628493:
64: 38432: loss: 0.1814817341:
64: 41632: loss: 0.1828798956:
64: 44832: loss: 0.1842940615:
64: 48032: loss: 0.1837404401:
64: 51232: loss: 0.1839400540:
64: 54432: loss: 0.1844894857:
64: 57632: loss: 0.1836619103:
64: 60832: loss: 0.1838613065:
64: 64032: loss: 0.1843843392:
64: 67232: loss: 0.1845348485:
64: 70432: loss: 0.1850845445:
64: 73632: loss: 0.1850578998:
64: 76832: loss: 0.1849354822:
64: 80032: loss: 0.1847593671:
64: 83232: loss: 0.1849850643:
64: 86432: loss: 0.1844246492:
64: 89632: loss: 0.1845658888:
64: 92832: loss: 0.1842845437:
64: 96032: loss: 0.1839908542:
64: 99232: loss: 0.1838866211:
64: 102432: loss: 0.1837302759:
64: 105632: loss: 0.1838014667:
64: 108832: loss: 0.1837779173:
64: 112032: loss: 0.1838582743:
64: 115232: loss: 0.1836299816:
64: 118432: loss: 0.1836567170:
64: 121632: loss: 0.1837222887:
64: 124832: loss: 0.1833237863:
64: 128032: loss: 0.1830903090:
64: 131232: loss: 0.1830756791:
64: 134432: loss: 0.1830187869:
Dev-Acc: 64: Accuracy: 0.9305941463: precision: 0.4111359285: recall: 0.4381907839: f1: 0.4242324471
Train-Acc: 64: Accuracy: 0.9294808507: precision: 0.7983463975: recall: 0.4887910065: f1: 0.6063448051
65: 3232: loss: 0.1740279285:
65: 6432: loss: 0.1732715403:
65: 9632: loss: 0.1740567286:
65: 12832: loss: 0.1764193629:
65: 16032: loss: 0.1769124864:
65: 19232: loss: 0.1791995563:
65: 22432: loss: 0.1800272892:
65: 25632: loss: 0.1812064725:
65: 28832: loss: 0.1810057287:
65: 32032: loss: 0.1819862389:
65: 35232: loss: 0.1814951123:
65: 38432: loss: 0.1806777580:
65: 41632: loss: 0.1804346839:
65: 44832: loss: 0.1802009962:
65: 48032: loss: 0.1806229459:
65: 51232: loss: 0.1805477698:
65: 54432: loss: 0.1799870466:
65: 57632: loss: 0.1799782197:
65: 60832: loss: 0.1805782113:
65: 64032: loss: 0.1809349428:
65: 67232: loss: 0.1814062891:
65: 70432: loss: 0.1807278936:
65: 73632: loss: 0.1810449157:
65: 76832: loss: 0.1809990189:
65: 80032: loss: 0.1815215365:
65: 83232: loss: 0.1815156643:
65: 86432: loss: 0.1818442698:
65: 89632: loss: 0.1816342731:
65: 92832: loss: 0.1812647208:
65: 96032: loss: 0.1817009275:
65: 99232: loss: 0.1815508741:
65: 102432: loss: 0.1814834776:
65: 105632: loss: 0.1818432561:
65: 108832: loss: 0.1818365465:
65: 112032: loss: 0.1821633177:
65: 115232: loss: 0.1821985294:
65: 118432: loss: 0.1824727843:
65: 121632: loss: 0.1824929337:
65: 124832: loss: 0.1825281015:
65: 128032: loss: 0.1826776851:
65: 131232: loss: 0.1829144640:
65: 134432: loss: 0.1824628306:
Dev-Acc: 65: Accuracy: 0.9304552078: precision: 0.4103908484: recall: 0.4392110185: f1: 0.4243121150
Train-Acc: 65: Accuracy: 0.9296708107: precision: 0.7985242220: recall: 0.4908947472: f1: 0.6080123768
66: 3232: loss: 0.1736834722:
66: 6432: loss: 0.1802069908:
66: 9632: loss: 0.1831017316:
66: 12832: loss: 0.1830072805:
66: 16032: loss: 0.1822344513:
66: 19232: loss: 0.1856524266:
66: 22432: loss: 0.1856541712:
66: 25632: loss: 0.1834497617:
66: 28832: loss: 0.1846840218:
66: 32032: loss: 0.1866071440:
66: 35232: loss: 0.1860702711:
66: 38432: loss: 0.1859591644:
66: 41632: loss: 0.1850385766:
66: 44832: loss: 0.1844760431:
66: 48032: loss: 0.1843061750:
66: 51232: loss: 0.1848695499:
66: 54432: loss: 0.1854546060:
66: 57632: loss: 0.1848764940:
66: 60832: loss: 0.1846797129:
66: 64032: loss: 0.1852694518:
66: 67232: loss: 0.1849441441:
66: 70432: loss: 0.1851349494:
66: 73632: loss: 0.1848155556:
66: 76832: loss: 0.1840877280:
66: 80032: loss: 0.1837746186:
66: 83232: loss: 0.1831472641:
66: 86432: loss: 0.1829900150:
66: 89632: loss: 0.1827334263:
66: 92832: loss: 0.1824065100:
66: 96032: loss: 0.1820874708:
66: 99232: loss: 0.1819519393:
66: 102432: loss: 0.1817665275:
66: 105632: loss: 0.1818619377:
66: 108832: loss: 0.1818215398:
66: 112032: loss: 0.1815638475:
66: 115232: loss: 0.1819318336:
66: 118432: loss: 0.1821454875:
66: 121632: loss: 0.1821227478:
66: 124832: loss: 0.1820769988:
66: 128032: loss: 0.1819480829:
66: 131232: loss: 0.1816037691:
66: 134432: loss: 0.1817083061:
Dev-Acc: 66: Accuracy: 0.9302269816: precision: 0.4089254629: recall: 0.4393810576: f1: 0.4236065574
Train-Acc: 66: Accuracy: 0.9298753142: precision: 0.7991257064: recall: 0.4927355203: f1: 0.6095973973
67: 3232: loss: 0.1796584700:
67: 6432: loss: 0.1787285673:
67: 9632: loss: 0.1811215019:
67: 12832: loss: 0.1770488394:
67: 16032: loss: 0.1783563384:
67: 19232: loss: 0.1786140752:
67: 22432: loss: 0.1785023438:
67: 25632: loss: 0.1781963493:
67: 28832: loss: 0.1795257507:
67: 32032: loss: 0.1792917136:
67: 35232: loss: 0.1797084949:
67: 38432: loss: 0.1788726520:
67: 41632: loss: 0.1785703023:
67: 44832: loss: 0.1785368478:
67: 48032: loss: 0.1788762149:
67: 51232: loss: 0.1800278180:
67: 54432: loss: 0.1800599805:
67: 57632: loss: 0.1800792563:
67: 60832: loss: 0.1808329203:
67: 64032: loss: 0.1808580632:
67: 67232: loss: 0.1803420084:
67: 70432: loss: 0.1805541179:
67: 73632: loss: 0.1801388640:
67: 76832: loss: 0.1799878286:
67: 80032: loss: 0.1797415939:
67: 83232: loss: 0.1796392798:
67: 86432: loss: 0.1798770044:
67: 89632: loss: 0.1804514066:
67: 92832: loss: 0.1807388730:
67: 96032: loss: 0.1808021695:
67: 99232: loss: 0.1807239652:
67: 102432: loss: 0.1807957459:
67: 105632: loss: 0.1807613286:
67: 108832: loss: 0.1810985489:
67: 112032: loss: 0.1814200420:
67: 115232: loss: 0.1813414056:
67: 118432: loss: 0.1815706010:
67: 121632: loss: 0.1816738130:
67: 124832: loss: 0.1819964817:
67: 128032: loss: 0.1815169512:
67: 131232: loss: 0.1813804644:
67: 134432: loss: 0.1813504696:
Dev-Acc: 67: Accuracy: 0.9299591184: precision: 0.4071856287: recall: 0.4393810576: f1: 0.4226711376
Train-Acc: 67: Accuracy: 0.9300433397: precision: 0.7996171027: recall: 0.4942475840: f1: 0.6108966806
68: 3232: loss: 0.1754117068:
68: 6432: loss: 0.1743718407:
68: 9632: loss: 0.1795254235:
68: 12832: loss: 0.1809831589:
68: 16032: loss: 0.1820279820:
68: 19232: loss: 0.1819012018:
68: 22432: loss: 0.1803257353:
68: 25632: loss: 0.1796865297:
68: 28832: loss: 0.1779021539:
68: 32032: loss: 0.1797310341:
68: 35232: loss: 0.1798546751:
68: 38432: loss: 0.1794735415:
68: 41632: loss: 0.1799299081:
68: 44832: loss: 0.1806174207:
68: 48032: loss: 0.1811135643:
68: 51232: loss: 0.1815163071:
68: 54432: loss: 0.1805983112:
68: 57632: loss: 0.1799429780:
68: 60832: loss: 0.1803102780:
68: 64032: loss: 0.1807767007:
68: 67232: loss: 0.1808682621:
68: 70432: loss: 0.1804583037:
68: 73632: loss: 0.1801733211:
68: 76832: loss: 0.1807154651:
68: 80032: loss: 0.1810218186:
68: 83232: loss: 0.1806983207:
68: 86432: loss: 0.1813433573:
68: 89632: loss: 0.1811532215:
68: 92832: loss: 0.1811776080:
68: 96032: loss: 0.1811171555:
68: 99232: loss: 0.1811249279:
68: 102432: loss: 0.1812848364:
68: 105632: loss: 0.1810971939:
68: 108832: loss: 0.1811376302:
68: 112032: loss: 0.1814869352:
68: 115232: loss: 0.1813735757:
68: 118432: loss: 0.1815025250:
68: 121632: loss: 0.1811769013:
68: 124832: loss: 0.1805147487:
68: 128032: loss: 0.1809826959:
68: 131232: loss: 0.1808783156:
68: 134432: loss: 0.1808088442:
Dev-Acc: 68: Accuracy: 0.9298102856: precision: 0.4061959428: recall: 0.4392110185: f1: 0.4220588235
Train-Acc: 68: Accuracy: 0.9301820993: precision: 0.7997666773: recall: 0.4957596476: f1: 0.6120941558
69: 3232: loss: 0.1817632607:
69: 6432: loss: 0.1758150593:
69: 9632: loss: 0.1775495813:
69: 12832: loss: 0.1779882321:
69: 16032: loss: 0.1755897272:
69: 19232: loss: 0.1760307795:
69: 22432: loss: 0.1756262104:
69: 25632: loss: 0.1760071671:
69: 28832: loss: 0.1765128827:
69: 32032: loss: 0.1760989830:
69: 35232: loss: 0.1773042913:
69: 38432: loss: 0.1779782074:
69: 41632: loss: 0.1783372666:
69: 44832: loss: 0.1787841360:
69: 48032: loss: 0.1790282878:
69: 51232: loss: 0.1793896023:
69: 54432: loss: 0.1797677273:
69: 57632: loss: 0.1800250525:
69: 60832: loss: 0.1797778672:
69: 64032: loss: 0.1802172635:
69: 67232: loss: 0.1802724928:
69: 70432: loss: 0.1796161202:
69: 73632: loss: 0.1790701570:
69: 76832: loss: 0.1796537998:
69: 80032: loss: 0.1799177252:
69: 83232: loss: 0.1799612137:
69: 86432: loss: 0.1803581490:
69: 89632: loss: 0.1804107151:
69: 92832: loss: 0.1801794963:
69: 96032: loss: 0.1797044813:
69: 99232: loss: 0.1797683685:
69: 102432: loss: 0.1795969226:
69: 105632: loss: 0.1795480627:
69: 108832: loss: 0.1794454213:
69: 112032: loss: 0.1800368681:
69: 115232: loss: 0.1801925822:
69: 118432: loss: 0.1799863076:
69: 121632: loss: 0.1797781919:
69: 124832: loss: 0.1796063597:
69: 128032: loss: 0.1797596888:
69: 131232: loss: 0.1801390527:
69: 134432: loss: 0.1799082357:
Dev-Acc: 69: Accuracy: 0.9295622110: precision: 0.4046351394: recall: 0.4393810576: f1: 0.4212928996
Train-Acc: 69: Accuracy: 0.9303281903: precision: 0.7996197317: recall: 0.4976661626: f1: 0.6135019045
70: 3232: loss: 0.1841513100:
70: 6432: loss: 0.1786959965:
70: 9632: loss: 0.1830749692:
70: 12832: loss: 0.1756646733:
70: 16032: loss: 0.1771296536:
70: 19232: loss: 0.1780695322:
70: 22432: loss: 0.1796399671:
70: 25632: loss: 0.1798523770:
70: 28832: loss: 0.1804343971:
70: 32032: loss: 0.1809761634:
70: 35232: loss: 0.1818758608:
70: 38432: loss: 0.1820558031:
70: 41632: loss: 0.1818641744:
70: 44832: loss: 0.1804976260:
70: 48032: loss: 0.1804747113:
70: 51232: loss: 0.1813183170:
70: 54432: loss: 0.1808303144:
70: 57632: loss: 0.1804688220:
70: 60832: loss: 0.1806636122:
70: 64032: loss: 0.1806942867:
70: 67232: loss: 0.1803277271:
70: 70432: loss: 0.1806057116:
70: 73632: loss: 0.1801390971:
70: 76832: loss: 0.1802821947:
70: 80032: loss: 0.1806180584:
70: 83232: loss: 0.1802834789:
70: 86432: loss: 0.1804185790:
70: 89632: loss: 0.1796165257:
70: 92832: loss: 0.1796590204:
70: 96032: loss: 0.1794684369:
70: 99232: loss: 0.1790274311:
70: 102432: loss: 0.1795032344:
70: 105632: loss: 0.1797141765:
70: 108832: loss: 0.1793591990:
70: 112032: loss: 0.1794314836:
70: 115232: loss: 0.1796629169:
70: 118432: loss: 0.1802247195:
70: 121632: loss: 0.1801750641:
70: 124832: loss: 0.1801487275:
70: 128032: loss: 0.1802253418:
70: 131232: loss: 0.1800849432:
70: 134432: loss: 0.1796428020:
Dev-Acc: 70: Accuracy: 0.9293141365: precision: 0.4032383621: recall: 0.4404012923: f1: 0.4210013004
Train-Acc: 70: Accuracy: 0.9305327535: precision: 0.7999579080: recall: 0.4997699034: f1: 0.6151978636
71: 3232: loss: 0.1784168213:
71: 6432: loss: 0.1865209464:
71: 9632: loss: 0.1904527087:
71: 12832: loss: 0.1866187244:
71: 16032: loss: 0.1866188277:
71: 19232: loss: 0.1853599247:
71: 22432: loss: 0.1832423279:
71: 25632: loss: 0.1820589551:
71: 28832: loss: 0.1808196728:
71: 32032: loss: 0.1815465460:
71: 35232: loss: 0.1817238089:
71: 38432: loss: 0.1817337605:
71: 41632: loss: 0.1823028383:
71: 44832: loss: 0.1813341885:
71: 48032: loss: 0.1817403900:
71: 51232: loss: 0.1820534944:
71: 54432: loss: 0.1813240215:
71: 57632: loss: 0.1806305505:
71: 60832: loss: 0.1806718681:
71: 64032: loss: 0.1803696710:
71: 67232: loss: 0.1802414535:
71: 70432: loss: 0.1803666860:
71: 73632: loss: 0.1802683911:
71: 76832: loss: 0.1797462973:
71: 80032: loss: 0.1804753236:
71: 83232: loss: 0.1800319263:
71: 86432: loss: 0.1798662321:
71: 89632: loss: 0.1803849620:
71: 92832: loss: 0.1805167766:
71: 96032: loss: 0.1799426766:
71: 99232: loss: 0.1796475321:
71: 102432: loss: 0.1795475727:
71: 105632: loss: 0.1792396244:
71: 108832: loss: 0.1795208337:
71: 112032: loss: 0.1801327048:
71: 115232: loss: 0.1800775150:
71: 118432: loss: 0.1801696660:
71: 121632: loss: 0.1799323208:
71: 124832: loss: 0.1797104555:
71: 128032: loss: 0.1794430320:
71: 131232: loss: 0.1793163611:
71: 134432: loss: 0.1789518553:
Dev-Acc: 71: Accuracy: 0.9292050004: precision: 0.4025792418: recall: 0.4405713314: f1: 0.4207193310
Train-Acc: 71: Accuracy: 0.9308322072: precision: 0.8010696309: recall: 0.5022023536: f1: 0.6173677617
72: 3232: loss: 0.1828333410:
72: 6432: loss: 0.1722363505:
72: 9632: loss: 0.1759239973:
72: 12832: loss: 0.1742502491:
72: 16032: loss: 0.1791293180:
72: 19232: loss: 0.1802285957:
72: 22432: loss: 0.1798366330:
72: 25632: loss: 0.1784850313:
72: 28832: loss: 0.1809910461:
72: 32032: loss: 0.1801374355:
72: 35232: loss: 0.1791029011:
72: 38432: loss: 0.1794464631:
72: 41632: loss: 0.1796952974:
72: 44832: loss: 0.1790177603:
72: 48032: loss: 0.1779899835:
72: 51232: loss: 0.1783257079:
72: 54432: loss: 0.1780093462:
72: 57632: loss: 0.1783800703:
72: 60832: loss: 0.1780989739:
72: 64032: loss: 0.1778485919:
72: 67232: loss: 0.1776935390:
72: 70432: loss: 0.1773618685:
72: 73632: loss: 0.1775573110:
72: 76832: loss: 0.1779841838:
72: 80032: loss: 0.1784634495:
72: 83232: loss: 0.1785138374:
72: 86432: loss: 0.1784021875:
72: 89632: loss: 0.1782757035:
72: 92832: loss: 0.1786245870:
72: 96032: loss: 0.1783574379:
72: 99232: loss: 0.1777122058:
72: 102432: loss: 0.1778520580:
72: 105632: loss: 0.1786902480:
72: 108832: loss: 0.1784032364:
72: 112032: loss: 0.1786754084:
72: 115232: loss: 0.1788512731:
72: 118432: loss: 0.1788666258:
72: 121632: loss: 0.1788484159:
72: 124832: loss: 0.1787280025:
72: 128032: loss: 0.1786079529:
72: 131232: loss: 0.1783453815:
72: 134432: loss: 0.1784908805:
Dev-Acc: 72: Accuracy: 0.9290859699: precision: 0.4018300248: recall: 0.4405713314: f1: 0.4203098386
Train-Acc: 72: Accuracy: 0.9310367703: precision: 0.8013999164: recall: 0.5043060943: f1: 0.6190533834
73: 3232: loss: 0.1909156910:
73: 6432: loss: 0.1811210532:
73: 9632: loss: 0.1840745070:
73: 12832: loss: 0.1785977376:
73: 16032: loss: 0.1805543412:
73: 19232: loss: 0.1792844288:
73: 22432: loss: 0.1799183639:
73: 25632: loss: 0.1780120597:
73: 28832: loss: 0.1786790782:
73: 32032: loss: 0.1771166836:
73: 35232: loss: 0.1772981229:
73: 38432: loss: 0.1775101841:
73: 41632: loss: 0.1762587181:
73: 44832: loss: 0.1757486729:
73: 48032: loss: 0.1757515000:
73: 51232: loss: 0.1761059071:
73: 54432: loss: 0.1761815217:
73: 57632: loss: 0.1763155517:
73: 60832: loss: 0.1762515858:
73: 64032: loss: 0.1765778690:
73: 67232: loss: 0.1757438521:
73: 70432: loss: 0.1758462617:
73: 73632: loss: 0.1757936079:
73: 76832: loss: 0.1763544945:
73: 80032: loss: 0.1761894626:
73: 83232: loss: 0.1763094383:
73: 86432: loss: 0.1766277503:
73: 89632: loss: 0.1766599509:
73: 92832: loss: 0.1767397233:
73: 96032: loss: 0.1766600205:
73: 99232: loss: 0.1763002709:
73: 102432: loss: 0.1762995151:
73: 105632: loss: 0.1769180545:
73: 108832: loss: 0.1765981079:
73: 112032: loss: 0.1768031817:
73: 115232: loss: 0.1768988110:
73: 118432: loss: 0.1770621110:
73: 121632: loss: 0.1770241650:
73: 124832: loss: 0.1771939702:
73: 128032: loss: 0.1775882279:
73: 131232: loss: 0.1777519272:
73: 134432: loss: 0.1778012332:
Dev-Acc: 73: Accuracy: 0.9289569855: precision: 0.4010521430: recall: 0.4407413705: f1: 0.4199611147
Train-Acc: 73: Accuracy: 0.9312705398: precision: 0.8021875000: recall: 0.5062783512: f1: 0.6207730442
74: 3232: loss: 0.1753356909:
74: 6432: loss: 0.1695815898:
74: 9632: loss: 0.1707653934:
74: 12832: loss: 0.1714164818:
74: 16032: loss: 0.1744531436:
74: 19232: loss: 0.1736856344:
74: 22432: loss: 0.1755841345:
74: 25632: loss: 0.1776437677:
74: 28832: loss: 0.1794765772:
74: 32032: loss: 0.1789662641:
74: 35232: loss: 0.1801513957:
74: 38432: loss: 0.1786696883:
74: 41632: loss: 0.1795388342:
74: 44832: loss: 0.1804405880:
74: 48032: loss: 0.1803291874:
74: 51232: loss: 0.1798239904:
74: 54432: loss: 0.1797154795:
74: 57632: loss: 0.1796320051:
74: 60832: loss: 0.1795313757:
74: 64032: loss: 0.1790223694:
74: 67232: loss: 0.1787695241:
74: 70432: loss: 0.1789116471:
74: 73632: loss: 0.1786844626:
74: 76832: loss: 0.1784080441:
74: 80032: loss: 0.1784334598:
74: 83232: loss: 0.1779047972:
74: 86432: loss: 0.1779196085:
74: 89632: loss: 0.1780634085:
74: 92832: loss: 0.1786062320:
74: 96032: loss: 0.1787962650:
74: 99232: loss: 0.1785477687:
74: 102432: loss: 0.1784201770:
74: 105632: loss: 0.1793023943:
74: 108832: loss: 0.1792889827:
74: 112032: loss: 0.1787688174:
74: 115232: loss: 0.1787635013:
74: 118432: loss: 0.1782875453:
74: 121632: loss: 0.1779303153:
74: 124832: loss: 0.1779894318:
74: 128032: loss: 0.1778394733:
74: 131232: loss: 0.1778012966:
74: 134432: loss: 0.1777978227:
Dev-Acc: 74: Accuracy: 0.9287089109: precision: 0.3995067818: recall: 0.4407413705: f1: 0.4191122969
Train-Acc: 74: Accuracy: 0.9314531088: precision: 0.8026384128: recall: 0.5079876405: f1: 0.6221918029
75: 3232: loss: 0.1841469175:
75: 6432: loss: 0.1788946396:
75: 9632: loss: 0.1757411229:
75: 12832: loss: 0.1784569450:
75: 16032: loss: 0.1791498776:
75: 19232: loss: 0.1772058234:
75: 22432: loss: 0.1760216382:
75: 25632: loss: 0.1762679698:
75: 28832: loss: 0.1748558214:
75: 32032: loss: 0.1746299672:
75: 35232: loss: 0.1748896575:
75: 38432: loss: 0.1755514512:
75: 41632: loss: 0.1759460221:
75: 44832: loss: 0.1753753494:
75: 48032: loss: 0.1756819361:
75: 51232: loss: 0.1762976276:
75: 54432: loss: 0.1758819371:
75: 57632: loss: 0.1762190674:
75: 60832: loss: 0.1764952957:
75: 64032: loss: 0.1766777669:
75: 67232: loss: 0.1771188290:
75: 70432: loss: 0.1774835665:
75: 73632: loss: 0.1774110955:
75: 76832: loss: 0.1765701441:
75: 80032: loss: 0.1772854152:
75: 83232: loss: 0.1768570407:
75: 86432: loss: 0.1765752699:
75: 89632: loss: 0.1770392250:
75: 92832: loss: 0.1767618168:
75: 96032: loss: 0.1767325298:
75: 99232: loss: 0.1760280186:
75: 102432: loss: 0.1755419868:
75: 105632: loss: 0.1757784750:
75: 108832: loss: 0.1757612913:
75: 112032: loss: 0.1756555693:
75: 115232: loss: 0.1756762428:
75: 118432: loss: 0.1753140528:
75: 121632: loss: 0.1755963078:
75: 124832: loss: 0.1759785033:
75: 128032: loss: 0.1760214704:
75: 131232: loss: 0.1763576053:
75: 134432: loss: 0.1767005567:
Dev-Acc: 75: Accuracy: 0.9285203815: precision: 0.3984027031: recall: 0.4410814487: f1: 0.4186571982
Train-Acc: 75: Accuracy: 0.9316065311: precision: 0.8025662252: recall: 0.5098941555: f1: 0.6235979899
76: 3232: loss: 0.1700515633:
76: 6432: loss: 0.1798244312:
76: 9632: loss: 0.1779958752:
76: 12832: loss: 0.1766339401:
76: 16032: loss: 0.1799813702:
76: 19232: loss: 0.1813000600:
76: 22432: loss: 0.1801057284:
76: 25632: loss: 0.1784244214:
76: 28832: loss: 0.1779337024:
76: 32032: loss: 0.1768168166:
76: 35232: loss: 0.1772644918:
76: 38432: loss: 0.1776947621:
76: 41632: loss: 0.1774299356:
76: 44832: loss: 0.1750352074:
76: 48032: loss: 0.1746822502:
76: 51232: loss: 0.1761912882:
76: 54432: loss: 0.1759325476:
76: 57632: loss: 0.1757845521:
76: 60832: loss: 0.1753679798:
76: 64032: loss: 0.1752247249:
76: 67232: loss: 0.1756009881:
76: 70432: loss: 0.1754963435:
76: 73632: loss: 0.1751465720:
76: 76832: loss: 0.1750161478:
76: 80032: loss: 0.1750862558:
76: 83232: loss: 0.1749110295:
76: 86432: loss: 0.1751896988:
76: 89632: loss: 0.1747538680:
76: 92832: loss: 0.1750811475:
76: 96032: loss: 0.1754309035:
76: 99232: loss: 0.1757602683:
76: 102432: loss: 0.1757154609:
76: 105632: loss: 0.1761502346:
76: 108832: loss: 0.1756394329:
76: 112032: loss: 0.1753710324:
76: 115232: loss: 0.1755408525:
76: 118432: loss: 0.1757233764:
76: 121632: loss: 0.1759472503:
76: 124832: loss: 0.1761354607:
76: 128032: loss: 0.1762727952:
76: 131232: loss: 0.1765388239:
76: 134432: loss: 0.1764580874:
Dev-Acc: 76: Accuracy: 0.9283021092: precision: 0.3972184013: recall: 0.4419316443: f1: 0.4183837733
Train-Acc: 76: Accuracy: 0.9318621755: precision: 0.8033412396: recall: 0.5121293801: f1: 0.6255018468
77: 3232: loss: 0.1846977505:
77: 6432: loss: 0.1823713875:
77: 9632: loss: 0.1810936497:
77: 12832: loss: 0.1775136990:
77: 16032: loss: 0.1773254989:
77: 19232: loss: 0.1790039265:
77: 22432: loss: 0.1804033957:
77: 25632: loss: 0.1812447063:
77: 28832: loss: 0.1811334781:
77: 32032: loss: 0.1809197353:
77: 35232: loss: 0.1812849532:
77: 38432: loss: 0.1797590376:
77: 41632: loss: 0.1793098072:
77: 44832: loss: 0.1787753509:
77: 48032: loss: 0.1784246329:
77: 51232: loss: 0.1779869059:
77: 54432: loss: 0.1773870831:
77: 57632: loss: 0.1774804250:
77: 60832: loss: 0.1770654422:
77: 64032: loss: 0.1761631105:
77: 67232: loss: 0.1758286914:
77: 70432: loss: 0.1759195801:
77: 73632: loss: 0.1761181804:
77: 76832: loss: 0.1762004025:
77: 80032: loss: 0.1763481669:
77: 83232: loss: 0.1762227931:
77: 86432: loss: 0.1760193632:
77: 89632: loss: 0.1759475800:
77: 92832: loss: 0.1754453475:
77: 96032: loss: 0.1756385964:
77: 99232: loss: 0.1759406929:
77: 102432: loss: 0.1758107939:
77: 105632: loss: 0.1757753309:
77: 108832: loss: 0.1756475341:
77: 112032: loss: 0.1754612954:
77: 115232: loss: 0.1754920524:
77: 118432: loss: 0.1754779641:
77: 121632: loss: 0.1755202340:
77: 124832: loss: 0.1757595974:
77: 128032: loss: 0.1757476572:
77: 131232: loss: 0.1757709950:
77: 134432: loss: 0.1757921689:
Dev-Acc: 77: Accuracy: 0.9282227159: precision: 0.3966702306: recall: 0.4415915661: f1: 0.4179272610
Train-Acc: 77: Accuracy: 0.9321397543: precision: 0.8044842127: recall: 0.5142331208: f1: 0.6274163792
78: 3232: loss: 0.1581436281:
78: 6432: loss: 0.1715235589:
78: 9632: loss: 0.1700350560:
78: 12832: loss: 0.1674269037:
78: 16032: loss: 0.1669661047:
78: 19232: loss: 0.1688905147:
78: 22432: loss: 0.1706817434:
78: 25632: loss: 0.1711191134:
78: 28832: loss: 0.1700144331:
78: 32032: loss: 0.1699608080:
78: 35232: loss: 0.1703517708:
78: 38432: loss: 0.1712809921:
78: 41632: loss: 0.1722182901:
78: 44832: loss: 0.1718411453:
78: 48032: loss: 0.1717336025:
78: 51232: loss: 0.1714001938:
78: 54432: loss: 0.1715693171:
78: 57632: loss: 0.1717664528:
78: 60832: loss: 0.1724330814:
78: 64032: loss: 0.1732595773:
78: 67232: loss: 0.1734283975:
78: 70432: loss: 0.1737478525:
78: 73632: loss: 0.1738548477:
78: 76832: loss: 0.1738003356:
78: 80032: loss: 0.1738177833:
78: 83232: loss: 0.1738009987:
78: 86432: loss: 0.1738005632:
78: 89632: loss: 0.1739420960:
78: 92832: loss: 0.1742260524:
78: 96032: loss: 0.1742632769:
78: 99232: loss: 0.1744584461:
78: 102432: loss: 0.1746488470:
78: 105632: loss: 0.1744560511:
78: 108832: loss: 0.1748472943:
78: 112032: loss: 0.1748121971:
78: 115232: loss: 0.1748921539:
78: 118432: loss: 0.1754700021:
78: 121632: loss: 0.1756460869:
78: 124832: loss: 0.1761157666:
78: 128032: loss: 0.1760840793:
78: 131232: loss: 0.1758951769:
78: 134432: loss: 0.1756172231:
Dev-Acc: 78: Accuracy: 0.9279845953: precision: 0.3952851711: recall: 0.4419316443: f1: 0.4173089274
Train-Acc: 78: Accuracy: 0.9323807955: precision: 0.8051455515: recall: 0.5164026034: f1: 0.6292305844
79: 3232: loss: 0.1697771881:
79: 6432: loss: 0.1699693404:
79: 9632: loss: 0.1739345788:
79: 12832: loss: 0.1759265949:
79: 16032: loss: 0.1748055964:
79: 19232: loss: 0.1757201585:
79: 22432: loss: 0.1755736252:
79: 25632: loss: 0.1745418592:
79: 28832: loss: 0.1730965869:
79: 32032: loss: 0.1746876171:
79: 35232: loss: 0.1753056406:
79: 38432: loss: 0.1741011857:
79: 41632: loss: 0.1727301861:
79: 44832: loss: 0.1717076911:
79: 48032: loss: 0.1713164185:
79: 51232: loss: 0.1710205582:
79: 54432: loss: 0.1720483071:
79: 57632: loss: 0.1724454785:
79: 60832: loss: 0.1728319691:
79: 64032: loss: 0.1735345758:
79: 67232: loss: 0.1734234835:
79: 70432: loss: 0.1740180316:
79: 73632: loss: 0.1747851832:
79: 76832: loss: 0.1744935178:
79: 80032: loss: 0.1740324987:
79: 83232: loss: 0.1739321724:
79: 86432: loss: 0.1739953092:
79: 89632: loss: 0.1739416370:
79: 92832: loss: 0.1737830898:
79: 96032: loss: 0.1741848280:
79: 99232: loss: 0.1739897523:
79: 102432: loss: 0.1740815067:
79: 105632: loss: 0.1743956609:
79: 108832: loss: 0.1746350410:
79: 112032: loss: 0.1744454183:
79: 115232: loss: 0.1745764677:
79: 118432: loss: 0.1747785796:
79: 121632: loss: 0.1745088959:
79: 124832: loss: 0.1743794254:
79: 128032: loss: 0.1744967999:
79: 131232: loss: 0.1745644728:
79: 134432: loss: 0.1745393818:
Dev-Acc: 79: Accuracy: 0.9278556108: precision: 0.3944731248: recall: 0.4417616052: f1: 0.4167803000
Train-Acc: 79: Accuracy: 0.9326218963: precision: 0.8058024313: recall: 0.5185720860: f1: 0.6310400000
80: 3232: loss: 0.1852606670:
80: 6432: loss: 0.1796945425:
80: 9632: loss: 0.1813551163:
80: 12832: loss: 0.1802128605:
80: 16032: loss: 0.1820945790:
80: 19232: loss: 0.1806704400:
80: 22432: loss: 0.1794350995:
80: 25632: loss: 0.1786923688:
80: 28832: loss: 0.1785459629:
80: 32032: loss: 0.1783132726:
80: 35232: loss: 0.1784802961:
80: 38432: loss: 0.1781245391:
80: 41632: loss: 0.1774488156:
80: 44832: loss: 0.1773776015:
80: 48032: loss: 0.1760579840:
80: 51232: loss: 0.1749860997:
80: 54432: loss: 0.1743616216:
80: 57632: loss: 0.1751604527:
80: 60832: loss: 0.1751688174:
80: 64032: loss: 0.1764403585:
80: 67232: loss: 0.1758990382:
80: 70432: loss: 0.1758869549:
80: 73632: loss: 0.1754194000:
80: 76832: loss: 0.1755092764:
80: 80032: loss: 0.1755581517:
80: 83232: loss: 0.1757928462:
80: 86432: loss: 0.1754210516:
80: 89632: loss: 0.1755926802:
80: 92832: loss: 0.1753791559:
80: 96032: loss: 0.1753341669:
80: 99232: loss: 0.1751147284:
80: 102432: loss: 0.1754424345:
80: 105632: loss: 0.1754477632:
80: 108832: loss: 0.1753564014:
80: 112032: loss: 0.1751043053:
80: 115232: loss: 0.1750494458:
80: 118432: loss: 0.1750877410:
80: 121632: loss: 0.1746019982:
80: 124832: loss: 0.1744430478:
80: 128032: loss: 0.1743183462:
80: 131232: loss: 0.1743126721:
80: 134432: loss: 0.1740164060:
Dev-Acc: 80: Accuracy: 0.9277464747: precision: 0.3937187073: recall: 0.4412514878: f1: 0.4161321360
Train-Acc: 80: Accuracy: 0.9327825904: precision: 0.8064878099: recall: 0.5197554401: f1: 0.6321260094
81: 3232: loss: 0.1646418532:
81: 6432: loss: 0.1686738008:
81: 9632: loss: 0.1684864479:
81: 12832: loss: 0.1732947864:
81: 16032: loss: 0.1744590967:
81: 19232: loss: 0.1723347629:
81: 22432: loss: 0.1736519512:
81: 25632: loss: 0.1740943618:
81: 28832: loss: 0.1747062115:
81: 32032: loss: 0.1752163609:
81: 35232: loss: 0.1750796838:
81: 38432: loss: 0.1745863437:
81: 41632: loss: 0.1736800397:
81: 44832: loss: 0.1746003400:
81: 48032: loss: 0.1747206417:
81: 51232: loss: 0.1745509047:
81: 54432: loss: 0.1743579283:
81: 57632: loss: 0.1745058390:
81: 60832: loss: 0.1737528004:
81: 64032: loss: 0.1741979189:
81: 67232: loss: 0.1743354334:
81: 70432: loss: 0.1739546491:
81: 73632: loss: 0.1736179757:
81: 76832: loss: 0.1736140145:
81: 80032: loss: 0.1738594979:
81: 83232: loss: 0.1742682625:
81: 86432: loss: 0.1751356014:
81: 89632: loss: 0.1744664928:
81: 92832: loss: 0.1743329641:
81: 96032: loss: 0.1739279118:
81: 99232: loss: 0.1738400940:
81: 102432: loss: 0.1734549705:
81: 105632: loss: 0.1734686128:
81: 108832: loss: 0.1735968511:
81: 112032: loss: 0.1735218786:
81: 115232: loss: 0.1739637383:
81: 118432: loss: 0.1738383537:
81: 121632: loss: 0.1738857497:
81: 124832: loss: 0.1738280027:
81: 128032: loss: 0.1738560562:
81: 131232: loss: 0.1737282446:
81: 134432: loss: 0.1737390445:
Dev-Acc: 81: Accuracy: 0.9274686575: precision: 0.3921834918: recall: 0.4419316443: f1: 0.4155740326
Train-Acc: 81: Accuracy: 0.9329652190: precision: 0.8062944162: recall: 0.5221221484: f1: 0.6338134951
82: 3232: loss: 0.1646433115:
82: 6432: loss: 0.1669900479:
82: 9632: loss: 0.1737135429:
82: 12832: loss: 0.1716676166:
82: 16032: loss: 0.1722568307:
82: 19232: loss: 0.1736735809:
82: 22432: loss: 0.1726034632:
82: 25632: loss: 0.1715072268:
82: 28832: loss: 0.1698212932:
82: 32032: loss: 0.1693486851:
82: 35232: loss: 0.1686484569:
82: 38432: loss: 0.1704702989:
82: 41632: loss: 0.1697940329:
82: 44832: loss: 0.1692693945:
82: 48032: loss: 0.1695644557:
82: 51232: loss: 0.1699770430:
82: 54432: loss: 0.1700869051:
82: 57632: loss: 0.1707826169:
82: 60832: loss: 0.1712556329:
82: 64032: loss: 0.1709900919:
82: 67232: loss: 0.1714855165:
82: 70432: loss: 0.1709669778:
82: 73632: loss: 0.1710015830:
82: 76832: loss: 0.1716122395:
82: 80032: loss: 0.1719211134:
82: 83232: loss: 0.1724306953:
82: 86432: loss: 0.1721161093:
82: 89632: loss: 0.1723355673:
82: 92832: loss: 0.1719283106:
82: 96032: loss: 0.1722691295:
82: 99232: loss: 0.1722140397:
82: 102432: loss: 0.1718428522:
82: 105632: loss: 0.1724713613:
82: 108832: loss: 0.1725803281:
82: 112032: loss: 0.1727143866:
82: 115232: loss: 0.1727894774:
82: 118432: loss: 0.1730800603:
82: 121632: loss: 0.1735796338:
82: 124832: loss: 0.1733997367:
82: 128032: loss: 0.1732832435:
82: 131232: loss: 0.1731263387:
82: 134432: loss: 0.1730929233:
Dev-Acc: 82: Accuracy: 0.9272007346: precision: 0.3906906907: recall: 0.4424417616: f1: 0.4149589347
Train-Acc: 82: Accuracy: 0.9332135320: precision: 0.8066504952: recall: 0.5246860824: f1: 0.6358095997
83: 3232: loss: 0.1668603636:
83: 6432: loss: 0.1596310415:
83: 9632: loss: 0.1646092962:
83: 12832: loss: 0.1684873611:
83: 16032: loss: 0.1701856182:
83: 19232: loss: 0.1684867563:
83: 22432: loss: 0.1688612498:
83: 25632: loss: 0.1716584550:
83: 28832: loss: 0.1711824303:
83: 32032: loss: 0.1714975610:
83: 35232: loss: 0.1725291415:
83: 38432: loss: 0.1729491505:
83: 41632: loss: 0.1722028125:
83: 44832: loss: 0.1718674404:
83: 48032: loss: 0.1724990253:
83: 51232: loss: 0.1725048273:
83: 54432: loss: 0.1733125768:
83: 57632: loss: 0.1737589810:
83: 60832: loss: 0.1739887467:
83: 64032: loss: 0.1745286716:
83: 67232: loss: 0.1747818843:
83: 70432: loss: 0.1745772291:
83: 73632: loss: 0.1746894592:
83: 76832: loss: 0.1741568553:
83: 80032: loss: 0.1739534657:
83: 83232: loss: 0.1739535024:
83: 86432: loss: 0.1736836035:
83: 89632: loss: 0.1731243820:
83: 92832: loss: 0.1731295020:
83: 96032: loss: 0.1735810609:
83: 99232: loss: 0.1733754987:
83: 102432: loss: 0.1735096160:
83: 105632: loss: 0.1735338794:
83: 108832: loss: 0.1733264742:
83: 112032: loss: 0.1739055024:
83: 115232: loss: 0.1734413992:
83: 118432: loss: 0.1730885766:
83: 121632: loss: 0.1730004076:
83: 124832: loss: 0.1731803680:
83: 128032: loss: 0.1730054635:
83: 131232: loss: 0.1730121577:
83: 134432: loss: 0.1730555056:
Dev-Acc: 83: Accuracy: 0.9269626141: precision: 0.3892878516: recall: 0.4424417616: f1: 0.4141663351
Train-Acc: 83: Accuracy: 0.9334180951: precision: 0.8071342201: recall: 0.5265925975: f1: 0.6373582654
84: 3232: loss: 0.1771152896:
84: 6432: loss: 0.1776208663:
84: 9632: loss: 0.1759174604:
84: 12832: loss: 0.1720202128:
84: 16032: loss: 0.1717963733:
84: 19232: loss: 0.1735844106:
84: 22432: loss: 0.1740593998:
84: 25632: loss: 0.1738956197:
84: 28832: loss: 0.1740510772:
84: 32032: loss: 0.1736514233:
84: 35232: loss: 0.1744698681:
84: 38432: loss: 0.1742576213:
84: 41632: loss: 0.1730436918:
84: 44832: loss: 0.1735612850:
84: 48032: loss: 0.1740207187:
84: 51232: loss: 0.1741876453:
84: 54432: loss: 0.1736170879:
84: 57632: loss: 0.1730326532:
84: 60832: loss: 0.1726131569:
84: 64032: loss: 0.1733082933:
84: 67232: loss: 0.1733479068:
84: 70432: loss: 0.1733163168:
84: 73632: loss: 0.1738653463:
84: 76832: loss: 0.1735224973:
84: 80032: loss: 0.1731572671:
84: 83232: loss: 0.1734309746:
84: 86432: loss: 0.1732138750:
84: 89632: loss: 0.1729243315:
84: 92832: loss: 0.1727559664:
84: 96032: loss: 0.1723930622:
84: 99232: loss: 0.1721713183:
84: 102432: loss: 0.1722947991:
84: 105632: loss: 0.1722214980:
84: 108832: loss: 0.1722688536:
84: 112032: loss: 0.1725544698:
84: 115232: loss: 0.1729320648:
84: 118432: loss: 0.1726539355:
84: 121632: loss: 0.1727753925:
84: 124832: loss: 0.1727404258:
84: 128032: loss: 0.1729724404:
84: 131232: loss: 0.1727562156:
84: 134432: loss: 0.1727796097:
Dev-Acc: 84: Accuracy: 0.9268435240: precision: 0.3884569378: recall: 0.4417616052: f1: 0.4133980428
Train-Acc: 84: Accuracy: 0.9337029457: precision: 0.8078891900: recall: 0.5291565315: f1: 0.6394692937
85: 3232: loss: 0.1619157010:
85: 6432: loss: 0.1688489133:
85: 9632: loss: 0.1647749977:
85: 12832: loss: 0.1688044507:
85: 16032: loss: 0.1663260114:
85: 19232: loss: 0.1682717370:
85: 22432: loss: 0.1707808086:
85: 25632: loss: 0.1719946682:
85: 28832: loss: 0.1713263400:
85: 32032: loss: 0.1726268354:
85: 35232: loss: 0.1729111892:
85: 38432: loss: 0.1734789909:
85: 41632: loss: 0.1731681142:
85: 44832: loss: 0.1730425338:
85: 48032: loss: 0.1729221083:
85: 51232: loss: 0.1732257964:
85: 54432: loss: 0.1728425664:
85: 57632: loss: 0.1719261420:
85: 60832: loss: 0.1725935000:
85: 64032: loss: 0.1721112270:
85: 67232: loss: 0.1720898115:
85: 70432: loss: 0.1717284951:
85: 73632: loss: 0.1715165384:
85: 76832: loss: 0.1713447294:
85: 80032: loss: 0.1712794694:
85: 83232: loss: 0.1713675604:
85: 86432: loss: 0.1709112659:
85: 89632: loss: 0.1706841529:
85: 92832: loss: 0.1707236307:
85: 96032: loss: 0.1709061602:
85: 99232: loss: 0.1711760932:
85: 102432: loss: 0.1708996562:
85: 105632: loss: 0.1708343501:
85: 108832: loss: 0.1711936966:
85: 112032: loss: 0.1714104887:
85: 115232: loss: 0.1713994791:
85: 118432: loss: 0.1713690116:
85: 121632: loss: 0.1715848580:
85: 124832: loss: 0.1715434029:
85: 128032: loss: 0.1714629515:
85: 131232: loss: 0.1717059791:
85: 134432: loss: 0.1720087359:
Dev-Acc: 85: Accuracy: 0.9266847968: precision: 0.3875968992: recall: 0.4421016834: f1: 0.4130590198
Train-Acc: 85: Accuracy: 0.9339805245: precision: 0.8091755985: recall: 0.5310630465: f1: 0.6412637930
86: 3232: loss: 0.1692731972:
86: 6432: loss: 0.1734580825:
86: 9632: loss: 0.1688115398:
86: 12832: loss: 0.1688962130:
86: 16032: loss: 0.1704037324:
86: 19232: loss: 0.1714634123:
86: 22432: loss: 0.1717648573:
86: 25632: loss: 0.1721723170:
86: 28832: loss: 0.1703652515:
86: 32032: loss: 0.1703637258:
86: 35232: loss: 0.1705477992:
86: 38432: loss: 0.1695401173:
86: 41632: loss: 0.1691967069:
86: 44832: loss: 0.1693879154:
86: 48032: loss: 0.1694558665:
86: 51232: loss: 0.1696182142:
86: 54432: loss: 0.1687858186:
86: 57632: loss: 0.1697700892:
86: 60832: loss: 0.1704416761:
86: 64032: loss: 0.1705063282:
86: 67232: loss: 0.1703173442:
86: 70432: loss: 0.1703616555:
86: 73632: loss: 0.1702642358:
86: 76832: loss: 0.1704367068:
86: 80032: loss: 0.1710244211:
86: 83232: loss: 0.1711470038:
86: 86432: loss: 0.1716107138:
86: 89632: loss: 0.1715501024:
86: 92832: loss: 0.1715394992:
86: 96032: loss: 0.1720327128:
86: 99232: loss: 0.1720843759:
86: 102432: loss: 0.1717094583:
86: 105632: loss: 0.1720363870:
86: 108832: loss: 0.1717807240:
86: 112032: loss: 0.1716259025:
86: 115232: loss: 0.1713245377:
86: 118432: loss: 0.1716009268:
86: 121632: loss: 0.1715922248:
86: 124832: loss: 0.1713569214:
86: 128032: loss: 0.1714363453:
86: 131232: loss: 0.1714669340:
86: 134432: loss: 0.1718760023:
Dev-Acc: 86: Accuracy: 0.9263474345: precision: 0.3856761566: recall: 0.4422717225: f1: 0.4120396040
Train-Acc: 86: Accuracy: 0.9342069626: precision: 0.8096426432: recall: 0.5332325291: f1: 0.6429902097
87: 3232: loss: 0.1749614973:
87: 6432: loss: 0.1664784527:
87: 9632: loss: 0.1707443913:
87: 12832: loss: 0.1717819296:
87: 16032: loss: 0.1705573468:
87: 19232: loss: 0.1683669027:
87: 22432: loss: 0.1688165749:
87: 25632: loss: 0.1680346885:
87: 28832: loss: 0.1700141206:
87: 32032: loss: 0.1695329705:
87: 35232: loss: 0.1704673713:
87: 38432: loss: 0.1697023439:
87: 41632: loss: 0.1694385655:
87: 44832: loss: 0.1692804102:
87: 48032: loss: 0.1694146745:
87: 51232: loss: 0.1691751808:
87: 54432: loss: 0.1687541903:
87: 57632: loss: 0.1698391958:
87: 60832: loss: 0.1695393875:
87: 64032: loss: 0.1701794036:
87: 67232: loss: 0.1706823353:
87: 70432: loss: 0.1707910346:
87: 73632: loss: 0.1708569950:
87: 76832: loss: 0.1710787466:
87: 80032: loss: 0.1711068815:
87: 83232: loss: 0.1706279664:
87: 86432: loss: 0.1709162879:
87: 89632: loss: 0.1709249111:
87: 92832: loss: 0.1711487085:
87: 96032: loss: 0.1712240088:
87: 99232: loss: 0.1709446041:
87: 102432: loss: 0.1709294942:
87: 105632: loss: 0.1709665710:
87: 108832: loss: 0.1711069944:
87: 112032: loss: 0.1711944611:
87: 115232: loss: 0.1715475068:
87: 118432: loss: 0.1714233506:
87: 121632: loss: 0.1713167629:
87: 124832: loss: 0.1710767666:
87: 128032: loss: 0.1711107344:
87: 131232: loss: 0.1711869132:
87: 134432: loss: 0.1708684545:
Dev-Acc: 87: Accuracy: 0.9260993600: precision: 0.3841490463: recall: 0.4417616052: f1: 0.4109459032
Train-Acc: 87: Accuracy: 0.9345576167: precision: 0.8118515563: recall: 0.5350075603: f1: 0.6449772142
88: 3232: loss: 0.1739274893:
88: 6432: loss: 0.1737761882:
88: 9632: loss: 0.1718118004:
88: 12832: loss: 0.1724074690:
88: 16032: loss: 0.1708437135:
88: 19232: loss: 0.1715171946:
88: 22432: loss: 0.1694092072:
88: 25632: loss: 0.1679494312:
88: 28832: loss: 0.1673988028:
88: 32032: loss: 0.1680341548:
88: 35232: loss: 0.1687889988:
88: 38432: loss: 0.1699252320:
88: 41632: loss: 0.1689649804:
88: 44832: loss: 0.1683746226:
88: 48032: loss: 0.1687610692:
88: 51232: loss: 0.1697737660:
88: 54432: loss: 0.1705823951:
88: 57632: loss: 0.1707306224:
88: 60832: loss: 0.1714705647:
88: 64032: loss: 0.1717766960:
88: 67232: loss: 0.1717598612:
88: 70432: loss: 0.1719278065:
88: 73632: loss: 0.1720037172:
88: 76832: loss: 0.1713734295:
88: 80032: loss: 0.1712143085:
88: 83232: loss: 0.1714238885:
88: 86432: loss: 0.1714430650:
88: 89632: loss: 0.1712631567:
88: 92832: loss: 0.1711444693:
88: 96032: loss: 0.1709119728:
88: 99232: loss: 0.1710739559:
88: 102432: loss: 0.1712099563:
88: 105632: loss: 0.1712382229:
88: 108832: loss: 0.1711600913:
88: 112032: loss: 0.1711296747:
88: 115232: loss: 0.1714353489:
88: 118432: loss: 0.1713682411:
88: 121632: loss: 0.1711498582:
88: 124832: loss: 0.1709600601:
88: 128032: loss: 0.1704408493:
88: 131232: loss: 0.1701885592:
88: 134432: loss: 0.1703594487:
Dev-Acc: 88: Accuracy: 0.9257223010: precision: 0.3820026467: recall: 0.4417616052: f1: 0.4097145561
Train-Acc: 88: Accuracy: 0.9348790050: precision: 0.8127359428: recall: 0.5378344619: f1: 0.6473078293
89: 3232: loss: 0.1760580042:
89: 6432: loss: 0.1716558615:
89: 9632: loss: 0.1759858471:
89: 12832: loss: 0.1775837703:
89: 16032: loss: 0.1744094176:
89: 19232: loss: 0.1723663431:
89: 22432: loss: 0.1736657875:
89: 25632: loss: 0.1726495291:
89: 28832: loss: 0.1717326041:
89: 32032: loss: 0.1714354377:
89: 35232: loss: 0.1719578574:
89: 38432: loss: 0.1712287824:
89: 41632: loss: 0.1716342795:
89: 44832: loss: 0.1705683708:
89: 48032: loss: 0.1707351847:
89: 51232: loss: 0.1720252619:
89: 54432: loss: 0.1713441881:
89: 57632: loss: 0.1711722755:
89: 60832: loss: 0.1709389693:
89: 64032: loss: 0.1710038252:
89: 67232: loss: 0.1714164143:
89: 70432: loss: 0.1712795318:
89: 73632: loss: 0.1714524637:
89: 76832: loss: 0.1715696662:
89: 80032: loss: 0.1714473914:
89: 83232: loss: 0.1709665728:
89: 86432: loss: 0.1709829085:
89: 89632: loss: 0.1711205764:
89: 92832: loss: 0.1706312392:
89: 96032: loss: 0.1707166593:
89: 99232: loss: 0.1704397172:
89: 102432: loss: 0.1705807898:
89: 105632: loss: 0.1706911186:
89: 108832: loss: 0.1706915445:
89: 112032: loss: 0.1708885404:
89: 115232: loss: 0.1703920267:
89: 118432: loss: 0.1700211327:
89: 121632: loss: 0.1700273029:
89: 124832: loss: 0.1703198315:
89: 128032: loss: 0.1705782742:
89: 131232: loss: 0.1698341669:
89: 134432: loss: 0.1697278044:
Dev-Acc: 89: Accuracy: 0.9256032705: precision: 0.3812600969: recall: 0.4414215270: f1: 0.4091410559
Train-Acc: 89: Accuracy: 0.9351127744: precision: 0.8140758388: recall: 0.5391492998: f1: 0.6486849911
90: 3232: loss: 0.1872794782:
90: 6432: loss: 0.1807852653:
90: 9632: loss: 0.1778665602:
90: 12832: loss: 0.1780085708:
90: 16032: loss: 0.1756661144:
90: 19232: loss: 0.1748877624:
90: 22432: loss: 0.1712140779:
90: 25632: loss: 0.1703871307:
90: 28832: loss: 0.1715754573:
90: 32032: loss: 0.1707468172:
90: 35232: loss: 0.1697207633:
90: 38432: loss: 0.1705554441:
90: 41632: loss: 0.1705874700:
90: 44832: loss: 0.1705265943:
90: 48032: loss: 0.1700625586:
90: 51232: loss: 0.1704226854:
90: 54432: loss: 0.1703164745:
90: 57632: loss: 0.1703918574:
90: 60832: loss: 0.1698314171:
90: 64032: loss: 0.1698703140:
90: 67232: loss: 0.1698278730:
90: 70432: loss: 0.1702968560:
90: 73632: loss: 0.1696369660:
90: 76832: loss: 0.1696227192:
90: 80032: loss: 0.1693976214:
90: 83232: loss: 0.1689805324:
90: 86432: loss: 0.1687315789:
90: 89632: loss: 0.1687619710:
90: 92832: loss: 0.1692095464:
90: 96032: loss: 0.1687416572:
90: 99232: loss: 0.1686573214:
90: 102432: loss: 0.1690060603:
90: 105632: loss: 0.1689527100:
90: 108832: loss: 0.1688197695:
90: 112032: loss: 0.1687601847:
90: 115232: loss: 0.1688661212:
90: 118432: loss: 0.1691195039:
90: 121632: loss: 0.1688274352:
90: 124832: loss: 0.1690028440:
90: 128032: loss: 0.1690596831:
90: 131232: loss: 0.1692695943:
90: 134432: loss: 0.1695694135:
Dev-Acc: 90: Accuracy: 0.9253948927: precision: 0.3801580334: recall: 0.4417616052: f1: 0.4086511994
Train-Acc: 90: Accuracy: 0.9353246093: precision: 0.8145472538: recall: 0.5411215568: f1: 0.6502607047
91: 3232: loss: 0.1682902375:
91: 6432: loss: 0.1763201061:
91: 9632: loss: 0.1730244361:
91: 12832: loss: 0.1729420008:
91: 16032: loss: 0.1746860823:
91: 19232: loss: 0.1716100580:
91: 22432: loss: 0.1715106899:
91: 25632: loss: 0.1710314997:
91: 28832: loss: 0.1687402639:
91: 32032: loss: 0.1688722888:
91: 35232: loss: 0.1709374691:
91: 38432: loss: 0.1719094430:
91: 41632: loss: 0.1719920519:
91: 44832: loss: 0.1714510469:
91: 48032: loss: 0.1712312369:
91: 51232: loss: 0.1707420869:
91: 54432: loss: 0.1704708106:
91: 57632: loss: 0.1714673115:
91: 60832: loss: 0.1717356933:
91: 64032: loss: 0.1713018626:
91: 67232: loss: 0.1715646773:
91: 70432: loss: 0.1713565440:
91: 73632: loss: 0.1715510581:
91: 76832: loss: 0.1711175727:
91: 80032: loss: 0.1705367172:
91: 83232: loss: 0.1702059958:
91: 86432: loss: 0.1694092222:
91: 89632: loss: 0.1696884414:
91: 92832: loss: 0.1699945558:
91: 96032: loss: 0.1697727228:
91: 99232: loss: 0.1700649160:
91: 102432: loss: 0.1700301889:
91: 105632: loss: 0.1695751455:
91: 108832: loss: 0.1693468849:
91: 112032: loss: 0.1692952931:
91: 115232: loss: 0.1690052023:
91: 118432: loss: 0.1691733748:
91: 121632: loss: 0.1691697299:
91: 124832: loss: 0.1692776945:
91: 128032: loss: 0.1695868311:
91: 131232: loss: 0.1694861077:
91: 134432: loss: 0.1693590319:
Dev-Acc: 91: Accuracy: 0.9250575304: precision: 0.3782406059: recall: 0.4415915661: f1: 0.4074684239
Train-Acc: 91: Accuracy: 0.9354999065: precision: 0.8151733676: recall: 0.5425021366: f1: 0.6514565406
92: 3232: loss: 0.1788504520:
92: 6432: loss: 0.1715554609:
92: 9632: loss: 0.1682572499:
92: 12832: loss: 0.1726210465:
92: 16032: loss: 0.1724854264:
92: 19232: loss: 0.1701469944:
92: 22432: loss: 0.1706818773:
92: 25632: loss: 0.1695879827:
92: 28832: loss: 0.1700321540:
92: 32032: loss: 0.1705990055:
92: 35232: loss: 0.1710604279:
92: 38432: loss: 0.1696704985:
92: 41632: loss: 0.1704983858:
92: 44832: loss: 0.1701699102:
92: 48032: loss: 0.1680546404:
92: 51232: loss: 0.1688110661:
92: 54432: loss: 0.1695585358:
92: 57632: loss: 0.1684934943:
92: 60832: loss: 0.1685647746:
92: 64032: loss: 0.1686954842:
92: 67232: loss: 0.1678159271:
92: 70432: loss: 0.1677473285:
92: 73632: loss: 0.1679205358:
92: 76832: loss: 0.1678240861:
92: 80032: loss: 0.1680963078:
92: 83232: loss: 0.1679647958:
92: 86432: loss: 0.1680965589:
92: 89632: loss: 0.1679783156:
92: 92832: loss: 0.1675658485:
92: 96032: loss: 0.1679444762:
92: 99232: loss: 0.1678119686:
92: 102432: loss: 0.1677489525:
92: 105632: loss: 0.1677986183:
92: 108832: loss: 0.1680502271:
92: 112032: loss: 0.1682231386:
92: 115232: loss: 0.1685512988:
92: 118432: loss: 0.1690226505:
92: 121632: loss: 0.1690225394:
92: 124832: loss: 0.1689032140:
92: 128032: loss: 0.1689300444:
92: 131232: loss: 0.1688196509:
92: 134432: loss: 0.1686656631:
Dev-Acc: 92: Accuracy: 0.9248491526: precision: 0.3771229496: recall: 0.4417616052: f1: 0.4068911511
Train-Acc: 92: Accuracy: 0.9357044101: precision: 0.8153725027: recall: 0.5446716192: f1: 0.6530821378
93: 3232: loss: 0.1696398221:
93: 6432: loss: 0.1662347854:
93: 9632: loss: 0.1670081152:
93: 12832: loss: 0.1686074433:
93: 16032: loss: 0.1691455046:
93: 19232: loss: 0.1712225804:
93: 22432: loss: 0.1698144970:
93: 25632: loss: 0.1681028068:
93: 28832: loss: 0.1678768450:
93: 32032: loss: 0.1663467387:
93: 35232: loss: 0.1674710690:
93: 38432: loss: 0.1685678604:
93: 41632: loss: 0.1685979718:
93: 44832: loss: 0.1684773521:
93: 48032: loss: 0.1688467618:
93: 51232: loss: 0.1687567861:
93: 54432: loss: 0.1691131431:
93: 57632: loss: 0.1699806423:
93: 60832: loss: 0.1691696886:
93: 64032: loss: 0.1692253888:
93: 67232: loss: 0.1690811290:
93: 70432: loss: 0.1690899520:
93: 73632: loss: 0.1686571459:
93: 76832: loss: 0.1687181009:
93: 80032: loss: 0.1688515162:
93: 83232: loss: 0.1687351481:
93: 86432: loss: 0.1680567980:
93: 89632: loss: 0.1683578913:
93: 92832: loss: 0.1681184090:
93: 96032: loss: 0.1682285522:
93: 99232: loss: 0.1681034861:
93: 102432: loss: 0.1676985168:
93: 105632: loss: 0.1672881121:
93: 108832: loss: 0.1671002866:
93: 112032: loss: 0.1673968938:
93: 115232: loss: 0.1678507466:
93: 118432: loss: 0.1677869530:
93: 121632: loss: 0.1678276995:
93: 124832: loss: 0.1682425862:
93: 128032: loss: 0.1681636584:
93: 131232: loss: 0.1678204246:
93: 134432: loss: 0.1678726419:
Dev-Acc: 93: Accuracy: 0.9246606231: precision: 0.3761932311: recall: 0.4422717225: f1: 0.4065650645
Train-Acc: 93: Accuracy: 0.9359601140: precision: 0.8164407778: recall: 0.5465123923: f1: 0.6547473713
94: 3232: loss: 0.1807336406:
94: 6432: loss: 0.1780591390:
94: 9632: loss: 0.1733759946:
94: 12832: loss: 0.1731755163:
94: 16032: loss: 0.1729617208:
94: 19232: loss: 0.1716516055:
94: 22432: loss: 0.1709851393:
94: 25632: loss: 0.1704234887:
94: 28832: loss: 0.1688938739:
94: 32032: loss: 0.1688388306:
94: 35232: loss: 0.1692573513:
94: 38432: loss: 0.1683537960:
94: 41632: loss: 0.1698230504:
94: 44832: loss: 0.1698456237:
94: 48032: loss: 0.1696687343:
94: 51232: loss: 0.1690358057:
94: 54432: loss: 0.1683724422:
94: 57632: loss: 0.1684294825:
94: 60832: loss: 0.1688922361:
94: 64032: loss: 0.1693378896:
94: 67232: loss: 0.1699825637:
94: 70432: loss: 0.1701496654:
94: 73632: loss: 0.1699426390:
94: 76832: loss: 0.1700525687:
94: 80032: loss: 0.1696195593:
94: 83232: loss: 0.1695447949:
94: 86432: loss: 0.1693486158:
94: 89632: loss: 0.1693863738:
94: 92832: loss: 0.1694937545:
94: 96032: loss: 0.1686381636:
94: 99232: loss: 0.1687576683:
94: 102432: loss: 0.1686713356:
94: 105632: loss: 0.1689685374:
94: 108832: loss: 0.1691369339:
94: 112032: loss: 0.1690165734:
94: 115232: loss: 0.1687487486:
94: 118432: loss: 0.1683665751:
94: 121632: loss: 0.1684689632:
94: 124832: loss: 0.1684518680:
94: 128032: loss: 0.1682937242:
94: 131232: loss: 0.1680574700:
94: 134432: loss: 0.1679563060:
Dev-Acc: 94: Accuracy: 0.9245217443: precision: 0.3754330254: recall: 0.4422717225: f1: 0.4061206964
Train-Acc: 94: Accuracy: 0.9362157583: precision: 0.8174424302: recall: 0.5484189074: f1: 0.6564368901
95: 3232: loss: 0.1752342500:
95: 6432: loss: 0.1734163184:
95: 9632: loss: 0.1700483819:
95: 12832: loss: 0.1669283780:
95: 16032: loss: 0.1674484212:
95: 19232: loss: 0.1675177070:
95: 22432: loss: 0.1653549101:
95: 25632: loss: 0.1673118916:
95: 28832: loss: 0.1676505811:
95: 32032: loss: 0.1676956776:
95: 35232: loss: 0.1667986122:
95: 38432: loss: 0.1666032960:
95: 41632: loss: 0.1651695422:
95: 44832: loss: 0.1647579051:
95: 48032: loss: 0.1646243116:
95: 51232: loss: 0.1650111731:
95: 54432: loss: 0.1650680326:
95: 57632: loss: 0.1642843921:
95: 60832: loss: 0.1641774591:
95: 64032: loss: 0.1643162474:
95: 67232: loss: 0.1651001573:
95: 70432: loss: 0.1661579688:
95: 73632: loss: 0.1658692537:
95: 76832: loss: 0.1665819077:
95: 80032: loss: 0.1667150772:
95: 83232: loss: 0.1662894062:
95: 86432: loss: 0.1662796197:
95: 89632: loss: 0.1662026613:
95: 92832: loss: 0.1662863068:
95: 96032: loss: 0.1663637229:
95: 99232: loss: 0.1662944526:
95: 102432: loss: 0.1663065609:
95: 105632: loss: 0.1661107420:
95: 108832: loss: 0.1661088835:
95: 112032: loss: 0.1662405353:
95: 115232: loss: 0.1663392589:
95: 118432: loss: 0.1665144267:
95: 121632: loss: 0.1665241010:
95: 124832: loss: 0.1668057695:
95: 128032: loss: 0.1669024609:
95: 131232: loss: 0.1670134656:
95: 134432: loss: 0.1670738296:
Dev-Acc: 95: Accuracy: 0.9243133664: precision: 0.3742984602: recall: 0.4422717225: f1: 0.4054559626
Train-Acc: 95: Accuracy: 0.9365152717: precision: 0.8191072827: recall: 0.5501281967: f1: 0.6581979785
96: 3232: loss: 0.1669290837:
96: 6432: loss: 0.1618071234:
96: 9632: loss: 0.1642568473:
96: 12832: loss: 0.1668782360:
96: 16032: loss: 0.1695445403:
96: 19232: loss: 0.1712488827:
96: 22432: loss: 0.1739070812:
96: 25632: loss: 0.1728178290:
96: 28832: loss: 0.1716705246:
96: 32032: loss: 0.1714577707:
96: 35232: loss: 0.1705992677:
96: 38432: loss: 0.1703228340:
96: 41632: loss: 0.1704571308:
96: 44832: loss: 0.1702702199:
96: 48032: loss: 0.1694992190:
96: 51232: loss: 0.1698020772:
96: 54432: loss: 0.1679735509:
96: 57632: loss: 0.1680569480:
96: 60832: loss: 0.1675346634:
96: 64032: loss: 0.1680221294:
96: 67232: loss: 0.1683044280:
96: 70432: loss: 0.1679521207:
96: 73632: loss: 0.1680707185:
96: 76832: loss: 0.1682830970:
96: 80032: loss: 0.1678414856:
96: 83232: loss: 0.1675766426:
96: 86432: loss: 0.1676905074:
96: 89632: loss: 0.1678683484:
96: 92832: loss: 0.1675352293:
96: 96032: loss: 0.1674776817:
96: 99232: loss: 0.1674976293:
96: 102432: loss: 0.1673628134:
96: 105632: loss: 0.1673614238:
96: 108832: loss: 0.1677709575:
96: 112032: loss: 0.1675347712:
96: 115232: loss: 0.1673536029:
96: 118432: loss: 0.1675025556:
96: 121632: loss: 0.1675508158:
96: 124832: loss: 0.1671255992:
96: 128032: loss: 0.1671464098:
96: 131232: loss: 0.1671383719:
96: 134432: loss: 0.1671623960:
Dev-Acc: 96: Accuracy: 0.9242637753: precision: 0.3740293356: recall: 0.4422717225: f1: 0.4052980132
Train-Acc: 96: Accuracy: 0.9366759658: precision: 0.8199334898: recall: 0.5511143252: f1: 0.6591704344
97: 3232: loss: 0.1504336308:
97: 6432: loss: 0.1591085680:
97: 9632: loss: 0.1645497469:
97: 12832: loss: 0.1629237213:
97: 16032: loss: 0.1681227475:
97: 19232: loss: 0.1692306677:
97: 22432: loss: 0.1685713370:
97: 25632: loss: 0.1697965415:
97: 28832: loss: 0.1717553463:
97: 32032: loss: 0.1716658993:
97: 35232: loss: 0.1713193941:
97: 38432: loss: 0.1713294227:
97: 41632: loss: 0.1706361545:
97: 44832: loss: 0.1699041275:
97: 48032: loss: 0.1698443677:
97: 51232: loss: 0.1694956032:
97: 54432: loss: 0.1699152928:
97: 57632: loss: 0.1699879467:
97: 60832: loss: 0.1692538873:
97: 64032: loss: 0.1696788838:
97: 67232: loss: 0.1693323981:
97: 70432: loss: 0.1692471728:
97: 73632: loss: 0.1686428619:
97: 76832: loss: 0.1690399220:
97: 80032: loss: 0.1691844636:
97: 83232: loss: 0.1689190656:
97: 86432: loss: 0.1685018236:
97: 89632: loss: 0.1678271165:
97: 92832: loss: 0.1675514388:
97: 96032: loss: 0.1672532903:
97: 99232: loss: 0.1673461168:
97: 102432: loss: 0.1671746324:
97: 105632: loss: 0.1667714146:
97: 108832: loss: 0.1665693464:
97: 112032: loss: 0.1664358678:
97: 115232: loss: 0.1663938053:
97: 118432: loss: 0.1663361579:
97: 121632: loss: 0.1664502231:
97: 124832: loss: 0.1665505764:
97: 128032: loss: 0.1666167847:
97: 131232: loss: 0.1667260649:
97: 134432: loss: 0.1665284349:
Dev-Acc: 97: Accuracy: 0.9240553975: precision: 0.3728302969: recall: 0.4419316443: f1: 0.4044506692
Train-Acc: 97: Accuracy: 0.9368585944: precision: 0.8211246944: recall: 0.5519689698: f1: 0.6601666929
98: 3232: loss: 0.1549511803:
98: 6432: loss: 0.1645725139:
98: 9632: loss: 0.1628556015:
98: 12832: loss: 0.1630050283:
98: 16032: loss: 0.1639905386:
98: 19232: loss: 0.1643208878:
98: 22432: loss: 0.1633407507:
98: 25632: loss: 0.1639554420:
98: 28832: loss: 0.1653663063:
98: 32032: loss: 0.1663551935:
98: 35232: loss: 0.1665846110:
98: 38432: loss: 0.1661535476:
98: 41632: loss: 0.1668533473:
98: 44832: loss: 0.1673152513:
98: 48032: loss: 0.1670872238:
98: 51232: loss: 0.1658968596:
98: 54432: loss: 0.1652095945:
98: 57632: loss: 0.1656462887:
98: 60832: loss: 0.1663790129:
98: 64032: loss: 0.1665455191:
98: 67232: loss: 0.1666418764:
98: 70432: loss: 0.1665366668:
98: 73632: loss: 0.1664465277:
98: 76832: loss: 0.1660857854:
98: 80032: loss: 0.1661061588:
98: 83232: loss: 0.1660006953:
98: 86432: loss: 0.1658739377:
98: 89632: loss: 0.1660851065:
98: 92832: loss: 0.1658994721:
98: 96032: loss: 0.1659389890:
98: 99232: loss: 0.1659917686:
98: 102432: loss: 0.1660974803:
98: 105632: loss: 0.1660403882:
98: 108832: loss: 0.1662337590:
98: 112032: loss: 0.1659957780:
98: 115232: loss: 0.1658191892:
98: 118432: loss: 0.1660562805:
98: 121632: loss: 0.1660334092:
98: 124832: loss: 0.1661900836:
98: 128032: loss: 0.1664349041:
98: 131232: loss: 0.1665693523:
98: 134432: loss: 0.1664966863:
Dev-Acc: 98: Accuracy: 0.9236684442: precision: 0.3708297690: recall: 0.4422717225: f1: 0.4034121753
Train-Acc: 98: Accuracy: 0.9370046854: precision: 0.8210977869: recall: 0.5536782592: f1: 0.6613789854
99: 3232: loss: 0.1668551705:
99: 6432: loss: 0.1657218005:
99: 9632: loss: 0.1664178294:
99: 12832: loss: 0.1685618517:
99: 16032: loss: 0.1650718809:
99: 19232: loss: 0.1636313033:
99: 22432: loss: 0.1642614734:
99: 25632: loss: 0.1643543378:
99: 28832: loss: 0.1649320652:
99: 32032: loss: 0.1651221762:
99: 35232: loss: 0.1650314603:
99: 38432: loss: 0.1654936482:
99: 41632: loss: 0.1659191728:
99: 44832: loss: 0.1663547001:
99: 48032: loss: 0.1663864231:
99: 51232: loss: 0.1663568277:
99: 54432: loss: 0.1652326346:
99: 57632: loss: 0.1651958397:
99: 60832: loss: 0.1658082665:
99: 64032: loss: 0.1657589604:
99: 67232: loss: 0.1656947711:
99: 70432: loss: 0.1654251908:
99: 73632: loss: 0.1661238762:
99: 76832: loss: 0.1665583623:
99: 80032: loss: 0.1662659731:
99: 83232: loss: 0.1664702300:
99: 86432: loss: 0.1661442067:
99: 89632: loss: 0.1660435676:
99: 92832: loss: 0.1661935040:
99: 96032: loss: 0.1662170788:
99: 99232: loss: 0.1662227344:
99: 102432: loss: 0.1662211639:
99: 105632: loss: 0.1660803319:
99: 108832: loss: 0.1658745426:
99: 112032: loss: 0.1659461958:
99: 115232: loss: 0.1655211574:
99: 118432: loss: 0.1653604673:
99: 121632: loss: 0.1654707628:
99: 124832: loss: 0.1657295355:
99: 128032: loss: 0.1657918734:
99: 131232: loss: 0.1658664976:
99: 134432: loss: 0.1658903267:
Dev-Acc: 99: Accuracy: 0.9233409762: precision: 0.3692045938: recall: 0.4427818398: f1: 0.4026596567
Train-Acc: 99: Accuracy: 0.9372165203: precision: 0.8219151421: recall: 0.5552560647: f1: 0.6627692549
100: 3232: loss: 0.1805978537:
100: 6432: loss: 0.1734118669:
100: 9632: loss: 0.1754947537:
100: 12832: loss: 0.1756111405:
100: 16032: loss: 0.1735204986:
100: 19232: loss: 0.1733434799:
100: 22432: loss: 0.1703453189:
100: 25632: loss: 0.1680508260:
100: 28832: loss: 0.1675744199:
100: 32032: loss: 0.1665007671:
100: 35232: loss: 0.1656342156:
100: 38432: loss: 0.1645722054:
100: 41632: loss: 0.1651607981:
100: 44832: loss: 0.1647239519:
100: 48032: loss: 0.1644446696:
100: 51232: loss: 0.1647297610:
100: 54432: loss: 0.1651944383:
100: 57632: loss: 0.1651235974:
100: 60832: loss: 0.1651384085:
100: 64032: loss: 0.1654670874:
100: 67232: loss: 0.1656534731:
100: 70432: loss: 0.1661511108:
100: 73632: loss: 0.1660051192:
100: 76832: loss: 0.1657822459:
100: 80032: loss: 0.1651435043:
100: 83232: loss: 0.1655592823:
100: 86432: loss: 0.1658840426:
100: 89632: loss: 0.1657969070:
100: 92832: loss: 0.1664467239:
100: 96032: loss: 0.1661932638:
100: 99232: loss: 0.1662239123:
100: 102432: loss: 0.1663021450:
100: 105632: loss: 0.1659055665:
100: 108832: loss: 0.1658373790:
100: 112032: loss: 0.1660664624:
100: 115232: loss: 0.1659556446:
100: 118432: loss: 0.1657181416:
100: 121632: loss: 0.1660622933:
100: 124832: loss: 0.1659366803:
100: 128032: loss: 0.1659863455:
100: 131232: loss: 0.1659910681:
100: 134432: loss: 0.1659335616:
Dev-Acc: 100: Accuracy: 0.9232516885: precision: 0.3687340697: recall: 0.4427818398: f1: 0.4023796647
Train-Acc: 100: Accuracy: 0.9373626113: precision: 0.8225116641: recall: 0.5563079350: f1: 0.6637123024
