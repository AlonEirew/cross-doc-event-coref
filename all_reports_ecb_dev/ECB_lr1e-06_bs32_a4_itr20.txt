1: 3232: loss: 0.6984812665:
1: 6432: loss: 0.6923418990:
1: 9632: loss: 0.6858554258:
1: 12832: loss: 0.6795354110:
1: 16032: loss: 0.6732547549:
1: 19232: loss: 0.6673275127:
1: 22432: loss: 0.6612572918:
1: 25632: loss: 0.6555202544:
1: 28832: loss: 0.6497905099:
1: 32032: loss: 0.6442298079:
1: 35232: loss: 0.6384177622:
1: 38432: loss: 0.6326141997:
1: 41632: loss: 0.6267118592:
1: 44832: loss: 0.6211963045:
1: 48032: loss: 0.6159551443:
1: 51232: loss: 0.6105007485:
1: 54432: loss: 0.6047322382:
1: 57632: loss: 0.5997528207:
1: 60832: loss: 0.5942329328:
1: 64032: loss: 0.5891281000:
1: 67232: loss: 0.5837989498:
1: 70432: loss: 0.5789518662:
1: 73632: loss: 0.5741122521:
Dev-Acc: 1: Accuracy: 0.9413696527: precision: 0.4803921569: recall: 0.0583234144: f1: 0.1040181956
Train-Acc: 1: Accuracy: 0.8213792443: precision: 0.9447483589: recall: 0.1135362567: f1: 0.2027114267
2: 3232: loss: 0.4542588040:
2: 6432: loss: 0.4503539377:
2: 9632: loss: 0.4463221406:
2: 12832: loss: 0.4395871638:
2: 16032: loss: 0.4336740643:
2: 19232: loss: 0.4298289384:
2: 22432: loss: 0.4285655874:
2: 25632: loss: 0.4245924122:
2: 28832: loss: 0.4219421599:
2: 32032: loss: 0.4190567162:
2: 35232: loss: 0.4154457454:
2: 38432: loss: 0.4112382965:
2: 41632: loss: 0.4091369712:
2: 44832: loss: 0.4064724237:
2: 48032: loss: 0.4028685489:
2: 51232: loss: 0.4005515119:
2: 54432: loss: 0.3978076862:
2: 57632: loss: 0.3941509101:
2: 60832: loss: 0.3914578281:
2: 64032: loss: 0.3887195526:
2: 67232: loss: 0.3863097912:
2: 70432: loss: 0.3835700228:
2: 73632: loss: 0.3812867971:
Dev-Acc: 2: Accuracy: 0.8980790377: precision: 0.2871546292: recall: 0.5036558408: f1: 0.3657693258
Train-Acc: 2: Accuracy: 0.8855828047: precision: 0.8994721984: recall: 0.4817566235: f1: 0.6274509804
3: 3232: loss: 0.3281331100:
3: 6432: loss: 0.3235464593:
3: 9632: loss: 0.3196658625:
3: 12832: loss: 0.3148734806:
3: 16032: loss: 0.3118877731:
3: 19232: loss: 0.3105593272:
3: 22432: loss: 0.3090514972:
3: 25632: loss: 0.3063347388:
3: 28832: loss: 0.3058830208:
3: 32032: loss: 0.3045882630:
3: 35232: loss: 0.3020901736:
3: 38432: loss: 0.3007884864:
3: 41632: loss: 0.2991735022:
3: 44832: loss: 0.2979395656:
3: 48032: loss: 0.2955290582:
3: 51232: loss: 0.2947040051:
3: 54432: loss: 0.2933882554:
3: 57632: loss: 0.2914684092:
3: 60832: loss: 0.2903465932:
3: 64032: loss: 0.2889990764:
3: 67232: loss: 0.2880371027:
3: 70432: loss: 0.2866886549:
3: 73632: loss: 0.2853922584:
Dev-Acc: 3: Accuracy: 0.8477635384: precision: 0.2106422018: recall: 0.5856146914: f1: 0.3098376141
Train-Acc: 3: Accuracy: 0.9053184986: precision: 0.9025125628: recall: 0.5903622379: f1: 0.7138031080
4: 3232: loss: 0.2415811494:
4: 6432: loss: 0.2416071454:
4: 9632: loss: 0.2479958749:
4: 12832: loss: 0.2472010291:
4: 16032: loss: 0.2479102564:
4: 19232: loss: 0.2473915901:
4: 22432: loss: 0.2464644779:
4: 25632: loss: 0.2465999703:
4: 28832: loss: 0.2460918220:
4: 32032: loss: 0.2455279202:
4: 35232: loss: 0.2442855701:
4: 38432: loss: 0.2458269871:
4: 41632: loss: 0.2444076995:
4: 44832: loss: 0.2421942912:
4: 48032: loss: 0.2418520823:
4: 51232: loss: 0.2412883065:
4: 54432: loss: 0.2410200185:
4: 57632: loss: 0.2408667125:
4: 60832: loss: 0.2404939796:
4: 64032: loss: 0.2404908351:
4: 67232: loss: 0.2395086961:
4: 70432: loss: 0.2386103063:
4: 73632: loss: 0.2386154510:
Dev-Acc: 4: Accuracy: 0.8111009598: precision: 0.1790505928: recall: 0.6240435300: f1: 0.2782621882
Train-Acc: 4: Accuracy: 0.9175990820: precision: 0.8980064080: recall: 0.6633357439: f1: 0.7630355050
5: 3232: loss: 0.2387569438:
5: 6432: loss: 0.2283999837:
5: 9632: loss: 0.2230415263:
5: 12832: loss: 0.2239831758:
5: 16032: loss: 0.2225747369:
5: 19232: loss: 0.2231824891:
5: 22432: loss: 0.2224125558:
5: 25632: loss: 0.2213294352:
5: 28832: loss: 0.2204699521:
5: 32032: loss: 0.2206593290:
5: 35232: loss: 0.2211227643:
5: 38432: loss: 0.2199058479:
5: 41632: loss: 0.2193139567:
5: 44832: loss: 0.2178545660:
5: 48032: loss: 0.2172501739:
5: 51232: loss: 0.2165271801:
5: 54432: loss: 0.2153495265:
5: 57632: loss: 0.2149054781:
5: 60832: loss: 0.2149537046:
5: 64032: loss: 0.2145926764:
5: 67232: loss: 0.2145083883:
5: 70432: loss: 0.2133204927:
5: 73632: loss: 0.2120753063:
Dev-Acc: 5: Accuracy: 0.7865533829: precision: 0.1630814329: recall: 0.6432579493: f1: 0.2601967123
Train-Acc: 5: Accuracy: 0.9268686771: precision: 0.9018072791: recall: 0.7118532641: f1: 0.7956499375
6: 3232: loss: 0.2026318067:
6: 6432: loss: 0.2088007008:
6: 9632: loss: 0.2053830153:
6: 12832: loss: 0.2022066588:
6: 16032: loss: 0.2014209171:
6: 19232: loss: 0.2020371794:
6: 22432: loss: 0.2010640804:
6: 25632: loss: 0.1999282413:
6: 28832: loss: 0.1991613349:
6: 32032: loss: 0.1981031562:
6: 35232: loss: 0.1975155296:
6: 38432: loss: 0.1983140577:
6: 41632: loss: 0.1977949159:
6: 44832: loss: 0.1987547706:
6: 48032: loss: 0.1980333540:
6: 51232: loss: 0.1979582389:
6: 54432: loss: 0.1974721345:
6: 57632: loss: 0.1971783081:
6: 60832: loss: 0.1971480344:
6: 64032: loss: 0.1973740665:
6: 67232: loss: 0.1962771901:
6: 70432: loss: 0.1953110709:
6: 73632: loss: 0.1946484065:
Dev-Acc: 6: Accuracy: 0.7575110793: precision: 0.1480160838: recall: 0.6634926033: f1: 0.2420370313
Train-Acc: 6: Accuracy: 0.9346919656: precision: 0.9014735852: recall: 0.7560975610: f1: 0.8224105259
7: 3232: loss: 0.1797724966:
7: 6432: loss: 0.1797295519:
7: 9632: loss: 0.1772538657:
7: 12832: loss: 0.1783623751:
7: 16032: loss: 0.1814310534:
7: 19232: loss: 0.1794777516:
7: 22432: loss: 0.1811972771:
7: 25632: loss: 0.1816957036:
7: 28832: loss: 0.1822939792:
7: 32032: loss: 0.1830142855:
7: 35232: loss: 0.1835530060:
7: 38432: loss: 0.1846790148:
7: 41632: loss: 0.1851128819:
7: 44832: loss: 0.1848622384:
7: 48032: loss: 0.1847429644:
7: 51232: loss: 0.1843575459:
7: 54432: loss: 0.1838936459:
7: 57632: loss: 0.1835656460:
7: 60832: loss: 0.1833271455:
7: 64032: loss: 0.1829589920:
7: 67232: loss: 0.1822584917:
7: 70432: loss: 0.1823967591:
7: 73632: loss: 0.1821288725:
Dev-Acc: 7: Accuracy: 0.7420225143: precision: 0.1420998328: recall: 0.6791362013: f1: 0.2350241262
Train-Acc: 7: Accuracy: 0.9379001856: precision: 0.9025794565: recall: 0.7729274867: f1: 0.8327371888
8: 3232: loss: 0.1658338689:
8: 6432: loss: 0.1712995901:
8: 9632: loss: 0.1753267572:
8: 12832: loss: 0.1749909333:
8: 16032: loss: 0.1772241779:
8: 19232: loss: 0.1752192025:
8: 22432: loss: 0.1766288697:
8: 25632: loss: 0.1763242031:
8: 28832: loss: 0.1768784127:
8: 32032: loss: 0.1754358573:
8: 35232: loss: 0.1757883355:
8: 38432: loss: 0.1762801103:
8: 41632: loss: 0.1757181833:
8: 44832: loss: 0.1753220428:
8: 48032: loss: 0.1750834372:
8: 51232: loss: 0.1740239346:
8: 54432: loss: 0.1735305507:
8: 57632: loss: 0.1728840766:
8: 60832: loss: 0.1726725114:
8: 64032: loss: 0.1723762202:
8: 67232: loss: 0.1729066253:
8: 70432: loss: 0.1729552391:
8: 73632: loss: 0.1726592432:
Dev-Acc: 8: Accuracy: 0.7286771536: precision: 0.1375548801: recall: 0.6925692909: f1: 0.2295229777
Train-Acc: 8: Accuracy: 0.9415028691: precision: 0.9044040283: recall: 0.7911379922: f1: 0.8439877968
9: 3232: loss: 0.1656331094:
9: 6432: loss: 0.1792522356:
9: 9632: loss: 0.1713906231:
9: 12832: loss: 0.1697352306:
9: 16032: loss: 0.1688596880:
9: 19232: loss: 0.1667364785:
9: 22432: loss: 0.1673693518:
9: 25632: loss: 0.1660947264:
9: 28832: loss: 0.1665435500:
9: 32032: loss: 0.1663567326:
9: 35232: loss: 0.1658240296:
9: 38432: loss: 0.1657989564:
9: 41632: loss: 0.1650918549:
9: 44832: loss: 0.1658600697:
9: 48032: loss: 0.1656006518:
9: 51232: loss: 0.1662349954:
9: 54432: loss: 0.1662940668:
9: 57632: loss: 0.1660736701:
9: 60832: loss: 0.1665464024:
9: 64032: loss: 0.1666530506:
9: 67232: loss: 0.1661062148:
9: 70432: loss: 0.1658760060:
9: 73632: loss: 0.1655283962:
Dev-Acc: 9: Accuracy: 0.7255715132: precision: 0.1362375973: recall: 0.6934194865: f1: 0.2277321718
Train-Acc: 9: Accuracy: 0.9440536499: precision: 0.9149371307: recall: 0.7940963776: f1: 0.8502446063
10: 3232: loss: 0.1568484987:
10: 6432: loss: 0.1598823927:
10: 9632: loss: 0.1604427827:
10: 12832: loss: 0.1621837839:
10: 16032: loss: 0.1620446793:
10: 19232: loss: 0.1625510671:
10: 22432: loss: 0.1622943046:
10: 25632: loss: 0.1618786708:
10: 28832: loss: 0.1620444697:
10: 32032: loss: 0.1614304591:
10: 35232: loss: 0.1613088077:
10: 38432: loss: 0.1617181696:
10: 41632: loss: 0.1616762267:
10: 44832: loss: 0.1608154804:
10: 48032: loss: 0.1601577844:
10: 51232: loss: 0.1595704792:
10: 54432: loss: 0.1596352139:
10: 57632: loss: 0.1595946084:
10: 60832: loss: 0.1599629215:
10: 64032: loss: 0.1593535396:
10: 67232: loss: 0.1597027491:
10: 70432: loss: 0.1592262359:
10: 73632: loss: 0.1588506013:
Dev-Acc: 10: Accuracy: 0.7129405141: precision: 0.1319302504: recall: 0.7024315593: f1: 0.2221385745
Train-Acc: 10: Accuracy: 0.9461311102: precision: 0.9110815209: recall: 0.8096772073: f1: 0.8573914860
11: 3232: loss: 0.1505838158:
11: 6432: loss: 0.1592958483:
11: 9632: loss: 0.1579211815:
11: 12832: loss: 0.1547407931:
11: 16032: loss: 0.1531838271:
11: 19232: loss: 0.1536155808:
11: 22432: loss: 0.1527530159:
11: 25632: loss: 0.1536039955:
11: 28832: loss: 0.1554882850:
11: 32032: loss: 0.1559448237:
11: 35232: loss: 0.1557798975:
11: 38432: loss: 0.1544103542:
11: 41632: loss: 0.1525995557:
11: 44832: loss: 0.1523839405:
11: 48032: loss: 0.1519592042:
11: 51232: loss: 0.1517228257:
11: 54432: loss: 0.1514596421:
11: 57632: loss: 0.1515727336:
11: 60832: loss: 0.1522858696:
11: 64032: loss: 0.1530477880:
11: 67232: loss: 0.1530057077:
11: 70432: loss: 0.1531941269:
11: 73632: loss: 0.1531613709:
Dev-Acc: 11: Accuracy: 0.7055981159: precision: 0.1287221425: recall: 0.7012412855: f1: 0.2175162847
Train-Acc: 11: Accuracy: 0.9481164813: precision: 0.9177482756: recall: 0.8134902373: f1: 0.8624799610
12: 3232: loss: 0.1506557042:
12: 6432: loss: 0.1423423728:
12: 9632: loss: 0.1413129109:
12: 12832: loss: 0.1405971613:
12: 16032: loss: 0.1407155976:
12: 19232: loss: 0.1419287484:
12: 22432: loss: 0.1434567959:
12: 25632: loss: 0.1441129795:
12: 28832: loss: 0.1446821534:
12: 32032: loss: 0.1456887292:
12: 35232: loss: 0.1462516209:
12: 38432: loss: 0.1454325849:
12: 41632: loss: 0.1473751368:
12: 44832: loss: 0.1474742726:
12: 48032: loss: 0.1472536690:
12: 51232: loss: 0.1463132438:
12: 54432: loss: 0.1469266893:
12: 57632: loss: 0.1469064979:
12: 60832: loss: 0.1461678925:
12: 64032: loss: 0.1465015034:
12: 67232: loss: 0.1472898922:
12: 70432: loss: 0.1478782205:
12: 73632: loss: 0.1480613050:
Dev-Acc: 12: Accuracy: 0.6984441876: precision: 0.1262769493: recall: 0.7041319503: f1: 0.2141490407
Train-Acc: 12: Accuracy: 0.9501281977: precision: 0.9231396383: recall: 0.8188153310: f1: 0.8678535345
13: 3232: loss: 0.1445269651:
13: 6432: loss: 0.1449986705:
13: 9632: loss: 0.1428857756:
13: 12832: loss: 0.1424708645:
13: 16032: loss: 0.1438301797:
13: 19232: loss: 0.1434660425:
13: 22432: loss: 0.1441787261:
13: 25632: loss: 0.1436119951:
13: 28832: loss: 0.1425127153:
13: 32032: loss: 0.1438523159:
13: 35232: loss: 0.1452545918:
13: 38432: loss: 0.1452440742:
13: 41632: loss: 0.1456227376:
13: 44832: loss: 0.1458281425:
13: 48032: loss: 0.1451317906:
13: 51232: loss: 0.1445908897:
13: 54432: loss: 0.1440959976:
13: 57632: loss: 0.1436555553:
13: 60832: loss: 0.1441281977:
13: 64032: loss: 0.1441915719:
13: 67232: loss: 0.1436277245:
13: 70432: loss: 0.1434334605:
13: 73632: loss: 0.1433102735:
Dev-Acc: 13: Accuracy: 0.6949118972: precision: 0.1248529101: recall: 0.7036218330: f1: 0.2120746207
Train-Acc: 13: Accuracy: 0.9516402483: precision: 0.9268635724: recall: 0.8231542962: f1: 0.8719359331
14: 3232: loss: 0.1461930106:
14: 6432: loss: 0.1367143824:
14: 9632: loss: 0.1334048608:
14: 12832: loss: 0.1339035096:
14: 16032: loss: 0.1350974593:
14: 19232: loss: 0.1369853495:
14: 22432: loss: 0.1388770744:
14: 25632: loss: 0.1401685781:
14: 28832: loss: 0.1401340381:
14: 32032: loss: 0.1404462065:
14: 35232: loss: 0.1395804827:
14: 38432: loss: 0.1395673543:
14: 41632: loss: 0.1399036121:
14: 44832: loss: 0.1394283525:
14: 48032: loss: 0.1398879045:
14: 51232: loss: 0.1405550202:
14: 54432: loss: 0.1412087438:
14: 57632: loss: 0.1412472432:
14: 60832: loss: 0.1410465838:
14: 64032: loss: 0.1402406322:
14: 67232: loss: 0.1400766682:
14: 70432: loss: 0.1399068488:
14: 73632: loss: 0.1395879294:
Dev-Acc: 14: Accuracy: 0.6919649839: precision: 0.1234287082: recall: 0.7012412855: f1: 0.2099101621
Train-Acc: 14: Accuracy: 0.9527710080: precision: 0.9327374302: recall: 0.8232200381: f1: 0.8745634865
15: 3232: loss: 0.1367190459:
15: 6432: loss: 0.1423313496:
15: 9632: loss: 0.1419093131:
15: 12832: loss: 0.1395645983:
15: 16032: loss: 0.1387112448:
15: 19232: loss: 0.1391165868:
15: 22432: loss: 0.1392837619:
15: 25632: loss: 0.1375365347:
15: 28832: loss: 0.1380043563:
15: 32032: loss: 0.1372324554:
15: 35232: loss: 0.1373896859:
15: 38432: loss: 0.1380064702:
15: 41632: loss: 0.1387213393:
15: 44832: loss: 0.1383991935:
15: 48032: loss: 0.1377174577:
15: 51232: loss: 0.1376330287:
15: 54432: loss: 0.1378157740:
15: 57632: loss: 0.1374139744:
15: 60832: loss: 0.1370341999:
15: 64032: loss: 0.1365960925:
15: 67232: loss: 0.1363720533:
15: 70432: loss: 0.1358661111:
15: 73632: loss: 0.1357430413:
Dev-Acc: 15: Accuracy: 0.6830945015: precision: 0.1214700755: recall: 0.7109335147: f1: 0.2074886479
Train-Acc: 15: Accuracy: 0.9539412260: precision: 0.9309481743: recall: 0.8313720334: f1: 0.8783469352
16: 3232: loss: 0.1296352751:
16: 6432: loss: 0.1329329122:
16: 9632: loss: 0.1359877825:
16: 12832: loss: 0.1378627227:
16: 16032: loss: 0.1358003779:
16: 19232: loss: 0.1346983247:
16: 22432: loss: 0.1353347122:
16: 25632: loss: 0.1364712188:
16: 28832: loss: 0.1352112519:
16: 32032: loss: 0.1354194377:
16: 35232: loss: 0.1353246730:
16: 38432: loss: 0.1355499073:
16: 41632: loss: 0.1349455500:
16: 44832: loss: 0.1340757656:
16: 48032: loss: 0.1350526422:
16: 51232: loss: 0.1345303699:
16: 54432: loss: 0.1341350850:
16: 57632: loss: 0.1339395581:
16: 60832: loss: 0.1329590371:
16: 64032: loss: 0.1324629053:
16: 67232: loss: 0.1322463560:
16: 70432: loss: 0.1325309065:
16: 73632: loss: 0.1327157523:
Dev-Acc: 16: Accuracy: 0.6787585020: precision: 0.1205568126: recall: 0.7156946098: f1: 0.2063538756
Train-Acc: 16: Accuracy: 0.9550194144: precision: 0.9314888011: recall: 0.8366313852: f1: 0.8815156028
17: 3232: loss: 0.1285463233:
17: 6432: loss: 0.1298095368:
17: 9632: loss: 0.1252273864:
17: 12832: loss: 0.1276100817:
17: 16032: loss: 0.1285203915:
17: 19232: loss: 0.1280456448:
17: 22432: loss: 0.1281089638:
17: 25632: loss: 0.1269572722:
17: 28832: loss: 0.1276914011:
17: 32032: loss: 0.1275997866:
17: 35232: loss: 0.1274337505:
17: 38432: loss: 0.1277901000:
17: 41632: loss: 0.1272901661:
17: 44832: loss: 0.1284593860:
17: 48032: loss: 0.1284792151:
17: 51232: loss: 0.1280931295:
17: 54432: loss: 0.1279651560:
17: 57632: loss: 0.1282830100:
17: 60832: loss: 0.1287645961:
17: 64032: loss: 0.1286802908:
17: 67232: loss: 0.1285592868:
17: 70432: loss: 0.1289128703:
17: 73632: loss: 0.1293455627:
Dev-Acc: 17: Accuracy: 0.6675761938: precision: 0.1183752418: recall: 0.7284475429: f1: 0.2036557248
Train-Acc: 17: Accuracy: 0.9562947750: precision: 0.9274361740: recall: 0.8478075077: f1: 0.8858359665
18: 3232: loss: 0.1286253198:
18: 6432: loss: 0.1305824836:
18: 9632: loss: 0.1327781848:
18: 12832: loss: 0.1286991491:
18: 16032: loss: 0.1272389342:
18: 19232: loss: 0.1263297548:
18: 22432: loss: 0.1252828009:
18: 25632: loss: 0.1261906448:
18: 28832: loss: 0.1261952414:
18: 32032: loss: 0.1260653955:
18: 35232: loss: 0.1266822812:
18: 38432: loss: 0.1262717269:
18: 41632: loss: 0.1262570156:
18: 44832: loss: 0.1263320796:
18: 48032: loss: 0.1266374462:
18: 51232: loss: 0.1265661236:
18: 54432: loss: 0.1270803816:
18: 57632: loss: 0.1272163745:
18: 60832: loss: 0.1266759209:
18: 64032: loss: 0.1263823576:
18: 67232: loss: 0.1262027967:
18: 70432: loss: 0.1263361920:
18: 73632: loss: 0.1260892408:
Dev-Acc: 18: Accuracy: 0.6632699370: precision: 0.1178906080: recall: 0.7359292637: f1: 0.2032258822
Train-Acc: 18: Accuracy: 0.9562290311: precision: 0.9224260523: recall: 0.8528696338: f1: 0.8862852263
19: 3232: loss: 0.1187196632:
19: 6432: loss: 0.1219597417:
19: 9632: loss: 0.1205783843:
19: 12832: loss: 0.1252443894:
19: 16032: loss: 0.1260289857:
19: 19232: loss: 0.1279476330:
19: 22432: loss: 0.1272078198:
19: 25632: loss: 0.1263476268:
19: 28832: loss: 0.1259629510:
19: 32032: loss: 0.1252032447:
19: 35232: loss: 0.1249167883:
19: 38432: loss: 0.1263890548:
19: 41632: loss: 0.1262055847:
19: 44832: loss: 0.1261091574:
19: 48032: loss: 0.1249070428:
19: 51232: loss: 0.1253898530:
19: 54432: loss: 0.1247908238:
19: 57632: loss: 0.1241723415:
19: 60832: loss: 0.1240175590:
19: 64032: loss: 0.1230748907:
19: 67232: loss: 0.1229490286:
19: 70432: loss: 0.1227611998:
19: 73632: loss: 0.1234230530:
Dev-Acc: 19: Accuracy: 0.6576043963: precision: 0.1165170797: recall: 0.7395000850: f1: 0.2013146322
Train-Acc: 19: Accuracy: 0.9574387074: precision: 0.9254548039: recall: 0.8561567287: f1: 0.8894580473
20: 3232: loss: 0.1128908349:
20: 6432: loss: 0.1186839528:
20: 9632: loss: 0.1218252515:
20: 12832: loss: 0.1215890847:
20: 16032: loss: 0.1218077249:
20: 19232: loss: 0.1208516457:
20: 22432: loss: 0.1228256296:
20: 25632: loss: 0.1213729103:
20: 28832: loss: 0.1200837622:
20: 32032: loss: 0.1196201900:
20: 35232: loss: 0.1193297430:
20: 38432: loss: 0.1197969958:
20: 41632: loss: 0.1201571499:
20: 44832: loss: 0.1205271826:
20: 48032: loss: 0.1207654411:
20: 51232: loss: 0.1207962585:
20: 54432: loss: 0.1209439472:
20: 57632: loss: 0.1211042683:
20: 60832: loss: 0.1212807112:
20: 64032: loss: 0.1211165221:
20: 67232: loss: 0.1206195029:
20: 70432: loss: 0.1203116246:
20: 73632: loss: 0.1200234371:
Dev-Acc: 20: Accuracy: 0.6576142907: precision: 0.1161499678: recall: 0.7364393811: f1: 0.2006532465
Train-Acc: 20: Accuracy: 0.9582012892: precision: 0.9306987400: recall: 0.8546446650: f1: 0.8910517838
