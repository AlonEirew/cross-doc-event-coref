1: 3232: loss: 0.7080008829:
1: 6432: loss: 0.7067370796:
1: 9632: loss: 0.7059941892:
1: 12832: loss: 0.7054001537:
1: 16032: loss: 0.7041897291:
1: 19232: loss: 0.7035516484:
1: 22432: loss: 0.7028332359:
1: 25632: loss: 0.7019185592:
1: 28832: loss: 0.7011548103:
1: 32032: loss: 0.7002539968:
1: 35232: loss: 0.6993748772:
1: 38432: loss: 0.6985942178:
1: 41632: loss: 0.6978656976:
1: 44832: loss: 0.6970326750:
1: 48032: loss: 0.6962567831:
1: 51232: loss: 0.6955720587:
1: 54432: loss: 0.6947807602:
1: 57632: loss: 0.6940289283:
1: 60832: loss: 0.6932546615:
1: 64032: loss: 0.6924656979:
1: 67232: loss: 0.6916678509:
1: 70432: loss: 0.6909439143:
1: 73632: loss: 0.6901980859:
1: 76832: loss: 0.6894049575:
1: 80032: loss: 0.6885904900:
1: 83232: loss: 0.6878553511:
1: 86432: loss: 0.6870985361:
1: 89632: loss: 0.6863708160:
1: 92832: loss: 0.6855749842:
1: 96032: loss: 0.6848600149:
1: 99232: loss: 0.6840841697:
1: 102432: loss: 0.6832790073:
1: 105632: loss: 0.6825314937:
1: 108832: loss: 0.6817884330:
1: 112032: loss: 0.6810045339:
1: 115232: loss: 0.6802265528:
1: 118432: loss: 0.6795180203:
1: 121632: loss: 0.6787626786:
Dev-Acc: 1: Accuracy: 0.9044888020: precision: 0.0935532885: recall: 0.0732868560: f1: 0.0821891686
Train-Acc: 1: Accuracy: 0.8589343429: precision: 0.3094170404: recall: 0.1043323910: f1: 0.1560471976
2: 3232: loss: 0.6500315952:
2: 6432: loss: 0.6489159024:
2: 9632: loss: 0.6485162914:
2: 12832: loss: 0.6476318814:
2: 16032: loss: 0.6468442281:
2: 19232: loss: 0.6459058429:
2: 22432: loss: 0.6451642331:
2: 25632: loss: 0.6445634463:
2: 28832: loss: 0.6437380003:
2: 32032: loss: 0.6430359756:
2: 35232: loss: 0.6421914347:
2: 38432: loss: 0.6414654587:
2: 41632: loss: 0.6407732251:
2: 44832: loss: 0.6398559637:
2: 48032: loss: 0.6392882407:
2: 51232: loss: 0.6384796445:
2: 54432: loss: 0.6376311725:
2: 57632: loss: 0.6367940780:
2: 60832: loss: 0.6361214715:
2: 64032: loss: 0.6353950460:
2: 67232: loss: 0.6345424520:
2: 70432: loss: 0.6339211500:
2: 73632: loss: 0.6332923159:
2: 76832: loss: 0.6326556078:
2: 80032: loss: 0.6320311865:
2: 83232: loss: 0.6314198004:
2: 86432: loss: 0.6308273958:
2: 89632: loss: 0.6301224929:
2: 92832: loss: 0.6293644729:
2: 96032: loss: 0.6286267787:
2: 99232: loss: 0.6279315456:
2: 102432: loss: 0.6272816748:
2: 105632: loss: 0.6266210024:
2: 108832: loss: 0.6259128416:
2: 112032: loss: 0.6253147221:
2: 115232: loss: 0.6245286190:
2: 118432: loss: 0.6239393896:
2: 121632: loss: 0.6232557120:
Dev-Acc: 2: Accuracy: 0.9415978789: precision: 0.1428571429: recall: 0.0001700391: f1: 0.0003396739
Train-Acc: 2: Accuracy: 0.8750411272: precision: 0.5757575758: recall: 0.0012490960: f1: 0.0024927840
3: 3232: loss: 0.5974996561:
3: 6432: loss: 0.5964575461:
3: 9632: loss: 0.5960693381:
3: 12832: loss: 0.5952170998:
3: 16032: loss: 0.5951915849:
3: 19232: loss: 0.5943828224:
3: 22432: loss: 0.5935312325:
3: 25632: loss: 0.5930022989:
3: 28832: loss: 0.5927036071:
3: 32032: loss: 0.5919678163:
3: 35232: loss: 0.5905762988:
3: 38432: loss: 0.5898363585:
3: 41632: loss: 0.5890758597:
3: 44832: loss: 0.5884172965:
3: 48032: loss: 0.5879160277:
3: 51232: loss: 0.5872686038:
3: 54432: loss: 0.5866374687:
3: 57632: loss: 0.5863008776:
3: 60832: loss: 0.5854409270:
3: 64032: loss: 0.5849938126:
3: 67232: loss: 0.5842748285:
3: 70432: loss: 0.5835070771:
3: 73632: loss: 0.5828098477:
3: 76832: loss: 0.5820613427:
3: 80032: loss: 0.5815043240:
3: 83232: loss: 0.5808154853:
3: 86432: loss: 0.5800832364:
3: 89632: loss: 0.5795360059:
3: 92832: loss: 0.5788284910:
3: 96032: loss: 0.5780822537:
3: 99232: loss: 0.5773602703:
3: 102432: loss: 0.5768198574:
3: 105632: loss: 0.5761046330:
3: 108832: loss: 0.5754465481:
3: 112032: loss: 0.5748180506:
3: 115232: loss: 0.5739774148:
3: 118432: loss: 0.5733215145:
3: 121632: loss: 0.5726793564:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.5477189240:
4: 6432: loss: 0.5475534002:
4: 9632: loss: 0.5470884130:
4: 12832: loss: 0.5465959034:
4: 16032: loss: 0.5462219269:
4: 19232: loss: 0.5449793876:
4: 22432: loss: 0.5444463368:
4: 25632: loss: 0.5439175465:
4: 28832: loss: 0.5435102072:
4: 32032: loss: 0.5430197700:
4: 35232: loss: 0.5425892184:
4: 38432: loss: 0.5422069644:
4: 41632: loss: 0.5412506672:
4: 44832: loss: 0.5405133170:
4: 48032: loss: 0.5398519861:
4: 51232: loss: 0.5394473654:
4: 54432: loss: 0.5387604448:
4: 57632: loss: 0.5380401005:
4: 60832: loss: 0.5373411122:
4: 64032: loss: 0.5368433174:
4: 67232: loss: 0.5362800609:
4: 70432: loss: 0.5357560499:
4: 73632: loss: 0.5348846960:
4: 76832: loss: 0.5344418181:
4: 80032: loss: 0.5336821803:
4: 83232: loss: 0.5332175153:
4: 86432: loss: 0.5326285464:
4: 89632: loss: 0.5322058689:
4: 92832: loss: 0.5315407149:
4: 96032: loss: 0.5309987117:
4: 99232: loss: 0.5306489232:
4: 102432: loss: 0.5299773120:
4: 105632: loss: 0.5291895024:
4: 108832: loss: 0.5286524983:
4: 112032: loss: 0.5280140656:
4: 115232: loss: 0.5272628152:
4: 118432: loss: 0.5268533355:
4: 121632: loss: 0.5263382227:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.5004494727:
5: 6432: loss: 0.5013572715:
5: 9632: loss: 0.5006152615:
5: 12832: loss: 0.5008632462:
5: 16032: loss: 0.5008214021:
5: 19232: loss: 0.5012656817:
5: 22432: loss: 0.5005599068:
5: 25632: loss: 0.4998062678:
5: 28832: loss: 0.4993159818:
5: 32032: loss: 0.4984034475:
5: 35232: loss: 0.4980479583:
5: 38432: loss: 0.4980650512:
5: 41632: loss: 0.4969978697:
5: 44832: loss: 0.4962611236:
5: 48032: loss: 0.4955149990:
5: 51232: loss: 0.4952341493:
5: 54432: loss: 0.4947010340:
5: 57632: loss: 0.4942879293:
5: 60832: loss: 0.4944096751:
5: 64032: loss: 0.4939631341:
5: 67232: loss: 0.4933971573:
5: 70432: loss: 0.4929821028:
5: 73632: loss: 0.4924060279:
5: 76832: loss: 0.4920885593:
5: 80032: loss: 0.4914823535:
5: 83232: loss: 0.4908344859:
5: 86432: loss: 0.4902159325:
5: 89632: loss: 0.4896237012:
5: 92832: loss: 0.4892700522:
5: 96032: loss: 0.4885386377:
5: 99232: loss: 0.4880861277:
5: 102432: loss: 0.4876638304:
5: 105632: loss: 0.4871536574:
5: 108832: loss: 0.4866716310:
5: 112032: loss: 0.4863295395:
5: 115232: loss: 0.4857739660:
5: 118432: loss: 0.4852839496:
5: 121632: loss: 0.4846314105:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.4657553783:
6: 6432: loss: 0.4676317115:
6: 9632: loss: 0.4662195057:
6: 12832: loss: 0.4643509446:
6: 16032: loss: 0.4626548199:
6: 19232: loss: 0.4621973169:
6: 22432: loss: 0.4617903212:
6: 25632: loss: 0.4623983758:
6: 28832: loss: 0.4624851588:
6: 32032: loss: 0.4618244720:
6: 35232: loss: 0.4616202809:
6: 38432: loss: 0.4608951113:
6: 41632: loss: 0.4602559238:
6: 44832: loss: 0.4594196634:
6: 48032: loss: 0.4592583213:
6: 51232: loss: 0.4591850041:
6: 54432: loss: 0.4582125650:
6: 57632: loss: 0.4571647793:
6: 60832: loss: 0.4568879615:
6: 64032: loss: 0.4563639925:
6: 67232: loss: 0.4557980359:
6: 70432: loss: 0.4551712666:
6: 73632: loss: 0.4548425908:
6: 76832: loss: 0.4540906577:
6: 80032: loss: 0.4541040048:
6: 83232: loss: 0.4536131129:
6: 86432: loss: 0.4529778379:
6: 89632: loss: 0.4527170681:
6: 92832: loss: 0.4523199082:
6: 96032: loss: 0.4519395883:
6: 99232: loss: 0.4515509969:
6: 102432: loss: 0.4511429297:
6: 105632: loss: 0.4504587751:
6: 108832: loss: 0.4498755993:
6: 112032: loss: 0.4494297889:
6: 115232: loss: 0.4491904403:
6: 118432: loss: 0.4487013577:
6: 121632: loss: 0.4480436282:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.4388877037:
7: 6432: loss: 0.4302075805:
7: 9632: loss: 0.4303661587:
7: 12832: loss: 0.4300377794:
7: 16032: loss: 0.4314824260:
7: 19232: loss: 0.4305281638:
7: 22432: loss: 0.4301006793:
7: 25632: loss: 0.4287410064:
7: 28832: loss: 0.4285541023:
7: 32032: loss: 0.4286567408:
7: 35232: loss: 0.4275362873:
7: 38432: loss: 0.4269494814:
7: 41632: loss: 0.4270675290:
7: 44832: loss: 0.4265626109:
7: 48032: loss: 0.4267983235:
7: 51232: loss: 0.4264000323:
7: 54432: loss: 0.4255943420:
7: 57632: loss: 0.4253209476:
7: 60832: loss: 0.4245749616:
7: 64032: loss: 0.4242953306:
7: 67232: loss: 0.4240589025:
7: 70432: loss: 0.4237700954:
7: 73632: loss: 0.4231466688:
7: 76832: loss: 0.4228617108:
7: 80032: loss: 0.4224586051:
7: 83232: loss: 0.4225716379:
7: 86432: loss: 0.4217912379:
7: 89632: loss: 0.4214609010:
7: 92832: loss: 0.4212005186:
7: 96032: loss: 0.4208274836:
7: 99232: loss: 0.4203019407:
7: 102432: loss: 0.4199227289:
7: 105632: loss: 0.4194924242:
7: 108832: loss: 0.4189179876:
7: 112032: loss: 0.4180500599:
7: 115232: loss: 0.4177453885:
7: 118432: loss: 0.4172059188:
7: 121632: loss: 0.4169586450:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.4027174580:
8: 6432: loss: 0.4028008795:
8: 9632: loss: 0.4037794051:
8: 12832: loss: 0.4034007939:
8: 16032: loss: 0.4038825932:
8: 19232: loss: 0.4043494302:
8: 22432: loss: 0.4030186392:
8: 25632: loss: 0.4025913142:
8: 28832: loss: 0.4031274087:
8: 32032: loss: 0.4021293499:
8: 35232: loss: 0.4013869852:
8: 38432: loss: 0.4013067636:
8: 41632: loss: 0.3999616398:
8: 44832: loss: 0.3999672830:
8: 48032: loss: 0.4000751480:
8: 51232: loss: 0.3995608070:
8: 54432: loss: 0.3994719319:
8: 57632: loss: 0.3980981409:
8: 60832: loss: 0.3980066945:
8: 64032: loss: 0.3975071888:
8: 67232: loss: 0.3963833352:
8: 70432: loss: 0.3964571900:
8: 73632: loss: 0.3959742896:
8: 76832: loss: 0.3953355710:
8: 80032: loss: 0.3952660079:
8: 83232: loss: 0.3950402969:
8: 86432: loss: 0.3943047922:
8: 89632: loss: 0.3937467209:
8: 92832: loss: 0.3938491904:
8: 96032: loss: 0.3935006934:
8: 99232: loss: 0.3931961380:
8: 102432: loss: 0.3932201637:
8: 105632: loss: 0.3927939936:
8: 108832: loss: 0.3923120766:
8: 112032: loss: 0.3921879078:
8: 115232: loss: 0.3920439465:
8: 118432: loss: 0.3918707383:
8: 121632: loss: 0.3915758011:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3801117969:
9: 6432: loss: 0.3758844776:
9: 9632: loss: 0.3763032397:
9: 12832: loss: 0.3783144629:
9: 16032: loss: 0.3771481662:
9: 19232: loss: 0.3760927546:
9: 22432: loss: 0.3761675124:
9: 25632: loss: 0.3768570413:
9: 28832: loss: 0.3776409992:
9: 32032: loss: 0.3766802871:
9: 35232: loss: 0.3753791233:
9: 38432: loss: 0.3760714016:
9: 41632: loss: 0.3766497965:
9: 44832: loss: 0.3763868620:
9: 48032: loss: 0.3756119710:
9: 51232: loss: 0.3756769989:
9: 54432: loss: 0.3753481199:
9: 57632: loss: 0.3755811637:
9: 60832: loss: 0.3752677718:
9: 64032: loss: 0.3760737343:
9: 67232: loss: 0.3758262224:
9: 70432: loss: 0.3755664755:
9: 73632: loss: 0.3754684730:
9: 76832: loss: 0.3752765504:
9: 80032: loss: 0.3748433001:
9: 83232: loss: 0.3743185721:
9: 86432: loss: 0.3744169015:
9: 89632: loss: 0.3736230256:
9: 92832: loss: 0.3736580842:
9: 96032: loss: 0.3731557480:
9: 99232: loss: 0.3726243568:
9: 102432: loss: 0.3721936502:
9: 105632: loss: 0.3719196591:
9: 108832: loss: 0.3717055652:
9: 112032: loss: 0.3715087455:
9: 115232: loss: 0.3715980807:
9: 118432: loss: 0.3711369357:
9: 121632: loss: 0.3709459782:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.3688473141:
10: 6432: loss: 0.3639612756:
10: 9632: loss: 0.3643459034:
10: 12832: loss: 0.3670163941:
10: 16032: loss: 0.3627119857:
10: 19232: loss: 0.3625517548:
10: 22432: loss: 0.3622293267:
10: 25632: loss: 0.3622915857:
10: 28832: loss: 0.3614240991:
10: 32032: loss: 0.3595288193:
10: 35232: loss: 0.3586772833:
10: 38432: loss: 0.3576285404:
10: 41632: loss: 0.3575672228:
10: 44832: loss: 0.3575216049:
10: 48032: loss: 0.3576193447:
10: 51232: loss: 0.3577498939:
10: 54432: loss: 0.3580185899:
10: 57632: loss: 0.3581160333:
10: 60832: loss: 0.3582466130:
10: 64032: loss: 0.3586828214:
10: 67232: loss: 0.3585674957:
10: 70432: loss: 0.3582989498:
10: 73632: loss: 0.3577578372:
10: 76832: loss: 0.3575239591:
10: 80032: loss: 0.3575135519:
10: 83232: loss: 0.3572897966:
10: 86432: loss: 0.3570886774:
10: 89632: loss: 0.3570394659:
10: 92832: loss: 0.3565384069:
10: 96032: loss: 0.3561065724:
10: 99232: loss: 0.3559399535:
10: 102432: loss: 0.3557581841:
10: 105632: loss: 0.3555265042:
10: 108832: loss: 0.3554014456:
10: 112032: loss: 0.3548910737:
10: 115232: loss: 0.3544239259:
10: 118432: loss: 0.3543708968:
10: 121632: loss: 0.3541785628:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.3537096782:
11: 6432: loss: 0.3452700478:
11: 9632: loss: 0.3460805122:
11: 12832: loss: 0.3450251526:
11: 16032: loss: 0.3466694195:
11: 19232: loss: 0.3459463515:
11: 22432: loss: 0.3448984093:
11: 25632: loss: 0.3443451882:
11: 28832: loss: 0.3445606887:
11: 32032: loss: 0.3429716795:
11: 35232: loss: 0.3428758175:
11: 38432: loss: 0.3434931951:
11: 41632: loss: 0.3432724726:
11: 44832: loss: 0.3432131776:
11: 48032: loss: 0.3431173786:
11: 51232: loss: 0.3441506132:
11: 54432: loss: 0.3440235485:
11: 57632: loss: 0.3444304057:
11: 60832: loss: 0.3437341280:
11: 64032: loss: 0.3435242090:
11: 67232: loss: 0.3438316744:
11: 70432: loss: 0.3436005451:
11: 73632: loss: 0.3431010363:
11: 76832: loss: 0.3426036499:
11: 80032: loss: 0.3420660638:
11: 83232: loss: 0.3419861057:
11: 86432: loss: 0.3417958601:
11: 89632: loss: 0.3411938091:
11: 92832: loss: 0.3411242026:
11: 96032: loss: 0.3412660094:
11: 99232: loss: 0.3413031945:
11: 102432: loss: 0.3406878951:
11: 105632: loss: 0.3404158225:
11: 108832: loss: 0.3403233777:
11: 112032: loss: 0.3400964999:
11: 115232: loss: 0.3397620718:
11: 118432: loss: 0.3399670914:
11: 121632: loss: 0.3399469764:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.3300218697:
12: 6432: loss: 0.3319852947:
12: 9632: loss: 0.3310701713:
12: 12832: loss: 0.3293474686:
12: 16032: loss: 0.3309043393:
12: 19232: loss: 0.3317950498:
12: 22432: loss: 0.3318340097:
12: 25632: loss: 0.3333913226:
12: 28832: loss: 0.3324469954:
12: 32032: loss: 0.3314508267:
12: 35232: loss: 0.3306865649:
12: 38432: loss: 0.3313280681:
12: 41632: loss: 0.3320869786:
12: 44832: loss: 0.3312545147:
12: 48032: loss: 0.3310965987:
12: 51232: loss: 0.3310451784:
12: 54432: loss: 0.3302280893:
12: 57632: loss: 0.3302221281:
12: 60832: loss: 0.3294702292:
12: 64032: loss: 0.3295562479:
12: 67232: loss: 0.3299165832:
12: 70432: loss: 0.3300884633:
12: 73632: loss: 0.3296625230:
12: 76832: loss: 0.3294591282:
12: 80032: loss: 0.3290781328:
12: 83232: loss: 0.3291626461:
12: 86432: loss: 0.3287087759:
12: 89632: loss: 0.3290778638:
12: 92832: loss: 0.3290988963:
12: 96032: loss: 0.3295830473:
12: 99232: loss: 0.3290517174:
12: 102432: loss: 0.3287959557:
12: 105632: loss: 0.3286430689:
12: 108832: loss: 0.3282535156:
12: 112032: loss: 0.3284997118:
12: 115232: loss: 0.3284489697:
12: 118432: loss: 0.3279110989:
12: 121632: loss: 0.3277945292:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.3262641540:
13: 6432: loss: 0.3240449121:
13: 9632: loss: 0.3187136309:
13: 12832: loss: 0.3210883698:
13: 16032: loss: 0.3192463502:
13: 19232: loss: 0.3203177624:
13: 22432: loss: 0.3187693319:
13: 25632: loss: 0.3217778200:
13: 28832: loss: 0.3233838893:
13: 32032: loss: 0.3221422814:
13: 35232: loss: 0.3210730511:
13: 38432: loss: 0.3203472658:
13: 41632: loss: 0.3211466619:
13: 44832: loss: 0.3201978111:
13: 48032: loss: 0.3194170777:
13: 51232: loss: 0.3191134801:
13: 54432: loss: 0.3190171566:
13: 57632: loss: 0.3188414793:
13: 60832: loss: 0.3188756779:
13: 64032: loss: 0.3185024945:
13: 67232: loss: 0.3183973703:
13: 70432: loss: 0.3176420368:
13: 73632: loss: 0.3176843507:
13: 76832: loss: 0.3175034529:
13: 80032: loss: 0.3180243360:
13: 83232: loss: 0.3179310978:
13: 86432: loss: 0.3182403688:
13: 89632: loss: 0.3182532106:
13: 92832: loss: 0.3180810096:
13: 96032: loss: 0.3177788387:
13: 99232: loss: 0.3176272785:
13: 102432: loss: 0.3174916749:
13: 105632: loss: 0.3177632504:
13: 108832: loss: 0.3175814195:
13: 112032: loss: 0.3174178228:
13: 115232: loss: 0.3172710356:
13: 118432: loss: 0.3170892004:
13: 121632: loss: 0.3170562884:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.8754191399: precision: 1.0000000000: recall: 0.0033528368: f1: 0.0066832656
14: 3232: loss: 0.3061810784:
14: 6432: loss: 0.3066966571:
14: 9632: loss: 0.3082650611:
14: 12832: loss: 0.3086553153:
14: 16032: loss: 0.3089452281:
14: 19232: loss: 0.3104877363:
14: 22432: loss: 0.3110012812:
14: 25632: loss: 0.3125384632:
14: 28832: loss: 0.3118524317:
14: 32032: loss: 0.3117658080:
14: 35232: loss: 0.3113943462:
14: 38432: loss: 0.3121233647:
14: 41632: loss: 0.3109832541:
14: 44832: loss: 0.3118473107:
14: 48032: loss: 0.3123410880:
14: 51232: loss: 0.3115987872:
14: 54432: loss: 0.3116331757:
14: 57632: loss: 0.3109685639:
14: 60832: loss: 0.3109651331:
14: 64032: loss: 0.3110181797:
14: 67232: loss: 0.3104370369:
14: 70432: loss: 0.3108094201:
14: 73632: loss: 0.3114639635:
14: 76832: loss: 0.3111062105:
14: 80032: loss: 0.3106760267:
14: 83232: loss: 0.3104257124:
14: 86432: loss: 0.3100186448:
14: 89632: loss: 0.3096649504:
14: 92832: loss: 0.3094580494:
14: 96032: loss: 0.3093619835:
14: 99232: loss: 0.3093647410:
14: 102432: loss: 0.3091500870:
14: 105632: loss: 0.3093135462:
14: 108832: loss: 0.3087682662:
14: 112032: loss: 0.3091525432:
14: 115232: loss: 0.3087000843:
14: 118432: loss: 0.3085510912:
14: 121632: loss: 0.3081538783:
Dev-Acc: 14: Accuracy: 0.9416871667: precision: 0.5588235294: recall: 0.0032307431: f1: 0.0064243449
Train-Acc: 14: Accuracy: 0.8772763610: precision: 0.8386308068: recall: 0.0225494708: f1: 0.0439180538
15: 3232: loss: 0.3170884232:
15: 6432: loss: 0.3108823457:
15: 9632: loss: 0.3083938187:
15: 12832: loss: 0.3068106234:
15: 16032: loss: 0.3044681279:
15: 19232: loss: 0.3046427896:
15: 22432: loss: 0.3060493375:
15: 25632: loss: 0.3068737416:
15: 28832: loss: 0.3062436446:
15: 32032: loss: 0.3050598511:
15: 35232: loss: 0.3041922110:
15: 38432: loss: 0.3053450579:
15: 41632: loss: 0.3042751226:
15: 44832: loss: 0.3047245529:
15: 48032: loss: 0.3037760328:
15: 51232: loss: 0.3038174127:
15: 54432: loss: 0.3037227159:
15: 57632: loss: 0.3036424343:
15: 60832: loss: 0.3033944727:
15: 64032: loss: 0.3026802285:
15: 67232: loss: 0.3027047259:
15: 70432: loss: 0.3027081268:
15: 73632: loss: 0.3024180218:
15: 76832: loss: 0.3022368444:
15: 80032: loss: 0.3023633876:
15: 83232: loss: 0.3010765114:
15: 86432: loss: 0.3007532384:
15: 89632: loss: 0.3009426620:
15: 92832: loss: 0.3010444884:
15: 96032: loss: 0.3007063823:
15: 99232: loss: 0.3000491895:
15: 102432: loss: 0.3002046184:
15: 105632: loss: 0.3000203627:
15: 108832: loss: 0.2999432302:
15: 112032: loss: 0.2996293936:
15: 115232: loss: 0.2993992215:
15: 118432: loss: 0.2993266513:
15: 121632: loss: 0.2994024481:
Dev-Acc: 15: Accuracy: 0.9423320889: precision: 0.6949152542: recall: 0.0209148104: f1: 0.0406074612
Train-Acc: 15: Accuracy: 0.8787062168: precision: 0.8263386397: recall: 0.0375386234: f1: 0.0718148661
16: 3232: loss: 0.3117953835:
16: 6432: loss: 0.3053474104:
16: 9632: loss: 0.3033000121:
16: 12832: loss: 0.3016415134:
16: 16032: loss: 0.3023219896:
16: 19232: loss: 0.3011047746:
16: 22432: loss: 0.2964201615:
16: 25632: loss: 0.2956210459:
16: 28832: loss: 0.2952596190:
16: 32032: loss: 0.2955590629:
16: 35232: loss: 0.2960096185:
16: 38432: loss: 0.2947499424:
16: 41632: loss: 0.2961860303:
16: 44832: loss: 0.2959781822:
16: 48032: loss: 0.2957835439:
16: 51232: loss: 0.2953474838:
16: 54432: loss: 0.2953420922:
16: 57632: loss: 0.2950380664:
16: 60832: loss: 0.2950429186:
16: 64032: loss: 0.2956595946:
16: 67232: loss: 0.2952987648:
16: 70432: loss: 0.2947729642:
16: 73632: loss: 0.2945741079:
16: 76832: loss: 0.2951296618:
16: 80032: loss: 0.2946181541:
16: 83232: loss: 0.2945018401:
16: 86432: loss: 0.2941369704:
16: 89632: loss: 0.2938821223:
16: 92832: loss: 0.2936567652:
16: 96032: loss: 0.2934270706:
16: 99232: loss: 0.2932734485:
16: 102432: loss: 0.2935405184:
16: 105632: loss: 0.2932891370:
16: 108832: loss: 0.2929130719:
16: 112032: loss: 0.2926676659:
16: 115232: loss: 0.2923987063:
16: 118432: loss: 0.2923361917:
16: 121632: loss: 0.2919518732:
Dev-Acc: 16: Accuracy: 0.9423916340: precision: 0.6080691643: recall: 0.0358782520: f1: 0.0677585100
Train-Acc: 16: Accuracy: 0.8814838529: precision: 0.8583106267: recall: 0.0621260930: f1: 0.1158656204
17: 3232: loss: 0.2945330848:
17: 6432: loss: 0.2870306778:
17: 9632: loss: 0.2853937437:
17: 12832: loss: 0.2894052524:
17: 16032: loss: 0.2876973585:
17: 19232: loss: 0.2898880144:
17: 22432: loss: 0.2910531221:
17: 25632: loss: 0.2907952348:
17: 28832: loss: 0.2895444699:
17: 32032: loss: 0.2890398624:
17: 35232: loss: 0.2902617192:
17: 38432: loss: 0.2893788553:
17: 41632: loss: 0.2881949549:
17: 44832: loss: 0.2885652494:
17: 48032: loss: 0.2883184082:
17: 51232: loss: 0.2880199395:
17: 54432: loss: 0.2887719403:
17: 57632: loss: 0.2893819432:
17: 60832: loss: 0.2891684904:
17: 64032: loss: 0.2885271552:
17: 67232: loss: 0.2881727822:
17: 70432: loss: 0.2877078268:
17: 73632: loss: 0.2871892793:
17: 76832: loss: 0.2871049927:
17: 80032: loss: 0.2871493537:
17: 83232: loss: 0.2878504802:
17: 86432: loss: 0.2870640506:
17: 89632: loss: 0.2871410472:
17: 92832: loss: 0.2871964073:
17: 96032: loss: 0.2872116817:
17: 99232: loss: 0.2866909196:
17: 102432: loss: 0.2862163937:
17: 105632: loss: 0.2858536460:
17: 108832: loss: 0.2855463381:
17: 112032: loss: 0.2857661054:
17: 115232: loss: 0.2856935678:
17: 118432: loss: 0.2854225915:
17: 121632: loss: 0.2852253044:
Dev-Acc: 17: Accuracy: 0.9422824979: precision: 0.5422163588: recall: 0.0698860738: f1: 0.1238138274
Train-Acc: 17: Accuracy: 0.8851736188: precision: 0.8711031175: recall: 0.0955229768: f1: 0.1721665975
18: 3232: loss: 0.2943151523:
18: 6432: loss: 0.2838540091:
18: 9632: loss: 0.2776072452:
18: 12832: loss: 0.2829873931:
18: 16032: loss: 0.2803637921:
18: 19232: loss: 0.2810313896:
18: 22432: loss: 0.2811525888:
18: 25632: loss: 0.2819835668:
18: 28832: loss: 0.2820038502:
18: 32032: loss: 0.2830561154:
18: 35232: loss: 0.2835119828:
18: 38432: loss: 0.2837524959:
18: 41632: loss: 0.2837130342:
18: 44832: loss: 0.2834077124:
18: 48032: loss: 0.2832498040:
18: 51232: loss: 0.2825335399:
18: 54432: loss: 0.2818047503:
18: 57632: loss: 0.2822821067:
18: 60832: loss: 0.2813993514:
18: 64032: loss: 0.2807134596:
18: 67232: loss: 0.2811277647:
18: 70432: loss: 0.2809959600:
18: 73632: loss: 0.2811818178:
18: 76832: loss: 0.2813367637:
18: 80032: loss: 0.2811538359:
18: 83232: loss: 0.2805382038:
18: 86432: loss: 0.2802321822:
18: 89632: loss: 0.2801781737:
18: 92832: loss: 0.2794765954:
18: 96032: loss: 0.2789941874:
18: 99232: loss: 0.2789486684:
18: 102432: loss: 0.2787075366:
18: 105632: loss: 0.2787267230:
18: 108832: loss: 0.2786415261:
18: 112032: loss: 0.2789824959:
18: 115232: loss: 0.2793251572:
18: 118432: loss: 0.2794410609:
18: 121632: loss: 0.2791631522:
Dev-Acc: 18: Accuracy: 0.9421634078: precision: 0.5243445693: recall: 0.0952219010: f1: 0.1611742697
Train-Acc: 18: Accuracy: 0.8880497813: precision: 0.8832046332: recall: 0.1203076721: f1: 0.2117687901
19: 3232: loss: 0.2695118677:
19: 6432: loss: 0.2653431945:
19: 9632: loss: 0.2703623015:
19: 12832: loss: 0.2713702206:
19: 16032: loss: 0.2732369334:
19: 19232: loss: 0.2726683140:
19: 22432: loss: 0.2732602347:
19: 25632: loss: 0.2734142571:
19: 28832: loss: 0.2732850748:
19: 32032: loss: 0.2751398822:
19: 35232: loss: 0.2742331423:
19: 38432: loss: 0.2732738695:
19: 41632: loss: 0.2727382872:
19: 44832: loss: 0.2725324839:
19: 48032: loss: 0.2726720331:
19: 51232: loss: 0.2722590558:
19: 54432: loss: 0.2717183451:
19: 57632: loss: 0.2727915083:
19: 60832: loss: 0.2726333624:
19: 64032: loss: 0.2732213630:
19: 67232: loss: 0.2738301156:
19: 70432: loss: 0.2732743960:
19: 73632: loss: 0.2736656639:
19: 76832: loss: 0.2736779116:
19: 80032: loss: 0.2733383808:
19: 83232: loss: 0.2737204604:
19: 86432: loss: 0.2735789328:
19: 89632: loss: 0.2740037740:
19: 92832: loss: 0.2736904608:
19: 96032: loss: 0.2736931787:
19: 99232: loss: 0.2735897148:
19: 102432: loss: 0.2741664229:
19: 105632: loss: 0.2735853446:
19: 108832: loss: 0.2732145051:
19: 112032: loss: 0.2732518762:
19: 115232: loss: 0.2736334441:
19: 118432: loss: 0.2730191333:
19: 121632: loss: 0.2729975613:
Dev-Acc: 19: Accuracy: 0.9434533119: precision: 0.5636363636: recall: 0.1370515219: f1: 0.2204896731
Train-Acc: 19: Accuracy: 0.8924462795: precision: 0.9010200227: recall: 0.1567944251: f1: 0.2671071789
20: 3232: loss: 0.2794553269:
20: 6432: loss: 0.2787742614:
20: 9632: loss: 0.2794547601:
20: 12832: loss: 0.2769827006:
20: 16032: loss: 0.2738631573:
20: 19232: loss: 0.2743426689:
20: 22432: loss: 0.2734929547:
20: 25632: loss: 0.2733254334:
20: 28832: loss: 0.2748639985:
20: 32032: loss: 0.2738134489:
20: 35232: loss: 0.2745286447:
20: 38432: loss: 0.2726713276:
20: 41632: loss: 0.2718740697:
20: 44832: loss: 0.2709616279:
20: 48032: loss: 0.2710061971:
20: 51232: loss: 0.2705240129:
20: 54432: loss: 0.2711449408:
20: 57632: loss: 0.2718551311:
20: 60832: loss: 0.2717133229:
20: 64032: loss: 0.2715072249:
20: 67232: loss: 0.2719711945:
20: 70432: loss: 0.2719538188:
20: 73632: loss: 0.2722030267:
20: 76832: loss: 0.2722329828:
20: 80032: loss: 0.2715021296:
20: 83232: loss: 0.2708068454:
20: 86432: loss: 0.2708477415:
20: 89632: loss: 0.2709808506:
20: 92832: loss: 0.2705716437:
20: 96032: loss: 0.2704580893:
20: 99232: loss: 0.2706168305:
20: 102432: loss: 0.2701688990:
20: 105632: loss: 0.2699785205:
20: 108832: loss: 0.2692550795:
20: 112032: loss: 0.2691845194:
20: 115232: loss: 0.2686783523:
20: 118432: loss: 0.2677727869:
20: 121632: loss: 0.2679990967:
Dev-Acc: 20: Accuracy: 0.9429274201: precision: 0.5321695761: recall: 0.1814317293: f1: 0.2706061375
Train-Acc: 20: Accuracy: 0.8958237767: precision: 0.8869883934: recall: 0.1909144698: f1: 0.3142007033
21: 3232: loss: 0.2706251508:
21: 6432: loss: 0.2692074570:
21: 9632: loss: 0.2664855426:
21: 12832: loss: 0.2642497853:
21: 16032: loss: 0.2631985538:
21: 19232: loss: 0.2640684089:
21: 22432: loss: 0.2638617796:
21: 25632: loss: 0.2637047456:
21: 28832: loss: 0.2628430233:
21: 32032: loss: 0.2621929563:
21: 35232: loss: 0.2624121588:
21: 38432: loss: 0.2620091783:
21: 41632: loss: 0.2625105236:
21: 44832: loss: 0.2623471681:
21: 48032: loss: 0.2641417030:
21: 51232: loss: 0.2634849928:
21: 54432: loss: 0.2638458081:
21: 57632: loss: 0.2645253307:
21: 60832: loss: 0.2650661169:
21: 64032: loss: 0.2653106183:
21: 67232: loss: 0.2646469878:
21: 70432: loss: 0.2642977210:
21: 73632: loss: 0.2641625466:
21: 76832: loss: 0.2636186458:
21: 80032: loss: 0.2634582728:
21: 83232: loss: 0.2635690530:
21: 86432: loss: 0.2633926750:
21: 89632: loss: 0.2635328293:
21: 92832: loss: 0.2634824262:
21: 96032: loss: 0.2631250254:
21: 99232: loss: 0.2630390589:
21: 102432: loss: 0.2632169653:
21: 105632: loss: 0.2634226328:
21: 108832: loss: 0.2631645087:
21: 112032: loss: 0.2632524818:
21: 115232: loss: 0.2635805766:
21: 118432: loss: 0.2635203019:
21: 121632: loss: 0.2634360776:
Dev-Acc: 21: Accuracy: 0.9398713708: precision: 0.4721444133: recall: 0.2579493283: f1: 0.3336265670
Train-Acc: 21: Accuracy: 0.8997272253: precision: 0.8507810678: recall: 0.2398921833: f1: 0.3742564103
22: 3232: loss: 0.2406039921:
22: 6432: loss: 0.2516571647:
22: 9632: loss: 0.2559081040:
22: 12832: loss: 0.2568032406:
22: 16032: loss: 0.2599860387:
22: 19232: loss: 0.2567127896:
22: 22432: loss: 0.2583706036:
22: 25632: loss: 0.2575889682:
22: 28832: loss: 0.2580168560:
22: 32032: loss: 0.2575934661:
22: 35232: loss: 0.2582783168:
22: 38432: loss: 0.2585885464:
22: 41632: loss: 0.2580249078:
22: 44832: loss: 0.2588062812:
22: 48032: loss: 0.2584356372:
22: 51232: loss: 0.2591057591:
22: 54432: loss: 0.2593421707:
22: 57632: loss: 0.2595558516:
22: 60832: loss: 0.2591746973:
22: 64032: loss: 0.2595878726:
22: 67232: loss: 0.2594153108:
22: 70432: loss: 0.2596292617:
22: 73632: loss: 0.2596210405:
22: 76832: loss: 0.2589654969:
22: 80032: loss: 0.2583422857:
22: 83232: loss: 0.2580882100:
22: 86432: loss: 0.2584871926:
22: 89632: loss: 0.2580646864:
22: 92832: loss: 0.2582233122:
22: 96032: loss: 0.2578932155:
22: 99232: loss: 0.2573618722:
22: 102432: loss: 0.2575334845:
22: 105632: loss: 0.2575913785:
22: 108832: loss: 0.2581636815:
22: 112032: loss: 0.2585185468:
22: 115232: loss: 0.2584568984:
22: 118432: loss: 0.2586743578:
22: 121632: loss: 0.2586652505:
Dev-Acc: 22: Accuracy: 0.9325388670: precision: 0.4060196560: recall: 0.3371875531: f1: 0.3684161635
Train-Acc: 22: Accuracy: 0.9017734528: precision: 0.7947882736: recall: 0.2887384130: f1: 0.4235906833
23: 3232: loss: 0.2603949701:
23: 6432: loss: 0.2600620976:
23: 9632: loss: 0.2564184424:
23: 12832: loss: 0.2580324444:
23: 16032: loss: 0.2576789871:
23: 19232: loss: 0.2563048880:
23: 22432: loss: 0.2557102246:
23: 25632: loss: 0.2567553772:
23: 28832: loss: 0.2582335849:
23: 32032: loss: 0.2586878325:
23: 35232: loss: 0.2581868579:
23: 38432: loss: 0.2586301887:
23: 41632: loss: 0.2591368123:
23: 44832: loss: 0.2589716706:
23: 48032: loss: 0.2593608846:
23: 51232: loss: 0.2588127576:
23: 54432: loss: 0.2584296197:
23: 57632: loss: 0.2589148246:
23: 60832: loss: 0.2581851761:
23: 64032: loss: 0.2583042268:
23: 67232: loss: 0.2577463905:
23: 70432: loss: 0.2577846573:
23: 73632: loss: 0.2575433230:
23: 76832: loss: 0.2573367652:
23: 80032: loss: 0.2567792054:
23: 83232: loss: 0.2560783563:
23: 86432: loss: 0.2566144618:
23: 89632: loss: 0.2558520985:
23: 92832: loss: 0.2554885290:
23: 96032: loss: 0.2551107439:
23: 99232: loss: 0.2549318689:
23: 102432: loss: 0.2544162892:
23: 105632: loss: 0.2544417956:
23: 108832: loss: 0.2547458888:
23: 112032: loss: 0.2545168853:
23: 115232: loss: 0.2548797409:
23: 118432: loss: 0.2549177981:
23: 121632: loss: 0.2546633782:
Dev-Acc: 23: Accuracy: 0.9272404313: precision: 0.3798808736: recall: 0.3904097943: f1: 0.3850733753
Train-Acc: 23: Accuracy: 0.9031375647: precision: 0.7513950073: recall: 0.3364012885: f1: 0.4647382044
24: 3232: loss: 0.2502899142:
24: 6432: loss: 0.2491109454:
24: 9632: loss: 0.2491317567:
24: 12832: loss: 0.2489735295:
24: 16032: loss: 0.2486733000:
24: 19232: loss: 0.2495730928:
24: 22432: loss: 0.2493254391:
24: 25632: loss: 0.2493386135:
24: 28832: loss: 0.2508804350:
24: 32032: loss: 0.2509840146:
24: 35232: loss: 0.2507708896:
24: 38432: loss: 0.2514535927:
24: 41632: loss: 0.2512174346:
24: 44832: loss: 0.2519936994:
24: 48032: loss: 0.2519308898:
24: 51232: loss: 0.2519278436:
24: 54432: loss: 0.2522486908:
24: 57632: loss: 0.2530430981:
24: 60832: loss: 0.2531084085:
24: 64032: loss: 0.2529850314:
24: 67232: loss: 0.2530850273:
24: 70432: loss: 0.2533677768:
24: 73632: loss: 0.2523256562:
24: 76832: loss: 0.2524429673:
24: 80032: loss: 0.2516770919:
24: 83232: loss: 0.2518010105:
24: 86432: loss: 0.2515068220:
24: 89632: loss: 0.2514846269:
24: 92832: loss: 0.2518000997:
24: 96032: loss: 0.2520234764:
24: 99232: loss: 0.2521257899:
24: 102432: loss: 0.2516610956:
24: 105632: loss: 0.2515866512:
24: 108832: loss: 0.2514540567:
24: 112032: loss: 0.2513131959:
24: 115232: loss: 0.2515862871:
24: 118432: loss: 0.2512586703:
24: 121632: loss: 0.2509762974:
Dev-Acc: 24: Accuracy: 0.9243431091: precision: 0.3671541743: recall: 0.4097942527: f1: 0.3873041382
Train-Acc: 24: Accuracy: 0.9038196206: precision: 0.7354007249: recall: 0.3601341135: f1: 0.4834951456
25: 3232: loss: 0.2508902925:
25: 6432: loss: 0.2420213749:
25: 9632: loss: 0.2389738032:
25: 12832: loss: 0.2440688429:
25: 16032: loss: 0.2444346349:
25: 19232: loss: 0.2461621777:
25: 22432: loss: 0.2462941999:
25: 25632: loss: 0.2481251935:
25: 28832: loss: 0.2480089842:
25: 32032: loss: 0.2464795536:
25: 35232: loss: 0.2461054401:
25: 38432: loss: 0.2459123522:
25: 41632: loss: 0.2470064031:
25: 44832: loss: 0.2468634457:
25: 48032: loss: 0.2474173229:
25: 51232: loss: 0.2470655940:
25: 54432: loss: 0.2464827552:
25: 57632: loss: 0.2469574723:
25: 60832: loss: 0.2472007532:
25: 64032: loss: 0.2471946164:
25: 67232: loss: 0.2468191289:
25: 70432: loss: 0.2466498100:
25: 73632: loss: 0.2473179930:
25: 76832: loss: 0.2476501286:
25: 80032: loss: 0.2480173692:
25: 83232: loss: 0.2479292599:
25: 86432: loss: 0.2472504387:
25: 89632: loss: 0.2475006752:
25: 92832: loss: 0.2474644489:
25: 96032: loss: 0.2475071479:
25: 99232: loss: 0.2473391109:
25: 102432: loss: 0.2469531224:
25: 105632: loss: 0.2475752578:
25: 108832: loss: 0.2474709832:
25: 112032: loss: 0.2473774363:
25: 115232: loss: 0.2473649301:
25: 118432: loss: 0.2473400837:
25: 121632: loss: 0.2471619407:
Dev-Acc: 25: Accuracy: 0.9228547812: precision: 0.3629918981: recall: 0.4266281245: f1: 0.3922457594
Train-Acc: 25: Accuracy: 0.9047647119: precision: 0.7279707956: recall: 0.3801853922: f1: 0.4995033470
26: 3232: loss: 0.2333553380:
26: 6432: loss: 0.2363542840:
26: 9632: loss: 0.2354630421:
26: 12832: loss: 0.2356957921:
26: 16032: loss: 0.2385085306:
26: 19232: loss: 0.2400757229:
26: 22432: loss: 0.2393765987:
26: 25632: loss: 0.2403123982:
26: 28832: loss: 0.2412695956:
26: 32032: loss: 0.2415805981:
26: 35232: loss: 0.2427577922:
26: 38432: loss: 0.2443406401:
26: 41632: loss: 0.2441476467:
26: 44832: loss: 0.2438392747:
26: 48032: loss: 0.2433616322:
26: 51232: loss: 0.2424377601:
26: 54432: loss: 0.2428676705:
26: 57632: loss: 0.2438027587:
26: 60832: loss: 0.2441070413:
26: 64032: loss: 0.2441900772:
26: 67232: loss: 0.2440853408:
26: 70432: loss: 0.2440879179:
26: 73632: loss: 0.2438993166:
26: 76832: loss: 0.2436318834:
26: 80032: loss: 0.2432207925:
26: 83232: loss: 0.2429406924:
26: 86432: loss: 0.2429700229:
26: 89632: loss: 0.2434255913:
26: 92832: loss: 0.2430267243:
26: 96032: loss: 0.2431769389:
26: 99232: loss: 0.2430617261:
26: 102432: loss: 0.2429603199:
26: 105632: loss: 0.2432029842:
26: 108832: loss: 0.2434175485:
26: 112032: loss: 0.2435432356:
26: 115232: loss: 0.2436012413:
26: 118432: loss: 0.2439772938:
26: 121632: loss: 0.2437425113:
Dev-Acc: 26: Accuracy: 0.9222297072: precision: 0.3618523225: recall: 0.4358102364: f1: 0.3954026535
Train-Acc: 26: Accuracy: 0.9055371284: precision: 0.7232636385: recall: 0.3957004799: f1: 0.5115369906
27: 3232: loss: 0.2480141985:
27: 6432: loss: 0.2455962801:
27: 9632: loss: 0.2465134685:
27: 12832: loss: 0.2454050361:
27: 16032: loss: 0.2447787762:
27: 19232: loss: 0.2439590176:
27: 22432: loss: 0.2459725546:
27: 25632: loss: 0.2470786572:
27: 28832: loss: 0.2475190641:
27: 32032: loss: 0.2471179702:
27: 35232: loss: 0.2467851025:
27: 38432: loss: 0.2456330210:
27: 41632: loss: 0.2457861518:
27: 44832: loss: 0.2449143969:
27: 48032: loss: 0.2454330878:
27: 51232: loss: 0.2460740771:
27: 54432: loss: 0.2452525780:
27: 57632: loss: 0.2450835617:
27: 60832: loss: 0.2448365874:
27: 64032: loss: 0.2449577501:
27: 67232: loss: 0.2448036405:
27: 70432: loss: 0.2443762580:
27: 73632: loss: 0.2438817444:
27: 76832: loss: 0.2442066196:
27: 80032: loss: 0.2437050448:
27: 83232: loss: 0.2439975455:
27: 86432: loss: 0.2444278428:
27: 89632: loss: 0.2437600668:
27: 92832: loss: 0.2428930799:
27: 96032: loss: 0.2429567319:
27: 99232: loss: 0.2424550991:
27: 102432: loss: 0.2419641539:
27: 105632: loss: 0.2420057690:
27: 108832: loss: 0.2417828055:
27: 112032: loss: 0.2413243777:
27: 115232: loss: 0.2413051181:
27: 118432: loss: 0.2410128608:
27: 121632: loss: 0.2408482661:
Dev-Acc: 27: Accuracy: 0.9212176204: precision: 0.3588759424: recall: 0.4451623873: f1: 0.3973891925
Train-Acc: 27: Accuracy: 0.9065561295: precision: 0.7233077460: recall: 0.4088488594: f1: 0.5224074930
28: 3232: loss: 0.2370154581:
28: 6432: loss: 0.2324837011:
28: 9632: loss: 0.2322547072:
28: 12832: loss: 0.2377789669:
28: 16032: loss: 0.2389895529:
28: 19232: loss: 0.2412877428:
28: 22432: loss: 0.2394375992:
28: 25632: loss: 0.2390887653:
28: 28832: loss: 0.2401353454:
28: 32032: loss: 0.2402270336:
28: 35232: loss: 0.2416090456:
28: 38432: loss: 0.2417734344:
28: 41632: loss: 0.2413795700:
28: 44832: loss: 0.2419695181:
28: 48032: loss: 0.2414708346:
28: 51232: loss: 0.2408631189:
28: 54432: loss: 0.2410458525:
28: 57632: loss: 0.2402219075:
28: 60832: loss: 0.2404049711:
28: 64032: loss: 0.2403598415:
28: 67232: loss: 0.2405559526:
28: 70432: loss: 0.2403515310:
28: 73632: loss: 0.2400725180:
28: 76832: loss: 0.2399869497:
28: 80032: loss: 0.2399424550:
28: 83232: loss: 0.2395856635:
28: 86432: loss: 0.2393056633:
28: 89632: loss: 0.2394550351:
28: 92832: loss: 0.2390199741:
28: 96032: loss: 0.2385954153:
28: 99232: loss: 0.2386634843:
28: 102432: loss: 0.2385495932:
28: 105632: loss: 0.2385006919:
28: 108832: loss: 0.2387200544:
28: 112032: loss: 0.2381100532:
28: 115232: loss: 0.2378323882:
28: 118432: loss: 0.2380085411:
28: 121632: loss: 0.2379275532:
Dev-Acc: 28: Accuracy: 0.9205925465: precision: 0.3577747989: recall: 0.4538343819: f1: 0.4001199310
Train-Acc: 28: Accuracy: 0.9078298807: precision: 0.7258848807: recall: 0.4219972388: f1: 0.5337158061
29: 3232: loss: 0.2471797329:
29: 6432: loss: 0.2435039585:
29: 9632: loss: 0.2369399337:
29: 12832: loss: 0.2380162691:
29: 16032: loss: 0.2369885301:
29: 19232: loss: 0.2381878430:
29: 22432: loss: 0.2414112389:
29: 25632: loss: 0.2384331919:
29: 28832: loss: 0.2368386973:
29: 32032: loss: 0.2369695299:
29: 35232: loss: 0.2368383414:
29: 38432: loss: 0.2363601113:
29: 41632: loss: 0.2366470976:
29: 44832: loss: 0.2366977796:
29: 48032: loss: 0.2361312048:
29: 51232: loss: 0.2369801469:
29: 54432: loss: 0.2364370830:
29: 57632: loss: 0.2363833385:
29: 60832: loss: 0.2365478815:
29: 64032: loss: 0.2369430788:
29: 67232: loss: 0.2367968794:
29: 70432: loss: 0.2372176773:
29: 73632: loss: 0.2367161585:
29: 76832: loss: 0.2365611346:
29: 80032: loss: 0.2361723130:
29: 83232: loss: 0.2357676836:
29: 86432: loss: 0.2353317090:
29: 89632: loss: 0.2349196796:
29: 92832: loss: 0.2348903942:
29: 96032: loss: 0.2350827601:
29: 99232: loss: 0.2348450814:
29: 102432: loss: 0.2347381191:
29: 105632: loss: 0.2349512494:
29: 108832: loss: 0.2347829708:
29: 112032: loss: 0.2350313004:
29: 115232: loss: 0.2353598875:
29: 118432: loss: 0.2353847416:
29: 121632: loss: 0.2352911581:
Dev-Acc: 29: Accuracy: 0.9197689891: precision: 0.3569482289: recall: 0.4677775888: f1: 0.4049161024
Train-Acc: 29: Accuracy: 0.9088571072: precision: 0.7280274518: recall: 0.4323844586: f1: 0.5425448546
30: 3232: loss: 0.2405327794:
30: 6432: loss: 0.2341243839:
30: 9632: loss: 0.2344892139:
30: 12832: loss: 0.2330541309:
30: 16032: loss: 0.2327850447:
30: 19232: loss: 0.2330817549:
30: 22432: loss: 0.2337203252:
30: 25632: loss: 0.2322149460:
30: 28832: loss: 0.2336554959:
30: 32032: loss: 0.2352624644:
30: 35232: loss: 0.2339592756:
30: 38432: loss: 0.2336873664:
30: 41632: loss: 0.2344950070:
30: 44832: loss: 0.2342961512:
30: 48032: loss: 0.2345916434:
30: 51232: loss: 0.2337493354:
30: 54432: loss: 0.2337396419:
30: 57632: loss: 0.2333491782:
30: 60832: loss: 0.2332885227:
30: 64032: loss: 0.2326089001:
30: 67232: loss: 0.2327004825:
30: 70432: loss: 0.2323259576:
30: 73632: loss: 0.2318820238:
30: 76832: loss: 0.2313721052:
30: 80032: loss: 0.2320348849:
30: 83232: loss: 0.2322249624:
30: 86432: loss: 0.2325365049:
30: 89632: loss: 0.2325707995:
30: 92832: loss: 0.2327008260:
30: 96032: loss: 0.2322393210:
30: 99232: loss: 0.2322925427:
30: 102432: loss: 0.2325885077:
30: 105632: loss: 0.2327166784:
30: 108832: loss: 0.2325894041:
30: 112032: loss: 0.2323277554:
30: 115232: loss: 0.2325340802:
30: 118432: loss: 0.2326920723:
30: 121632: loss: 0.2324275163:
Dev-Acc: 30: Accuracy: 0.9189057946: precision: 0.3549367089: recall: 0.4767896616: f1: 0.4069370873
Train-Acc: 30: Accuracy: 0.9099747539: precision: 0.7311535955: recall: 0.4425087108: f1: 0.5513371831
31: 3232: loss: 0.2404789080:
31: 6432: loss: 0.2319490205:
31: 9632: loss: 0.2310934311:
31: 12832: loss: 0.2308275835:
31: 16032: loss: 0.2329253914:
31: 19232: loss: 0.2345868069:
31: 22432: loss: 0.2355602104:
31: 25632: loss: 0.2357201492:
31: 28832: loss: 0.2345914112:
31: 32032: loss: 0.2337007577:
31: 35232: loss: 0.2339826936:
31: 38432: loss: 0.2329427415:
31: 41632: loss: 0.2327482143:
31: 44832: loss: 0.2327212535:
31: 48032: loss: 0.2326702831:
31: 51232: loss: 0.2327621238:
31: 54432: loss: 0.2329065433:
31: 57632: loss: 0.2326518922:
31: 60832: loss: 0.2322282960:
31: 64032: loss: 0.2325792182:
31: 67232: loss: 0.2321841473:
31: 70432: loss: 0.2310482071:
31: 73632: loss: 0.2309518615:
31: 76832: loss: 0.2313426824:
31: 80032: loss: 0.2310001703:
31: 83232: loss: 0.2309620427:
31: 86432: loss: 0.2306737663:
31: 89632: loss: 0.2308997462:
31: 92832: loss: 0.2309737167:
31: 96032: loss: 0.2309357221:
31: 99232: loss: 0.2306410133:
31: 102432: loss: 0.2305012654:
31: 105632: loss: 0.2303205053:
31: 108832: loss: 0.2304200948:
31: 112032: loss: 0.2307761786:
31: 115232: loss: 0.2304684125:
31: 118432: loss: 0.2300797131:
31: 121632: loss: 0.2300533271:
Dev-Acc: 31: Accuracy: 0.9185882807: precision: 0.3550399202: recall: 0.4839313042: f1: 0.4095848025
Train-Acc: 31: Accuracy: 0.9106732607: precision: 0.7324622470: recall: 0.4496088357: f1: 0.5571940688
32: 3232: loss: 0.2237894343:
32: 6432: loss: 0.2406022415:
32: 9632: loss: 0.2366300404:
32: 12832: loss: 0.2340357638:
32: 16032: loss: 0.2343858406:
32: 19232: loss: 0.2315179575:
32: 22432: loss: 0.2322780774:
32: 25632: loss: 0.2326874878:
32: 28832: loss: 0.2316817188:
32: 32032: loss: 0.2310899765:
32: 35232: loss: 0.2324559084:
32: 38432: loss: 0.2315072942:
32: 41632: loss: 0.2303074380:
32: 44832: loss: 0.2298213385:
32: 48032: loss: 0.2301634192:
32: 51232: loss: 0.2297093899:
32: 54432: loss: 0.2287769956:
32: 57632: loss: 0.2279749904:
32: 60832: loss: 0.2289783234:
32: 64032: loss: 0.2288525858:
32: 67232: loss: 0.2283474398:
32: 70432: loss: 0.2281103583:
32: 73632: loss: 0.2283062048:
32: 76832: loss: 0.2288657893:
32: 80032: loss: 0.2285327997:
32: 83232: loss: 0.2283828494:
32: 86432: loss: 0.2283078481:
32: 89632: loss: 0.2278379627:
32: 92832: loss: 0.2272126408:
32: 96032: loss: 0.2272423032:
32: 99232: loss: 0.2270859828:
32: 102432: loss: 0.2276983746:
32: 105632: loss: 0.2275007284:
32: 108832: loss: 0.2274536971:
32: 112032: loss: 0.2275931718:
32: 115232: loss: 0.2274121091:
32: 118432: loss: 0.2275602211:
32: 121632: loss: 0.2276325629:
Dev-Acc: 32: Accuracy: 0.9182409644: precision: 0.3545443335: recall: 0.4888624384: f1: 0.4110078628
Train-Acc: 32: Accuracy: 0.9113963842: precision: 0.7348605366: recall: 0.4555256065: f1: 0.5624188312
33: 3232: loss: 0.2306437566:
33: 6432: loss: 0.2299633991:
33: 9632: loss: 0.2322741136:
33: 12832: loss: 0.2289164357:
33: 16032: loss: 0.2261222364:
33: 19232: loss: 0.2262376947:
33: 22432: loss: 0.2240350821:
33: 25632: loss: 0.2224745814:
33: 28832: loss: 0.2244863840:
33: 32032: loss: 0.2245289668:
33: 35232: loss: 0.2254023938:
33: 38432: loss: 0.2241727887:
33: 41632: loss: 0.2235652985:
33: 44832: loss: 0.2248521160:
33: 48032: loss: 0.2251976435:
33: 51232: loss: 0.2246950267:
33: 54432: loss: 0.2252711508:
33: 57632: loss: 0.2251307817:
33: 60832: loss: 0.2247781762:
33: 64032: loss: 0.2259841284:
33: 67232: loss: 0.2255102894:
33: 70432: loss: 0.2257340460:
33: 73632: loss: 0.2254178567:
33: 76832: loss: 0.2254884619:
33: 80032: loss: 0.2257473857:
33: 83232: loss: 0.2254454555:
33: 86432: loss: 0.2254214427:
33: 89632: loss: 0.2254658891:
33: 92832: loss: 0.2253697217:
33: 96032: loss: 0.2255879529:
33: 99232: loss: 0.2256106424:
33: 102432: loss: 0.2258012276:
33: 105632: loss: 0.2258225215:
33: 108832: loss: 0.2261395432:
33: 112032: loss: 0.2264107891:
33: 115232: loss: 0.2263717151:
33: 118432: loss: 0.2260512342:
33: 121632: loss: 0.2258521323:
Dev-Acc: 33: Accuracy: 0.9180921316: precision: 0.3549608993: recall: 0.4939636116: f1: 0.4130821187
Train-Acc: 33: Accuracy: 0.9123414159: precision: 0.7384550798: recall: 0.4625599895: f1: 0.5688184648
34: 3232: loss: 0.2306178668:
34: 6432: loss: 0.2284236161:
34: 9632: loss: 0.2280707944:
34: 12832: loss: 0.2259562287:
34: 16032: loss: 0.2222000178:
34: 19232: loss: 0.2228283021:
34: 22432: loss: 0.2217410452:
34: 25632: loss: 0.2238091844:
34: 28832: loss: 0.2234398020:
34: 32032: loss: 0.2240385138:
34: 35232: loss: 0.2256242115:
34: 38432: loss: 0.2245992469:
34: 41632: loss: 0.2242436436:
34: 44832: loss: 0.2237922086:
34: 48032: loss: 0.2243561119:
34: 51232: loss: 0.2246093390:
34: 54432: loss: 0.2248350404:
34: 57632: loss: 0.2244474051:
34: 60832: loss: 0.2246378849:
34: 64032: loss: 0.2240322656:
34: 67232: loss: 0.2240671033:
34: 70432: loss: 0.2240722024:
34: 73632: loss: 0.2244087554:
34: 76832: loss: 0.2240110897:
34: 80032: loss: 0.2239916925:
34: 83232: loss: 0.2246865470:
34: 86432: loss: 0.2243185121:
34: 89632: loss: 0.2244520588:
34: 92832: loss: 0.2242703485:
34: 96032: loss: 0.2241250813:
34: 99232: loss: 0.2244034254:
34: 102432: loss: 0.2246572005:
34: 105632: loss: 0.2242230068:
34: 108832: loss: 0.2242862638:
34: 112032: loss: 0.2242970661:
34: 115232: loss: 0.2239344888:
34: 118432: loss: 0.2241296225:
34: 121632: loss: 0.2236896168:
Dev-Acc: 34: Accuracy: 0.9178341627: precision: 0.3544395924: recall: 0.4968542765: f1: 0.4137345133
Train-Acc: 34: Accuracy: 0.9128098488: precision: 0.7395106715: recall: 0.4669646966: f1: 0.5724532560
35: 3232: loss: 0.2278552538:
35: 6432: loss: 0.2179409251:
35: 9632: loss: 0.2245323770:
35: 12832: loss: 0.2271648552:
35: 16032: loss: 0.2272606948:
35: 19232: loss: 0.2209990402:
35: 22432: loss: 0.2207430995:
35: 25632: loss: 0.2227476131:
35: 28832: loss: 0.2210083175:
35: 32032: loss: 0.2209763686:
35: 35232: loss: 0.2205123500:
35: 38432: loss: 0.2215624016:
35: 41632: loss: 0.2214450286:
35: 44832: loss: 0.2221023666:
35: 48032: loss: 0.2217165864:
35: 51232: loss: 0.2220941454:
35: 54432: loss: 0.2222376105:
35: 57632: loss: 0.2215496050:
35: 60832: loss: 0.2211365799:
35: 64032: loss: 0.2211014590:
35: 67232: loss: 0.2213612173:
35: 70432: loss: 0.2218958173:
35: 73632: loss: 0.2219330928:
35: 76832: loss: 0.2227462783:
35: 80032: loss: 0.2225271569:
35: 83232: loss: 0.2224761457:
35: 86432: loss: 0.2219568144:
35: 89632: loss: 0.2218835878:
35: 92832: loss: 0.2221224352:
35: 96032: loss: 0.2215929243:
35: 99232: loss: 0.2208535895:
35: 102432: loss: 0.2209236589:
35: 105632: loss: 0.2208951064:
35: 108832: loss: 0.2211172732:
35: 112032: loss: 0.2214441684:
35: 115232: loss: 0.2213421509:
35: 118432: loss: 0.2211482837:
35: 121632: loss: 0.2213728270:
Dev-Acc: 35: Accuracy: 0.9179234505: precision: 0.3560505719: recall: 0.5028056453: f1: 0.4168898914
Train-Acc: 35: Accuracy: 0.9134426117: precision: 0.7433416563: recall: 0.4697258563: f1: 0.5756757846
36: 3232: loss: 0.2283125924:
36: 6432: loss: 0.2270990798:
36: 9632: loss: 0.2229561789:
36: 12832: loss: 0.2225423433:
36: 16032: loss: 0.2223616384:
36: 19232: loss: 0.2217749752:
36: 22432: loss: 0.2234192510:
36: 25632: loss: 0.2248030873:
36: 28832: loss: 0.2256656817:
36: 32032: loss: 0.2246507281:
36: 35232: loss: 0.2254238085:
36: 38432: loss: 0.2262215462:
36: 41632: loss: 0.2251370366:
36: 44832: loss: 0.2249627973:
36: 48032: loss: 0.2248717280:
36: 51232: loss: 0.2244548682:
36: 54432: loss: 0.2231928476:
36: 57632: loss: 0.2224433543:
36: 60832: loss: 0.2219197168:
36: 64032: loss: 0.2218494690:
36: 67232: loss: 0.2210959240:
36: 70432: loss: 0.2212044338:
36: 73632: loss: 0.2208866336:
36: 76832: loss: 0.2209326640:
36: 80032: loss: 0.2210305044:
36: 83232: loss: 0.2206847659:
36: 86432: loss: 0.2203175821:
36: 89632: loss: 0.2201033358:
36: 92832: loss: 0.2195044827:
36: 96032: loss: 0.2196291813:
36: 99232: loss: 0.2198666236:
36: 102432: loss: 0.2197723837:
36: 105632: loss: 0.2202344679:
36: 108832: loss: 0.2205361514:
36: 112032: loss: 0.2203110674:
36: 115232: loss: 0.2201404851:
36: 118432: loss: 0.2196479044:
36: 121632: loss: 0.2195798116:
Dev-Acc: 36: Accuracy: 0.9182906151: precision: 0.3584315612: recall: 0.5067165448: f1: 0.4198661501
Train-Acc: 36: Accuracy: 0.9141082764: precision: 0.7461466846: recall: 0.4741963053: f1: 0.5798697645
37: 3232: loss: 0.2165790246:
37: 6432: loss: 0.2193589976:
37: 9632: loss: 0.2161007730:
37: 12832: loss: 0.2117092677:
37: 16032: loss: 0.2120302608:
37: 19232: loss: 0.2149345307:
37: 22432: loss: 0.2171710758:
37: 25632: loss: 0.2175434199:
37: 28832: loss: 0.2184369034:
37: 32032: loss: 0.2169127341:
37: 35232: loss: 0.2177879153:
37: 38432: loss: 0.2179682284:
37: 41632: loss: 0.2185505227:
37: 44832: loss: 0.2182719487:
37: 48032: loss: 0.2183286165:
37: 51232: loss: 0.2182046068:
37: 54432: loss: 0.2175418831:
37: 57632: loss: 0.2175739667:
37: 60832: loss: 0.2181076488:
37: 64032: loss: 0.2179222367:
37: 67232: loss: 0.2182943591:
37: 70432: loss: 0.2188156241:
37: 73632: loss: 0.2189769246:
37: 76832: loss: 0.2195138769:
37: 80032: loss: 0.2198590470:
37: 83232: loss: 0.2192367577:
37: 86432: loss: 0.2194224539:
37: 89632: loss: 0.2191968876:
37: 92832: loss: 0.2189210605:
37: 96032: loss: 0.2188158187:
37: 99232: loss: 0.2185941107:
37: 102432: loss: 0.2184621529:
37: 105632: loss: 0.2188002932:
37: 108832: loss: 0.2186456182:
37: 112032: loss: 0.2185589176:
37: 115232: loss: 0.2181593231:
37: 118432: loss: 0.2183653288:
37: 121632: loss: 0.2180228194:
Dev-Acc: 37: Accuracy: 0.9185783267: precision: 0.3604274223: recall: 0.5104574052: f1: 0.4225193526
Train-Acc: 37: Accuracy: 0.9150779247: precision: 0.7520934560: recall: 0.4782723029: f1: 0.5847130686
38: 3232: loss: 0.2069931143:
38: 6432: loss: 0.2142422231:
38: 9632: loss: 0.2122507703:
38: 12832: loss: 0.2130293011:
38: 16032: loss: 0.2133202434:
38: 19232: loss: 0.2174672845:
38: 22432: loss: 0.2145496789:
38: 25632: loss: 0.2147106386:
38: 28832: loss: 0.2147431765:
38: 32032: loss: 0.2148126552:
38: 35232: loss: 0.2151967792:
38: 38432: loss: 0.2141122285:
38: 41632: loss: 0.2143661608:
38: 44832: loss: 0.2147639199:
38: 48032: loss: 0.2145844899:
38: 51232: loss: 0.2136911788:
38: 54432: loss: 0.2137715710:
38: 57632: loss: 0.2136137287:
38: 60832: loss: 0.2136986620:
38: 64032: loss: 0.2138049300:
38: 67232: loss: 0.2136262078:
38: 70432: loss: 0.2134558791:
38: 73632: loss: 0.2133940558:
38: 76832: loss: 0.2134217232:
38: 80032: loss: 0.2142993408:
38: 83232: loss: 0.2143694093:
38: 86432: loss: 0.2147693708:
38: 89632: loss: 0.2150896118:
38: 92832: loss: 0.2157408374:
38: 96032: loss: 0.2157695350:
38: 99232: loss: 0.2155512875:
38: 102432: loss: 0.2159997279:
38: 105632: loss: 0.2156587894:
38: 108832: loss: 0.2155854720:
38: 112032: loss: 0.2157139712:
38: 115232: loss: 0.2160406001:
38: 118432: loss: 0.2164665430:
38: 121632: loss: 0.2166965096:
Dev-Acc: 38: Accuracy: 0.9186974168: precision: 0.3615467497: recall: 0.5135181092: f1: 0.4243360967
Train-Acc: 38: Accuracy: 0.9158175588: precision: 0.7544308985: recall: 0.4841233318: f1: 0.5897805542
39: 3232: loss: 0.2192511513:
39: 6432: loss: 0.2228297378:
39: 9632: loss: 0.2225101944:
39: 12832: loss: 0.2197005121:
39: 16032: loss: 0.2182438202:
39: 19232: loss: 0.2175712925:
39: 22432: loss: 0.2175652948:
39: 25632: loss: 0.2163982416:
39: 28832: loss: 0.2166635084:
39: 32032: loss: 0.2181979117:
39: 35232: loss: 0.2184728070:
39: 38432: loss: 0.2193593427:
39: 41632: loss: 0.2193239102:
39: 44832: loss: 0.2182823481:
39: 48032: loss: 0.2186594844:
39: 51232: loss: 0.2186598184:
39: 54432: loss: 0.2190048425:
39: 57632: loss: 0.2188490424:
39: 60832: loss: 0.2186103072:
39: 64032: loss: 0.2176795012:
39: 67232: loss: 0.2169832408:
39: 70432: loss: 0.2167585876:
39: 73632: loss: 0.2167223830:
39: 76832: loss: 0.2160631943:
39: 80032: loss: 0.2151938434:
39: 83232: loss: 0.2151859340:
39: 86432: loss: 0.2151423132:
39: 89632: loss: 0.2147109386:
39: 92832: loss: 0.2145126982:
39: 96032: loss: 0.2145064647:
39: 99232: loss: 0.2148185411:
39: 102432: loss: 0.2150933021:
39: 105632: loss: 0.2150325347:
39: 108832: loss: 0.2144302715:
39: 112032: loss: 0.2144339501:
39: 115232: loss: 0.2144457769:
39: 118432: loss: 0.2147286478:
39: 121632: loss: 0.2146439175:
Dev-Acc: 39: Accuracy: 0.9192827940: precision: 0.3640530760: recall: 0.5131780309: f1: 0.4259403006
Train-Acc: 39: Accuracy: 0.9164831638: precision: 0.7580249438: recall: 0.4874761686: f1: 0.5933661425
40: 3232: loss: 0.2241359508:
40: 6432: loss: 0.2205934259:
40: 9632: loss: 0.2138419337:
40: 12832: loss: 0.2117250489:
40: 16032: loss: 0.2111041820:
40: 19232: loss: 0.2138635992:
40: 22432: loss: 0.2149177515:
40: 25632: loss: 0.2156910650:
40: 28832: loss: 0.2145454295:
40: 32032: loss: 0.2142284621:
40: 35232: loss: 0.2135588838:
40: 38432: loss: 0.2136743162:
40: 41632: loss: 0.2137822885:
40: 44832: loss: 0.2145207312:
40: 48032: loss: 0.2158400885:
40: 51232: loss: 0.2148334448:
40: 54432: loss: 0.2142986386:
40: 57632: loss: 0.2140401705:
40: 60832: loss: 0.2140158894:
40: 64032: loss: 0.2144624410:
40: 67232: loss: 0.2135372879:
40: 70432: loss: 0.2140320833:
40: 73632: loss: 0.2136855388:
40: 76832: loss: 0.2136958573:
40: 80032: loss: 0.2133854131:
40: 83232: loss: 0.2128828119:
40: 86432: loss: 0.2135125429:
40: 89632: loss: 0.2137814422:
40: 92832: loss: 0.2137350143:
40: 96032: loss: 0.2135791891:
40: 99232: loss: 0.2133765160:
40: 102432: loss: 0.2131501934:
40: 105632: loss: 0.2130818057:
40: 108832: loss: 0.2130427252:
40: 112032: loss: 0.2127448820:
40: 115232: loss: 0.2127867420:
40: 118432: loss: 0.2127979640:
40: 121632: loss: 0.2132030630:
Dev-Acc: 40: Accuracy: 0.9196400046: precision: 0.3662243667: recall: 0.5162387349: f1: 0.4284807000
Train-Acc: 40: Accuracy: 0.9170995355: precision: 0.7622606737: recall: 0.4894484255: f1: 0.5961245896
41: 3232: loss: 0.2013609546:
41: 6432: loss: 0.2067727408:
41: 9632: loss: 0.2057291401:
41: 12832: loss: 0.2065998127:
41: 16032: loss: 0.2067734970:
41: 19232: loss: 0.2075202653:
41: 22432: loss: 0.2088632773:
41: 25632: loss: 0.2096185380:
41: 28832: loss: 0.2110788455:
41: 32032: loss: 0.2119352911:
41: 35232: loss: 0.2121531579:
41: 38432: loss: 0.2124067736:
41: 41632: loss: 0.2133872688:
41: 44832: loss: 0.2127644959:
41: 48032: loss: 0.2126465942:
41: 51232: loss: 0.2127282890:
41: 54432: loss: 0.2128616680:
41: 57632: loss: 0.2138432586:
41: 60832: loss: 0.2140897691:
41: 64032: loss: 0.2140535053:
41: 67232: loss: 0.2140547117:
41: 70432: loss: 0.2144452606:
41: 73632: loss: 0.2147518077:
41: 76832: loss: 0.2142235503:
41: 80032: loss: 0.2142409812:
41: 83232: loss: 0.2144134065:
41: 86432: loss: 0.2138560024:
41: 89632: loss: 0.2136574641:
41: 92832: loss: 0.2137418854:
41: 96032: loss: 0.2137061281:
41: 99232: loss: 0.2133245989:
41: 102432: loss: 0.2132893885:
41: 105632: loss: 0.2128141043:
41: 108832: loss: 0.2129459870:
41: 112032: loss: 0.2129265641:
41: 115232: loss: 0.2124541116:
41: 118432: loss: 0.2120353020:
41: 121632: loss: 0.2116570432:
Dev-Acc: 41: Accuracy: 0.9204337597: precision: 0.3703299369: recall: 0.5191293998: f1: 0.4322831858
Train-Acc: 41: Accuracy: 0.9177158475: precision: 0.7682700248: recall: 0.4893169417: f1: 0.5978553356
42: 3232: loss: 0.2200439768:
42: 6432: loss: 0.2202952944:
42: 9632: loss: 0.2178396177:
42: 12832: loss: 0.2155950222:
42: 16032: loss: 0.2119169268:
42: 19232: loss: 0.2107787348:
42: 22432: loss: 0.2089159107:
42: 25632: loss: 0.2093303782:
42: 28832: loss: 0.2096218326:
42: 32032: loss: 0.2089990828:
42: 35232: loss: 0.2103176562:
42: 38432: loss: 0.2109393061:
42: 41632: loss: 0.2108200292:
42: 44832: loss: 0.2106541048:
42: 48032: loss: 0.2119428777:
42: 51232: loss: 0.2113878529:
42: 54432: loss: 0.2116894146:
42: 57632: loss: 0.2115193288:
42: 60832: loss: 0.2111357895:
42: 64032: loss: 0.2113054732:
42: 67232: loss: 0.2107289701:
42: 70432: loss: 0.2111777214:
42: 73632: loss: 0.2113966560:
42: 76832: loss: 0.2115712674:
42: 80032: loss: 0.2115261276:
42: 83232: loss: 0.2112383709:
42: 86432: loss: 0.2111151864:
42: 89632: loss: 0.2109577327:
42: 92832: loss: 0.2109961086:
42: 96032: loss: 0.2107491646:
42: 99232: loss: 0.2108901387:
42: 102432: loss: 0.2102478450:
42: 105632: loss: 0.2099214720:
42: 108832: loss: 0.2098669812:
42: 112032: loss: 0.2102178126:
42: 115232: loss: 0.2104902645:
42: 118432: loss: 0.2105037609:
42: 121632: loss: 0.2103209419:
Dev-Acc: 42: Accuracy: 0.9210290909: precision: 0.3727651237: recall: 0.5175990478: f1: 0.4334021499
Train-Acc: 42: Accuracy: 0.9184390306: precision: 0.7755421184: recall: 0.4890539741: f1: 0.5998467927
43: 3232: loss: 0.2034080840:
43: 6432: loss: 0.2105145479:
43: 9632: loss: 0.2084353279:
43: 12832: loss: 0.2111477217:
43: 16032: loss: 0.2098693469:
43: 19232: loss: 0.2085181147:
43: 22432: loss: 0.2091653894:
43: 25632: loss: 0.2085620252:
43: 28832: loss: 0.2087182599:
43: 32032: loss: 0.2090269638:
43: 35232: loss: 0.2091113790:
43: 38432: loss: 0.2088830558:
43: 41632: loss: 0.2090547160:
43: 44832: loss: 0.2101405914:
43: 48032: loss: 0.2088958720:
43: 51232: loss: 0.2093116677:
43: 54432: loss: 0.2088154726:
43: 57632: loss: 0.2084175902:
43: 60832: loss: 0.2086177680:
43: 64032: loss: 0.2079105539:
43: 67232: loss: 0.2085330225:
43: 70432: loss: 0.2082220422:
43: 73632: loss: 0.2080996070:
43: 76832: loss: 0.2083787375:
43: 80032: loss: 0.2086825358:
43: 83232: loss: 0.2085251596:
43: 86432: loss: 0.2088558135:
43: 89632: loss: 0.2084226895:
43: 92832: loss: 0.2086737226:
43: 96032: loss: 0.2081741542:
43: 99232: loss: 0.2079196629:
43: 102432: loss: 0.2080825913:
43: 105632: loss: 0.2081193077:
43: 108832: loss: 0.2084585089:
43: 112032: loss: 0.2083391650:
43: 115232: loss: 0.2089455189:
43: 118432: loss: 0.2089737446:
43: 121632: loss: 0.2086522841:
Dev-Acc: 43: Accuracy: 0.9214954376: precision: 0.3742103307: recall: 0.5136881483: f1: 0.4329941235
Train-Acc: 43: Accuracy: 0.9186855555: precision: 0.7790258241: recall: 0.4878706199: f1: 0.5999919149
44: 3232: loss: 0.2120517205:
44: 6432: loss: 0.2177949896:
44: 9632: loss: 0.2121110620:
44: 12832: loss: 0.2119532590:
44: 16032: loss: 0.2108539653:
44: 19232: loss: 0.2102801348:
44: 22432: loss: 0.2101056271:
44: 25632: loss: 0.2106403470:
44: 28832: loss: 0.2090461861:
44: 32032: loss: 0.2090663329:
44: 35232: loss: 0.2082807623:
44: 38432: loss: 0.2079195907:
44: 41632: loss: 0.2082415131:
44: 44832: loss: 0.2080721811:
44: 48032: loss: 0.2077298541:
44: 51232: loss: 0.2077075636:
44: 54432: loss: 0.2082020719:
44: 57632: loss: 0.2073901154:
44: 60832: loss: 0.2071044256:
44: 64032: loss: 0.2067066348:
44: 67232: loss: 0.2064141914:
44: 70432: loss: 0.2064419406:
44: 73632: loss: 0.2064070494:
44: 76832: loss: 0.2066057154:
44: 80032: loss: 0.2066338715:
44: 83232: loss: 0.2071051044:
44: 86432: loss: 0.2066193852:
44: 89632: loss: 0.2065537039:
44: 92832: loss: 0.2062347603:
44: 96032: loss: 0.2069019528:
44: 99232: loss: 0.2070431830:
44: 102432: loss: 0.2069765763:
44: 105632: loss: 0.2068318139:
44: 108832: loss: 0.2068565254:
44: 112032: loss: 0.2069609977:
44: 115232: loss: 0.2070845773:
44: 118432: loss: 0.2071740452:
44: 121632: loss: 0.2074080135:
Dev-Acc: 44: Accuracy: 0.9221106172: precision: 0.3766445308: recall: 0.5111375616: f1: 0.4337036503
Train-Acc: 44: Accuracy: 0.9192525744: precision: 0.7835105823: recall: 0.4891854579: f1: 0.6023150397
45: 3232: loss: 0.2007500357:
45: 6432: loss: 0.2055344883:
45: 9632: loss: 0.2031038442:
45: 12832: loss: 0.2072800185:
45: 16032: loss: 0.2073037018:
45: 19232: loss: 0.2058975869:
45: 22432: loss: 0.2054150991:
45: 25632: loss: 0.2051474267:
45: 28832: loss: 0.2052930623:
45: 32032: loss: 0.2083631188:
45: 35232: loss: 0.2078522460:
45: 38432: loss: 0.2079936246:
45: 41632: loss: 0.2078099335:
45: 44832: loss: 0.2070369775:
45: 48032: loss: 0.2065346467:
45: 51232: loss: 0.2063684114:
45: 54432: loss: 0.2067455494:
45: 57632: loss: 0.2059529862:
45: 60832: loss: 0.2059309883:
45: 64032: loss: 0.2058222092:
45: 67232: loss: 0.2063424370:
45: 70432: loss: 0.2062022685:
45: 73632: loss: 0.2060797112:
45: 76832: loss: 0.2059474478:
45: 80032: loss: 0.2061268696:
45: 83232: loss: 0.2063125916:
45: 86432: loss: 0.2060428404:
45: 89632: loss: 0.2061178272:
45: 92832: loss: 0.2059013949:
45: 96032: loss: 0.2059881558:
45: 99232: loss: 0.2061768715:
45: 102432: loss: 0.2064584279:
45: 105632: loss: 0.2061776716:
45: 108832: loss: 0.2061972191:
45: 112032: loss: 0.2063058822:
45: 115232: loss: 0.2064642497:
45: 118432: loss: 0.2064966987:
45: 121632: loss: 0.2062840545:
Dev-Acc: 45: Accuracy: 0.9224976301: precision: 0.3779408045: recall: 0.5080768577: f1: 0.4334518024
Train-Acc: 45: Accuracy: 0.9197784662: precision: 0.7874854912: recall: 0.4906317796: f1: 0.6045852236
46: 3232: loss: 0.2069370071:
46: 6432: loss: 0.2072100292:
46: 9632: loss: 0.2082597290:
46: 12832: loss: 0.2074857888:
46: 16032: loss: 0.2112761947:
46: 19232: loss: 0.2092047618:
46: 22432: loss: 0.2070685651:
46: 25632: loss: 0.2070258456:
46: 28832: loss: 0.2064532150:
46: 32032: loss: 0.2061509864:
46: 35232: loss: 0.2066384498:
46: 38432: loss: 0.2052319543:
46: 41632: loss: 0.2057042609:
46: 44832: loss: 0.2053305006:
46: 48032: loss: 0.2049806854:
46: 51232: loss: 0.2054098240:
46: 54432: loss: 0.2052502298:
46: 57632: loss: 0.2057180600:
46: 60832: loss: 0.2054838864:
46: 64032: loss: 0.2055016051:
46: 67232: loss: 0.2050838730:
46: 70432: loss: 0.2044482536:
46: 73632: loss: 0.2048391569:
46: 76832: loss: 0.2046988940:
46: 80032: loss: 0.2043039846:
46: 83232: loss: 0.2047211550:
46: 86432: loss: 0.2045350661:
46: 89632: loss: 0.2046806219:
46: 92832: loss: 0.2048801235:
46: 96032: loss: 0.2047670838:
46: 99232: loss: 0.2049446932:
46: 102432: loss: 0.2046666295:
46: 105632: loss: 0.2051039399:
46: 108832: loss: 0.2053488551:
46: 112032: loss: 0.2053397589:
46: 115232: loss: 0.2050737089:
46: 118432: loss: 0.2051373092:
46: 121632: loss: 0.2052600574:
Dev-Acc: 46: Accuracy: 0.9228349924: precision: 0.3786171575: recall: 0.5028056453: f1: 0.4319626032
Train-Acc: 46: Accuracy: 0.9201483130: precision: 0.7936083796: recall: 0.4881335875: f1: 0.6044694102
47: 3232: loss: 0.2094633539:
47: 6432: loss: 0.2063516446:
47: 9632: loss: 0.2060256202:
47: 12832: loss: 0.2044336526:
47: 16032: loss: 0.2049012577:
47: 19232: loss: 0.2068277558:
47: 22432: loss: 0.2063541575:
47: 25632: loss: 0.2059504026:
47: 28832: loss: 0.2054104747:
47: 32032: loss: 0.2053096919:
47: 35232: loss: 0.2063476604:
47: 38432: loss: 0.2066843172:
47: 41632: loss: 0.2062465304:
47: 44832: loss: 0.2064641164:
47: 48032: loss: 0.2070225635:
47: 51232: loss: 0.2062707008:
47: 54432: loss: 0.2063840819:
47: 57632: loss: 0.2068601016:
47: 60832: loss: 0.2058485634:
47: 64032: loss: 0.2057823160:
47: 67232: loss: 0.2057281001:
47: 70432: loss: 0.2056434670:
47: 73632: loss: 0.2050734172:
47: 76832: loss: 0.2049437493:
47: 80032: loss: 0.2047831737:
47: 83232: loss: 0.2051289063:
47: 86432: loss: 0.2053784962:
47: 89632: loss: 0.2057223658:
47: 92832: loss: 0.2058275008:
47: 96032: loss: 0.2053925008:
47: 99232: loss: 0.2051509565:
47: 102432: loss: 0.2049651338:
47: 105632: loss: 0.2051277584:
47: 108832: loss: 0.2051529876:
47: 112032: loss: 0.2049435701:
47: 115232: loss: 0.2044107568:
47: 118432: loss: 0.2042090610:
47: 121632: loss: 0.2039517689:
Dev-Acc: 47: Accuracy: 0.9228746295: precision: 0.3783749036: recall: 0.5004250978: f1: 0.4309246651
Train-Acc: 47: Accuracy: 0.9204030633: precision: 0.7955493741: recall: 0.4888567484: f1: 0.6055867742
48: 3232: loss: 0.1950351494:
48: 6432: loss: 0.1943990072:
48: 9632: loss: 0.1945356033:
48: 12832: loss: 0.2000999730:
48: 16032: loss: 0.2005157001:
48: 19232: loss: 0.2003831060:
48: 22432: loss: 0.2001021964:
48: 25632: loss: 0.2012935802:
48: 28832: loss: 0.2011436863:
48: 32032: loss: 0.2015353490:
48: 35232: loss: 0.2019677341:
48: 38432: loss: 0.2022526693:
48: 41632: loss: 0.2017751115:
48: 44832: loss: 0.2012433215:
48: 48032: loss: 0.1999771196:
48: 51232: loss: 0.1999593781:
48: 54432: loss: 0.2002285307:
48: 57632: loss: 0.2006906119:
48: 60832: loss: 0.2011304568:
48: 64032: loss: 0.2012129224:
48: 67232: loss: 0.2017803046:
48: 70432: loss: 0.2020857716:
48: 73632: loss: 0.2022377084:
48: 76832: loss: 0.2030730244:
48: 80032: loss: 0.2030306315:
48: 83232: loss: 0.2031371121:
48: 86432: loss: 0.2031200913:
48: 89632: loss: 0.2031184289:
48: 92832: loss: 0.2034147211:
48: 96032: loss: 0.2035002884:
48: 99232: loss: 0.2033884286:
48: 102432: loss: 0.2029560539:
48: 105632: loss: 0.2027767854:
48: 108832: loss: 0.2031293277:
48: 112032: loss: 0.2031368254:
48: 115232: loss: 0.2029476026:
48: 118432: loss: 0.2029237396:
48: 121632: loss: 0.2029763448:
Dev-Acc: 48: Accuracy: 0.9231822491: precision: 0.3792969257: recall: 0.4971943547: f1: 0.4303164091
Train-Acc: 48: Accuracy: 0.9206084609: precision: 0.7981947131: recall: 0.4883308132: f1: 0.6059468940
49: 3232: loss: 0.1931690065:
49: 6432: loss: 0.1956627515:
49: 9632: loss: 0.1963060919:
49: 12832: loss: 0.1982653017:
49: 16032: loss: 0.2001678016:
49: 19232: loss: 0.2003551489:
49: 22432: loss: 0.2005282665:
49: 25632: loss: 0.2006828648:
49: 28832: loss: 0.1994663977:
49: 32032: loss: 0.2005675152:
49: 35232: loss: 0.1999790374:
49: 38432: loss: 0.1993951208:
49: 41632: loss: 0.1991021880:
49: 44832: loss: 0.1993763638:
49: 48032: loss: 0.1989362155:
49: 51232: loss: 0.1986557743:
49: 54432: loss: 0.1991775249:
49: 57632: loss: 0.1989825924:
49: 60832: loss: 0.1994684953:
49: 64032: loss: 0.1992084028:
49: 67232: loss: 0.1994401404:
49: 70432: loss: 0.1994726224:
49: 73632: loss: 0.1997050562:
49: 76832: loss: 0.2003243564:
49: 80032: loss: 0.2002978512:
49: 83232: loss: 0.2004006245:
49: 86432: loss: 0.2006588287:
49: 89632: loss: 0.2007222912:
49: 92832: loss: 0.2005215276:
49: 96032: loss: 0.2007245330:
49: 99232: loss: 0.2010701734:
49: 102432: loss: 0.2011888424:
49: 105632: loss: 0.2012019434:
49: 108832: loss: 0.2018816700:
49: 112032: loss: 0.2019798349:
49: 115232: loss: 0.2020105896:
49: 118432: loss: 0.2020897688:
49: 121632: loss: 0.2016946905:
Dev-Acc: 49: Accuracy: 0.9233806729: precision: 0.3804390181: recall: 0.4980445502: f1: 0.4313696613
Train-Acc: 49: Accuracy: 0.9208221436: precision: 0.7999784807: recall: 0.4887910065: f1: 0.6068149357
50: 3232: loss: 0.2056067504:
50: 6432: loss: 0.2099649755:
50: 9632: loss: 0.2029606856:
50: 12832: loss: 0.2054761742:
50: 16032: loss: 0.2061327087:
50: 19232: loss: 0.2025289878:
50: 22432: loss: 0.2040775881:
50: 25632: loss: 0.2036945975:
50: 28832: loss: 0.2032927399:
50: 32032: loss: 0.2029321530:
50: 35232: loss: 0.2029578082:
50: 38432: loss: 0.2036945933:
50: 41632: loss: 0.2035066989:
50: 44832: loss: 0.2038344301:
50: 48032: loss: 0.2038178939:
50: 51232: loss: 0.2042050873:
50: 54432: loss: 0.2035878345:
50: 57632: loss: 0.2036487412:
50: 60832: loss: 0.2043193377:
50: 64032: loss: 0.2038011771:
50: 67232: loss: 0.2041894872:
50: 70432: loss: 0.2040188644:
50: 73632: loss: 0.2040782229:
50: 76832: loss: 0.2033600846:
50: 80032: loss: 0.2025447953:
50: 83232: loss: 0.2026487235:
50: 86432: loss: 0.2025675910:
50: 89632: loss: 0.2025960584:
50: 92832: loss: 0.2024943024:
50: 96032: loss: 0.2017676680:
50: 99232: loss: 0.2016534224:
50: 102432: loss: 0.2013867831:
50: 105632: loss: 0.2015755767:
50: 108832: loss: 0.2013710092:
50: 112032: loss: 0.2011246708:
50: 115232: loss: 0.2012110016:
50: 118432: loss: 0.2009697668:
50: 121632: loss: 0.2008534153:
Dev-Acc: 50: Accuracy: 0.9236981869: precision: 0.3815634411: recall: 0.4954939636: f1: 0.4311288652
Train-Acc: 50: Accuracy: 0.9212083817: precision: 0.8029960125: recall: 0.4898428769: f1: 0.6084932626
