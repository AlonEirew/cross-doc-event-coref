1: 6464: loss: 0.7103455919:
1: 12864: loss: 0.7084513560:
1: 19264: loss: 0.7076287494:
1: 25664: loss: 0.7067472759:
1: 32064: loss: 0.7056696334:
1: 38464: loss: 0.7046820866:
1: 44864: loss: 0.7036425604:
1: 51264: loss: 0.7024596646:
1: 57664: loss: 0.7012821661:
1: 64064: loss: 0.7002514358:
1: 70464: loss: 0.6991572657:
1: 76864: loss: 0.6980992384:
1: 83264: loss: 0.6970197784:
1: 89664: loss: 0.6960132023:
1: 96064: loss: 0.6949460652:
1: 102464: loss: 0.6939196550:
1: 108864: loss: 0.6928645955:
1: 115264: loss: 0.6918160071:
1: 121664: loss: 0.6908178183:
1: 128064: loss: 0.6897616465:
1: 134464: loss: 0.6886945021:
1: 140864: loss: 0.6875887389:
1: 147264: loss: 0.6865526988:
1: 153664: loss: 0.6855086809:
1: 160064: loss: 0.6844472415:
1: 166464: loss: 0.6833958245:
Dev-Acc: 1: Accuracy: 0.8635696173: precision: 0.0669235003: recall: 0.1033837783: f1: 0.0812508352
Train-Acc: 1: Accuracy: 0.8547701836: precision: 0.1655258703: recall: 0.1478535271: f1: 0.1561914022
2: 6464: loss: 0.6566570824:
2: 12864: loss: 0.6549714777:
2: 19264: loss: 0.6538878204:
2: 25664: loss: 0.6528686379:
2: 32064: loss: 0.6516770128:
2: 38464: loss: 0.6505542213:
2: 44864: loss: 0.6494580547:
2: 51264: loss: 0.6483878700:
2: 57664: loss: 0.6474376225:
2: 64064: loss: 0.6465518799:
2: 70464: loss: 0.6457055526:
2: 76864: loss: 0.6447446420:
2: 83264: loss: 0.6437779438:
2: 89664: loss: 0.6427896363:
2: 96064: loss: 0.6417983172:
2: 102464: loss: 0.6407979544:
2: 108864: loss: 0.6398904063:
2: 115264: loss: 0.6389208888:
2: 121664: loss: 0.6379448838:
2: 128064: loss: 0.6370579797:
2: 134464: loss: 0.6360934752:
2: 140864: loss: 0.6351026511:
2: 147264: loss: 0.6341144392:
2: 153664: loss: 0.6331524752:
2: 160064: loss: 0.6322175481:
2: 166464: loss: 0.6312339828:
Dev-Acc: 2: Accuracy: 0.9407941699: precision: 0.0865384615: recall: 0.0015303520: f1: 0.0030075188
Train-Acc: 2: Accuracy: 0.9083617926: precision: 0.2095238095: recall: 0.0028926435: f1: 0.0057065041
3: 6464: loss: 0.6051158625:
3: 12864: loss: 0.6044894278:
3: 19264: loss: 0.6032032041:
3: 25664: loss: 0.6024976173:
3: 32064: loss: 0.6016234738:
3: 38464: loss: 0.6005592682:
3: 44864: loss: 0.5996794677:
3: 51264: loss: 0.5987293579:
3: 57664: loss: 0.5976802001:
3: 64064: loss: 0.5966368869:
3: 70464: loss: 0.5959632313:
3: 76864: loss: 0.5951198086:
3: 83264: loss: 0.5941168025:
3: 89664: loss: 0.5932799840:
3: 96064: loss: 0.5924622261:
3: 102464: loss: 0.5914090621:
3: 108864: loss: 0.5906478286:
3: 115264: loss: 0.5896688745:
3: 121664: loss: 0.5888903081:
3: 128064: loss: 0.5879926365:
3: 134464: loss: 0.5871018437:
3: 140864: loss: 0.5862818873:
3: 147264: loss: 0.5854565494:
3: 153664: loss: 0.5845905132:
3: 160064: loss: 0.5836073377:
3: 166464: loss: 0.5826117846:
Dev-Acc: 3: Accuracy: 0.9416574240: precision: 1.0000000000: recall: 0.0001700391: f1: 0.0003400204
Train-Acc: 3: Accuracy: 0.9090968966: precision: 1.0000000000: recall: 0.0000657419: f1: 0.0001314752
4: 6464: loss: 0.5564049292:
4: 12864: loss: 0.5567048481:
4: 19264: loss: 0.5557817815:
4: 25664: loss: 0.5551203048:
4: 32064: loss: 0.5547234734:
4: 38464: loss: 0.5537974902:
4: 44864: loss: 0.5532264349:
4: 51264: loss: 0.5523483599:
4: 57664: loss: 0.5515233952:
4: 64064: loss: 0.5510507433:
4: 70464: loss: 0.5503184391:
4: 76864: loss: 0.5494456649:
4: 83264: loss: 0.5487068438:
4: 89664: loss: 0.5476320896:
4: 96064: loss: 0.5466526450:
4: 102464: loss: 0.5456965989:
4: 108864: loss: 0.5451312568:
4: 115264: loss: 0.5444121799:
4: 121664: loss: 0.5436728004:
4: 128064: loss: 0.5426074548:
4: 134464: loss: 0.5418181086:
4: 140864: loss: 0.5408347983:
4: 147264: loss: 0.5401285140:
4: 153664: loss: 0.5391068743:
4: 160064: loss: 0.5381784658:
4: 166464: loss: 0.5372281751:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 6464: loss: 0.5149609801:
5: 12864: loss: 0.5137985381:
5: 19264: loss: 0.5123490330:
5: 25664: loss: 0.5118648771:
5: 32064: loss: 0.5106138178:
5: 38464: loss: 0.5097203790:
5: 44864: loss: 0.5085794506:
5: 51264: loss: 0.5081507470:
5: 57664: loss: 0.5076836637:
5: 64064: loss: 0.5070102626:
5: 70464: loss: 0.5062051625:
5: 76864: loss: 0.5057783037:
5: 83264: loss: 0.5049042961:
5: 89664: loss: 0.5042176285:
5: 96064: loss: 0.5034574654:
5: 102464: loss: 0.5026793750:
5: 108864: loss: 0.5019998881:
5: 115264: loss: 0.5012215595:
5: 121664: loss: 0.5002562144:
5: 128064: loss: 0.4996010642:
5: 134464: loss: 0.4991436893:
5: 140864: loss: 0.4981693087:
5: 147264: loss: 0.4972910538:
5: 153664: loss: 0.4964591237:
5: 160064: loss: 0.4957664420:
5: 166464: loss: 0.4950926457:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 6464: loss: 0.4751521748:
6: 12864: loss: 0.4774927638:
6: 19264: loss: 0.4769695834:
6: 25664: loss: 0.4747963658:
6: 32064: loss: 0.4730085456:
6: 38464: loss: 0.4724928399:
6: 44864: loss: 0.4717028589:
6: 51264: loss: 0.4707498509:
6: 57664: loss: 0.4699207952:
6: 64064: loss: 0.4690014614:
6: 70464: loss: 0.4679773086:
6: 76864: loss: 0.4669673758:
6: 83264: loss: 0.4664574002:
6: 89664: loss: 0.4655393239:
6: 96064: loss: 0.4648955946:
6: 102464: loss: 0.4638465698:
6: 108864: loss: 0.4632393391:
6: 115264: loss: 0.4623240293:
6: 121664: loss: 0.4615222066:
6: 128064: loss: 0.4606548197:
6: 134464: loss: 0.4597258260:
6: 140864: loss: 0.4592177666:
6: 147264: loss: 0.4584817261:
6: 153664: loss: 0.4577744547:
6: 160064: loss: 0.4571633721:
6: 166464: loss: 0.4563978036:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 6464: loss: 0.4428767735:
7: 12864: loss: 0.4375998136:
7: 19264: loss: 0.4399290921:
7: 25664: loss: 0.4385883281:
7: 32064: loss: 0.4359189169:
7: 38464: loss: 0.4349619828:
7: 44864: loss: 0.4332035846:
7: 51264: loss: 0.4335888514:
7: 57664: loss: 0.4334608736:
7: 64064: loss: 0.4327599083:
7: 70464: loss: 0.4318595968:
7: 76864: loss: 0.4307970703:
7: 83264: loss: 0.4301949428:
7: 89664: loss: 0.4295131592:
7: 96064: loss: 0.4292460081:
7: 102464: loss: 0.4286299081:
7: 108864: loss: 0.4275409037:
7: 115264: loss: 0.4270241534:
7: 121664: loss: 0.4260097826:
7: 128064: loss: 0.4253403372:
7: 134464: loss: 0.4247967229:
7: 140864: loss: 0.4240012193:
7: 147264: loss: 0.4233946639:
7: 153664: loss: 0.4225231803:
7: 160064: loss: 0.4222220958:
7: 166464: loss: 0.4216304038:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.4049591550:
8: 12864: loss: 0.4033327563:
8: 19264: loss: 0.4036033963:
8: 25664: loss: 0.4015514179:
8: 32064: loss: 0.4017812105:
8: 38464: loss: 0.4012382992:
8: 44864: loss: 0.4006171477:
8: 51264: loss: 0.4009699815:
8: 57664: loss: 0.4003037294:
8: 64064: loss: 0.3997752927:
8: 70464: loss: 0.3989483958:
8: 76864: loss: 0.3986558378:
8: 83264: loss: 0.3986122835:
8: 89664: loss: 0.3981826187:
8: 96064: loss: 0.3971225669:
8: 102464: loss: 0.3962745069:
8: 108864: loss: 0.3957228278:
8: 115264: loss: 0.3950391419:
8: 121664: loss: 0.3944984438:
8: 128064: loss: 0.3939775872:
8: 134464: loss: 0.3934188270:
8: 140864: loss: 0.3928315922:
8: 147264: loss: 0.3923972530:
8: 153664: loss: 0.3920528945:
8: 160064: loss: 0.3919260096:
8: 166464: loss: 0.3917013415:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.3719277740:
9: 12864: loss: 0.3775726390:
9: 19264: loss: 0.3745405360:
9: 25664: loss: 0.3733900290:
9: 32064: loss: 0.3725252240:
9: 38464: loss: 0.3727424444:
9: 44864: loss: 0.3732873014:
9: 51264: loss: 0.3732935284:
9: 57664: loss: 0.3738848721:
9: 64064: loss: 0.3738211322:
9: 70464: loss: 0.3730441673:
9: 76864: loss: 0.3719638554:
9: 83264: loss: 0.3712253310:
9: 89664: loss: 0.3709756427:
9: 96064: loss: 0.3705057847:
9: 102464: loss: 0.3698965168:
9: 108864: loss: 0.3694967587:
9: 115264: loss: 0.3690635209:
9: 121664: loss: 0.3690676464:
9: 128064: loss: 0.3688542261:
9: 134464: loss: 0.3681570320:
9: 140864: loss: 0.3679109864:
9: 147264: loss: 0.3674116243:
9: 153664: loss: 0.3667722468:
9: 160064: loss: 0.3665597526:
9: 166464: loss: 0.3660700670:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.3472563776:
10: 12864: loss: 0.3499901471:
10: 19264: loss: 0.3510222397:
10: 25664: loss: 0.3488184938:
10: 32064: loss: 0.3508327906:
10: 38464: loss: 0.3499298323:
10: 44864: loss: 0.3499962375:
10: 51264: loss: 0.3501044756:
10: 57664: loss: 0.3491038069:
10: 64064: loss: 0.3497383503:
10: 70464: loss: 0.3487424240:
10: 76864: loss: 0.3482613833:
10: 83264: loss: 0.3481637173:
10: 89664: loss: 0.3474353747:
10: 96064: loss: 0.3477036959:
10: 102464: loss: 0.3476916941:
10: 108864: loss: 0.3479862503:
10: 115264: loss: 0.3480295523:
10: 121664: loss: 0.3474106639:
10: 128064: loss: 0.3468409002:
10: 134464: loss: 0.3463796347:
10: 140864: loss: 0.3461498634:
10: 147264: loss: 0.3457069373:
10: 153664: loss: 0.3452270022:
10: 160064: loss: 0.3450479166:
10: 166464: loss: 0.3450239275:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.3354729354:
11: 12864: loss: 0.3369998960:
11: 19264: loss: 0.3331095175:
11: 25664: loss: 0.3342922366:
11: 32064: loss: 0.3319954353:
11: 38464: loss: 0.3324398737:
11: 44864: loss: 0.3331717316:
11: 51264: loss: 0.3323472765:
11: 57664: loss: 0.3315712099:
11: 64064: loss: 0.3321924344:
11: 70464: loss: 0.3324099624:
11: 76864: loss: 0.3322689628:
11: 83264: loss: 0.3321015078:
11: 89664: loss: 0.3324261555:
11: 96064: loss: 0.3316875072:
11: 102464: loss: 0.3321459213:
11: 108864: loss: 0.3321014971:
11: 115264: loss: 0.3315276441:
11: 121664: loss: 0.3307538054:
11: 128064: loss: 0.3307985080:
11: 134464: loss: 0.3306167939:
11: 140864: loss: 0.3300363188:
11: 147264: loss: 0.3294433648:
11: 153664: loss: 0.3288262536:
11: 160064: loss: 0.3286243945:
11: 166464: loss: 0.3279110207:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.3154646212:
12: 12864: loss: 0.3163886508:
12: 19264: loss: 0.3187063380:
12: 25664: loss: 0.3172530954:
12: 32064: loss: 0.3162692596:
12: 38464: loss: 0.3166158638:
12: 44864: loss: 0.3165836275:
12: 51264: loss: 0.3166468197:
12: 57664: loss: 0.3162936256:
12: 64064: loss: 0.3152570666:
12: 70464: loss: 0.3149834666:
12: 76864: loss: 0.3150088454:
12: 83264: loss: 0.3151835394:
12: 89664: loss: 0.3144782559:
12: 96064: loss: 0.3142330522:
12: 102464: loss: 0.3144428310:
12: 108864: loss: 0.3144614527:
12: 115264: loss: 0.3156834755:
12: 121664: loss: 0.3151251490:
12: 128064: loss: 0.3147922508:
12: 134464: loss: 0.3147561787:
12: 140864: loss: 0.3149353822:
12: 147264: loss: 0.3142575082:
12: 153664: loss: 0.3140743269:
12: 160064: loss: 0.3137926455:
12: 166464: loss: 0.3135572831:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.3156759761:
13: 12864: loss: 0.3120733965:
13: 19264: loss: 0.3045553389:
13: 25664: loss: 0.3039333492:
13: 32064: loss: 0.3053009362:
13: 38464: loss: 0.3062910864:
13: 44864: loss: 0.3054236462:
13: 51264: loss: 0.3048537484:
13: 57664: loss: 0.3039931552:
13: 64064: loss: 0.3035246724:
13: 70464: loss: 0.3032057642:
13: 76864: loss: 0.3026623268:
13: 83264: loss: 0.3022188180:
13: 89664: loss: 0.3026049453:
13: 96064: loss: 0.3033432542:
13: 102464: loss: 0.3037036885:
13: 108864: loss: 0.3026864393:
13: 115264: loss: 0.3027086155:
13: 121664: loss: 0.3024298162:
13: 128064: loss: 0.3024690569:
13: 134464: loss: 0.3029438260:
13: 140864: loss: 0.3032046805:
13: 147264: loss: 0.3023670302:
13: 153664: loss: 0.3023767197:
13: 160064: loss: 0.3022572404:
13: 166464: loss: 0.3020647731:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.2942127433:
14: 12864: loss: 0.2955009933:
14: 19264: loss: 0.2944305598:
14: 25664: loss: 0.2961225289:
14: 32064: loss: 0.2962430214:
14: 38464: loss: 0.2960621880:
14: 44864: loss: 0.2959111923:
14: 51264: loss: 0.2955654662:
14: 57664: loss: 0.2952595863:
14: 64064: loss: 0.2939503056:
14: 70464: loss: 0.2936011617:
14: 76864: loss: 0.2939761667:
14: 83264: loss: 0.2933451881:
14: 89664: loss: 0.2938570203:
14: 96064: loss: 0.2939919285:
14: 102464: loss: 0.2936646112:
14: 108864: loss: 0.2928976588:
14: 115264: loss: 0.2931855368:
14: 121664: loss: 0.2932709632:
14: 128064: loss: 0.2934652758:
14: 134464: loss: 0.2931781014:
14: 140864: loss: 0.2931061065:
14: 147264: loss: 0.2929809763:
14: 153664: loss: 0.2929984774:
14: 160064: loss: 0.2925480090:
14: 166464: loss: 0.2922837061:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.2899284685:
15: 12864: loss: 0.2865861582:
15: 19264: loss: 0.2863846529:
15: 25664: loss: 0.2888140628:
15: 32064: loss: 0.2859882656:
15: 38464: loss: 0.2857428069:
15: 44864: loss: 0.2849001598:
15: 51264: loss: 0.2835940914:
15: 57664: loss: 0.2848019652:
15: 64064: loss: 0.2849980626:
15: 70464: loss: 0.2837726205:
15: 76864: loss: 0.2836723430:
15: 83264: loss: 0.2844323186:
15: 89664: loss: 0.2846484944:
15: 96064: loss: 0.2846887947:
15: 102464: loss: 0.2846173371:
15: 108864: loss: 0.2840933240:
15: 115264: loss: 0.2835464561:
15: 121664: loss: 0.2840339683:
15: 128064: loss: 0.2837155850:
15: 134464: loss: 0.2836057073:
15: 140864: loss: 0.2838297512:
15: 147264: loss: 0.2840431743:
15: 153664: loss: 0.2836406360:
15: 160064: loss: 0.2836464687:
15: 166464: loss: 0.2833140435:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.2811923152:
16: 12864: loss: 0.2821639995:
16: 19264: loss: 0.2819766227:
16: 25664: loss: 0.2799834382:
16: 32064: loss: 0.2813983724:
16: 38464: loss: 0.2804159270:
16: 44864: loss: 0.2820312536:
16: 51264: loss: 0.2790641994:
16: 57664: loss: 0.2781651874:
16: 64064: loss: 0.2782932249:
16: 70464: loss: 0.2775752273:
16: 76864: loss: 0.2767004663:
16: 83264: loss: 0.2765919901:
16: 89664: loss: 0.2767346617:
16: 96064: loss: 0.2764870155:
16: 102464: loss: 0.2765551337:
16: 108864: loss: 0.2757349659:
16: 115264: loss: 0.2755152679:
16: 121664: loss: 0.2755308682:
16: 128064: loss: 0.2755696879:
16: 134464: loss: 0.2752362064:
16: 140864: loss: 0.2753760469:
16: 147264: loss: 0.2756068281:
16: 153664: loss: 0.2755501163:
16: 160064: loss: 0.2760226131:
16: 166464: loss: 0.2761174002:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.2765798996:
17: 12864: loss: 0.2739765871:
17: 19264: loss: 0.2762933526:
17: 25664: loss: 0.2754389332:
17: 32064: loss: 0.2764580990:
17: 38464: loss: 0.2753039976:
17: 44864: loss: 0.2758864650:
17: 51264: loss: 0.2752525107:
17: 57664: loss: 0.2736124933:
17: 64064: loss: 0.2729729434:
17: 70464: loss: 0.2727182950:
17: 76864: loss: 0.2725722787:
17: 83264: loss: 0.2721462462:
17: 89664: loss: 0.2715309433:
17: 96064: loss: 0.2709996430:
17: 102464: loss: 0.2710251963:
17: 108864: loss: 0.2714767342:
17: 115264: loss: 0.2713671326:
17: 121664: loss: 0.2717910685:
17: 128064: loss: 0.2719485609:
17: 134464: loss: 0.2711013257:
17: 140864: loss: 0.2708510506:
17: 147264: loss: 0.2704747005:
17: 153664: loss: 0.2701419435:
17: 160064: loss: 0.2698626787:
17: 166464: loss: 0.2697559183:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.2658586164:
18: 12864: loss: 0.2651686342:
18: 19264: loss: 0.2657745286:
18: 25664: loss: 0.2663556908:
18: 32064: loss: 0.2653230055:
18: 38464: loss: 0.2654775761:
18: 44864: loss: 0.2660119920:
18: 51264: loss: 0.2643511117:
18: 57664: loss: 0.2654377787:
18: 64064: loss: 0.2638772097:
18: 70464: loss: 0.2640358009:
18: 76864: loss: 0.2633109293:
18: 83264: loss: 0.2633012501:
18: 89664: loss: 0.2634317539:
18: 96064: loss: 0.2643644014:
18: 102464: loss: 0.2646864238:
18: 108864: loss: 0.2642466164:
18: 115264: loss: 0.2641810779:
18: 121664: loss: 0.2636430879:
18: 128064: loss: 0.2632769228:
18: 134464: loss: 0.2637161698:
18: 140864: loss: 0.2634537076:
18: 147264: loss: 0.2630926009:
18: 153664: loss: 0.2634894599:
18: 160064: loss: 0.2636543037:
18: 166464: loss: 0.2638379219:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 6464: loss: 0.2637501178:
19: 12864: loss: 0.2644402598:
19: 19264: loss: 0.2631451406:
19: 25664: loss: 0.2627117131:
19: 32064: loss: 0.2605744296:
19: 38464: loss: 0.2596886405:
19: 44864: loss: 0.2599035354:
19: 51264: loss: 0.2591867802:
19: 57664: loss: 0.2593096111:
19: 64064: loss: 0.2583402953:
19: 70464: loss: 0.2586645495:
19: 76864: loss: 0.2586138956:
19: 83264: loss: 0.2590544019:
19: 89664: loss: 0.2595931689:
19: 96064: loss: 0.2592316334:
19: 102464: loss: 0.2590338526:
19: 108864: loss: 0.2593101074:
19: 115264: loss: 0.2591589003:
19: 121664: loss: 0.2592352694:
19: 128064: loss: 0.2594852391:
19: 134464: loss: 0.2592029140:
19: 140864: loss: 0.2594332506:
19: 147264: loss: 0.2589684358:
19: 153664: loss: 0.2588307358:
19: 160064: loss: 0.2586279765:
19: 166464: loss: 0.2584433540:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.9091267586: precision: 1.0000000000: recall: 0.0003944514: f1: 0.0007885917
20: 6464: loss: 0.2642231545:
20: 12864: loss: 0.2618745964:
20: 19264: loss: 0.2559662489:
20: 25664: loss: 0.2541673642:
20: 32064: loss: 0.2555151217:
20: 38464: loss: 0.2549035316:
20: 44864: loss: 0.2550032373:
20: 51264: loss: 0.2547298656:
20: 57664: loss: 0.2549081758:
20: 64064: loss: 0.2545258871:
20: 70464: loss: 0.2539687811:
20: 76864: loss: 0.2539776119:
20: 83264: loss: 0.2535329984:
20: 89664: loss: 0.2529329449:
20: 96064: loss: 0.2530313655:
20: 102464: loss: 0.2532582796:
20: 108864: loss: 0.2535137030:
20: 115264: loss: 0.2535697576:
20: 121664: loss: 0.2535185799:
20: 128064: loss: 0.2533910864:
20: 134464: loss: 0.2529662614:
20: 140864: loss: 0.2528764002:
20: 147264: loss: 0.2532066009:
20: 153664: loss: 0.2533153940:
20: 160064: loss: 0.2532771880:
20: 166464: loss: 0.2534952359:
Dev-Acc: 20: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 20: Accuracy: 0.9093897343: precision: 1.0000000000: recall: 0.0032870949: f1: 0.0065526505
21: 6464: loss: 0.2521069253:
21: 12864: loss: 0.2517280234:
21: 19264: loss: 0.2530057057:
21: 25664: loss: 0.2506106210:
21: 32064: loss: 0.2487362012:
21: 38464: loss: 0.2484659298:
21: 44864: loss: 0.2496394368:
21: 51264: loss: 0.2480212584:
21: 57664: loss: 0.2472365794:
21: 64064: loss: 0.2471758828:
21: 70464: loss: 0.2478889937:
21: 76864: loss: 0.2495606548:
21: 83264: loss: 0.2489994764:
21: 89664: loss: 0.2495462834:
21: 96064: loss: 0.2484060819:
21: 102464: loss: 0.2484022821:
21: 108864: loss: 0.2487443183:
21: 115264: loss: 0.2487922630:
21: 121664: loss: 0.2482444966:
21: 128064: loss: 0.2483649934:
21: 134464: loss: 0.2486388319:
21: 140864: loss: 0.2482634988:
21: 147264: loss: 0.2485137541:
21: 153664: loss: 0.2487849227:
21: 160064: loss: 0.2487578341:
21: 166464: loss: 0.2488245802:
Dev-Acc: 21: Accuracy: 0.9416375756: precision: 0.4666666667: recall: 0.0011902738: f1: 0.0023744912
Train-Acc: 21: Accuracy: 0.9101547599: precision: 0.9198113208: recall: 0.0128196700: f1: 0.0252869092
22: 6464: loss: 0.2535873750:
22: 12864: loss: 0.2496309723:
22: 19264: loss: 0.2492228046:
22: 25664: loss: 0.2496449007:
22: 32064: loss: 0.2489222375:
22: 38464: loss: 0.2500172456:
22: 44864: loss: 0.2502075728:
22: 51264: loss: 0.2499722560:
22: 57664: loss: 0.2487389693:
22: 64064: loss: 0.2482471464:
22: 70464: loss: 0.2483595066:
22: 76864: loss: 0.2478900913:
22: 83264: loss: 0.2473090840:
22: 89664: loss: 0.2467406392:
22: 96064: loss: 0.2458646912:
22: 102464: loss: 0.2453894490:
22: 108864: loss: 0.2457783178:
22: 115264: loss: 0.2455559124:
22: 121664: loss: 0.2455778681:
22: 128064: loss: 0.2454002831:
22: 134464: loss: 0.2457403780:
22: 140864: loss: 0.2451813568:
22: 147264: loss: 0.2453178008:
22: 153664: loss: 0.2454046630:
22: 160064: loss: 0.2452977821:
22: 166464: loss: 0.2451292227:
Dev-Acc: 22: Accuracy: 0.9418458939: precision: 0.6470588235: recall: 0.0074817208: f1: 0.0147924021
Train-Acc: 22: Accuracy: 0.9110512137: precision: 0.7981818182: recall: 0.0288606929: f1: 0.0557071252
23: 6464: loss: 0.2438828901:
23: 12864: loss: 0.2404398159:
23: 19264: loss: 0.2381419684:
23: 25664: loss: 0.2408260446:
23: 32064: loss: 0.2418279230:
23: 38464: loss: 0.2400201716:
23: 44864: loss: 0.2396094510:
23: 51264: loss: 0.2398865041:
23: 57664: loss: 0.2401179818:
23: 64064: loss: 0.2395933934:
23: 70464: loss: 0.2391149741:
23: 76864: loss: 0.2396803339:
23: 83264: loss: 0.2402731945:
23: 89664: loss: 0.2407931333:
23: 96064: loss: 0.2403314007:
23: 102464: loss: 0.2408611238:
23: 108864: loss: 0.2409483425:
23: 115264: loss: 0.2405520874:
23: 121664: loss: 0.2408152814:
23: 128064: loss: 0.2408682648:
23: 134464: loss: 0.2410697452:
23: 140864: loss: 0.2409513740:
23: 147264: loss: 0.2409409626:
23: 153664: loss: 0.2410243196:
23: 160064: loss: 0.2409947899:
23: 166464: loss: 0.2413263729:
Dev-Acc: 23: Accuracy: 0.9420245290: precision: 0.7065217391: recall: 0.0110525421: f1: 0.0217646074
Train-Acc: 23: Accuracy: 0.9113321304: precision: 0.8048780488: recall: 0.0325422392: f1: 0.0625552888
24: 6464: loss: 0.2421559124:
24: 12864: loss: 0.2498519547:
24: 19264: loss: 0.2446670264:
24: 25664: loss: 0.2439471502:
24: 32064: loss: 0.2432509679:
24: 38464: loss: 0.2434501882:
24: 44864: loss: 0.2428265671:
24: 51264: loss: 0.2426240829:
24: 57664: loss: 0.2410146745:
24: 64064: loss: 0.2407225262:
24: 70464: loss: 0.2402868294:
24: 76864: loss: 0.2396272396:
24: 83264: loss: 0.2396690067:
24: 89664: loss: 0.2398494693:
24: 96064: loss: 0.2397577159:
24: 102464: loss: 0.2398943235:
24: 108864: loss: 0.2394531053:
24: 115264: loss: 0.2389574652:
24: 121664: loss: 0.2390251429:
24: 128064: loss: 0.2385677570:
24: 134464: loss: 0.2380117030:
24: 140864: loss: 0.2379897558:
24: 147264: loss: 0.2379289878:
24: 153664: loss: 0.2379266623:
24: 160064: loss: 0.2378569997:
24: 166464: loss: 0.2375702137:
Dev-Acc: 24: Accuracy: 0.9423717856: precision: 0.7016574586: recall: 0.0215949668: f1: 0.0419003629
Train-Acc: 24: Accuracy: 0.9124138951: precision: 0.8423645320: recall: 0.0449674578: f1: 0.0853772702
25: 6464: loss: 0.2345402702:
25: 12864: loss: 0.2325706214:
25: 19264: loss: 0.2340086875:
25: 25664: loss: 0.2331495927:
25: 32064: loss: 0.2340556662:
25: 38464: loss: 0.2330732680:
25: 44864: loss: 0.2334711036:
25: 51264: loss: 0.2329881635:
25: 57664: loss: 0.2330827883:
25: 64064: loss: 0.2332850462:
25: 70464: loss: 0.2330637151:
25: 76864: loss: 0.2337777604:
25: 83264: loss: 0.2340172335:
25: 89664: loss: 0.2335138219:
25: 96064: loss: 0.2342685891:
25: 102464: loss: 0.2343822585:
25: 108864: loss: 0.2339587943:
25: 115264: loss: 0.2343532174:
25: 121664: loss: 0.2347047278:
25: 128064: loss: 0.2347942455:
25: 134464: loss: 0.2348596906:
25: 140864: loss: 0.2348619076:
25: 147264: loss: 0.2349086948:
25: 153664: loss: 0.2347813515:
25: 160064: loss: 0.2347609081:
25: 166464: loss: 0.2345222941:
Dev-Acc: 25: Accuracy: 0.9422130585: precision: 0.5845697329: recall: 0.0334977045: f1: 0.0633644259
Train-Acc: 25: Accuracy: 0.9132744670: precision: 0.8472222222: recall: 0.0561435803: f1: 0.1053085887
26: 6464: loss: 0.2194408869:
26: 12864: loss: 0.2243498899:
26: 19264: loss: 0.2266330833:
26: 25664: loss: 0.2286871835:
26: 32064: loss: 0.2303275838:
26: 38464: loss: 0.2300970394:
26: 44864: loss: 0.2296256656:
26: 51264: loss: 0.2297313351:
26: 57664: loss: 0.2299926514:
26: 64064: loss: 0.2302623168:
26: 70464: loss: 0.2304144188:
26: 76864: loss: 0.2305967014:
26: 83264: loss: 0.2302346626:
26: 89664: loss: 0.2306701413:
26: 96064: loss: 0.2307256230:
26: 102464: loss: 0.2307424818:
26: 108864: loss: 0.2312071505:
26: 115264: loss: 0.2312922268:
26: 121664: loss: 0.2313083945:
26: 128064: loss: 0.2314243962:
26: 134464: loss: 0.2318226142:
26: 140864: loss: 0.2318250533:
26: 147264: loss: 0.2315515464:
26: 153664: loss: 0.2316523506:
26: 160064: loss: 0.2316271516:
26: 166464: loss: 0.2315876892:
Dev-Acc: 26: Accuracy: 0.9420840740: precision: 0.5504587156: recall: 0.0408093862: f1: 0.0759854361
Train-Acc: 26: Accuracy: 0.9138482213: precision: 0.8378607810: recall: 0.0648872526: f1: 0.1204466410
27: 6464: loss: 0.2285871936:
27: 12864: loss: 0.2309385954:
27: 19264: loss: 0.2323556537:
27: 25664: loss: 0.2320155486:
27: 32064: loss: 0.2322097473:
27: 38464: loss: 0.2316188608:
27: 44864: loss: 0.2322070157:
27: 51264: loss: 0.2302132140:
27: 57664: loss: 0.2291894551:
27: 64064: loss: 0.2294251995:
27: 70464: loss: 0.2293585868:
27: 76864: loss: 0.2292753984:
27: 83264: loss: 0.2293965158:
27: 89664: loss: 0.2294907851:
27: 96064: loss: 0.2286615901:
27: 102464: loss: 0.2284011188:
27: 108864: loss: 0.2284578881:
27: 115264: loss: 0.2279957820:
27: 121664: loss: 0.2280612992:
27: 128064: loss: 0.2280405477:
27: 134464: loss: 0.2279460697:
27: 140864: loss: 0.2284297853:
27: 147264: loss: 0.2287727533:
27: 153664: loss: 0.2283676230:
27: 160064: loss: 0.2285547056:
27: 166464: loss: 0.2286862356:
Dev-Acc: 27: Accuracy: 0.9424213767: precision: 0.5691489362: recall: 0.0545825540: f1: 0.0996121024
Train-Acc: 27: Accuracy: 0.9150494933: precision: 0.8469032707: recall: 0.0800078890: f1: 0.1462037482
28: 6464: loss: 0.2297653928:
28: 12864: loss: 0.2233564844:
28: 19264: loss: 0.2227937395:
28: 25664: loss: 0.2268550325:
28: 32064: loss: 0.2262367931:
28: 38464: loss: 0.2265595742:
28: 44864: loss: 0.2277926910:
28: 51264: loss: 0.2278584743:
28: 57664: loss: 0.2273852231:
28: 64064: loss: 0.2273144496:
28: 70464: loss: 0.2274385940:
28: 76864: loss: 0.2271613277:
28: 83264: loss: 0.2277538043:
28: 89664: loss: 0.2277476347:
28: 96064: loss: 0.2275959936:
28: 102464: loss: 0.2278447996:
28: 108864: loss: 0.2273957236:
28: 115264: loss: 0.2270406640:
28: 121664: loss: 0.2271580459:
28: 128064: loss: 0.2275786723:
28: 134464: loss: 0.2270857487:
28: 140864: loss: 0.2268566998:
28: 147264: loss: 0.2267366091:
28: 153664: loss: 0.2263498207:
28: 160064: loss: 0.2263412729:
28: 166464: loss: 0.2258425761:
Dev-Acc: 28: Accuracy: 0.9429472685: precision: 0.5797807552: recall: 0.0809386159: f1: 0.1420471501
Train-Acc: 28: Accuracy: 0.9165735245: precision: 0.8473917869: recall: 0.1003878772: f1: 0.1795097866
29: 6464: loss: 0.2154245478:
29: 12864: loss: 0.2159005882:
29: 19264: loss: 0.2185292783:
29: 25664: loss: 0.2175029008:
29: 32064: loss: 0.2199869888:
29: 38464: loss: 0.2211746114:
29: 44864: loss: 0.2212645176:
29: 51264: loss: 0.2223369940:
29: 57664: loss: 0.2232368281:
29: 64064: loss: 0.2235077603:
29: 70464: loss: 0.2232140703:
29: 76864: loss: 0.2232278273:
29: 83264: loss: 0.2231439951:
29: 89664: loss: 0.2235161550:
29: 96064: loss: 0.2236868765:
29: 102464: loss: 0.2239023539:
29: 108864: loss: 0.2239761144:
29: 115264: loss: 0.2234916989:
29: 121664: loss: 0.2233667405:
29: 128064: loss: 0.2233067750:
29: 134464: loss: 0.2235644255:
29: 140864: loss: 0.2232210094:
29: 147264: loss: 0.2233760997:
29: 153664: loss: 0.2234324000:
29: 160064: loss: 0.2235473705:
29: 166464: loss: 0.2235681009:
Dev-Acc: 29: Accuracy: 0.9432052374: precision: 0.5745489079: recall: 0.1028736609: f1: 0.1745024517
Train-Acc: 29: Accuracy: 0.9179601073: precision: 0.8366606171: recall: 0.1212280586: f1: 0.2117714614
30: 6464: loss: 0.2139321619:
30: 12864: loss: 0.2148679943:
30: 19264: loss: 0.2178599325:
30: 25664: loss: 0.2169612923:
30: 32064: loss: 0.2178290917:
30: 38464: loss: 0.2185839364:
30: 44864: loss: 0.2200087213:
30: 51264: loss: 0.2209041971:
30: 57664: loss: 0.2206944985:
30: 64064: loss: 0.2207327403:
30: 70464: loss: 0.2202167161:
30: 76864: loss: 0.2194398149:
30: 83264: loss: 0.2193170644:
30: 89664: loss: 0.2192462933:
30: 96064: loss: 0.2196601931:
30: 102464: loss: 0.2202080732:
30: 108864: loss: 0.2198002304:
30: 115264: loss: 0.2204158105:
30: 121664: loss: 0.2213919557:
30: 128064: loss: 0.2206505229:
30: 134464: loss: 0.2214526307:
30: 140864: loss: 0.2210687033:
30: 147264: loss: 0.2214186443:
30: 153664: loss: 0.2210643866:
30: 160064: loss: 0.2206707947:
30: 166464: loss: 0.2212493420:
Dev-Acc: 30: Accuracy: 0.9433441758: precision: 0.5671641791: recall: 0.1227682367: f1: 0.2018451216
Train-Acc: 30: Accuracy: 0.9195319414: precision: 0.8267863823: recall: 0.1452895931: f1: 0.2471482890
31: 6464: loss: 0.2387268848:
31: 12864: loss: 0.2272306418:
31: 19264: loss: 0.2239841949:
31: 25664: loss: 0.2233518241:
31: 32064: loss: 0.2219678628:
31: 38464: loss: 0.2214426543:
31: 44864: loss: 0.2227698371:
31: 51264: loss: 0.2217667540:
31: 57664: loss: 0.2212160642:
31: 64064: loss: 0.2211901284:
31: 70464: loss: 0.2217543700:
31: 76864: loss: 0.2209773184:
31: 83264: loss: 0.2209611769:
31: 89664: loss: 0.2208218472:
31: 96064: loss: 0.2200152978:
31: 102464: loss: 0.2193756549:
31: 108864: loss: 0.2187394183:
31: 115264: loss: 0.2188335426:
31: 121664: loss: 0.2187043002:
31: 128064: loss: 0.2188365989:
31: 134464: loss: 0.2191731039:
31: 140864: loss: 0.2187321532:
31: 147264: loss: 0.2188279292:
31: 153664: loss: 0.2189194749:
31: 160064: loss: 0.2186912435:
31: 166464: loss: 0.2189878267:
Dev-Acc: 31: Accuracy: 0.9435227513: precision: 0.5676449535: recall: 0.1348410134: f1: 0.2179170102
Train-Acc: 31: Accuracy: 0.9209244847: precision: 0.8057442866: recall: 0.1715206101: f1: 0.2828337579
32: 6464: loss: 0.2208291668:
32: 12864: loss: 0.2240662217:
32: 19264: loss: 0.2218892541:
32: 25664: loss: 0.2206450833:
32: 32064: loss: 0.2212616562:
32: 38464: loss: 0.2213981060:
32: 44864: loss: 0.2217041186:
32: 51264: loss: 0.2209565862:
32: 57664: loss: 0.2204061539:
32: 64064: loss: 0.2202243126:
32: 70464: loss: 0.2200670660:
32: 76864: loss: 0.2183320431:
32: 83264: loss: 0.2183156840:
32: 89664: loss: 0.2185512674:
32: 96064: loss: 0.2179579816:
32: 102464: loss: 0.2181418122:
32: 108864: loss: 0.2178195330:
32: 115264: loss: 0.2179677513:
32: 121664: loss: 0.2179722511:
32: 128064: loss: 0.2183958200:
32: 134464: loss: 0.2179528719:
32: 140864: loss: 0.2177699465:
32: 147264: loss: 0.2178825830:
32: 153664: loss: 0.2177511043:
32: 160064: loss: 0.2177850031:
32: 166464: loss: 0.2173684592:
Dev-Acc: 32: Accuracy: 0.9439792037: precision: 0.5771503611: recall: 0.1494643768: f1: 0.2374392220
Train-Acc: 32: Accuracy: 0.9216476083: precision: 0.7877293892: recall: 0.1890736967: f1: 0.3049517549
33: 6464: loss: 0.2137454155:
33: 12864: loss: 0.2127425102:
33: 19264: loss: 0.2133281144:
33: 25664: loss: 0.2148407614:
33: 32064: loss: 0.2145504038:
33: 38464: loss: 0.2142941400:
33: 44864: loss: 0.2142481742:
33: 51264: loss: 0.2149780258:
33: 57664: loss: 0.2148445017:
33: 64064: loss: 0.2139295496:
33: 70464: loss: 0.2139961928:
33: 76864: loss: 0.2130781869:
33: 83264: loss: 0.2142063936:
33: 89664: loss: 0.2141668295:
33: 96064: loss: 0.2135971091:
33: 102464: loss: 0.2138214887:
33: 108864: loss: 0.2144392913:
33: 115264: loss: 0.2149435441:
33: 121664: loss: 0.2148825077:
33: 128064: loss: 0.2147248697:
33: 134464: loss: 0.2150115661:
33: 140864: loss: 0.2153548128:
33: 147264: loss: 0.2151528819:
33: 153664: loss: 0.2150971847:
33: 160064: loss: 0.2153644141:
33: 166464: loss: 0.2151699221:
Dev-Acc: 33: Accuracy: 0.9439196587: precision: 0.5677114134: recall: 0.1632375446: f1: 0.2535657686
Train-Acc: 33: Accuracy: 0.9219404459: precision: 0.7686156922: recall: 0.2022220761: f1: 0.3201998647
34: 6464: loss: 0.2196149246:
34: 12864: loss: 0.2125187161:
34: 19264: loss: 0.2108087741:
34: 25664: loss: 0.2111412193:
34: 32064: loss: 0.2092553816:
34: 38464: loss: 0.2112513125:
34: 44864: loss: 0.2097014039:
34: 51264: loss: 0.2103166460:
34: 57664: loss: 0.2101795610:
34: 64064: loss: 0.2111241650:
34: 70464: loss: 0.2123638456:
34: 76864: loss: 0.2124460663:
34: 83264: loss: 0.2129322572:
34: 89664: loss: 0.2132795334:
34: 96064: loss: 0.2142727594:
34: 102464: loss: 0.2134185950:
34: 108864: loss: 0.2132890157:
34: 115264: loss: 0.2133768103:
34: 121664: loss: 0.2134257479:
34: 128064: loss: 0.2140042415:
34: 134464: loss: 0.2138012814:
34: 140864: loss: 0.2136122793:
34: 147264: loss: 0.2135928388:
34: 153664: loss: 0.2134753655:
34: 160064: loss: 0.2136115641:
34: 166464: loss: 0.2135875504:
Dev-Acc: 34: Accuracy: 0.9430564046: precision: 0.5350444225: recall: 0.1843223942: f1: 0.2741874289
Train-Acc: 34: Accuracy: 0.9224544764: precision: 0.7556012803: recall: 0.2172769706: f1: 0.3375031912
35: 6464: loss: 0.2102024110:
35: 12864: loss: 0.2143489712:
35: 19264: loss: 0.2134347561:
35: 25664: loss: 0.2125557143:
35: 32064: loss: 0.2118949877:
35: 38464: loss: 0.2121891010:
35: 44864: loss: 0.2116234125:
35: 51264: loss: 0.2113852284:
35: 57664: loss: 0.2122137567:
35: 64064: loss: 0.2128901159:
35: 70464: loss: 0.2134845068:
35: 76864: loss: 0.2142251876:
35: 83264: loss: 0.2146006438:
35: 89664: loss: 0.2142034285:
35: 96064: loss: 0.2137296071:
35: 102464: loss: 0.2135001492:
35: 108864: loss: 0.2133205956:
35: 115264: loss: 0.2130403186:
35: 121664: loss: 0.2132566331:
35: 128064: loss: 0.2132701018:
35: 134464: loss: 0.2126237065:
35: 140864: loss: 0.2123623022:
35: 147264: loss: 0.2118877609:
35: 153664: loss: 0.2118025254:
35: 160064: loss: 0.2117511781:
35: 166464: loss: 0.2116148943:
Dev-Acc: 35: Accuracy: 0.9424114823: precision: 0.5156313439: recall: 0.2159496684: f1: 0.3044103547
Train-Acc: 35: Accuracy: 0.9236198664: precision: 0.7570310848: recall: 0.2353559924: f1: 0.3590772317
36: 6464: loss: 0.2076453942:
36: 12864: loss: 0.2124671202:
36: 19264: loss: 0.2114782952:
36: 25664: loss: 0.2102753881:
36: 32064: loss: 0.2102086709:
36: 38464: loss: 0.2102953020:
36: 44864: loss: 0.2099406365:
36: 51264: loss: 0.2096008806:
36: 57664: loss: 0.2095061286:
36: 64064: loss: 0.2087657426:
36: 70464: loss: 0.2086288970:
36: 76864: loss: 0.2085740856:
36: 83264: loss: 0.2093617867:
36: 89664: loss: 0.2086882090:
36: 96064: loss: 0.2090325540:
36: 102464: loss: 0.2090144599:
36: 108864: loss: 0.2090391615:
36: 115264: loss: 0.2092017642:
36: 121664: loss: 0.2095714583:
36: 128064: loss: 0.2094437310:
36: 134464: loss: 0.2094427829:
36: 140864: loss: 0.2091169963:
36: 147264: loss: 0.2092470282:
36: 153664: loss: 0.2095326367:
36: 160064: loss: 0.2094048485:
36: 166464: loss: 0.2095455160:
Dev-Acc: 36: Accuracy: 0.9415680766: precision: 0.4986273164: recall: 0.2470668254: f1: 0.3304150085
Train-Acc: 36: Accuracy: 0.9246239066: precision: 0.7588129855: recall: 0.2504766288: f1: 0.3766310795
37: 6464: loss: 0.1935947651:
37: 12864: loss: 0.1977050197:
37: 19264: loss: 0.2029017606:
37: 25664: loss: 0.2072716812:
37: 32064: loss: 0.2090767266:
37: 38464: loss: 0.2091052774:
37: 44864: loss: 0.2091205160:
37: 51264: loss: 0.2090588599:
37: 57664: loss: 0.2080045252:
37: 64064: loss: 0.2070851314:
37: 70464: loss: 0.2072083035:
37: 76864: loss: 0.2073022547:
37: 83264: loss: 0.2071781652:
37: 89664: loss: 0.2076386079:
37: 96064: loss: 0.2069606744:
37: 102464: loss: 0.2069374145:
37: 108864: loss: 0.2068499372:
37: 115264: loss: 0.2068728934:
37: 121664: loss: 0.2071963328:
37: 128064: loss: 0.2079076122:
37: 134464: loss: 0.2076466437:
37: 140864: loss: 0.2076994534:
37: 147264: loss: 0.2076172020:
37: 153664: loss: 0.2077104140:
37: 160064: loss: 0.2076380229:
37: 166464: loss: 0.2081344853:
Dev-Acc: 37: Accuracy: 0.9412902594: precision: 0.4942528736: recall: 0.2632205407: f1: 0.3435038278
Train-Acc: 37: Accuracy: 0.9256638288: precision: 0.7612587149: recall: 0.2655972651: f1: 0.3938005654
38: 6464: loss: 0.2005063491:
38: 12864: loss: 0.2094532184:
38: 19264: loss: 0.2053713764:
38: 25664: loss: 0.2080030531:
38: 32064: loss: 0.2071245924:
38: 38464: loss: 0.2056900594:
38: 44864: loss: 0.2045779897:
38: 51264: loss: 0.2056319342:
38: 57664: loss: 0.2066697959:
38: 64064: loss: 0.2069995408:
38: 70464: loss: 0.2065844980:
38: 76864: loss: 0.2072364953:
38: 83264: loss: 0.2072191386:
38: 89664: loss: 0.2071537709:
38: 96064: loss: 0.2073466779:
38: 102464: loss: 0.2068794589:
38: 108864: loss: 0.2067486837:
38: 115264: loss: 0.2067907428:
38: 121664: loss: 0.2069903931:
38: 128064: loss: 0.2073605153:
38: 134464: loss: 0.2072699906:
38: 140864: loss: 0.2072154660:
38: 147264: loss: 0.2069013831:
38: 153664: loss: 0.2065255529:
38: 160064: loss: 0.2062652929:
38: 166464: loss: 0.2062642656:
Dev-Acc: 38: Accuracy: 0.9408735633: precision: 0.4885023585: recall: 0.2817548036: f1: 0.3573816456
Train-Acc: 38: Accuracy: 0.9262495637: precision: 0.7616183707: recall: 0.2747353889: f1: 0.4038071311
39: 6464: loss: 0.2036773815:
39: 12864: loss: 0.2054120632:
39: 19264: loss: 0.2054627280:
39: 25664: loss: 0.2056604284:
39: 32064: loss: 0.2049123950:
39: 38464: loss: 0.2052070629:
39: 44864: loss: 0.2068030940:
39: 51264: loss: 0.2065427484:
39: 57664: loss: 0.2051402503:
39: 64064: loss: 0.2047680672:
39: 70464: loss: 0.2053504867:
39: 76864: loss: 0.2058095870:
39: 83264: loss: 0.2056373813:
39: 89664: loss: 0.2053922105:
39: 96064: loss: 0.2061024524:
39: 102464: loss: 0.2067218337:
39: 108864: loss: 0.2062683199:
39: 115264: loss: 0.2058300064:
39: 121664: loss: 0.2055159658:
39: 128064: loss: 0.2054197705:
39: 134464: loss: 0.2057243784:
39: 140864: loss: 0.2054704771:
39: 147264: loss: 0.2048346644:
39: 153664: loss: 0.2053513052:
39: 160064: loss: 0.2052818177:
39: 166464: loss: 0.2051759048:
Dev-Acc: 39: Accuracy: 0.9407544732: precision: 0.4872737557: recall: 0.2929773848: f1: 0.3659339492
Train-Acc: 39: Accuracy: 0.9267694950: precision: 0.7604790419: recall: 0.2838735126: f1: 0.4134233329
40: 6464: loss: 0.2029027699:
40: 12864: loss: 0.2009790305:
40: 19264: loss: 0.2033131200:
40: 25664: loss: 0.2049002375:
40: 32064: loss: 0.2052767190:
40: 38464: loss: 0.2043023586:
40: 44864: loss: 0.2036801098:
40: 51264: loss: 0.2047810175:
40: 57664: loss: 0.2047457933:
40: 64064: loss: 0.2045288284:
40: 70464: loss: 0.2035807470:
40: 76864: loss: 0.2036269665:
40: 83264: loss: 0.2037619042:
40: 89664: loss: 0.2032395738:
40: 96064: loss: 0.2026938326:
40: 102464: loss: 0.2036851733:
40: 108864: loss: 0.2035959998:
40: 115264: loss: 0.2034284025:
40: 121664: loss: 0.2034067049:
40: 128064: loss: 0.2033845959:
40: 134464: loss: 0.2033076274:
40: 140864: loss: 0.2030335749:
40: 147264: loss: 0.2032966370:
40: 153664: loss: 0.2029328513:
40: 160064: loss: 0.2031191222:
40: 166464: loss: 0.2033082068:
Dev-Acc: 40: Accuracy: 0.9404072165: precision: 0.4828438100: recall: 0.2990987927: f1: 0.3693826123
Train-Acc: 40: Accuracy: 0.9273253083: precision: 0.7614395887: recall: 0.2920912498: f1: 0.4222179987
41: 6464: loss: 0.2050591587:
41: 12864: loss: 0.2034458192:
41: 19264: loss: 0.2054221276:
41: 25664: loss: 0.2044093507:
41: 32064: loss: 0.2042243065:
41: 38464: loss: 0.2030434259:
41: 44864: loss: 0.2039186358:
41: 51264: loss: 0.2044023714:
41: 57664: loss: 0.2050891911:
41: 64064: loss: 0.2058162274:
41: 70464: loss: 0.2055759013:
41: 76864: loss: 0.2050541005:
41: 83264: loss: 0.2041472929:
41: 89664: loss: 0.2029881973:
41: 96064: loss: 0.2029535419:
41: 102464: loss: 0.2029682903:
41: 108864: loss: 0.2031468025:
41: 115264: loss: 0.2032159128:
41: 121664: loss: 0.2032089728:
41: 128064: loss: 0.2031677838:
41: 134464: loss: 0.2026241018:
41: 140864: loss: 0.2032034439:
41: 147264: loss: 0.2031238406:
41: 153664: loss: 0.2025000582:
41: 160064: loss: 0.2025889634:
41: 166464: loss: 0.2022292073:
Dev-Acc: 41: Accuracy: 0.9402285814: precision: 0.4807744017: recall: 0.3040299269: f1: 0.3725000000
Train-Acc: 41: Accuracy: 0.9275882840: precision: 0.7593430535: recall: 0.2978765367: f1: 0.4278968741
42: 6464: loss: 0.1973941776:
42: 12864: loss: 0.1992007658:
42: 19264: loss: 0.1976398531:
42: 25664: loss: 0.1981533150:
42: 32064: loss: 0.1990030225:
42: 38464: loss: 0.1989124061:
42: 44864: loss: 0.1987877042:
42: 51264: loss: 0.2003528492:
42: 57664: loss: 0.2006197632:
42: 64064: loss: 0.2002853376:
42: 70464: loss: 0.2004609153:
42: 76864: loss: 0.2005703066:
42: 83264: loss: 0.2006813717:
42: 89664: loss: 0.1997787092:
42: 96064: loss: 0.1997255041:
42: 102464: loss: 0.1996149146:
42: 108864: loss: 0.2000946795:
42: 115264: loss: 0.1999873014:
42: 121664: loss: 0.1996161097:
42: 128064: loss: 0.2002582857:
42: 134464: loss: 0.2009027406:
42: 140864: loss: 0.2004043797:
42: 147264: loss: 0.2003016965:
42: 153664: loss: 0.2007891816:
42: 160064: loss: 0.2005553857:
42: 166464: loss: 0.2008533540:
Dev-Acc: 42: Accuracy: 0.9398813248: precision: 0.4766893662: recall: 0.3094711784: f1: 0.3752964223
Train-Acc: 42: Accuracy: 0.9278333187: precision: 0.7554578038: recall: 0.3048451778: f1: 0.4343997377
43: 6464: loss: 0.1959671780:
43: 12864: loss: 0.2016385411:
43: 19264: loss: 0.2030012809:
43: 25664: loss: 0.2043754688:
43: 32064: loss: 0.2035117108:
43: 38464: loss: 0.2036406761:
43: 44864: loss: 0.2033440808:
43: 51264: loss: 0.2026354067:
43: 57664: loss: 0.2017892508:
43: 64064: loss: 0.2013849212:
43: 70464: loss: 0.2014795204:
43: 76864: loss: 0.2008087731:
43: 83264: loss: 0.2007318040:
43: 89664: loss: 0.2015343181:
43: 96064: loss: 0.2009823995:
43: 102464: loss: 0.2006831605:
43: 108864: loss: 0.2013427820:
43: 115264: loss: 0.2012267540:
43: 121664: loss: 0.2010599672:
43: 128064: loss: 0.2008092531:
43: 134464: loss: 0.2006223104:
43: 140864: loss: 0.2005606161:
43: 147264: loss: 0.2004605266:
43: 153664: loss: 0.2001451262:
43: 160064: loss: 0.2000382958:
43: 166464: loss: 0.1997422055:
Dev-Acc: 43: Accuracy: 0.9397820830: precision: 0.4757607014: recall: 0.3137221561: f1: 0.3781125115
Train-Acc: 43: Accuracy: 0.9282038808: precision: 0.7544557607: recall: 0.3116823352: f1: 0.4411258432
44: 6464: loss: 0.2089307352:
44: 12864: loss: 0.2071328161:
44: 19264: loss: 0.2011558933:
44: 25664: loss: 0.2003727390:
44: 32064: loss: 0.2003836245:
44: 38464: loss: 0.1991165974:
44: 44864: loss: 0.1999143227:
44: 51264: loss: 0.1993473311:
44: 57664: loss: 0.2011314012:
44: 64064: loss: 0.2005337114:
44: 70464: loss: 0.2008323420:
44: 76864: loss: 0.2003141204:
44: 83264: loss: 0.2008546889:
44: 89664: loss: 0.2005488794:
44: 96064: loss: 0.2006172110:
44: 102464: loss: 0.2002960550:
44: 108864: loss: 0.1993129545:
44: 115264: loss: 0.1987322056:
44: 121664: loss: 0.1989667377:
44: 128064: loss: 0.1995179298:
44: 134464: loss: 0.1989328964:
44: 140864: loss: 0.1986147720:
44: 147264: loss: 0.1986799164:
44: 153664: loss: 0.1983627693:
44: 160064: loss: 0.1982617036:
44: 166464: loss: 0.1985148880:
Dev-Acc: 44: Accuracy: 0.9395638108: precision: 0.4734311741: recall: 0.3181431729: f1: 0.3805552731
Train-Acc: 44: Accuracy: 0.9284130335: precision: 0.7515170375: recall: 0.3175333640: f1: 0.4464368241
45: 6464: loss: 0.2017042415:
45: 12864: loss: 0.1972305503:
45: 19264: loss: 0.1982810668:
45: 25664: loss: 0.1969280435:
45: 32064: loss: 0.1982218811:
45: 38464: loss: 0.1979706856:
45: 44864: loss: 0.1990334504:
45: 51264: loss: 0.1993575522:
45: 57664: loss: 0.1990256625:
45: 64064: loss: 0.1983309578:
45: 70464: loss: 0.1982076969:
45: 76864: loss: 0.1977661589:
45: 83264: loss: 0.1971597411:
45: 89664: loss: 0.1972855961:
45: 96064: loss: 0.1973356547:
45: 102464: loss: 0.1970766152:
45: 108864: loss: 0.1974892660:
45: 115264: loss: 0.1975425537:
45: 121664: loss: 0.1974173512:
45: 128064: loss: 0.1971501447:
45: 134464: loss: 0.1967502440:
45: 140864: loss: 0.1970034101:
45: 147264: loss: 0.1969425681:
45: 153664: loss: 0.1971026111:
45: 160064: loss: 0.1971503266:
45: 166464: loss: 0.1971362495:
Dev-Acc: 45: Accuracy: 0.9394050241: precision: 0.4718204489: recall: 0.3217139942: f1: 0.3825700131
Train-Acc: 45: Accuracy: 0.9288134575: precision: 0.7515243902: recall: 0.3241075537: f1: 0.4528960544
46: 6464: loss: 0.2090151276:
46: 12864: loss: 0.2085395898:
46: 19264: loss: 0.2014038869:
46: 25664: loss: 0.2015444522:
46: 32064: loss: 0.1994030691:
46: 38464: loss: 0.2002341813:
46: 44864: loss: 0.2012178028:
46: 51264: loss: 0.2001337163:
46: 57664: loss: 0.1997822955:
46: 64064: loss: 0.1988172136:
46: 70464: loss: 0.1975439115:
46: 76864: loss: 0.1973797382:
46: 83264: loss: 0.1977239159:
46: 89664: loss: 0.1976864099:
46: 96064: loss: 0.1973101740:
46: 102464: loss: 0.1968741837:
46: 108864: loss: 0.1966691778:
46: 115264: loss: 0.1972538851:
46: 121664: loss: 0.1972667791:
46: 128064: loss: 0.1975334668:
46: 134464: loss: 0.1976524718:
46: 140864: loss: 0.1974373262:
46: 147264: loss: 0.1970014861:
46: 153664: loss: 0.1966943348:
46: 160064: loss: 0.1966998016:
46: 166464: loss: 0.1962511735:
Dev-Acc: 46: Accuracy: 0.9392859936: precision: 0.4707472960: recall: 0.3256248937: f1: 0.3849633129
Train-Acc: 46: Accuracy: 0.9292497635: precision: 0.7504083148: recall: 0.3322595490: f1: 0.4605850725
47: 6464: loss: 0.1796209229:
47: 12864: loss: 0.1886090851:
47: 19264: loss: 0.1893468415:
47: 25664: loss: 0.1936541358:
47: 32064: loss: 0.1924363038:
47: 38464: loss: 0.1932339022:
47: 44864: loss: 0.1931187563:
47: 51264: loss: 0.1938013526:
47: 57664: loss: 0.1939896959:
47: 64064: loss: 0.1942682002:
47: 70464: loss: 0.1950660633:
47: 76864: loss: 0.1949315616:
47: 83264: loss: 0.1953154273:
47: 89664: loss: 0.1951442136:
47: 96064: loss: 0.1948374754:
47: 102464: loss: 0.1958791580:
47: 108864: loss: 0.1958247971:
47: 115264: loss: 0.1958635247:
47: 121664: loss: 0.1960128060:
47: 128064: loss: 0.1953655844:
47: 134464: loss: 0.1952271395:
47: 140864: loss: 0.1954059989:
47: 147264: loss: 0.1954437235:
47: 153664: loss: 0.1954365572:
47: 160064: loss: 0.1951644688:
47: 166464: loss: 0.1951470106:
Dev-Acc: 47: Accuracy: 0.9390776157: precision: 0.4685297691: recall: 0.3278354021: f1: 0.3857543017
Train-Acc: 47: Accuracy: 0.9295725226: precision: 0.7499635303: recall: 0.3379790941: f1: 0.4659657391
48: 6464: loss: 0.1851459745:
48: 12864: loss: 0.1884138953:
48: 19264: loss: 0.1903262327:
48: 25664: loss: 0.1917637975:
48: 32064: loss: 0.1925510999:
48: 38464: loss: 0.1923249822:
48: 44864: loss: 0.1930889592:
48: 51264: loss: 0.1926905594:
48: 57664: loss: 0.1934035603:
48: 64064: loss: 0.1941147389:
48: 70464: loss: 0.1938310540:
48: 76864: loss: 0.1945958008:
48: 83264: loss: 0.1944676776:
48: 89664: loss: 0.1950974311:
48: 96064: loss: 0.1948315166:
48: 102464: loss: 0.1947053820:
48: 108864: loss: 0.1944730889:
48: 115264: loss: 0.1948160917:
48: 121664: loss: 0.1945513105:
48: 128064: loss: 0.1945582959:
48: 134464: loss: 0.1942325200:
48: 140864: loss: 0.1942143836:
48: 147264: loss: 0.1944696529:
48: 153664: loss: 0.1941984616:
48: 160064: loss: 0.1938963757:
48: 166464: loss: 0.1940306303:
Dev-Acc: 48: Accuracy: 0.9390180707: precision: 0.4682634731: recall: 0.3324264581: f1: 0.3888225935
Train-Acc: 48: Accuracy: 0.9297757149: precision: 0.7479581602: recall: 0.3431727040: f1: 0.4704821992
49: 6464: loss: 0.2026615183:
49: 12864: loss: 0.1977660633:
49: 19264: loss: 0.1965629626:
49: 25664: loss: 0.1975132334:
49: 32064: loss: 0.1947771508:
49: 38464: loss: 0.1952895167:
49: 44864: loss: 0.1956998486:
49: 51264: loss: 0.1940555902:
49: 57664: loss: 0.1936393913:
49: 64064: loss: 0.1927597853:
49: 70464: loss: 0.1918586728:
49: 76864: loss: 0.1916112123:
49: 83264: loss: 0.1911589626:
49: 89664: loss: 0.1920794768:
49: 96064: loss: 0.1922267496:
49: 102464: loss: 0.1916830602:
49: 108864: loss: 0.1925427243:
49: 115264: loss: 0.1928566042:
49: 121664: loss: 0.1932287132:
49: 128064: loss: 0.1929901373:
49: 134464: loss: 0.1929847093:
49: 140864: loss: 0.1928694937:
49: 147264: loss: 0.1925876373:
49: 153664: loss: 0.1927361624:
49: 160064: loss: 0.1925529202:
49: 166464: loss: 0.1928579012:
Dev-Acc: 49: Accuracy: 0.9387303591: precision: 0.4654117647: recall: 0.3363373576: f1: 0.3904846511
Train-Acc: 49: Accuracy: 0.9300087690: precision: 0.7463400901: recall: 0.3485635395: f1: 0.4751960565
50: 6464: loss: 0.1946101588:
50: 12864: loss: 0.1920025776:
50: 19264: loss: 0.1930619403:
50: 25664: loss: 0.1905184782:
50: 32064: loss: 0.1901424901:
50: 38464: loss: 0.1901034415:
50: 44864: loss: 0.1892930428:
50: 51264: loss: 0.1895944613:
50: 57664: loss: 0.1900675634:
50: 64064: loss: 0.1900876812:
50: 70464: loss: 0.1897324183:
50: 76864: loss: 0.1909778417:
50: 83264: loss: 0.1912221691:
50: 89664: loss: 0.1906142944:
50: 96064: loss: 0.1902819887:
50: 102464: loss: 0.1900005198:
50: 108864: loss: 0.1914208116:
50: 115264: loss: 0.1915120659:
50: 121664: loss: 0.1917870097:
50: 128064: loss: 0.1915894795:
50: 134464: loss: 0.1920480350:
50: 140864: loss: 0.1917831932:
50: 147264: loss: 0.1920184119:
50: 153664: loss: 0.1920647097:
50: 160064: loss: 0.1921453461:
50: 166464: loss: 0.1922348293:
Dev-Acc: 50: Accuracy: 0.9384624362: precision: 0.4628386201: recall: 0.3399081789: f1: 0.3919607843
Train-Acc: 50: Accuracy: 0.9302060008: precision: 0.7449729580: recall: 0.3531654724: f1: 0.4791722415
