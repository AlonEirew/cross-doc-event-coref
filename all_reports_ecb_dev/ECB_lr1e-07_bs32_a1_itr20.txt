1: 3232: loss: 0.6856834090:
1: 6432: loss: 0.6863197669:
1: 9632: loss: 0.6858091597:
1: 12832: loss: 0.6856227331:
1: 16032: loss: 0.6854847814:
1: 19232: loss: 0.6851266650:
1: 22432: loss: 0.6847639657:
1: 25632: loss: 0.6843263487:
1: 28832: loss: 0.6840547786:
Dev-Acc: 1: Accuracy: 0.2390260398: precision: 0.0649184679: recall: 0.8983166128: f1: 0.1210864084
Train-Acc: 1: Accuracy: 0.5890802741: precision: 0.5544264139: recall: 0.9074354086: f1: 0.6883087740
2: 3232: loss: 0.6795878816:
2: 6432: loss: 0.6795517313:
2: 9632: loss: 0.6795452265:
2: 12832: loss: 0.6792762849:
2: 16032: loss: 0.6790207874:
2: 19232: loss: 0.6785222523:
2: 22432: loss: 0.6778826576:
2: 25632: loss: 0.6776729175:
2: 28832: loss: 0.6773007864:
Dev-Acc: 2: Accuracy: 0.2279230803: precision: 0.0657014514: recall: 0.9251827920: f1: 0.1226901178
Train-Acc: 2: Accuracy: 0.6222470999: precision: 0.5757402957: recall: 0.9292617185: f1: 0.7109803330
3: 3232: loss: 0.6743227214:
3: 6432: loss: 0.6740197808:
3: 9632: loss: 0.6734722813:
3: 12832: loss: 0.6728363825:
3: 16032: loss: 0.6721517239:
3: 19232: loss: 0.6717103149:
3: 22432: loss: 0.6715303213:
3: 25632: loss: 0.6711503977:
3: 28832: loss: 0.6708482823:
Dev-Acc: 3: Accuracy: 0.2213843465: precision: 0.0660042329: recall: 0.9386158817: f1: 0.1233354187
Train-Acc: 3: Accuracy: 0.6558083296: precision: 0.5992379198: recall: 0.9408322924: f1: 0.7321515361
4: 3232: loss: 0.6656861424:
4: 6432: loss: 0.6658364999:
4: 9632: loss: 0.6656269083:
4: 12832: loss: 0.6651502724:
4: 16032: loss: 0.6650491085:
4: 19232: loss: 0.6648173428:
4: 22432: loss: 0.6640937321:
4: 25632: loss: 0.6637535440:
4: 28832: loss: 0.6634881517:
Dev-Acc: 4: Accuracy: 0.2180901766: precision: 0.0661772580: recall: 0.9457575242: f1: 0.1236989592
Train-Acc: 4: Accuracy: 0.6869370937: precision: 0.6231538828: recall: 0.9458944185: f1: 0.7513315927
5: 3232: loss: 0.6566804636:
5: 6432: loss: 0.6581742239:
5: 9632: loss: 0.6579141670:
5: 12832: loss: 0.6572501168:
5: 16032: loss: 0.6570054226:
5: 19232: loss: 0.6567631266:
5: 22432: loss: 0.6567326090:
5: 25632: loss: 0.6566970060:
5: 28832: loss: 0.6562100332:
Dev-Acc: 5: Accuracy: 0.2174055427: precision: 0.0663291982: recall: 0.9491583064: f1: 0.1239934694
Train-Acc: 5: Accuracy: 0.7181645036: precision: 0.6495426074: recall: 0.9476037078: f1: 0.7707609219
6: 3232: loss: 0.6514588219:
6: 6432: loss: 0.6510798141:
6: 9632: loss: 0.6510440769:
6: 12832: loss: 0.6511884072:
6: 16032: loss: 0.6508238058:
6: 19232: loss: 0.6506169054:
6: 22432: loss: 0.6500878606:
6: 25632: loss: 0.6496828275:
6: 28832: loss: 0.6494491467:
Dev-Acc: 6: Accuracy: 0.2172070891: precision: 0.0664370546: recall: 0.9511987757: f1: 0.1241993317
Train-Acc: 6: Accuracy: 0.7456117868: precision: 0.6750866998: recall: 0.9470120308: f1: 0.7882568607
7: 3232: loss: 0.6466568136:
7: 6432: loss: 0.6456422848:
7: 9632: loss: 0.6456031998:
7: 12832: loss: 0.6448118019:
7: 16032: loss: 0.6443095151:
7: 19232: loss: 0.6436280736:
7: 22432: loss: 0.6432099406:
7: 25632: loss: 0.6428025382:
7: 28832: loss: 0.6423703275:
Dev-Acc: 7: Accuracy: 0.2200646847: precision: 0.0667889821: recall: 0.9532392450: f1: 0.1248316020
Train-Acc: 7: Accuracy: 0.7720728517: precision: 0.7022826140: recall: 0.9445795806: f1: 0.8056069526
8: 3232: loss: 0.6373617870:
8: 6432: loss: 0.6367749554:
8: 9632: loss: 0.6368476121:
8: 12832: loss: 0.6365278657:
8: 16032: loss: 0.6362734412:
8: 19232: loss: 0.6361586821:
8: 22432: loss: 0.6358935055:
8: 25632: loss: 0.6353981785:
8: 28832: loss: 0.6347542741:
Dev-Acc: 8: Accuracy: 0.2221285105: precision: 0.0668617847: recall: 0.9517088930: f1: 0.1249455861
Train-Acc: 8: Accuracy: 0.7980738282: precision: 0.7313265306: recall: 0.9423443561: f1: 0.8235327914
9: 3232: loss: 0.6319483405:
9: 6432: loss: 0.6310278407:
9: 9632: loss: 0.6302349929:
9: 12832: loss: 0.6297426826:
9: 16032: loss: 0.6286629891:
9: 19232: loss: 0.6279410585:
9: 22432: loss: 0.6275194559:
9: 25632: loss: 0.6274411962:
9: 28832: loss: 0.6272250616:
Dev-Acc: 9: Accuracy: 0.2278040200: precision: 0.0673426187: recall: 0.9520489713: f1: 0.1257877178
Train-Acc: 9: Accuracy: 0.8195713758: precision: 0.7583439626: recall: 0.9380711327: f1: 0.8386869251
10: 3232: loss: 0.6236827588:
10: 6432: loss: 0.6242123649:
10: 9632: loss: 0.6231655892:
10: 12832: loss: 0.6218891115:
10: 16032: loss: 0.6211769053:
10: 19232: loss: 0.6205302681:
10: 22432: loss: 0.6202311816:
10: 25632: loss: 0.6196405309:
10: 28832: loss: 0.6189823712:
Dev-Acc: 10: Accuracy: 0.2342336029: precision: 0.0677143966: recall: 0.9494983846: f1: 0.1264134926
Train-Acc: 10: Accuracy: 0.8404773474: precision: 0.7867980950: recall: 0.9340608770: f1: 0.8541284679
11: 3232: loss: 0.6149471986:
11: 6432: loss: 0.6157528746:
11: 9632: loss: 0.6149542481:
11: 12832: loss: 0.6137265040:
11: 16032: loss: 0.6133349776:
11: 19232: loss: 0.6126989933:
11: 22432: loss: 0.6120643616:
11: 25632: loss: 0.6116045602:
11: 28832: loss: 0.6112710550:
Dev-Acc: 11: Accuracy: 0.2410203964: precision: 0.0681751468: recall: 0.9477979935: f1: 0.1272007394
Train-Acc: 11: Accuracy: 0.8563210964: precision: 0.8108154605: recall: 0.9295246861: f1: 0.8661214739
12: 3232: loss: 0.6065361422:
12: 6432: loss: 0.6072817844:
12: 9632: loss: 0.6050560151:
12: 12832: loss: 0.6043683597:
12: 16032: loss: 0.6038182060:
12: 19232: loss: 0.6042621870:
12: 22432: loss: 0.6040478724:
12: 25632: loss: 0.6037345818:
12: 28832: loss: 0.6034321143:
Dev-Acc: 12: Accuracy: 0.2485612780: precision: 0.0685591462: recall: 0.9437170549: f1: 0.1278315848
Train-Acc: 12: Accuracy: 0.8698639274: precision: 0.8327025429: recall: 0.9257116560: f1: 0.8767472993
13: 3232: loss: 0.5950803530:
13: 6432: loss: 0.5979855391:
13: 9632: loss: 0.5982991878:
13: 12832: loss: 0.5971633662:
13: 16032: loss: 0.5966932496:
13: 19232: loss: 0.5965647189:
13: 22432: loss: 0.5963355917:
13: 25632: loss: 0.5957728294:
13: 28832: loss: 0.5953373563:
Dev-Acc: 13: Accuracy: 0.2573821247: precision: 0.0692666109: recall: 0.9428668594: f1: 0.1290525287
Train-Acc: 13: Accuracy: 0.8812701702: precision: 0.8538653975: recall: 0.9199921110: f1: 0.8856962025
14: 3232: loss: 0.5853394169:
14: 6432: loss: 0.5879856512:
14: 9632: loss: 0.5893128840:
14: 12832: loss: 0.5903253309:
14: 16032: loss: 0.5898926120:
14: 19232: loss: 0.5892437838:
14: 22432: loss: 0.5881904085:
14: 25632: loss: 0.5874174362:
14: 28832: loss: 0.5869250968:
Dev-Acc: 14: Accuracy: 0.2659152150: precision: 0.0698341313: recall: 0.9399761945: f1: 0.1300094073
Train-Acc: 14: Accuracy: 0.8902439475: precision: 0.8727238478: recall: 0.9137466307: f1: 0.8927642355
15: 3232: loss: 0.5807354760:
15: 6432: loss: 0.5802130112:
15: 9632: loss: 0.5808920517:
15: 12832: loss: 0.5806736352:
15: 16032: loss: 0.5811336095:
15: 19232: loss: 0.5803136051:
15: 22432: loss: 0.5797056762:
15: 25632: loss: 0.5792244039:
15: 28832: loss: 0.5786083742:
Dev-Acc: 15: Accuracy: 0.2741010487: precision: 0.0704709127: recall: 0.9384458425: f1: 0.1310973075
Train-Acc: 15: Accuracy: 0.8969824910: precision: 0.8878042515: recall: 0.9088159884: f1: 0.8981872523
16: 3232: loss: 0.5714641833:
16: 6432: loss: 0.5738089252:
16: 9632: loss: 0.5740272184:
16: 12832: loss: 0.5731098327:
16: 16032: loss: 0.5713621912:
16: 19232: loss: 0.5706449347:
16: 22432: loss: 0.5706869268:
16: 25632: loss: 0.5706421616:
16: 28832: loss: 0.5697015827:
Dev-Acc: 16: Accuracy: 0.2829516530: precision: 0.0711387891: recall: 0.9362353341: f1: 0.1322302141
Train-Acc: 16: Accuracy: 0.9028006196: precision: 0.9024037830: recall: 0.9032936691: f1: 0.9028485068
17: 3232: loss: 0.5659607172:
17: 6432: loss: 0.5646673906:
17: 9632: loss: 0.5646282023:
17: 12832: loss: 0.5637900386:
17: 16032: loss: 0.5633419524:
17: 19232: loss: 0.5628060923:
17: 22432: loss: 0.5620026516:
17: 25632: loss: 0.5615517781:
17: 28832: loss: 0.5611747736:
Dev-Acc: 17: Accuracy: 0.2915740609: precision: 0.0717787161: recall: 0.9336847475: f1: 0.1333090556
Train-Acc: 17: Accuracy: 0.9066137075: precision: 0.9128838451: recall: 0.8990204457: f1: 0.9058991090
18: 3232: loss: 0.5615324342:
18: 6432: loss: 0.5597221376:
18: 9632: loss: 0.5572676063:
18: 12832: loss: 0.5552174577:
18: 16032: loss: 0.5543831066:
18: 19232: loss: 0.5535839000:
18: 22432: loss: 0.5531144654:
18: 25632: loss: 0.5520621653:
18: 28832: loss: 0.5518563053:
Dev-Acc: 18: Accuracy: 0.3003453016: precision: 0.0724708622: recall: 0.9314742391: f1: 0.1344789493
Train-Acc: 18: Accuracy: 0.9098021388: precision: 0.9220664906: recall: 0.8952731576: f1: 0.9084723149
19: 3232: loss: 0.5419659418:
19: 6432: loss: 0.5424252361:
19: 9632: loss: 0.5432018292:
19: 12832: loss: 0.5445335541:
19: 16032: loss: 0.5441341251:
19: 19232: loss: 0.5438849141:
19: 22432: loss: 0.5439682644:
19: 25632: loss: 0.5436551883:
19: 28832: loss: 0.5425482641:
Dev-Acc: 19: Accuracy: 0.3089974523: precision: 0.0730652311: recall: 0.9277333787: f1: 0.1354619262
Train-Acc: 19: Accuracy: 0.9114457369: precision: 0.9281658343: recall: 0.8919203208: f1: 0.9096821778
20: 3232: loss: 0.5390534073:
20: 6432: loss: 0.5390297478:
20: 9632: loss: 0.5399782790:
20: 12832: loss: 0.5398309066:
20: 16032: loss: 0.5387093436:
20: 19232: loss: 0.5379911054:
20: 22432: loss: 0.5364926397:
20: 25632: loss: 0.5356927757:
20: 28832: loss: 0.5346452765:
Dev-Acc: 20: Accuracy: 0.3179175258: precision: 0.0737360312: recall: 0.9245026356: f1: 0.1365788713
Train-Acc: 20: Accuracy: 0.9126948118: precision: 0.9336188437: recall: 0.8885674841: f1: 0.9105362436
