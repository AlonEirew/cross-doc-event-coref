1: 3232: loss: 0.7105157268:
1: 6432: loss: 0.7094010925:
1: 9632: loss: 0.7092537721:
1: 12832: loss: 0.7093711440:
1: 16032: loss: 0.7092011253:
1: 19232: loss: 0.7092944033:
1: 22432: loss: 0.7092786750:
1: 25632: loss: 0.7091325395:
1: 28832: loss: 0.7089303839:
1: 32032: loss: 0.7088309550:
1: 35232: loss: 0.7089612011:
1: 38432: loss: 0.7089349243:
1: 41632: loss: 0.7087769536:
1: 44832: loss: 0.7086667338:
1: 48032: loss: 0.7085314891:
1: 51232: loss: 0.7084558691:
1: 54432: loss: 0.7083147407:
1: 57632: loss: 0.7082531976:
1: 60832: loss: 0.7081821687:
1: 64032: loss: 0.7080347606:
1: 67232: loss: 0.7079276294:
1: 70432: loss: 0.7078581356:
1: 73632: loss: 0.7078041775:
1: 76832: loss: 0.7077692948:
1: 80032: loss: 0.7076739623:
1: 83232: loss: 0.7076452954:
1: 86432: loss: 0.7075942532:
1: 89632: loss: 0.7075324645:
1: 92832: loss: 0.7074080384:
1: 96032: loss: 0.7073489559:
1: 99232: loss: 0.7072030706:
1: 102432: loss: 0.7070569845:
1: 105632: loss: 0.7069466486:
1: 108832: loss: 0.7068366629:
1: 112032: loss: 0.7067324622:
1: 115232: loss: 0.7066502709:
1: 118432: loss: 0.7065947241:
1: 121632: loss: 0.7064879872:
1: 124832: loss: 0.7063893989:
1: 128032: loss: 0.7062824330:
1: 131232: loss: 0.7062026779:
1: 134432: loss: 0.7060961891:
Dev-Acc: 1: Accuracy: 0.3421177864: precision: 0.0651582538: recall: 0.7697670464: f1: 0.1201465007
Train-Acc: 1: Accuracy: 0.3785418570: precision: 0.1251099998: recall: 0.7664190389: f1: 0.2151061415
2: 3232: loss: 0.7020423132:
2: 6432: loss: 0.7023662484:
2: 9632: loss: 0.7026342082:
2: 12832: loss: 0.7025317486:
2: 16032: loss: 0.7024514477:
2: 19232: loss: 0.7025591702:
2: 22432: loss: 0.7024801114:
2: 25632: loss: 0.7022776693:
2: 28832: loss: 0.7022370280:
2: 32032: loss: 0.7021084245:
2: 35232: loss: 0.7021287867:
2: 38432: loss: 0.7018655035:
2: 41632: loss: 0.7016706467:
2: 44832: loss: 0.7015294845:
2: 48032: loss: 0.7014117824:
2: 51232: loss: 0.7012466886:
2: 54432: loss: 0.7011263014:
2: 57632: loss: 0.7010659249:
2: 60832: loss: 0.7008946612:
2: 64032: loss: 0.7008149145:
2: 67232: loss: 0.7006282910:
2: 70432: loss: 0.7004776638:
2: 73632: loss: 0.7003870704:
2: 76832: loss: 0.7003209005:
2: 80032: loss: 0.7002576215:
2: 83232: loss: 0.7001392881:
2: 86432: loss: 0.7000057930:
2: 89632: loss: 0.6998969694:
2: 92832: loss: 0.6998045594:
2: 96032: loss: 0.6997140462:
2: 99232: loss: 0.6996323595:
2: 102432: loss: 0.6995327243:
2: 105632: loss: 0.6994428270:
2: 108832: loss: 0.6993449964:
2: 112032: loss: 0.6992513377:
2: 115232: loss: 0.6991955397:
2: 118432: loss: 0.6990785299:
2: 121632: loss: 0.6989796269:
2: 124832: loss: 0.6988877340:
2: 128032: loss: 0.6988164258:
2: 131232: loss: 0.6987315846:
2: 134432: loss: 0.6986382880:
Dev-Acc: 2: Accuracy: 0.4523634613: precision: 0.0637032843: recall: 0.6121407924: f1: 0.1153975606
Train-Acc: 2: Accuracy: 0.4756791592: precision: 0.1282822973: recall: 0.6417066597: f1: 0.2138202210
3: 3232: loss: 0.6948663461:
3: 6432: loss: 0.6945558611:
3: 9632: loss: 0.6945962250:
3: 12832: loss: 0.6947624415:
3: 16032: loss: 0.6946477523:
3: 19232: loss: 0.6945827272:
3: 22432: loss: 0.6944584996:
3: 25632: loss: 0.6943645525:
3: 28832: loss: 0.6942578654:
3: 32032: loss: 0.6941256114:
3: 35232: loss: 0.6939218739:
3: 38432: loss: 0.6938342553:
3: 41632: loss: 0.6937331193:
3: 44832: loss: 0.6935277290:
3: 48032: loss: 0.6934510847:
3: 51232: loss: 0.6934307161:
3: 54432: loss: 0.6933081481:
3: 57632: loss: 0.6932722388:
3: 60832: loss: 0.6932173045:
3: 64032: loss: 0.6930595665:
3: 67232: loss: 0.6929335288:
3: 70432: loss: 0.6928781573:
3: 73632: loss: 0.6927348261:
3: 76832: loss: 0.6926708414:
3: 80032: loss: 0.6925781547:
3: 83232: loss: 0.6925136417:
3: 86432: loss: 0.6923784836:
3: 89632: loss: 0.6923206156:
3: 92832: loss: 0.6922413566:
3: 96032: loss: 0.6921531913:
3: 99232: loss: 0.6920454731:
3: 102432: loss: 0.6919734864:
3: 105632: loss: 0.6919280112:
3: 108832: loss: 0.6918359247:
3: 112032: loss: 0.6917164045:
3: 115232: loss: 0.6916346871:
3: 118432: loss: 0.6915534780:
3: 121632: loss: 0.6914694140:
3: 124832: loss: 0.6913839039:
3: 128032: loss: 0.6913348046:
3: 131232: loss: 0.6912625715:
3: 134432: loss: 0.6911782117:
Dev-Acc: 3: Accuracy: 0.5668359995: precision: 0.0604184607: recall: 0.4414215270: f1: 0.1062888962
Train-Acc: 3: Accuracy: 0.5777105689: precision: 0.1339451433: recall: 0.5123923476: f1: 0.2123734656
4: 3232: loss: 0.6872705221:
4: 6432: loss: 0.6862538934:
4: 9632: loss: 0.6865587274:
4: 12832: loss: 0.6865202378:
4: 16032: loss: 0.6865894096:
4: 19232: loss: 0.6865890949:
4: 22432: loss: 0.6865727028:
4: 25632: loss: 0.6866000847:
4: 28832: loss: 0.6864440213:
4: 32032: loss: 0.6865045924:
4: 35232: loss: 0.6864214414:
4: 38432: loss: 0.6862969912:
4: 41632: loss: 0.6863123591:
4: 44832: loss: 0.6862225846:
4: 48032: loss: 0.6862120083:
4: 51232: loss: 0.6861027275:
4: 54432: loss: 0.6860197211:
4: 57632: loss: 0.6859496656:
4: 60832: loss: 0.6858557028:
4: 64032: loss: 0.6858345701:
4: 67232: loss: 0.6858149050:
4: 70432: loss: 0.6857177285:
4: 73632: loss: 0.6855965778:
4: 76832: loss: 0.6855718127:
4: 80032: loss: 0.6855297005:
4: 83232: loss: 0.6854128243:
4: 86432: loss: 0.6852906717:
4: 89632: loss: 0.6852038911:
4: 92832: loss: 0.6850790633:
4: 96032: loss: 0.6849955107:
4: 99232: loss: 0.6849092933:
4: 102432: loss: 0.6848570068:
4: 105632: loss: 0.6847323110:
4: 108832: loss: 0.6846768127:
4: 112032: loss: 0.6846002119:
4: 115232: loss: 0.6845518655:
4: 118432: loss: 0.6844380259:
4: 121632: loss: 0.6843299885:
4: 124832: loss: 0.6842093822:
4: 128032: loss: 0.6841157444:
4: 131232: loss: 0.6840250949:
4: 134432: loss: 0.6839443257:
Dev-Acc: 4: Accuracy: 0.6707215309: precision: 0.0637760808: recall: 0.3393980616: f1: 0.1073753295
Train-Acc: 4: Accuracy: 0.6686389446: precision: 0.1454707930: recall: 0.4066793768: f1: 0.2142894258
5: 3232: loss: 0.6800872713:
5: 6432: loss: 0.6799079898:
5: 9632: loss: 0.6796625692:
5: 12832: loss: 0.6798214184:
5: 16032: loss: 0.6800420749:
5: 19232: loss: 0.6798397890:
5: 22432: loss: 0.6797379100:
5: 25632: loss: 0.6797052221:
5: 28832: loss: 0.6795208651:
5: 32032: loss: 0.6795036619:
5: 35232: loss: 0.6792896796:
5: 38432: loss: 0.6790801174:
5: 41632: loss: 0.6790460050:
5: 44832: loss: 0.6788845056:
5: 48032: loss: 0.6788017892:
5: 51232: loss: 0.6787468991:
5: 54432: loss: 0.6786716044:
5: 57632: loss: 0.6786065658:
5: 60832: loss: 0.6785434957:
5: 64032: loss: 0.6785164205:
5: 67232: loss: 0.6784046398:
5: 70432: loss: 0.6783214781:
5: 73632: loss: 0.6782439981:
5: 76832: loss: 0.6781027706:
5: 80032: loss: 0.6780239992:
5: 83232: loss: 0.6779285964:
5: 86432: loss: 0.6778689701:
5: 89632: loss: 0.6777858396:
5: 92832: loss: 0.6776676642:
5: 96032: loss: 0.6775896038:
5: 99232: loss: 0.6775061349:
5: 102432: loss: 0.6774672909:
5: 105632: loss: 0.6773667917:
5: 108832: loss: 0.6772746119:
5: 112032: loss: 0.6771820961:
5: 115232: loss: 0.6771049246:
5: 118432: loss: 0.6770601755:
5: 121632: loss: 0.6770033385:
5: 124832: loss: 0.6769439730:
5: 128032: loss: 0.6769017605:
5: 131232: loss: 0.6768430971:
5: 134432: loss: 0.6767660770:
Dev-Acc: 5: Accuracy: 0.7552191019: precision: 0.0666943407: recall: 0.2458765516: f1: 0.1049270735
Train-Acc: 5: Accuracy: 0.7441544533: precision: 0.1629354927: recall: 0.3148379462: f1: 0.2147389189
6: 3232: loss: 0.6729621106:
6: 6432: loss: 0.6726456022:
6: 9632: loss: 0.6723890297:
6: 12832: loss: 0.6727313672:
6: 16032: loss: 0.6728713969:
6: 19232: loss: 0.6728524899:
6: 22432: loss: 0.6726884073:
6: 25632: loss: 0.6726808010:
6: 28832: loss: 0.6726023069:
6: 32032: loss: 0.6725069481:
6: 35232: loss: 0.6723454195:
6: 38432: loss: 0.6721812439:
6: 41632: loss: 0.6721766185:
6: 44832: loss: 0.6720743659:
6: 48032: loss: 0.6719504039:
6: 51232: loss: 0.6718944168:
6: 54432: loss: 0.6718160079:
6: 57632: loss: 0.6717488318:
6: 60832: loss: 0.6716121899:
6: 64032: loss: 0.6714757650:
6: 67232: loss: 0.6713729057:
6: 70432: loss: 0.6713266082:
6: 73632: loss: 0.6713190871:
6: 76832: loss: 0.6712446988:
6: 80032: loss: 0.6711663525:
6: 83232: loss: 0.6710682964:
6: 86432: loss: 0.6709799887:
6: 89632: loss: 0.6709210765:
6: 92832: loss: 0.6708332856:
6: 96032: loss: 0.6707852056:
6: 99232: loss: 0.6706930775:
6: 102432: loss: 0.6705478241:
6: 105632: loss: 0.6704646556:
6: 108832: loss: 0.6704036932:
6: 112032: loss: 0.6703389235:
6: 115232: loss: 0.6702625115:
6: 118432: loss: 0.6701854556:
6: 121632: loss: 0.6701129567:
6: 124832: loss: 0.6700037404:
6: 128032: loss: 0.6699413787:
6: 131232: loss: 0.6698384941:
6: 134432: loss: 0.6697425942:
Dev-Acc: 6: Accuracy: 0.8146530986: precision: 0.0686796522: recall: 0.1732698521: f1: 0.0983685684
Train-Acc: 6: Accuracy: 0.8007801771: precision: 0.1855906579: recall: 0.2340411544: f1: 0.2070188701
7: 3232: loss: 0.6664821070:
7: 6432: loss: 0.6662538177:
7: 9632: loss: 0.6659252016:
7: 12832: loss: 0.6659254442:
7: 16032: loss: 0.6656713041:
7: 19232: loss: 0.6656986395:
7: 22432: loss: 0.6655081288:
7: 25632: loss: 0.6655974445:
7: 28832: loss: 0.6655637705:
7: 32032: loss: 0.6654606397:
7: 35232: loss: 0.6654223744:
7: 38432: loss: 0.6653849006:
7: 41632: loss: 0.6653582606:
7: 44832: loss: 0.6652919060:
7: 48032: loss: 0.6651901945:
7: 51232: loss: 0.6649946720:
7: 54432: loss: 0.6649227247:
7: 57632: loss: 0.6648655207:
7: 60832: loss: 0.6647173491:
7: 64032: loss: 0.6646659028:
7: 67232: loss: 0.6645908645:
7: 70432: loss: 0.6645102186:
7: 73632: loss: 0.6644808791:
7: 76832: loss: 0.6643710245:
7: 80032: loss: 0.6642942211:
7: 83232: loss: 0.6642200981:
7: 86432: loss: 0.6641539365:
7: 89632: loss: 0.6640608597:
7: 92832: loss: 0.6639464093:
7: 96032: loss: 0.6638168512:
7: 99232: loss: 0.6637547653:
7: 102432: loss: 0.6636830857:
7: 105632: loss: 0.6635417204:
7: 108832: loss: 0.6634903703:
7: 112032: loss: 0.6634174379:
7: 115232: loss: 0.6633209706:
7: 118432: loss: 0.6632436665:
7: 121632: loss: 0.6631826331:
7: 124832: loss: 0.6630877529:
7: 128032: loss: 0.6629512836:
7: 131232: loss: 0.6628879071:
7: 134432: loss: 0.6628259025:
Dev-Acc: 7: Accuracy: 0.8587176204: precision: 0.0717360115: recall: 0.1190273763: f1: 0.0895197903
Train-Acc: 7: Accuracy: 0.8370550871: precision: 0.2105091384: recall: 0.1696140951: f1: 0.1878617978
8: 3232: loss: 0.6595999634:
8: 6432: loss: 0.6605323389:
8: 9632: loss: 0.6601575331:
8: 12832: loss: 0.6600862072:
8: 16032: loss: 0.6596005601:
8: 19232: loss: 0.6592920261:
8: 22432: loss: 0.6591875842:
8: 25632: loss: 0.6589718210:
8: 28832: loss: 0.6590026368:
8: 32032: loss: 0.6589024326:
8: 35232: loss: 0.6587586730:
8: 38432: loss: 0.6586574946:
8: 41632: loss: 0.6586325137:
8: 44832: loss: 0.6585533640:
8: 48032: loss: 0.6584469573:
8: 51232: loss: 0.6583920535:
8: 54432: loss: 0.6582679711:
8: 57632: loss: 0.6581595874:
8: 60832: loss: 0.6580592030:
8: 64032: loss: 0.6578913778:
8: 67232: loss: 0.6578375680:
8: 70432: loss: 0.6577099579:
8: 73632: loss: 0.6576248547:
8: 76832: loss: 0.6575848125:
8: 80032: loss: 0.6575044500:
8: 83232: loss: 0.6573966124:
8: 86432: loss: 0.6573333178:
8: 89632: loss: 0.6572261761:
8: 92832: loss: 0.6570742858:
8: 96032: loss: 0.6569776140:
8: 99232: loss: 0.6568802959:
8: 102432: loss: 0.6568044365:
8: 105632: loss: 0.6567184501:
8: 108832: loss: 0.6566662079:
8: 112032: loss: 0.6566167206:
8: 115232: loss: 0.6565560409:
8: 118432: loss: 0.6564349146:
8: 121632: loss: 0.6563036782:
8: 124832: loss: 0.6562153449:
8: 128032: loss: 0.6561417638:
8: 131232: loss: 0.6560484720:
8: 134432: loss: 0.6559869323:
Dev-Acc: 8: Accuracy: 0.8912624717: precision: 0.0819888047: recall: 0.0846794763: f1: 0.0833124216
Train-Acc: 8: Accuracy: 0.8601012826: precision: 0.2377910845: recall: 0.1174807705: f1: 0.1572648068
9: 3232: loss: 0.6535390741:
9: 6432: loss: 0.6533467758:
9: 9632: loss: 0.6530073659:
9: 12832: loss: 0.6527040483:
9: 16032: loss: 0.6522726271:
9: 19232: loss: 0.6523204171:
9: 22432: loss: 0.6522572619:
9: 25632: loss: 0.6522660971:
9: 28832: loss: 0.6519010054:
9: 32032: loss: 0.6517339248:
9: 35232: loss: 0.6517314332:
9: 38432: loss: 0.6517768464:
9: 41632: loss: 0.6516990725:
9: 44832: loss: 0.6515945598:
9: 48032: loss: 0.6514928893:
9: 51232: loss: 0.6514892618:
9: 54432: loss: 0.6514290881:
9: 57632: loss: 0.6514155957:
9: 60832: loss: 0.6512934233:
9: 64032: loss: 0.6512062452:
9: 67232: loss: 0.6510748355:
9: 70432: loss: 0.6509506283:
9: 73632: loss: 0.6508715436:
9: 76832: loss: 0.6508174493:
9: 80032: loss: 0.6506886445:
9: 83232: loss: 0.6505807378:
9: 86432: loss: 0.6504832210:
9: 89632: loss: 0.6503545038:
9: 92832: loss: 0.6502874944:
9: 96032: loss: 0.6501684196:
9: 99232: loss: 0.6500876821:
9: 102432: loss: 0.6500343681:
9: 105632: loss: 0.6499473070:
9: 108832: loss: 0.6498759294:
9: 112032: loss: 0.6498032042:
9: 115232: loss: 0.6496921277:
9: 118432: loss: 0.6496669391:
9: 121632: loss: 0.6496308452:
9: 124832: loss: 0.6495538974:
9: 128032: loss: 0.6494769223:
9: 131232: loss: 0.6494061413:
9: 134432: loss: 0.6493221452:
Dev-Acc: 9: Accuracy: 0.9140537977: precision: 0.0800362428: recall: 0.0450603639: f1: 0.0576588338
Train-Acc: 9: Accuracy: 0.8732715249: precision: 0.2548165138: recall: 0.0730392479: f1: 0.1135353329
10: 3232: loss: 0.6456314081:
10: 6432: loss: 0.6466044152:
10: 9632: loss: 0.6464960955:
10: 12832: loss: 0.6459070933:
10: 16032: loss: 0.6454247872:
10: 19232: loss: 0.6452463345:
10: 22432: loss: 0.6450686092:
10: 25632: loss: 0.6447574422:
10: 28832: loss: 0.6449051322:
10: 32032: loss: 0.6447659322:
10: 35232: loss: 0.6445794767:
10: 38432: loss: 0.6444995334:
10: 41632: loss: 0.6444109132:
10: 44832: loss: 0.6444031623:
10: 48032: loss: 0.6443618189:
10: 51232: loss: 0.6444127820:
10: 54432: loss: 0.6442463941:
10: 57632: loss: 0.6442119446:
10: 60832: loss: 0.6441648815:
10: 64032: loss: 0.6440592599:
10: 67232: loss: 0.6440294360:
10: 70432: loss: 0.6439600575:
10: 73632: loss: 0.6439291067:
10: 76832: loss: 0.6437857334:
10: 80032: loss: 0.6436610779:
10: 83232: loss: 0.6435646212:
10: 86432: loss: 0.6434666436:
10: 89632: loss: 0.6433579207:
10: 92832: loss: 0.6432896099:
10: 96032: loss: 0.6431248853:
10: 99232: loss: 0.6430123875:
10: 102432: loss: 0.6429474235:
10: 105632: loss: 0.6429217395:
10: 108832: loss: 0.6428852189:
10: 112032: loss: 0.6427964930:
10: 115232: loss: 0.6427609451:
10: 118432: loss: 0.6427249549:
10: 121632: loss: 0.6426724042:
10: 124832: loss: 0.6426214385:
10: 128032: loss: 0.6425265272:
10: 131232: loss: 0.6425247978:
10: 134432: loss: 0.6424836272:
Dev-Acc: 10: Accuracy: 0.9270122051: precision: 0.0643827525: recall: 0.0185342629: f1: 0.0287826776
Train-Acc: 10: Accuracy: 0.8808683753: precision: 0.2796950241: recall: 0.0458221024: f1: 0.0787437158
11: 3232: loss: 0.6392279422:
11: 6432: loss: 0.6395267695:
11: 9632: loss: 0.6388764234:
11: 12832: loss: 0.6390121208:
11: 16032: loss: 0.6390197985:
11: 19232: loss: 0.6389815504:
11: 22432: loss: 0.6391254807:
11: 25632: loss: 0.6390015765:
11: 28832: loss: 0.6387168152:
11: 32032: loss: 0.6385522079:
11: 35232: loss: 0.6385210126:
11: 38432: loss: 0.6382570052:
11: 41632: loss: 0.6381074431:
11: 44832: loss: 0.6380967408:
11: 48032: loss: 0.6380159800:
11: 51232: loss: 0.6377808933:
11: 54432: loss: 0.6377748752:
11: 57632: loss: 0.6376845114:
11: 60832: loss: 0.6376754548:
11: 64032: loss: 0.6376153664:
11: 67232: loss: 0.6376083530:
11: 70432: loss: 0.6374356980:
11: 73632: loss: 0.6373644980:
11: 76832: loss: 0.6372755231:
11: 80032: loss: 0.6372184729:
11: 83232: loss: 0.6372738933:
11: 86432: loss: 0.6372041240:
11: 89632: loss: 0.6371348379:
11: 92832: loss: 0.6370299638:
11: 96032: loss: 0.6369428459:
11: 99232: loss: 0.6368285420:
11: 102432: loss: 0.6367177463:
11: 105632: loss: 0.6365832572:
11: 108832: loss: 0.6365032201:
11: 112032: loss: 0.6364100710:
11: 115232: loss: 0.6363738362:
11: 118432: loss: 0.6363053399:
11: 121632: loss: 0.6362610020:
11: 124832: loss: 0.6361275490:
11: 128032: loss: 0.6360508221:
11: 131232: loss: 0.6359742789:
11: 134432: loss: 0.6358520592:
Dev-Acc: 11: Accuracy: 0.9334517121: precision: 0.0634249471: recall: 0.0102023465: f1: 0.0175772667
Train-Acc: 11: Accuracy: 0.8846814036: precision: 0.3046132972: recall: 0.0295181119: f1: 0.0538207971
12: 3232: loss: 0.6319915456:
12: 6432: loss: 0.6332965955:
12: 9632: loss: 0.6332606298:
12: 12832: loss: 0.6330858009:
12: 16032: loss: 0.6328744923:
12: 19232: loss: 0.6323664510:
12: 22432: loss: 0.6321728682:
12: 25632: loss: 0.6320932006:
12: 28832: loss: 0.6320015453:
12: 32032: loss: 0.6320481204:
12: 35232: loss: 0.6318830746:
12: 38432: loss: 0.6317639185:
12: 41632: loss: 0.6318079016:
12: 44832: loss: 0.6317153476:
12: 48032: loss: 0.6315788021:
12: 51232: loss: 0.6314489972:
12: 54432: loss: 0.6313801869:
12: 57632: loss: 0.6313065753:
12: 60832: loss: 0.6311984016:
12: 64032: loss: 0.6310516747:
12: 67232: loss: 0.6310340874:
12: 70432: loss: 0.6310269623:
12: 73632: loss: 0.6309840995:
12: 76832: loss: 0.6309036745:
12: 80032: loss: 0.6307445020:
12: 83232: loss: 0.6306261001:
12: 86432: loss: 0.6305368223:
12: 89632: loss: 0.6303990569:
12: 92832: loss: 0.6303568225:
12: 96032: loss: 0.6302761424:
12: 99232: loss: 0.6301401228:
12: 102432: loss: 0.6300089872:
12: 105632: loss: 0.6300153241:
12: 108832: loss: 0.6299614706:
12: 112032: loss: 0.6298674769:
12: 115232: loss: 0.6298504027:
12: 118432: loss: 0.6298089921:
12: 121632: loss: 0.6297129282:
12: 124832: loss: 0.6296246408:
12: 128032: loss: 0.6295154601:
12: 131232: loss: 0.6294907231:
12: 134432: loss: 0.6294208447:
Dev-Acc: 12: Accuracy: 0.9379068017: precision: 0.0946236559: recall: 0.0074817208: f1: 0.0138670028
Train-Acc: 12: Accuracy: 0.8870115876: precision: 0.3411619283: recall: 0.0181447637: f1: 0.0344569288
13: 3232: loss: 0.6252662784:
13: 6432: loss: 0.6251192775:
13: 9632: loss: 0.6256693786:
13: 12832: loss: 0.6260613452:
13: 16032: loss: 0.6261030802:
13: 19232: loss: 0.6262412354:
13: 22432: loss: 0.6258373314:
13: 25632: loss: 0.6257031311:
13: 28832: loss: 0.6255672567:
13: 32032: loss: 0.6254022332:
13: 35232: loss: 0.6251316604:
13: 38432: loss: 0.6251749727:
13: 41632: loss: 0.6251130397:
13: 44832: loss: 0.6250878918:
13: 48032: loss: 0.6249639587:
13: 51232: loss: 0.6248920786:
13: 54432: loss: 0.6248515876:
13: 57632: loss: 0.6247783472:
13: 60832: loss: 0.6246848172:
13: 64032: loss: 0.6245920358:
13: 67232: loss: 0.6244596736:
13: 70432: loss: 0.6243977740:
13: 73632: loss: 0.6244329270:
13: 76832: loss: 0.6243567409:
13: 80032: loss: 0.6242783899:
13: 83232: loss: 0.6242254056:
13: 86432: loss: 0.6240974894:
13: 89632: loss: 0.6240133788:
13: 92832: loss: 0.6239850411:
13: 96032: loss: 0.6239803094:
13: 99232: loss: 0.6239042096:
13: 102432: loss: 0.6238615136:
13: 105632: loss: 0.6238306483:
13: 108832: loss: 0.6237049820:
13: 112032: loss: 0.6236377172:
13: 115232: loss: 0.6235765240:
13: 118432: loss: 0.6234727197:
13: 121632: loss: 0.6233806909:
13: 124832: loss: 0.6233044530:
13: 128032: loss: 0.6232381734:
13: 131232: loss: 0.6232051143:
13: 134432: loss: 0.6230908336:
Dev-Acc: 13: Accuracy: 0.9399507642: precision: 0.1165919283: recall: 0.0044210168: f1: 0.0085190039
Train-Acc: 13: Accuracy: 0.8880415559: precision: 0.3432432432: recall: 0.0083492210: f1: 0.0163019062
14: 3232: loss: 0.6184993529:
14: 6432: loss: 0.6204963520:
14: 9632: loss: 0.6195622881:
14: 12832: loss: 0.6194774273:
14: 16032: loss: 0.6193178107:
14: 19232: loss: 0.6196737490:
14: 22432: loss: 0.6195111422:
14: 25632: loss: 0.6192993698:
14: 28832: loss: 0.6193649893:
14: 32032: loss: 0.6192831365:
14: 35232: loss: 0.6190986650:
14: 38432: loss: 0.6191570661:
14: 41632: loss: 0.6189833991:
14: 44832: loss: 0.6189120093:
14: 48032: loss: 0.6189177268:
14: 51232: loss: 0.6187060983:
14: 54432: loss: 0.6187969350:
14: 57632: loss: 0.6186328970:
14: 60832: loss: 0.6184857632:
14: 64032: loss: 0.6182775040:
14: 67232: loss: 0.6182509414:
14: 70432: loss: 0.6181497694:
14: 73632: loss: 0.6180895115:
14: 76832: loss: 0.6179938986:
14: 80032: loss: 0.6179382140:
14: 83232: loss: 0.6179258084:
14: 86432: loss: 0.6178796147:
14: 89632: loss: 0.6177661877:
14: 92832: loss: 0.6176907739:
14: 96032: loss: 0.6176339979:
14: 99232: loss: 0.6175758716:
14: 102432: loss: 0.6175094883:
14: 105632: loss: 0.6173665910:
14: 108832: loss: 0.6173320841:
14: 112032: loss: 0.6172476426:
14: 115232: loss: 0.6171958416:
14: 118432: loss: 0.6171671330:
14: 121632: loss: 0.6171234053:
14: 124832: loss: 0.6170983532:
14: 128032: loss: 0.6169868081:
14: 131232: loss: 0.6168950703:
14: 134432: loss: 0.6168152036:
Dev-Acc: 14: Accuracy: 0.9409033060: precision: 0.0879120879: recall: 0.0013603129: f1: 0.0026791695
Train-Acc: 14: Accuracy: 0.8884798288: precision: 0.3556701031: recall: 0.0045361909: f1: 0.0089581305
15: 3232: loss: 0.6139136076:
15: 6432: loss: 0.6136053032:
15: 9632: loss: 0.6139517301:
15: 12832: loss: 0.6135513474:
15: 16032: loss: 0.6134770961:
15: 19232: loss: 0.6135064471:
15: 22432: loss: 0.6131928215:
15: 25632: loss: 0.6136317064:
15: 28832: loss: 0.6136425137:
15: 32032: loss: 0.6137894859:
15: 35232: loss: 0.6134750941:
15: 38432: loss: 0.6131713691:
15: 41632: loss: 0.6129762506:
15: 44832: loss: 0.6127247524:
15: 48032: loss: 0.6125842884:
15: 51232: loss: 0.6123688935:
15: 54432: loss: 0.6122805820:
15: 57632: loss: 0.6120990319:
15: 60832: loss: 0.6121509212:
15: 64032: loss: 0.6120180504:
15: 67232: loss: 0.6121176371:
15: 70432: loss: 0.6120633330:
15: 73632: loss: 0.6120381946:
15: 76832: loss: 0.6119471277:
15: 80032: loss: 0.6119292968:
15: 83232: loss: 0.6119089773:
15: 86432: loss: 0.6117376780:
15: 89632: loss: 0.6116386361:
15: 92832: loss: 0.6116475537:
15: 96032: loss: 0.6114860769:
15: 99232: loss: 0.6113617251:
15: 102432: loss: 0.6113659145:
15: 105632: loss: 0.6112435645:
15: 108832: loss: 0.6111615320:
15: 112032: loss: 0.6110531542:
15: 115232: loss: 0.6110067346:
15: 118432: loss: 0.6108710513:
15: 121632: loss: 0.6107747061:
15: 124832: loss: 0.6106598800:
15: 128032: loss: 0.6105885472:
15: 131232: loss: 0.6105407533:
15: 134432: loss: 0.6104794473:
Dev-Acc: 15: Accuracy: 0.9413002133: precision: 0.1111111111: recall: 0.0008501955: f1: 0.0016874789
Train-Acc: 15: Accuracy: 0.8887428045: precision: 0.4019607843: recall: 0.0026954178: f1: 0.0053549272
16: 3232: loss: 0.6041239631:
16: 6432: loss: 0.6055908316:
16: 9632: loss: 0.6068100373:
16: 12832: loss: 0.6063055874:
16: 16032: loss: 0.6059424289:
16: 19232: loss: 0.6063895111:
16: 22432: loss: 0.6066274146:
16: 25632: loss: 0.6065831421:
16: 28832: loss: 0.6063935837:
16: 32032: loss: 0.6064858300:
16: 35232: loss: 0.6064765240:
16: 38432: loss: 0.6064537999:
16: 41632: loss: 0.6064184117:
16: 44832: loss: 0.6063214449:
16: 48032: loss: 0.6062759328:
16: 51232: loss: 0.6060627629:
16: 54432: loss: 0.6060321919:
16: 57632: loss: 0.6060184783:
16: 60832: loss: 0.6059102408:
16: 64032: loss: 0.6058115897:
16: 67232: loss: 0.6056510311:
16: 70432: loss: 0.6056315639:
16: 73632: loss: 0.6055330915:
16: 76832: loss: 0.6054622786:
16: 80032: loss: 0.6054145075:
16: 83232: loss: 0.6053639250:
16: 86432: loss: 0.6053072491:
16: 89632: loss: 0.6053995205:
16: 92832: loss: 0.6052178759:
16: 96032: loss: 0.6051695445:
16: 99232: loss: 0.6050852640:
16: 102432: loss: 0.6050839639:
16: 105632: loss: 0.6049526780:
16: 108832: loss: 0.6048752140:
16: 112032: loss: 0.6047545594:
16: 115232: loss: 0.6047450242:
16: 118432: loss: 0.6046102155:
16: 121632: loss: 0.6045520328:
16: 124832: loss: 0.6045069155:
16: 128032: loss: 0.6044995200:
16: 131232: loss: 0.6043861642:
16: 134432: loss: 0.6042727424:
Dev-Acc: 16: Accuracy: 0.9414887428: precision: 0.1000000000: recall: 0.0003400782: f1: 0.0006778512
Train-Acc: 16: Accuracy: 0.8888669610: precision: 0.4680851064: recall: 0.0014463217: f1: 0.0028837331
17: 3232: loss: 0.6002779919:
17: 6432: loss: 0.6015504685:
17: 9632: loss: 0.6011968233:
17: 12832: loss: 0.6002735084:
17: 16032: loss: 0.5998112429:
17: 19232: loss: 0.6002345323:
17: 22432: loss: 0.6002328558:
17: 25632: loss: 0.6004347060:
17: 28832: loss: 0.6003366915:
17: 32032: loss: 0.6003622565:
17: 35232: loss: 0.6002828774:
17: 38432: loss: 0.6002297248:
17: 41632: loss: 0.6001714374:
17: 44832: loss: 0.6000010521:
17: 48032: loss: 0.6000073768:
17: 51232: loss: 0.6000378130:
17: 54432: loss: 0.5998891582:
17: 57632: loss: 0.5997547055:
17: 60832: loss: 0.5996499251:
17: 64032: loss: 0.5996622538:
17: 67232: loss: 0.5994981080:
17: 70432: loss: 0.5993502143:
17: 73632: loss: 0.5993332757:
17: 76832: loss: 0.5992296554:
17: 80032: loss: 0.5991172770:
17: 83232: loss: 0.5990596688:
17: 86432: loss: 0.5989906554:
17: 89632: loss: 0.5988330390:
17: 92832: loss: 0.5988493618:
17: 96032: loss: 0.5987492786:
17: 99232: loss: 0.5985757861:
17: 102432: loss: 0.5984579987:
17: 105632: loss: 0.5983903577:
17: 108832: loss: 0.5983311211:
17: 112032: loss: 0.5982909118:
17: 115232: loss: 0.5981769019:
17: 118432: loss: 0.5980815943:
17: 121632: loss: 0.5981101557:
17: 124832: loss: 0.5980213675:
17: 128032: loss: 0.5979572460:
17: 131232: loss: 0.5979121921:
17: 134432: loss: 0.5978700581:
Dev-Acc: 17: Accuracy: 0.9415978789: precision: 0.1428571429: recall: 0.0001700391: f1: 0.0003396739
Train-Acc: 17: Accuracy: 0.8888888955: precision: 0.5000000000: recall: 0.0009861285: f1: 0.0019683748
18: 3232: loss: 0.5939655876:
18: 6432: loss: 0.5927166128:
18: 9632: loss: 0.5932672503:
18: 12832: loss: 0.5935849889:
18: 16032: loss: 0.5941245712:
18: 19232: loss: 0.5942593441:
18: 22432: loss: 0.5941077486:
18: 25632: loss: 0.5939008174:
18: 28832: loss: 0.5939384100:
18: 32032: loss: 0.5939667419:
18: 35232: loss: 0.5940195382:
18: 38432: loss: 0.5937181796:
18: 41632: loss: 0.5934450865:
18: 44832: loss: 0.5931439404:
18: 48032: loss: 0.5932402021:
18: 51232: loss: 0.5932274554:
18: 54432: loss: 0.5932964747:
18: 57632: loss: 0.5932668062:
18: 60832: loss: 0.5934157815:
18: 64032: loss: 0.5933731409:
18: 67232: loss: 0.5933927771:
18: 70432: loss: 0.5932832419:
18: 73632: loss: 0.5933506879:
18: 76832: loss: 0.5931973988:
18: 80032: loss: 0.5929899493:
18: 83232: loss: 0.5929158638:
18: 86432: loss: 0.5928511292:
18: 89632: loss: 0.5927106813:
18: 92832: loss: 0.5927673115:
18: 96032: loss: 0.5927382241:
18: 99232: loss: 0.5927361066:
18: 102432: loss: 0.5926491096:
18: 105632: loss: 0.5925919914:
18: 108832: loss: 0.5924915641:
18: 112032: loss: 0.5924296037:
18: 115232: loss: 0.5923837553:
18: 118432: loss: 0.5923556748:
18: 121632: loss: 0.5922538210:
18: 124832: loss: 0.5921692823:
18: 128032: loss: 0.5920885521:
18: 131232: loss: 0.5920790484:
18: 134432: loss: 0.5919999105:
Dev-Acc: 18: Accuracy: 0.9416177273: precision: 0.2000000000: recall: 0.0001700391: f1: 0.0003397893
Train-Acc: 18: Accuracy: 0.8888816237: precision: 0.4782608696: recall: 0.0007231609: f1: 0.0014441381
19: 3232: loss: 0.5910249859:
19: 6432: loss: 0.5913028517:
19: 9632: loss: 0.5908196914:
19: 12832: loss: 0.5898693138:
19: 16032: loss: 0.5900042511:
19: 19232: loss: 0.5896769365:
19: 22432: loss: 0.5895932466:
19: 25632: loss: 0.5893909203:
19: 28832: loss: 0.5889491416:
19: 32032: loss: 0.5885371351:
19: 35232: loss: 0.5884989698:
19: 38432: loss: 0.5883312000:
19: 41632: loss: 0.5882346666:
19: 44832: loss: 0.5881348962:
19: 48032: loss: 0.5879563587:
19: 51232: loss: 0.5879727108:
19: 54432: loss: 0.5878246010:
19: 57632: loss: 0.5875480382:
19: 60832: loss: 0.5876015509:
19: 64032: loss: 0.5875382679:
19: 67232: loss: 0.5873738220:
19: 70432: loss: 0.5874225657:
19: 73632: loss: 0.5873560902:
19: 76832: loss: 0.5872253436:
19: 80032: loss: 0.5872581757:
19: 83232: loss: 0.5872330260:
19: 86432: loss: 0.5871451317:
19: 89632: loss: 0.5870920463:
19: 92832: loss: 0.5869609211:
19: 96032: loss: 0.5868755993:
19: 99232: loss: 0.5868150765:
19: 102432: loss: 0.5866223237:
19: 105632: loss: 0.5865155844:
19: 108832: loss: 0.5863783429:
19: 112032: loss: 0.5863271902:
19: 115232: loss: 0.5861781653:
19: 118432: loss: 0.5861896999:
19: 121632: loss: 0.5861727521:
19: 124832: loss: 0.5861003943:
19: 128032: loss: 0.5860179272:
19: 131232: loss: 0.5860934099:
19: 134432: loss: 0.5859618239:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.5000000000: recall: 0.0001700391: f1: 0.0003399626
Train-Acc: 19: Accuracy: 0.8889327049: precision: 0.7500000000: recall: 0.0005916771: f1: 0.0011824213
20: 3232: loss: 0.5853265005:
20: 6432: loss: 0.5845961931:
20: 9632: loss: 0.5848171208:
20: 12832: loss: 0.5836788505:
20: 16032: loss: 0.5833062531:
20: 19232: loss: 0.5830266184:
20: 22432: loss: 0.5829594318:
20: 25632: loss: 0.5825452723:
20: 28832: loss: 0.5825427195:
20: 32032: loss: 0.5823406748:
20: 35232: loss: 0.5820913629:
20: 38432: loss: 0.5820255374:
20: 41632: loss: 0.5820359567:
20: 44832: loss: 0.5821764950:
20: 48032: loss: 0.5819168233:
20: 51232: loss: 0.5816933376:
20: 54432: loss: 0.5817216054:
20: 57632: loss: 0.5817130951:
20: 60832: loss: 0.5815756954:
20: 64032: loss: 0.5813317590:
20: 67232: loss: 0.5814880449:
20: 70432: loss: 0.5815011126:
20: 73632: loss: 0.5815340083:
20: 76832: loss: 0.5814621230:
20: 80032: loss: 0.5813111481:
20: 83232: loss: 0.5811415134:
20: 86432: loss: 0.5810063892:
20: 89632: loss: 0.5810332733:
20: 92832: loss: 0.5811013921:
20: 96032: loss: 0.5810021274:
20: 99232: loss: 0.5810679439:
20: 102432: loss: 0.5809577982:
20: 105632: loss: 0.5808549734:
20: 108832: loss: 0.5808640568:
20: 112032: loss: 0.5808305606:
20: 115232: loss: 0.5807839596:
20: 118432: loss: 0.5806905749:
20: 121632: loss: 0.5805991526:
20: 124832: loss: 0.5804754294:
20: 128032: loss: 0.5802940551:
20: 131232: loss: 0.5802744854:
20: 134432: loss: 0.5801788621:
Dev-Acc: 20: Accuracy: 0.9416574240: precision: 1.0000000000: recall: 0.0001700391: f1: 0.0003400204
Train-Acc: 20: Accuracy: 0.8889181018: precision: 1.0000000000: recall: 0.0002629676: f1: 0.0005257969
