1: 6464: loss: 0.7101154518:
1: 12864: loss: 0.7090834016:
1: 19264: loss: 0.7080456412:
1: 25664: loss: 0.7069274053:
1: 32064: loss: 0.7058862802:
1: 38464: loss: 0.7049527779:
1: 44864: loss: 0.7039571129:
1: 51264: loss: 0.7029434896:
1: 57664: loss: 0.7020210694:
1: 64064: loss: 0.7008899724:
1: 70464: loss: 0.6997987125:
1: 76864: loss: 0.6987765153:
1: 83264: loss: 0.6977061049:
1: 89664: loss: 0.6966994394:
1: 96064: loss: 0.6956880055:
1: 102464: loss: 0.6946710028:
1: 108864: loss: 0.6937234224:
1: 115264: loss: 0.6926533040:
1: 121664: loss: 0.6916799066:
1: 128064: loss: 0.6906944517:
1: 134464: loss: 0.6897092930:
1: 140864: loss: 0.6887307835:
1: 147264: loss: 0.6877303798:
Dev-Acc: 1: Accuracy: 0.8343189359: precision: 0.0649183493: recall: 0.1372215610: f1: 0.0881389253
Train-Acc: 1: Accuracy: 0.8198606372: precision: 0.1649257834: recall: 0.1972256919: f1: 0.1796353403
2: 6464: loss: 0.6621802372:
2: 12864: loss: 0.6610523337:
2: 19264: loss: 0.6601460157:
2: 25664: loss: 0.6590211490:
2: 32064: loss: 0.6582789561:
2: 38464: loss: 0.6573859018:
2: 44864: loss: 0.6566469970:
2: 51264: loss: 0.6557449967:
2: 57664: loss: 0.6546895483:
2: 64064: loss: 0.6537879000:
2: 70464: loss: 0.6528180683:
2: 76864: loss: 0.6518952147:
2: 83264: loss: 0.6508846533:
2: 89664: loss: 0.6499117038:
2: 96064: loss: 0.6490488523:
2: 102464: loss: 0.6480926296:
2: 108864: loss: 0.6470857361:
2: 115264: loss: 0.6461214911:
2: 121664: loss: 0.6451668797:
2: 128064: loss: 0.6441297138:
2: 134464: loss: 0.6431772817:
2: 140864: loss: 0.6422370407:
2: 147264: loss: 0.6413253669:
Dev-Acc: 2: Accuracy: 0.9393653870: precision: 0.1006944444: recall: 0.0049311342: f1: 0.0094018479
Train-Acc: 2: Accuracy: 0.8980671763: precision: 0.2439024390: recall: 0.0092038656: f1: 0.0177383592
3: 6464: loss: 0.6181995380:
3: 12864: loss: 0.6167504355:
3: 19264: loss: 0.6162159477:
3: 25664: loss: 0.6150235093:
3: 32064: loss: 0.6144187058:
3: 38464: loss: 0.6133448332:
3: 44864: loss: 0.6123938575:
3: 51264: loss: 0.6113636117:
3: 57664: loss: 0.6106864072:
3: 64064: loss: 0.6098219430:
3: 70464: loss: 0.6088130137:
3: 76864: loss: 0.6080163956:
3: 83264: loss: 0.6072078841:
3: 89664: loss: 0.6062824686:
3: 96064: loss: 0.6054073955:
3: 102464: loss: 0.6045155331:
3: 108864: loss: 0.6036526648:
3: 115264: loss: 0.6028280595:
3: 121664: loss: 0.6020360783:
3: 128064: loss: 0.6010793004:
3: 134464: loss: 0.6002045562:
3: 140864: loss: 0.5994029213:
3: 147264: loss: 0.5986003001:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.5000000000: recall: 0.0001700391: f1: 0.0003399626
Train-Acc: 3: Accuracy: 0.9000065923: precision: 0.5454545455: recall: 0.0003944514: f1: 0.0007883327
4: 6464: loss: 0.5760164279:
4: 12864: loss: 0.5751459211:
4: 19264: loss: 0.5740777449:
4: 25664: loss: 0.5734095879:
4: 32064: loss: 0.5731181147:
4: 38464: loss: 0.5724531985:
4: 44864: loss: 0.5713883422:
4: 51264: loss: 0.5703561558:
4: 57664: loss: 0.5694248998:
4: 64064: loss: 0.5686625982:
4: 70464: loss: 0.5680959654:
4: 76864: loss: 0.5671256149:
4: 83264: loss: 0.5661291905:
4: 89664: loss: 0.5653217883:
4: 96064: loss: 0.5644800807:
4: 102464: loss: 0.5636257099:
4: 108864: loss: 0.5627738546:
4: 115264: loss: 0.5618267895:
4: 121664: loss: 0.5608742706:
4: 128064: loss: 0.5599840300:
4: 134464: loss: 0.5593151948:
4: 140864: loss: 0.5586317565:
4: 147264: loss: 0.5578201347:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 6464: loss: 0.5361688432:
5: 12864: loss: 0.5358919477:
5: 19264: loss: 0.5348037536:
5: 25664: loss: 0.5343531337:
5: 32064: loss: 0.5332343321:
5: 38464: loss: 0.5331136008:
5: 44864: loss: 0.5326232831:
5: 51264: loss: 0.5322262345:
5: 57664: loss: 0.5316340129:
5: 64064: loss: 0.5308333424:
5: 70464: loss: 0.5301127536:
5: 76864: loss: 0.5291327920:
5: 83264: loss: 0.5283306455:
5: 89664: loss: 0.5276743135:
5: 96064: loss: 0.5269788953:
5: 102464: loss: 0.5262718238:
5: 108864: loss: 0.5253637271:
5: 115264: loss: 0.5245055265:
5: 121664: loss: 0.5235244574:
5: 128064: loss: 0.5224757849:
5: 134464: loss: 0.5216499412:
5: 140864: loss: 0.5207293767:
5: 147264: loss: 0.5199481851:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 6464: loss: 0.4971549714:
6: 12864: loss: 0.4997183144:
6: 19264: loss: 0.4990434919:
6: 25664: loss: 0.4987636048:
6: 32064: loss: 0.4974894434:
6: 38464: loss: 0.4967866390:
6: 44864: loss: 0.4961707799:
6: 51264: loss: 0.4960364541:
6: 57664: loss: 0.4957045880:
6: 64064: loss: 0.4947415146:
6: 70464: loss: 0.4934744514:
6: 76864: loss: 0.4928099054:
6: 83264: loss: 0.4922001810:
6: 89664: loss: 0.4915001076:
6: 96064: loss: 0.4907536440:
6: 102464: loss: 0.4895716649:
6: 108864: loss: 0.4889953609:
6: 115264: loss: 0.4883618236:
6: 121664: loss: 0.4876741849:
6: 128064: loss: 0.4869143747:
6: 134464: loss: 0.4861243839:
6: 140864: loss: 0.4852997937:
6: 147264: loss: 0.4845845005:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 6464: loss: 0.4681190372:
7: 12864: loss: 0.4671439521:
7: 19264: loss: 0.4662729323:
7: 25664: loss: 0.4653504524:
7: 32064: loss: 0.4658559044:
7: 38464: loss: 0.4638764289:
7: 44864: loss: 0.4628825550:
7: 51264: loss: 0.4624600452:
7: 57664: loss: 0.4617032364:
7: 64064: loss: 0.4610001190:
7: 70464: loss: 0.4601709987:
7: 76864: loss: 0.4591932526:
7: 83264: loss: 0.4588061979:
7: 89664: loss: 0.4581009318:
7: 96064: loss: 0.4574555904:
7: 102464: loss: 0.4567389285:
7: 108864: loss: 0.4558792685:
7: 115264: loss: 0.4554561093:
7: 121664: loss: 0.4547948332:
7: 128064: loss: 0.4542426523:
7: 134464: loss: 0.4534278875:
7: 140864: loss: 0.4527749041:
7: 147264: loss: 0.4521161820:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.4387595627:
8: 12864: loss: 0.4367874891:
8: 19264: loss: 0.4358108544:
8: 25664: loss: 0.4343718939:
8: 32064: loss: 0.4346767333:
8: 38464: loss: 0.4346681497:
8: 44864: loss: 0.4345952914:
8: 51264: loss: 0.4335944900:
8: 57664: loss: 0.4319753147:
8: 64064: loss: 0.4311572122:
8: 70464: loss: 0.4299259806:
8: 76864: loss: 0.4294504257:
8: 83264: loss: 0.4289102788:
8: 89664: loss: 0.4279444519:
8: 96064: loss: 0.4273063903:
8: 102464: loss: 0.4263009592:
8: 108864: loss: 0.4260811777:
8: 115264: loss: 0.4256115832:
8: 121664: loss: 0.4251414835:
8: 128064: loss: 0.4243208888:
8: 134464: loss: 0.4237561604:
8: 140864: loss: 0.4231384183:
8: 147264: loss: 0.4228839749:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.4079076082:
9: 12864: loss: 0.4080186902:
9: 19264: loss: 0.4068326656:
9: 25664: loss: 0.4057258023:
9: 32064: loss: 0.4046074426:
9: 38464: loss: 0.4031750943:
9: 44864: loss: 0.4027662269:
9: 51264: loss: 0.4022746631:
9: 57664: loss: 0.4023660064:
9: 64064: loss: 0.4020127721:
9: 70464: loss: 0.4017006688:
9: 76864: loss: 0.4020191776:
9: 83264: loss: 0.4014797983:
9: 89664: loss: 0.4008554200:
9: 96064: loss: 0.4004766352:
9: 102464: loss: 0.4006014802:
9: 108864: loss: 0.4003460812:
9: 115264: loss: 0.4001226417:
9: 121664: loss: 0.3997814412:
9: 128064: loss: 0.3993993003:
9: 134464: loss: 0.3988933740:
9: 140864: loss: 0.3980178437:
9: 147264: loss: 0.3973976375:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.3863253698:
10: 12864: loss: 0.3820098650:
10: 19264: loss: 0.3813394373:
10: 25664: loss: 0.3826755106:
10: 32064: loss: 0.3829210525:
10: 38464: loss: 0.3808250647:
10: 44864: loss: 0.3802064815:
10: 51264: loss: 0.3800316115:
10: 57664: loss: 0.3800769250:
10: 64064: loss: 0.3799592595:
10: 70464: loss: 0.3798649477:
10: 76864: loss: 0.3786494505:
10: 83264: loss: 0.3787961458:
10: 89664: loss: 0.3785727860:
10: 96064: loss: 0.3786570230:
10: 102464: loss: 0.3778567054:
10: 108864: loss: 0.3776903186:
10: 115264: loss: 0.3773251781:
10: 121664: loss: 0.3769066269:
10: 128064: loss: 0.3765231337:
10: 134464: loss: 0.3761352195:
10: 140864: loss: 0.3753793579:
10: 147264: loss: 0.3753045033:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.3708743635:
11: 12864: loss: 0.3664788677:
11: 19264: loss: 0.3659280575:
11: 25664: loss: 0.3639677801:
11: 32064: loss: 0.3633801884:
11: 38464: loss: 0.3622855853:
11: 44864: loss: 0.3607780904:
11: 51264: loss: 0.3609474485:
11: 57664: loss: 0.3596768504:
11: 64064: loss: 0.3593080049:
11: 70464: loss: 0.3593340668:
11: 76864: loss: 0.3595959906:
11: 83264: loss: 0.3595906812:
11: 89664: loss: 0.3590423299:
11: 96064: loss: 0.3589652799:
11: 102464: loss: 0.3584538222:
11: 108864: loss: 0.3584147214:
11: 115264: loss: 0.3582416803:
11: 121664: loss: 0.3577587618:
11: 128064: loss: 0.3576326673:
11: 134464: loss: 0.3570992219:
11: 140864: loss: 0.3565415989:
11: 147264: loss: 0.3567863567:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.3553563252:
12: 12864: loss: 0.3478299502:
12: 19264: loss: 0.3487106913:
12: 25664: loss: 0.3478624952:
12: 32064: loss: 0.3477858797:
12: 38464: loss: 0.3479429791:
12: 44864: loss: 0.3480089550:
12: 51264: loss: 0.3465698741:
12: 57664: loss: 0.3461709226:
12: 64064: loss: 0.3461610691:
12: 70464: loss: 0.3440625999:
12: 76864: loss: 0.3444231065:
12: 83264: loss: 0.3446309891:
12: 89664: loss: 0.3440191544:
12: 96064: loss: 0.3436178076:
12: 102464: loss: 0.3436573071:
12: 108864: loss: 0.3434929749:
12: 115264: loss: 0.3427444851:
12: 121664: loss: 0.3425879386:
12: 128064: loss: 0.3422470562:
12: 134464: loss: 0.3418252511:
12: 140864: loss: 0.3419089679:
12: 147264: loss: 0.3414246836:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.3417544302:
13: 12864: loss: 0.3347549030:
13: 19264: loss: 0.3359781657:
13: 25664: loss: 0.3353518950:
13: 32064: loss: 0.3328680047:
13: 38464: loss: 0.3322001088:
13: 44864: loss: 0.3319906123:
13: 51264: loss: 0.3321914040:
13: 57664: loss: 0.3317393894:
13: 64064: loss: 0.3311871853:
13: 70464: loss: 0.3315955698:
13: 76864: loss: 0.3316318136:
13: 83264: loss: 0.3315097370:
13: 89664: loss: 0.3307332255:
13: 96064: loss: 0.3307893384:
13: 102464: loss: 0.3306637097:
13: 108864: loss: 0.3305194350:
13: 115264: loss: 0.3303838918:
13: 121664: loss: 0.3295670622:
13: 128064: loss: 0.3295815411:
13: 134464: loss: 0.3294478580:
13: 140864: loss: 0.3291843231:
13: 147264: loss: 0.3283589043:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.3288388340:
14: 12864: loss: 0.3257270944:
14: 19264: loss: 0.3231349791:
14: 25664: loss: 0.3224537957:
14: 32064: loss: 0.3236453422:
14: 38464: loss: 0.3228847844:
14: 44864: loss: 0.3224541158:
14: 51264: loss: 0.3224164995:
14: 57664: loss: 0.3211879516:
14: 64064: loss: 0.3217602779:
14: 70464: loss: 0.3205996727:
14: 76864: loss: 0.3202254518:
14: 83264: loss: 0.3207020488:
14: 89664: loss: 0.3203195483:
14: 96064: loss: 0.3202840340:
14: 102464: loss: 0.3197614296:
14: 108864: loss: 0.3194848879:
14: 115264: loss: 0.3194189985:
14: 121664: loss: 0.3186316836:
14: 128064: loss: 0.3180752431:
14: 134464: loss: 0.3178455815:
14: 140864: loss: 0.3174682403:
14: 147264: loss: 0.3168732686:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.3036046849:
15: 12864: loss: 0.3071806476:
15: 19264: loss: 0.3072752257:
15: 25664: loss: 0.3074493359:
15: 32064: loss: 0.3101492966:
15: 38464: loss: 0.3108012828:
15: 44864: loss: 0.3108550100:
15: 51264: loss: 0.3098090953:
15: 57664: loss: 0.3097104442:
15: 64064: loss: 0.3093271995:
15: 70464: loss: 0.3096873102:
15: 76864: loss: 0.3107172220:
15: 83264: loss: 0.3106184840:
15: 89664: loss: 0.3103134303:
15: 96064: loss: 0.3106970424:
15: 102464: loss: 0.3101504533:
15: 108864: loss: 0.3097931213:
15: 115264: loss: 0.3097070921:
15: 121664: loss: 0.3090261628:
15: 128064: loss: 0.3081359885:
15: 134464: loss: 0.3081544823:
15: 140864: loss: 0.3076396060:
15: 147264: loss: 0.3075018019:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.3046423182:
16: 12864: loss: 0.2986404123:
16: 19264: loss: 0.2982275104:
16: 25664: loss: 0.2985490220:
16: 32064: loss: 0.3005838503:
16: 38464: loss: 0.3004012815:
16: 44864: loss: 0.3016370810:
16: 51264: loss: 0.3005561542:
16: 57664: loss: 0.2997685470:
16: 64064: loss: 0.3001702840:
16: 70464: loss: 0.3001739516:
16: 76864: loss: 0.3007288123:
16: 83264: loss: 0.3009007358:
16: 89664: loss: 0.3016823694:
16: 96064: loss: 0.3013533369:
16: 102464: loss: 0.3005733899:
16: 108864: loss: 0.3005052468:
16: 115264: loss: 0.3000244788:
16: 121664: loss: 0.2998533614:
16: 128064: loss: 0.2999266905:
16: 134464: loss: 0.2995439651:
16: 140864: loss: 0.2989566445:
16: 147264: loss: 0.2992532485:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.3004249200:
17: 12864: loss: 0.2952615345:
17: 19264: loss: 0.2948085088:
17: 25664: loss: 0.2937689892:
17: 32064: loss: 0.2944371136:
17: 38464: loss: 0.2919535640:
17: 44864: loss: 0.2919298820:
17: 51264: loss: 0.2926090395:
17: 57664: loss: 0.2940553734:
17: 64064: loss: 0.2946338928:
17: 70464: loss: 0.2942977428:
17: 76864: loss: 0.2933565275:
17: 83264: loss: 0.2924157328:
17: 89664: loss: 0.2933100270:
17: 96064: loss: 0.2930432385:
17: 102464: loss: 0.2935965826:
17: 108864: loss: 0.2934033012:
17: 115264: loss: 0.2935143626:
17: 121664: loss: 0.2928265283:
17: 128064: loss: 0.2926673677:
17: 134464: loss: 0.2922984016:
17: 140864: loss: 0.2922874048:
17: 147264: loss: 0.2920108028:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.2878292498:
18: 12864: loss: 0.2808023223:
18: 19264: loss: 0.2827822673:
18: 25664: loss: 0.2850368871:
18: 32064: loss: 0.2856704293:
18: 38464: loss: 0.2852313510:
18: 44864: loss: 0.2853479560:
18: 51264: loss: 0.2851395425:
18: 57664: loss: 0.2857284279:
18: 64064: loss: 0.2856220455:
18: 70464: loss: 0.2858234558:
18: 76864: loss: 0.2859291477:
18: 83264: loss: 0.2858712779:
18: 89664: loss: 0.2861693189:
18: 96064: loss: 0.2866695422:
18: 102464: loss: 0.2862607949:
18: 108864: loss: 0.2859417125:
18: 115264: loss: 0.2853038761:
18: 121664: loss: 0.2853043133:
18: 128064: loss: 0.2850335532:
18: 134464: loss: 0.2846564554:
18: 140864: loss: 0.2840965970:
18: 147264: loss: 0.2846070751:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 6464: loss: 0.2826374587:
19: 12864: loss: 0.2807701850:
19: 19264: loss: 0.2848830083:
19: 25664: loss: 0.2845109293:
19: 32064: loss: 0.2845823729:
19: 38464: loss: 0.2817572510:
19: 44864: loss: 0.2813236551:
19: 51264: loss: 0.2795280563:
19: 57664: loss: 0.2784008228:
19: 64064: loss: 0.2780118037:
19: 70464: loss: 0.2784284592:
19: 76864: loss: 0.2783486849:
19: 83264: loss: 0.2773713144:
19: 89664: loss: 0.2777571149:
19: 96064: loss: 0.2784915822:
19: 102464: loss: 0.2789285532:
19: 108864: loss: 0.2789664741:
19: 115264: loss: 0.2783003523:
19: 121664: loss: 0.2786789727:
19: 128064: loss: 0.2789845124:
19: 134464: loss: 0.2784730998:
19: 140864: loss: 0.2787287735:
19: 147264: loss: 0.2790835300:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.9000197053: precision: 1.0000000000: recall: 0.0001972257: f1: 0.0003943736
20: 6464: loss: 0.2749191704:
20: 12864: loss: 0.2748423479:
20: 19264: loss: 0.2752631489:
20: 25664: loss: 0.2732130214:
20: 32064: loss: 0.2739111111:
20: 38464: loss: 0.2739964476:
20: 44864: loss: 0.2743799942:
20: 51264: loss: 0.2753942066:
20: 57664: loss: 0.2762448201:
20: 64064: loss: 0.2755461972:
20: 70464: loss: 0.2761430109:
20: 76864: loss: 0.2762787838:
20: 83264: loss: 0.2757717454:
20: 89664: loss: 0.2756380149:
20: 96064: loss: 0.2755276501:
20: 102464: loss: 0.2750039159:
20: 108864: loss: 0.2756983263:
20: 115264: loss: 0.2749928150:
20: 121664: loss: 0.2741953099:
20: 128064: loss: 0.2737431510:
20: 134464: loss: 0.2736171197:
20: 140864: loss: 0.2730712645:
20: 147264: loss: 0.2732592579:
Dev-Acc: 20: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 20: Accuracy: 0.9003089666: precision: 1.0000000000: recall: 0.0030898692: f1: 0.0061607026
