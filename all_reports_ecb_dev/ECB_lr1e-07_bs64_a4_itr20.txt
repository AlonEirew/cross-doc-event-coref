1: 6464: loss: 0.7044577992:
1: 12864: loss: 0.7025826871:
1: 19264: loss: 0.7017775204:
1: 25664: loss: 0.7008810948:
1: 32064: loss: 0.7002657472:
1: 38464: loss: 0.6995476846:
1: 44864: loss: 0.6988540087:
1: 51264: loss: 0.6981089910:
1: 57664: loss: 0.6973341364:
1: 64064: loss: 0.6964877589:
1: 70464: loss: 0.6957403098:
Dev-Acc: 1: Accuracy: 0.5109838843: precision: 0.0687373316: recall: 0.5881652780: f1: 0.1230895148
Train-Acc: 1: Accuracy: 0.5913615227: precision: 0.2712226067: recall: 0.6183682861: f1: 0.3770619951
2: 6464: loss: 0.6850014567:
2: 12864: loss: 0.6842531723:
2: 19264: loss: 0.6837961183:
2: 25664: loss: 0.6830647331:
2: 32064: loss: 0.6822445992:
2: 38464: loss: 0.6813859792:
2: 44864: loss: 0.6807392463:
2: 51264: loss: 0.6800298814:
2: 57664: loss: 0.6792947353:
2: 64064: loss: 0.6785129144:
2: 70464: loss: 0.6778034945:
Dev-Acc: 2: Accuracy: 0.7510418296: precision: 0.0845588235: recall: 0.3324264581: f1: 0.1348229371
Train-Acc: 2: Accuracy: 0.7776871920: precision: 0.4367593352: recall: 0.3852475182: f1: 0.4093894090
3: 6464: loss: 0.6684244037:
3: 12864: loss: 0.6676578194:
3: 19264: loss: 0.6667233034:
3: 25664: loss: 0.6660143794:
3: 32064: loss: 0.6653775183:
3: 38464: loss: 0.6647804166:
3: 44864: loss: 0.6640257813:
3: 51264: loss: 0.6632827097:
3: 57664: loss: 0.6624124712:
3: 64064: loss: 0.6617483986:
3: 70464: loss: 0.6610082153:
Dev-Acc: 3: Accuracy: 0.8653357625: precision: 0.1197468605: recall: 0.2059173610: f1: 0.1514317869
Train-Acc: 3: Accuracy: 0.8289264441: precision: 0.6935938050: recall: 0.2590888173: f1: 0.3772555401
4: 6464: loss: 0.6493157756:
4: 12864: loss: 0.6495528951:
4: 19264: loss: 0.6491787831:
4: 25664: loss: 0.6485446498:
4: 32064: loss: 0.6477689291:
4: 38464: loss: 0.6473894482:
4: 44864: loss: 0.6466588891:
4: 51264: loss: 0.6460327040:
4: 57664: loss: 0.6453883215:
4: 64064: loss: 0.6447921131:
4: 70464: loss: 0.6442410219:
Dev-Acc: 4: Accuracy: 0.9146888256: precision: 0.1876293401: recall: 0.1387519129: f1: 0.1595307918
Train-Acc: 4: Accuracy: 0.8292025328: precision: 0.8535498249: recall: 0.1762540267: f1: 0.2921752398
5: 6464: loss: 0.6358648330:
5: 12864: loss: 0.6351985162:
5: 19264: loss: 0.6347920358:
5: 25664: loss: 0.6341428810:
5: 32064: loss: 0.6332672619:
5: 38464: loss: 0.6325681896:
5: 44864: loss: 0.6319395454:
5: 51264: loss: 0.6311604254:
5: 57664: loss: 0.6304321631:
5: 64064: loss: 0.6295900905:
5: 70464: loss: 0.6287662486:
Dev-Acc: 5: Accuracy: 0.9323503375: precision: 0.2593733950: recall: 0.0858697500: f1: 0.1290240164
Train-Acc: 5: Accuracy: 0.8234435320: precision: 0.9160055996: recall: 0.1290513444: f1: 0.2262302639
6: 6464: loss: 0.6186808854:
6: 12864: loss: 0.6181928763:
6: 19264: loss: 0.6179887637:
6: 25664: loss: 0.6176064178:
6: 32064: loss: 0.6165556070:
6: 38464: loss: 0.6161212701:
6: 44864: loss: 0.6155687728:
6: 51264: loss: 0.6152235550:
6: 57664: loss: 0.6143123974:
6: 64064: loss: 0.6138217002:
6: 70464: loss: 0.6129170688:
Dev-Acc: 6: Accuracy: 0.9384822845: precision: 0.3374108053: recall: 0.0562829451: f1: 0.0964733314
Train-Acc: 6: Accuracy: 0.8188547492: precision: 0.9334945586: recall: 0.1015054894: f1: 0.1831010969
7: 6464: loss: 0.6050536835:
7: 12864: loss: 0.6033471602:
7: 19264: loss: 0.6031928974:
7: 25664: loss: 0.6023838158:
7: 32064: loss: 0.6017640958:
7: 38464: loss: 0.6011653562:
7: 44864: loss: 0.6004342433:
7: 51264: loss: 0.5997764359:
7: 57664: loss: 0.5992760221:
7: 64064: loss: 0.5985681121:
7: 70464: loss: 0.5977602520:
Dev-Acc: 7: Accuracy: 0.9402385354: precision: 0.3812709030: recall: 0.0387689169: f1: 0.0703812317
Train-Acc: 7: Accuracy: 0.8165932298: precision: 0.9286684783: recall: 0.0898691736: f1: 0.1638793982
8: 6464: loss: 0.5886454743:
8: 12864: loss: 0.5883973488:
8: 19264: loss: 0.5873450089:
8: 25664: loss: 0.5871287115:
8: 32064: loss: 0.5865837295:
8: 38464: loss: 0.5858115489:
8: 44864: loss: 0.5854468815:
8: 51264: loss: 0.5848148613:
8: 57664: loss: 0.5841097324:
8: 64064: loss: 0.5835433644:
8: 70464: loss: 0.5832380081:
Dev-Acc: 8: Accuracy: 0.9408040643: precision: 0.4105263158: recall: 0.0331576263: f1: 0.0613593455
Train-Acc: 8: Accuracy: 0.8154098988: precision: 0.9252539913: recall: 0.0838209191: f1: 0.1537163180
9: 6464: loss: 0.5769577813:
9: 12864: loss: 0.5746857497:
9: 19264: loss: 0.5738200998:
9: 25664: loss: 0.5728033778:
9: 32064: loss: 0.5722360296:
9: 38464: loss: 0.5718848094:
9: 44864: loss: 0.5712395352:
9: 51264: loss: 0.5706935262:
9: 57664: loss: 0.5700340525:
9: 64064: loss: 0.5696057178:
9: 70464: loss: 0.5687583743:
Dev-Acc: 9: Accuracy: 0.9410918355: precision: 0.4285714286: recall: 0.0285665703: f1: 0.0535628886
Train-Acc: 9: Accuracy: 0.8152521253: precision: 0.9252199413: recall: 0.0829662744: f1: 0.1522775264
10: 6464: loss: 0.5591515923:
10: 12864: loss: 0.5592024043:
10: 19264: loss: 0.5592270577:
10: 25664: loss: 0.5591682863:
10: 32064: loss: 0.5584358423:
10: 38464: loss: 0.5575596091:
10: 44864: loss: 0.5570057236:
10: 51264: loss: 0.5560249831:
10: 57664: loss: 0.5556686140:
10: 64064: loss: 0.5550854372:
10: 70464: loss: 0.5543121426:
Dev-Acc: 10: Accuracy: 0.9413101077: precision: 0.4508670520: recall: 0.0265261010: f1: 0.0501043841
Train-Acc: 10: Accuracy: 0.8156465888: precision: 0.9268292683: recall: 0.0849385313: f1: 0.1556157784
11: 6464: loss: 0.5456386185:
11: 12864: loss: 0.5446346034:
11: 19264: loss: 0.5447867196:
11: 25664: loss: 0.5443563304:
11: 32064: loss: 0.5443662071:
11: 38464: loss: 0.5431829399:
11: 44864: loss: 0.5423511892:
11: 51264: loss: 0.5417279453:
11: 57664: loss: 0.5410983944:
11: 64064: loss: 0.5405837514:
11: 70464: loss: 0.5400697989:
Dev-Acc: 11: Accuracy: 0.9413002133: precision: 0.4501424501: recall: 0.0268661792: f1: 0.0507060334
Train-Acc: 11: Accuracy: 0.8159358501: precision: 0.9279661017: recall: 0.0863848531: f1: 0.1580561737
12: 6464: loss: 0.5307978934:
12: 12864: loss: 0.5298324034:
12: 19264: loss: 0.5287620589:
12: 25664: loss: 0.5292075736:
12: 32064: loss: 0.5288206733:
12: 38464: loss: 0.5283017004:
12: 44864: loss: 0.5283481410:
12: 51264: loss: 0.5274958177:
12: 57664: loss: 0.5273838309:
12: 64064: loss: 0.5266017605:
12: 70464: loss: 0.5262887035:
Dev-Acc: 12: Accuracy: 0.9413299561: precision: 0.4600000000: recall: 0.0312871961: f1: 0.0585893966
Train-Acc: 12: Accuracy: 0.8167510033: precision: 0.9309878214: recall: 0.0904608507: f1: 0.1648990353
13: 6464: loss: 0.5195397559:
13: 12864: loss: 0.5159929286:
13: 19264: loss: 0.5168632628:
13: 25664: loss: 0.5164646841:
13: 32064: loss: 0.5152157916:
13: 38464: loss: 0.5149187395:
13: 44864: loss: 0.5151290591:
13: 51264: loss: 0.5143488513:
13: 57664: loss: 0.5139099525:
13: 64064: loss: 0.5139269082:
13: 70464: loss: 0.5131794375:
Dev-Acc: 13: Accuracy: 0.9413696527: precision: 0.4709543568: recall: 0.0385988777: f1: 0.0713499921
Train-Acc: 13: Accuracy: 0.8180527091: precision: 0.9358730159: recall: 0.0969035566: f1: 0.1756225426
14: 6464: loss: 0.5028426671:
14: 12864: loss: 0.5030381803:
14: 19264: loss: 0.5029545747:
14: 25664: loss: 0.5043711292:
14: 32064: loss: 0.5047355423:
14: 38464: loss: 0.5043556166:
14: 44864: loss: 0.5029396532:
14: 51264: loss: 0.5026937823:
14: 57664: loss: 0.5023194493:
14: 64064: loss: 0.5012312872:
14: 70464: loss: 0.5004392847:
Dev-Acc: 14: Accuracy: 0.9415482283: precision: 0.4921383648: recall: 0.0532222411: f1: 0.0960564677
Train-Acc: 14: Accuracy: 0.8198146224: precision: 0.9403857393: recall: 0.1057787128: f1: 0.1901666470
15: 6464: loss: 0.4941274986:
15: 12864: loss: 0.4920453307:
15: 19264: loss: 0.4929678529:
15: 25664: loss: 0.4922624380:
15: 32064: loss: 0.4911752265:
15: 38464: loss: 0.4907908834:
15: 44864: loss: 0.4901476330:
15: 51264: loss: 0.4891470226:
15: 57664: loss: 0.4888454047:
15: 64064: loss: 0.4880813656:
15: 70464: loss: 0.4872672310:
Dev-Acc: 15: Accuracy: 0.9413696527: precision: 0.4819587629: recall: 0.0635946268: f1: 0.1123629262
Train-Acc: 15: Accuracy: 0.8214976192: precision: 0.9445350734: recall: 0.1141936756: f1: 0.2037536657
16: 6464: loss: 0.4796963564:
16: 12864: loss: 0.4806674448:
16: 19264: loss: 0.4796477368:
16: 25664: loss: 0.4805458745:
16: 32064: loss: 0.4786653977:
16: 38464: loss: 0.4784450993:
16: 44864: loss: 0.4778114274:
16: 51264: loss: 0.4773342579:
16: 57664: loss: 0.4765200569:
16: 64064: loss: 0.4759783614:
16: 70464: loss: 0.4753927651:
Dev-Acc: 16: Accuracy: 0.9408040643: precision: 0.4530386740: recall: 0.0697160347: f1: 0.1208370174
Train-Acc: 16: Accuracy: 0.8236933947: precision: 0.9487051793: recall: 0.1252383144: f1: 0.2212672048
17: 6464: loss: 0.4714303079:
17: 12864: loss: 0.4679293349:
17: 19264: loss: 0.4682657803:
17: 25664: loss: 0.4663387363:
17: 32064: loss: 0.4655988804:
17: 38464: loss: 0.4647929328:
17: 44864: loss: 0.4648117339:
17: 51264: loss: 0.4645623194:
17: 57664: loss: 0.4640895946:
17: 64064: loss: 0.4633328366:
17: 70464: loss: 0.4628928202:
Dev-Acc: 17: Accuracy: 0.9403178692: precision: 0.4358237548: recall: 0.0773677946: f1: 0.1314079422
Train-Acc: 17: Accuracy: 0.8267832398: precision: 0.9540793580: recall: 0.1406876602: f1: 0.2452159963
18: 6464: loss: 0.4598786619:
18: 12864: loss: 0.4583227538:
18: 19264: loss: 0.4565972256:
18: 25664: loss: 0.4545515116:
18: 32064: loss: 0.4546026195:
18: 38464: loss: 0.4537456450:
18: 44864: loss: 0.4531675703:
18: 51264: loss: 0.4525599203:
18: 57664: loss: 0.4527449064:
18: 64064: loss: 0.4519877129:
18: 70464: loss: 0.4519962435:
Dev-Acc: 18: Accuracy: 0.9400103092: precision: 0.4350904799: recall: 0.0940316273: f1: 0.1546420582
Train-Acc: 18: Accuracy: 0.8304647803: precision: 0.9588118812: recall: 0.1591611334: f1: 0.2730040595
19: 6464: loss: 0.4414563942:
19: 12864: loss: 0.4396431242:
19: 19264: loss: 0.4425737490:
19: 25664: loss: 0.4413097239:
19: 32064: loss: 0.4414680906:
19: 38464: loss: 0.4415717533:
19: 44864: loss: 0.4415861358:
19: 51264: loss: 0.4406792742:
19: 57664: loss: 0.4408944011:
19: 64064: loss: 0.4402647421:
19: 70464: loss: 0.4400821547:
Dev-Acc: 19: Accuracy: 0.9401392937: precision: 0.4530284302: recall: 0.1246386669: f1: 0.1954927324
Train-Acc: 19: Accuracy: 0.8343304396: precision: 0.9627791563: recall: 0.1785549931: f1: 0.3012422360
20: 6464: loss: 0.4370207486:
20: 12864: loss: 0.4389536731:
20: 19264: loss: 0.4382350986:
20: 25664: loss: 0.4362972427:
20: 32064: loss: 0.4354819950:
20: 38464: loss: 0.4341456523:
20: 44864: loss: 0.4329938423:
20: 51264: loss: 0.4322988532:
20: 57664: loss: 0.4317054410:
20: 64064: loss: 0.4307631440:
20: 70464: loss: 0.4299612985:
Dev-Acc: 20: Accuracy: 0.9395241141: precision: 0.4452405322: recall: 0.1479340248: f1: 0.2220804084
Train-Acc: 20: Accuracy: 0.8384327292: precision: 0.9658909786: recall: 0.1991979489: f1: 0.3302812296
