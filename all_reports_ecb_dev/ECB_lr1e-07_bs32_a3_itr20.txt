1: 3232: loss: 0.7017942864:
1: 6432: loss: 0.6999697348:
1: 9632: loss: 0.6997013072:
1: 12832: loss: 0.6988148682:
1: 16032: loss: 0.6983039929:
1: 19232: loss: 0.6978373500:
1: 22432: loss: 0.6972234327:
1: 25632: loss: 0.6965876189:
1: 28832: loss: 0.6959487863:
1: 32032: loss: 0.6952961274:
1: 35232: loss: 0.6946470956:
1: 38432: loss: 0.6941352284:
1: 41632: loss: 0.6935809990:
1: 44832: loss: 0.6930306723:
1: 48032: loss: 0.6925214826:
1: 51232: loss: 0.6919634309:
1: 54432: loss: 0.6914025821:
1: 57632: loss: 0.6908592912:
1: 60832: loss: 0.6903096416:
Dev-Acc: 1: Accuracy: 0.5475373268: precision: 0.0749689680: recall: 0.5956469988: f1: 0.1331761933
Train-Acc: 1: Accuracy: 0.6825159788: precision: 0.4101138354: recall: 0.6158043521: f1: 0.4923392289
2: 3232: loss: 0.6788155603:
2: 6432: loss: 0.6782613742:
2: 9632: loss: 0.6780509555:
2: 12832: loss: 0.6775443037:
2: 16032: loss: 0.6767772225:
2: 19232: loss: 0.6759266305:
2: 22432: loss: 0.6755187359:
2: 25632: loss: 0.6749114193:
2: 28832: loss: 0.6744581235:
2: 32032: loss: 0.6739165281:
2: 35232: loss: 0.6734141768:
2: 38432: loss: 0.6728829449:
2: 41632: loss: 0.6722487088:
2: 44832: loss: 0.6717322601:
2: 48032: loss: 0.6711755999:
2: 51232: loss: 0.6706248947:
2: 54432: loss: 0.6700332205:
2: 57632: loss: 0.6696225210:
2: 60832: loss: 0.6690597520:
Dev-Acc: 2: Accuracy: 0.7821876407: precision: 0.1033223083: recall: 0.3558918551: f1: 0.1601499732
Train-Acc: 2: Accuracy: 0.8134738207: precision: 0.7345154239: recall: 0.3976069949: f1: 0.5159309021
3: 3232: loss: 0.6596694958:
3: 6432: loss: 0.6577717108:
3: 9632: loss: 0.6569434863:
3: 12832: loss: 0.6565271328:
3: 16032: loss: 0.6560790790:
3: 19232: loss: 0.6556856422:
3: 22432: loss: 0.6553774324:
3: 25632: loss: 0.6549009884:
3: 28832: loss: 0.6542622847:
3: 32032: loss: 0.6538753947:
3: 35232: loss: 0.6533203264:
3: 38432: loss: 0.6528981516:
3: 41632: loss: 0.6523978137:
3: 44832: loss: 0.6518599520:
3: 48032: loss: 0.6513300460:
3: 51232: loss: 0.6507651127:
3: 54432: loss: 0.6502157659:
3: 57632: loss: 0.6496947457:
3: 60832: loss: 0.6491470941:
Dev-Acc: 3: Accuracy: 0.8769943714: precision: 0.1561114630: recall: 0.2514878422: f1: 0.1926408336
Train-Acc: 3: Accuracy: 0.8250443935: precision: 0.9492325856: recall: 0.3171389126: f1: 0.4754348790
4: 3232: loss: 0.6373895115:
4: 6432: loss: 0.6379626018:
4: 9632: loss: 0.6375390836:
4: 12832: loss: 0.6373923625:
4: 16032: loss: 0.6370078720:
4: 19232: loss: 0.6364760848:
4: 22432: loss: 0.6359322341:
4: 25632: loss: 0.6350672301:
4: 28832: loss: 0.6346002613:
4: 32032: loss: 0.6342698355:
4: 35232: loss: 0.6338031688:
4: 38432: loss: 0.6331145240:
4: 41632: loss: 0.6326574871:
4: 44832: loss: 0.6322687079:
4: 48032: loss: 0.6319124007:
4: 51232: loss: 0.6314895745:
4: 54432: loss: 0.6308264592:
4: 57632: loss: 0.6302835876:
4: 60832: loss: 0.6297420936:
Dev-Acc: 4: Accuracy: 0.9088248014: precision: 0.2132454924: recall: 0.2091481041: f1: 0.2111769251
Train-Acc: 4: Accuracy: 0.8170567751: precision: 0.9901489668: recall: 0.2709223588: f1: 0.4254374645
5: 3232: loss: 0.6192476094:
5: 6432: loss: 0.6192175919:
5: 9632: loss: 0.6178597158:
5: 12832: loss: 0.6177155036:
5: 16032: loss: 0.6170386711:
5: 19232: loss: 0.6165831897:
5: 22432: loss: 0.6156953787:
5: 25632: loss: 0.6153337971:
5: 28832: loss: 0.6150216366:
5: 32032: loss: 0.6145725059:
5: 35232: loss: 0.6141350176:
5: 38432: loss: 0.6137070068:
5: 41632: loss: 0.6131886684:
5: 44832: loss: 0.6129045263:
5: 48032: loss: 0.6126104744:
5: 51232: loss: 0.6120848694:
5: 54432: loss: 0.6116421222:
5: 57632: loss: 0.6111172634:
5: 60832: loss: 0.6106773321:
Dev-Acc: 5: Accuracy: 0.9217931032: precision: 0.2634192480: recall: 0.1894235674: f1: 0.2203758655
Train-Acc: 5: Accuracy: 0.8108605742: precision: 0.9991911566: recall: 0.2436394714: f1: 0.3917547569
6: 3232: loss: 0.6013785666:
6: 6432: loss: 0.6004180682:
6: 9632: loss: 0.5992971400:
6: 12832: loss: 0.5986242670:
6: 16032: loss: 0.5985466620:
6: 19232: loss: 0.5987399514:
6: 22432: loss: 0.5978326878:
6: 25632: loss: 0.5972001057:
6: 28832: loss: 0.5965098766:
6: 32032: loss: 0.5963918754:
6: 35232: loss: 0.5960508023:
6: 38432: loss: 0.5953953506:
6: 41632: loss: 0.5950241742:
6: 44832: loss: 0.5946361497:
6: 48032: loss: 0.5941585596:
6: 51232: loss: 0.5932962414:
6: 54432: loss: 0.5927108810:
6: 57632: loss: 0.5923339194:
6: 60832: loss: 0.5916868853:
Dev-Acc: 6: Accuracy: 0.9279746413: precision: 0.3013264129: recall: 0.1776908689: f1: 0.2235533212
Train-Acc: 6: Accuracy: 0.8092827797: precision: 1.0000000000: recall: 0.2371310236: f1: 0.3833563609
7: 3232: loss: 0.5824496138:
7: 6432: loss: 0.5802067867:
7: 9632: loss: 0.5793821079:
7: 12832: loss: 0.5800201866:
7: 16032: loss: 0.5793965575:
7: 19232: loss: 0.5793264777:
7: 22432: loss: 0.5787057820:
7: 25632: loss: 0.5784764080:
7: 28832: loss: 0.5781477843:
7: 32032: loss: 0.5776764050:
7: 35232: loss: 0.5774647528:
7: 38432: loss: 0.5770270266:
7: 41632: loss: 0.5767804059:
7: 44832: loss: 0.5759679251:
7: 48032: loss: 0.5750172836:
7: 51232: loss: 0.5747301975:
7: 54432: loss: 0.5742535369:
7: 57632: loss: 0.5734312851:
7: 60832: loss: 0.5729458728:
Dev-Acc: 7: Accuracy: 0.9302766323: precision: 0.3269927536: recall: 0.1841523550: f1: 0.2356140542
Train-Acc: 7: Accuracy: 0.8103182316: precision: 1.0000000000: recall: 0.2412727631: f1: 0.3887505958
8: 3232: loss: 0.5642477661:
8: 6432: loss: 0.5612133873:
8: 9632: loss: 0.5602075662:
8: 12832: loss: 0.5604291765:
8: 16032: loss: 0.5597437649:
8: 19232: loss: 0.5591066089:
8: 22432: loss: 0.5589573921:
8: 25632: loss: 0.5591022327:
8: 28832: loss: 0.5583831731:
8: 32032: loss: 0.5580513333:
8: 35232: loss: 0.5577446074:
8: 38432: loss: 0.5573239008:
8: 41632: loss: 0.5564321226:
8: 44832: loss: 0.5565963419:
8: 48032: loss: 0.5561725361:
8: 51232: loss: 0.5556245932:
8: 54432: loss: 0.5551398651:
8: 57632: loss: 0.5546889942:
8: 60832: loss: 0.5544833891:
Dev-Acc: 8: Accuracy: 0.9307131767: precision: 0.3410271206: recall: 0.2009862268: f1: 0.2529153739
Train-Acc: 8: Accuracy: 0.8135889173: precision: 1.0000000000: recall: 0.2543554007: f1: 0.4055555556
9: 3232: loss: 0.5477674621:
9: 6432: loss: 0.5460208350:
9: 9632: loss: 0.5459872818:
9: 12832: loss: 0.5443468535:
9: 16032: loss: 0.5436038550:
9: 19232: loss: 0.5427039948:
9: 22432: loss: 0.5421985717:
9: 25632: loss: 0.5421769714:
9: 28832: loss: 0.5420038534:
9: 32032: loss: 0.5408968505:
9: 35232: loss: 0.5398226786:
9: 38432: loss: 0.5395303253:
9: 41632: loss: 0.5392813154:
9: 44832: loss: 0.5392363607:
9: 48032: loss: 0.5382487244:
9: 51232: loss: 0.5374596929:
9: 54432: loss: 0.5370439170:
9: 57632: loss: 0.5364465097:
9: 60832: loss: 0.5359165097:
Dev-Acc: 9: Accuracy: 0.9298301339: precision: 0.3417486048: recall: 0.2186702942: f1: 0.2666943177
Train-Acc: 9: Accuracy: 0.8176155686: precision: 1.0000000000: recall: 0.2704621655: f1: 0.4257697283
10: 3232: loss: 0.5195851642:
10: 6432: loss: 0.5203166084:
10: 9632: loss: 0.5245624577:
10: 12832: loss: 0.5246197175:
10: 16032: loss: 0.5241358610:
10: 19232: loss: 0.5246545668:
10: 22432: loss: 0.5237106991:
10: 25632: loss: 0.5234917680:
10: 28832: loss: 0.5232760333:
10: 32032: loss: 0.5231237249:
10: 35232: loss: 0.5229120292:
10: 38432: loss: 0.5225248294:
10: 41632: loss: 0.5220266415:
10: 44832: loss: 0.5211107076:
10: 48032: loss: 0.5203692080:
10: 51232: loss: 0.5193735873:
10: 54432: loss: 0.5191038932:
10: 57632: loss: 0.5184513889:
10: 60832: loss: 0.5176281432:
Dev-Acc: 10: Accuracy: 0.9278159142: precision: 0.3403573065: recall: 0.2526781160: f1: 0.2900361081
Train-Acc: 10: Accuracy: 0.8225462437: precision: 1.0000000000: recall: 0.2901847347: f1: 0.4498343949
11: 3232: loss: 0.5008969700:
11: 6432: loss: 0.4998030704:
11: 9632: loss: 0.4988846417:
11: 12832: loss: 0.5020752184:
11: 16032: loss: 0.5025250691:
11: 19232: loss: 0.5027300799:
11: 22432: loss: 0.5036665791:
11: 25632: loss: 0.5026683680:
11: 28832: loss: 0.5030550276:
11: 32032: loss: 0.5025380999:
11: 35232: loss: 0.5025333804:
11: 38432: loss: 0.5017052341:
11: 41632: loss: 0.5014099867:
11: 44832: loss: 0.5006277340:
11: 48032: loss: 0.5003962405:
11: 51232: loss: 0.5000907846:
11: 54432: loss: 0.4998505698:
11: 57632: loss: 0.4995516162:
11: 60832: loss: 0.4993421784:
Dev-Acc: 11: Accuracy: 0.9247400165: precision: 0.3348196976: recall: 0.2936575412: f1: 0.3128906604
Train-Acc: 11: Accuracy: 0.8284630179: precision: 1.0000000000: recall: 0.3138518178: f1: 0.4777583187
12: 3232: loss: 0.4914213023:
12: 6432: loss: 0.4908342163:
12: 9632: loss: 0.4918121919:
12: 12832: loss: 0.4899567585:
12: 16032: loss: 0.4891126535:
12: 19232: loss: 0.4886514022:
12: 22432: loss: 0.4888986837:
12: 25632: loss: 0.4882638461:
12: 28832: loss: 0.4871099032:
12: 32032: loss: 0.4855978947:
12: 35232: loss: 0.4850697916:
12: 38432: loss: 0.4841218673:
12: 41632: loss: 0.4837788824:
12: 44832: loss: 0.4834357310:
12: 48032: loss: 0.4836572368:
12: 51232: loss: 0.4835949539:
12: 54432: loss: 0.4832871295:
12: 57632: loss: 0.4827574979:
12: 60832: loss: 0.4819846968:
Dev-Acc: 12: Accuracy: 0.9197094440: precision: 0.3162094763: recall: 0.3234143853: f1: 0.3197713517
Train-Acc: 12: Accuracy: 0.8348563910: precision: 1.0000000000: recall: 0.3394254158: f1: 0.5068224207
13: 3232: loss: 0.4728356236:
13: 6432: loss: 0.4723006625:
13: 9632: loss: 0.4713244351:
13: 12832: loss: 0.4696838138:
13: 16032: loss: 0.4694044894:
13: 19232: loss: 0.4703164592:
13: 22432: loss: 0.4700385068:
13: 25632: loss: 0.4696926111:
13: 28832: loss: 0.4682895546:
13: 32032: loss: 0.4678900786:
13: 35232: loss: 0.4682719615:
13: 38432: loss: 0.4673532630:
13: 41632: loss: 0.4671650575:
13: 44832: loss: 0.4670628991:
13: 48032: loss: 0.4662715488:
13: 51232: loss: 0.4659614239:
13: 54432: loss: 0.4657313284:
13: 57632: loss: 0.4649493672:
13: 60832: loss: 0.4645817838:
Dev-Acc: 13: Accuracy: 0.9148674011: precision: 0.3045619117: recall: 0.3575922462: f1: 0.3289535429
Train-Acc: 13: Accuracy: 0.8417428732: precision: 1.0000000000: recall: 0.3669712708: f1: 0.5369114606
14: 3232: loss: 0.4519917983:
14: 6432: loss: 0.4524304768:
14: 9632: loss: 0.4523944091:
14: 12832: loss: 0.4523018934:
14: 16032: loss: 0.4517223066:
14: 19232: loss: 0.4514061657:
14: 22432: loss: 0.4514720933:
14: 25632: loss: 0.4512587492:
14: 28832: loss: 0.4511351633:
14: 32032: loss: 0.4508963764:
14: 35232: loss: 0.4503241107:
14: 38432: loss: 0.4499228196:
14: 41632: loss: 0.4494334396:
14: 44832: loss: 0.4492281213:
14: 48032: loss: 0.4488445909:
14: 51232: loss: 0.4482336344:
14: 54432: loss: 0.4481543418:
14: 57632: loss: 0.4477247147:
14: 60832: loss: 0.4473373324:
Dev-Acc: 14: Accuracy: 0.9081997275: precision: 0.2841040092: recall: 0.3771467438: f1: 0.3240794857
Train-Acc: 14: Accuracy: 0.8476102948: precision: 0.9991595226: recall: 0.3907698376: f1: 0.5618147448
15: 3232: loss: 0.4307503009:
15: 6432: loss: 0.4344103189:
15: 9632: loss: 0.4332440353:
15: 12832: loss: 0.4340636788:
15: 16032: loss: 0.4361497586:
15: 19232: loss: 0.4360923331:
15: 22432: loss: 0.4365302191:
15: 25632: loss: 0.4367707216:
15: 28832: loss: 0.4367466547:
15: 32032: loss: 0.4359765070:
15: 35232: loss: 0.4359524375:
15: 38432: loss: 0.4356381923:
15: 41632: loss: 0.4355836442:
15: 44832: loss: 0.4347874312:
15: 48032: loss: 0.4337691267:
15: 51232: loss: 0.4329041362:
15: 54432: loss: 0.4320424100:
15: 57632: loss: 0.4315236602:
15: 60832: loss: 0.4311921477:
Dev-Acc: 15: Accuracy: 0.8950726390: precision: 0.2637406885: recall: 0.4455024656: f1: 0.3313310149
Train-Acc: 15: Accuracy: 0.8569785357: precision: 0.9806527839: recall: 0.4365261981: f1: 0.6041306524
16: 3232: loss: 0.4218706000:
16: 6432: loss: 0.4213641180:
16: 9632: loss: 0.4224446060:
16: 12832: loss: 0.4205273309:
16: 16032: loss: 0.4204242742:
16: 19232: loss: 0.4213201642:
16: 22432: loss: 0.4199085563:
16: 25632: loss: 0.4200752505:
16: 28832: loss: 0.4191605873:
16: 32032: loss: 0.4183762072:
16: 35232: loss: 0.4184245656:
16: 38432: loss: 0.4186458643:
16: 41632: loss: 0.4180970128:
16: 44832: loss: 0.4177400684:
16: 48032: loss: 0.4175791233:
16: 51232: loss: 0.4177458015:
16: 54432: loss: 0.4167072138:
16: 57632: loss: 0.4160578576:
16: 60832: loss: 0.4153236522:
Dev-Acc: 16: Accuracy: 0.8839696646: precision: 0.2490285813: recall: 0.4903927903: f1: 0.3303172603
Train-Acc: 16: Accuracy: 0.8622707725: precision: 0.9484048838: recall: 0.4749194662: f1: 0.6329069564
17: 3232: loss: 0.4181110612:
17: 6432: loss: 0.4073387167:
17: 9632: loss: 0.4052777806:
17: 12832: loss: 0.4054160845:
17: 16032: loss: 0.4048269240:
17: 19232: loss: 0.4065850310:
17: 22432: loss: 0.4067322892:
17: 25632: loss: 0.4062710559:
17: 28832: loss: 0.4045582439:
17: 32032: loss: 0.4041758896:
17: 35232: loss: 0.4036443233:
17: 38432: loss: 0.4031032999:
17: 41632: loss: 0.4026173328:
17: 44832: loss: 0.4018729539:
17: 48032: loss: 0.4017143964:
17: 51232: loss: 0.4015851993:
17: 54432: loss: 0.4008994604:
17: 57632: loss: 0.4004066890:
17: 60832: loss: 0.4004144156:
Dev-Acc: 17: Accuracy: 0.8743352294: precision: 0.2348757230: recall: 0.5109675225: f1: 0.3218206158
Train-Acc: 17: Accuracy: 0.8676615953: precision: 0.9462660516: recall: 0.4989810006: f1: 0.6534090909
18: 3232: loss: 0.3880496973:
18: 6432: loss: 0.3904308826:
18: 9632: loss: 0.3896778498:
18: 12832: loss: 0.3894256021:
18: 16032: loss: 0.3892562196:
18: 19232: loss: 0.3894060791:
18: 22432: loss: 0.3888764103:
18: 25632: loss: 0.3883829777:
18: 28832: loss: 0.3881086452:
18: 32032: loss: 0.3873811283:
18: 35232: loss: 0.3881090194:
18: 38432: loss: 0.3881510135:
18: 41632: loss: 0.3886769482:
18: 44832: loss: 0.3882039770:
18: 48032: loss: 0.3873796835:
18: 51232: loss: 0.3871192731:
18: 54432: loss: 0.3867868674:
18: 57632: loss: 0.3864695079:
18: 60832: loss: 0.3859123551:
Dev-Acc: 18: Accuracy: 0.8646610379: precision: 0.2234262494: recall: 0.5329025676: f1: 0.3148483022
Train-Acc: 18: Accuracy: 0.8718197942: precision: 0.9452186449: recall: 0.5172572480: f1: 0.6686212025
19: 3232: loss: 0.3817121157:
19: 6432: loss: 0.3778406163:
19: 9632: loss: 0.3758512828:
19: 12832: loss: 0.3753242769:
19: 16032: loss: 0.3749005786:
19: 19232: loss: 0.3745997324:
19: 22432: loss: 0.3747557313:
19: 25632: loss: 0.3741065001:
19: 28832: loss: 0.3739385594:
19: 32032: loss: 0.3733912155:
19: 35232: loss: 0.3727956972:
19: 38432: loss: 0.3723311020:
19: 41632: loss: 0.3727588039:
19: 44832: loss: 0.3730641605:
19: 48032: loss: 0.3720775681:
19: 51232: loss: 0.3715444656:
19: 54432: loss: 0.3721239204:
19: 57632: loss: 0.3717441331:
19: 60832: loss: 0.3720068442:
Dev-Acc: 19: Accuracy: 0.8572392464: precision: 0.2165656027: recall: 0.5526271042: f1: 0.3111834546
Train-Acc: 19: Accuracy: 0.8755835295: precision: 0.9461637277: recall: 0.5326408520: f1: 0.6815849247
20: 3232: loss: 0.3567203233:
20: 6432: loss: 0.3592421715:
20: 9632: loss: 0.3647362154:
20: 12832: loss: 0.3634164967:
20: 16032: loss: 0.3631683609:
20: 19232: loss: 0.3606994698:
20: 22432: loss: 0.3599548686:
20: 25632: loss: 0.3604918795:
20: 28832: loss: 0.3606248310:
20: 32032: loss: 0.3613817392:
20: 35232: loss: 0.3606431285:
20: 38432: loss: 0.3610078655:
20: 41632: loss: 0.3607182691:
20: 44832: loss: 0.3607137189:
20: 48032: loss: 0.3606058930:
20: 51232: loss: 0.3603044998:
20: 54432: loss: 0.3601551388:
20: 57632: loss: 0.3597052375:
20: 60832: loss: 0.3594633176:
Dev-Acc: 20: Accuracy: 0.8502242565: precision: 0.2083438845: recall: 0.5595987077: f1: 0.3036398026
Train-Acc: 20: Accuracy: 0.8785254359: precision: 0.9469593050: recall: 0.5446058773: f1: 0.6915146709
