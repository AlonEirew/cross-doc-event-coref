1: 6464: loss: 0.7072848785:
1: 12864: loss: 0.7055666488:
1: 19264: loss: 0.7044935685:
1: 25664: loss: 0.7037413451:
1: 32064: loss: 0.7027362286:
1: 38464: loss: 0.7019035049:
1: 44864: loss: 0.7010689671:
1: 51264: loss: 0.7002632593:
1: 57664: loss: 0.6993413040:
1: 64064: loss: 0.6985086076:
1: 70464: loss: 0.6976889437:
1: 76864: loss: 0.6969306577:
1: 83264: loss: 0.6961164574:
1: 89664: loss: 0.6952191573:
1: 96064: loss: 0.6943643673:
1: 102464: loss: 0.6935082103:
Dev-Acc: 1: Accuracy: 0.6735196114: precision: 0.0674520601: recall: 0.3582724027: f1: 0.1135298238
Train-Acc: 1: Accuracy: 0.6861293912: precision: 0.2056322545: recall: 0.4181184669: f1: 0.2756827048
2: 6464: loss: 0.6773782724:
2: 12864: loss: 0.6769733104:
2: 19264: loss: 0.6760749942:
2: 25664: loss: 0.6751647556:
2: 32064: loss: 0.6742103280:
2: 38464: loss: 0.6735021681:
2: 44864: loss: 0.6727542144:
2: 51264: loss: 0.6719971333:
2: 57664: loss: 0.6712229421:
2: 64064: loss: 0.6704899045:
2: 70464: loss: 0.6697132296:
2: 76864: loss: 0.6689730772:
2: 83264: loss: 0.6681863492:
2: 89664: loss: 0.6673687880:
2: 96064: loss: 0.6665376659:
2: 102464: loss: 0.6657359240:
Dev-Acc: 2: Accuracy: 0.8906770945: precision: 0.1050284484: recall: 0.1161367114: f1: 0.1103036176
Train-Acc: 2: Accuracy: 0.8459103703: precision: 0.3959276018: recall: 0.1495628164: f1: 0.2171112278
3: 6464: loss: 0.6513409764:
3: 12864: loss: 0.6502641425:
3: 19264: loss: 0.6497303909:
3: 25664: loss: 0.6490370768:
3: 32064: loss: 0.6484498103:
3: 38464: loss: 0.6475432012:
3: 44864: loss: 0.6466460834:
3: 51264: loss: 0.6459660506:
3: 57664: loss: 0.6451322080:
3: 64064: loss: 0.6444584837:
3: 70464: loss: 0.6437060397:
3: 76864: loss: 0.6429138176:
3: 83264: loss: 0.6421688482:
3: 89664: loss: 0.6415147537:
3: 96064: loss: 0.6407599425:
3: 102464: loss: 0.6399382875:
Dev-Acc: 3: Accuracy: 0.9370236993: precision: 0.1064189189: recall: 0.0107124639: f1: 0.0194654720
Train-Acc: 3: Accuracy: 0.8587770462: precision: 0.5931477516: recall: 0.0364210111: f1: 0.0686280582
4: 6464: loss: 0.6256105757:
4: 12864: loss: 0.6257256061:
4: 19264: loss: 0.6242876707:
4: 25664: loss: 0.6235766953:
4: 32064: loss: 0.6231215330:
4: 38464: loss: 0.6226441164:
4: 44864: loss: 0.6218462738:
4: 51264: loss: 0.6209599063:
4: 57664: loss: 0.6201977948:
4: 64064: loss: 0.6194075673:
4: 70464: loss: 0.6185867536:
4: 76864: loss: 0.6178523494:
4: 83264: loss: 0.6173171696:
4: 89664: loss: 0.6165267000:
4: 96064: loss: 0.6157001629:
4: 102464: loss: 0.6150723029:
Dev-Acc: 4: Accuracy: 0.9414589405: precision: 0.0476190476: recall: 0.0001700391: f1: 0.0003388682
Train-Acc: 4: Accuracy: 0.8576500416: precision: 0.8139534884: recall: 0.0046019328: f1: 0.0091521213
5: 6464: loss: 0.6030048358:
5: 12864: loss: 0.6012245646:
5: 19264: loss: 0.6010731033:
5: 25664: loss: 0.5997183602:
5: 32064: loss: 0.5988802814:
5: 38464: loss: 0.5983970129:
5: 44864: loss: 0.5974002893:
5: 51264: loss: 0.5965442246:
5: 57664: loss: 0.5960280496:
5: 64064: loss: 0.5952061320:
5: 70464: loss: 0.5946600112:
5: 76864: loss: 0.5938792926:
5: 83264: loss: 0.5931279872:
5: 89664: loss: 0.5924685950:
5: 96064: loss: 0.5916447178:
5: 102464: loss: 0.5908912787:
Dev-Acc: 5: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8572649360: precision: 1.0000000000: recall: 0.0008546447: f1: 0.0017078297
6: 6464: loss: 0.5796650368:
6: 12864: loss: 0.5778663838:
6: 19264: loss: 0.5767064059:
6: 25664: loss: 0.5760368846:
6: 32064: loss: 0.5756147480:
6: 38464: loss: 0.5747712609:
6: 44864: loss: 0.5739948870:
6: 51264: loss: 0.5732851022:
6: 57664: loss: 0.5722002712:
6: 64064: loss: 0.5716000137:
6: 70464: loss: 0.5712660124:
6: 76864: loss: 0.5707352363:
6: 83264: loss: 0.5698018587:
6: 89664: loss: 0.5692293680:
6: 96064: loss: 0.5686495427:
6: 102464: loss: 0.5679849324:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8571710587: precision: 1.0000000000: recall: 0.0001972257: f1: 0.0003943736
7: 6464: loss: 0.5559725285:
7: 12864: loss: 0.5562995410:
7: 19264: loss: 0.5556193038:
7: 25664: loss: 0.5550720854:
7: 32064: loss: 0.5541450079:
7: 38464: loss: 0.5534334770:
7: 44864: loss: 0.5525102870:
7: 51264: loss: 0.5516399408:
7: 57664: loss: 0.5510867088:
7: 64064: loss: 0.5504429629:
7: 70464: loss: 0.5497882006:
7: 76864: loss: 0.5490075078:
7: 83264: loss: 0.5482538279:
7: 89664: loss: 0.5473345175:
7: 96064: loss: 0.5465789222:
7: 102464: loss: 0.5460673371:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.5342527145:
8: 12864: loss: 0.5331588630:
8: 19264: loss: 0.5307103073:
8: 25664: loss: 0.5309906475:
8: 32064: loss: 0.5303507943:
8: 38464: loss: 0.5289479680:
8: 44864: loss: 0.5292329316:
8: 51264: loss: 0.5285554170:
8: 57664: loss: 0.5280345240:
8: 64064: loss: 0.5275784633:
8: 70464: loss: 0.5271639381:
8: 76864: loss: 0.5267117485:
8: 83264: loss: 0.5267328165:
8: 89664: loss: 0.5261634288:
8: 96064: loss: 0.5252178310:
8: 102464: loss: 0.5245559490:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.5107899728:
9: 12864: loss: 0.5102227530:
9: 19264: loss: 0.5105773336:
9: 25664: loss: 0.5102922938:
9: 32064: loss: 0.5099715084:
9: 38464: loss: 0.5098163147:
9: 44864: loss: 0.5095119017:
9: 51264: loss: 0.5090860121:
9: 57664: loss: 0.5083910062:
9: 64064: loss: 0.5076764106:
9: 70464: loss: 0.5070401433:
9: 76864: loss: 0.5062625882:
9: 83264: loss: 0.5057613564:
9: 89664: loss: 0.5055254106:
9: 96064: loss: 0.5049715205:
9: 102464: loss: 0.5044912121:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.4928883368:
10: 12864: loss: 0.4922211945:
10: 19264: loss: 0.4923930618:
10: 25664: loss: 0.4914933444:
10: 32064: loss: 0.4916141096:
10: 38464: loss: 0.4904121261:
10: 44864: loss: 0.4898875272:
10: 51264: loss: 0.4892106175:
10: 57664: loss: 0.4881665381:
10: 64064: loss: 0.4883410317:
10: 70464: loss: 0.4878154940:
10: 76864: loss: 0.4876322151:
10: 83264: loss: 0.4877468547:
10: 89664: loss: 0.4874497560:
10: 96064: loss: 0.4863265306:
10: 102464: loss: 0.4856810961:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.4702488890:
11: 12864: loss: 0.4748450580:
11: 19264: loss: 0.4738234718:
11: 25664: loss: 0.4752926409:
11: 32064: loss: 0.4740353629:
11: 38464: loss: 0.4739726879:
11: 44864: loss: 0.4732094016:
11: 51264: loss: 0.4724725418:
11: 57664: loss: 0.4720522120:
11: 64064: loss: 0.4716117114:
11: 70464: loss: 0.4712811903:
11: 76864: loss: 0.4703105922:
11: 83264: loss: 0.4698148554:
11: 89664: loss: 0.4691547190:
11: 96064: loss: 0.4684980812:
11: 102464: loss: 0.4678382745:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.4586854398:
12: 12864: loss: 0.4594404922:
12: 19264: loss: 0.4577037085:
12: 25664: loss: 0.4581952737:
12: 32064: loss: 0.4581592855:
12: 38464: loss: 0.4566386526:
12: 44864: loss: 0.4555900872:
12: 51264: loss: 0.4549041333:
12: 57664: loss: 0.4545895088:
12: 64064: loss: 0.4541812408:
12: 70464: loss: 0.4533187968:
12: 76864: loss: 0.4528614144:
12: 83264: loss: 0.4523222565:
12: 89664: loss: 0.4519246677:
12: 96064: loss: 0.4514560717:
12: 102464: loss: 0.4511082595:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.4455851376:
13: 12864: loss: 0.4455176467:
13: 19264: loss: 0.4444881544:
13: 25664: loss: 0.4424860652:
13: 32064: loss: 0.4429903523:
13: 38464: loss: 0.4412994953:
13: 44864: loss: 0.4405778128:
13: 51264: loss: 0.4399640762:
13: 57664: loss: 0.4382994483:
13: 64064: loss: 0.4376467755:
13: 70464: loss: 0.4374002822:
13: 76864: loss: 0.4371763427:
13: 83264: loss: 0.4370985092:
13: 89664: loss: 0.4367649356:
13: 96064: loss: 0.4362753190:
13: 102464: loss: 0.4358691071:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.4237418088:
14: 12864: loss: 0.4262690234:
14: 19264: loss: 0.4250667570:
14: 25664: loss: 0.4276812731:
14: 32064: loss: 0.4271400380:
14: 38464: loss: 0.4247611983:
14: 44864: loss: 0.4243845430:
14: 51264: loss: 0.4238767047:
14: 57664: loss: 0.4234073029:
14: 64064: loss: 0.4238334416:
14: 70464: loss: 0.4235165300:
14: 76864: loss: 0.4230129801:
14: 83264: loss: 0.4226584870:
14: 89664: loss: 0.4222479516:
14: 96064: loss: 0.4214693304:
14: 102464: loss: 0.4211493785:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.4165909553:
15: 12864: loss: 0.4169144367:
15: 19264: loss: 0.4128130445:
15: 25664: loss: 0.4117717411:
15: 32064: loss: 0.4124035287:
15: 38464: loss: 0.4133547883:
15: 44864: loss: 0.4124139399:
15: 51264: loss: 0.4113179126:
15: 57664: loss: 0.4107269297:
15: 64064: loss: 0.4101583934:
15: 70464: loss: 0.4093590419:
15: 76864: loss: 0.4091431700:
15: 83264: loss: 0.4090438906:
15: 89664: loss: 0.4086743439:
15: 96064: loss: 0.4080682454:
15: 102464: loss: 0.4078485253:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.3981312394:
16: 12864: loss: 0.4013133253:
16: 19264: loss: 0.3997023066:
16: 25664: loss: 0.3992322963:
16: 32064: loss: 0.3992847797:
16: 38464: loss: 0.3992288137:
16: 44864: loss: 0.3982540547:
16: 51264: loss: 0.3979451681:
16: 57664: loss: 0.3977050202:
16: 64064: loss: 0.3977699005:
16: 70464: loss: 0.3977455303:
16: 76864: loss: 0.3966759027:
16: 83264: loss: 0.3963498853:
16: 89664: loss: 0.3959784792:
16: 96064: loss: 0.3954612878:
16: 102464: loss: 0.3957318414:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.3896611682:
17: 12864: loss: 0.3889089002:
17: 19264: loss: 0.3883260899:
17: 25664: loss: 0.3898813960:
17: 32064: loss: 0.3892641711:
17: 38464: loss: 0.3868996020:
17: 44864: loss: 0.3866981243:
17: 51264: loss: 0.3866066286:
17: 57664: loss: 0.3867384701:
17: 64064: loss: 0.3863171767:
17: 70464: loss: 0.3854717252:
17: 76864: loss: 0.3857490925:
17: 83264: loss: 0.3853524925:
17: 89664: loss: 0.3850300190:
17: 96064: loss: 0.3847386282:
17: 102464: loss: 0.3843114294:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.3828252080:
18: 12864: loss: 0.3789777721:
18: 19264: loss: 0.3795084813:
18: 25664: loss: 0.3751913368:
18: 32064: loss: 0.3755873269:
18: 38464: loss: 0.3755188290:
18: 44864: loss: 0.3755878737:
18: 51264: loss: 0.3755665998:
18: 57664: loss: 0.3751601271:
18: 64064: loss: 0.3751992667:
18: 70464: loss: 0.3739950755:
18: 76864: loss: 0.3746945220:
18: 83264: loss: 0.3743522534:
18: 89664: loss: 0.3745154170:
18: 96064: loss: 0.3748071969:
18: 102464: loss: 0.3745526919:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.5000000000: recall: 0.0001700391: f1: 0.0003399626
Train-Acc: 18: Accuracy: 0.8572086096: precision: 1.0000000000: recall: 0.0004601933: f1: 0.0009199632
19: 6464: loss: 0.3713379353:
19: 12864: loss: 0.3693924706:
19: 19264: loss: 0.3656673159:
19: 25664: loss: 0.3670089068:
19: 32064: loss: 0.3674741758:
19: 38464: loss: 0.3668335643:
19: 44864: loss: 0.3674711247:
19: 51264: loss: 0.3669643899:
19: 57664: loss: 0.3657820766:
19: 64064: loss: 0.3650383505:
19: 70464: loss: 0.3649423044:
19: 76864: loss: 0.3654426863:
19: 83264: loss: 0.3655207291:
19: 89664: loss: 0.3654335861:
19: 96064: loss: 0.3653365926:
19: 102464: loss: 0.3645494694:
Dev-Acc: 19: Accuracy: 0.9416871667: precision: 0.8333333333: recall: 0.0008501955: f1: 0.0016986581
Train-Acc: 19: Accuracy: 0.8577721119: precision: 1.0000000000: recall: 0.0044047071: f1: 0.0087707815
20: 6464: loss: 0.3605850366:
20: 12864: loss: 0.3597892482:
20: 19264: loss: 0.3590190725:
20: 25664: loss: 0.3603112843:
20: 32064: loss: 0.3598645476:
20: 38464: loss: 0.3594538559:
20: 44864: loss: 0.3586281732:
20: 51264: loss: 0.3581003692:
20: 57664: loss: 0.3576255069:
20: 64064: loss: 0.3575079100:
20: 70464: loss: 0.3580507363:
20: 76864: loss: 0.3581493050:
20: 83264: loss: 0.3578533979:
20: 89664: loss: 0.3571566691:
20: 96064: loss: 0.3567242802:
20: 102464: loss: 0.3559629294:
Dev-Acc: 20: Accuracy: 0.9417367578: precision: 0.9090909091: recall: 0.0017003911: f1: 0.0033944331
Train-Acc: 20: Accuracy: 0.8584858775: precision: 0.9798657718: recall: 0.0095983170: f1: 0.0190104167
