1: 3232: loss: 0.7105288881:
1: 6432: loss: 0.7092092535:
1: 9632: loss: 0.7079713285:
1: 12832: loss: 0.7075009923:
1: 16032: loss: 0.7062812995:
1: 19232: loss: 0.7056110281:
1: 22432: loss: 0.7047547272:
1: 25632: loss: 0.7039073820:
1: 28832: loss: 0.7029753726:
1: 32032: loss: 0.7021795981:
1: 35232: loss: 0.7012719380:
1: 38432: loss: 0.7004658545:
1: 41632: loss: 0.6996242658:
1: 44832: loss: 0.6987325051:
1: 48032: loss: 0.6978519938:
1: 51232: loss: 0.6970539159:
1: 54432: loss: 0.6961859761:
1: 57632: loss: 0.6953588679:
1: 60832: loss: 0.6945074996:
1: 64032: loss: 0.6936315516:
1: 67232: loss: 0.6927707227:
1: 70432: loss: 0.6919806966:
1: 73632: loss: 0.6911724639:
1: 76832: loss: 0.6902995907:
1: 80032: loss: 0.6894337087:
1: 83232: loss: 0.6886236897:
1: 86432: loss: 0.6878167075:
1: 89632: loss: 0.6870143524:
1: 92832: loss: 0.6861537863:
1: 96032: loss: 0.6853312573:
1: 99232: loss: 0.6844983427:
1: 102432: loss: 0.6836248587:
1: 105632: loss: 0.6828137855:
1: 108832: loss: 0.6820147881:
1: 112032: loss: 0.6811753188:
1: 115232: loss: 0.6803192818:
1: 118432: loss: 0.6795369993:
1: 121632: loss: 0.6787301738:
Dev-Acc: 1: Accuracy: 0.9111565351: precision: 0.0927643785: recall: 0.0595136881: f1: 0.0725088046
Train-Acc: 1: Accuracy: 0.8621063828: precision: 0.3155419704: recall: 0.0882256262: f1: 0.1378956021
2: 3232: loss: 0.6475424516:
2: 6432: loss: 0.6466330487:
2: 9632: loss: 0.6456541065:
2: 12832: loss: 0.6448304248:
2: 16032: loss: 0.6440309654:
2: 19232: loss: 0.6430975342:
2: 22432: loss: 0.6422589752:
2: 25632: loss: 0.6416766467:
2: 28832: loss: 0.6409394436:
2: 32032: loss: 0.6401535151:
2: 35232: loss: 0.6392794331:
2: 38432: loss: 0.6385037291:
2: 41632: loss: 0.6377742527:
2: 44832: loss: 0.6367509583:
2: 48032: loss: 0.6360067987:
2: 51232: loss: 0.6351947609:
2: 54432: loss: 0.6343592847:
2: 57632: loss: 0.6334956946:
2: 60832: loss: 0.6327315422:
2: 64032: loss: 0.6319676432:
2: 67232: loss: 0.6310741588:
2: 70432: loss: 0.6304013920:
2: 73632: loss: 0.6296968725:
2: 76832: loss: 0.6290221371:
2: 80032: loss: 0.6283488612:
2: 83232: loss: 0.6276466427:
2: 86432: loss: 0.6270169489:
2: 89632: loss: 0.6262463235:
2: 92832: loss: 0.6254515652:
2: 96032: loss: 0.6246711704:
2: 99232: loss: 0.6239216067:
2: 102432: loss: 0.6232307900:
2: 105632: loss: 0.6225367656:
2: 108832: loss: 0.6217929227:
2: 112032: loss: 0.6211295619:
2: 115232: loss: 0.6202686306:
2: 118432: loss: 0.6196406177:
2: 121632: loss: 0.6189431138:
Dev-Acc: 2: Accuracy: 0.9416077733: precision: 0.1666666667: recall: 0.0001700391: f1: 0.0003397316
Train-Acc: 2: Accuracy: 0.8750329018: precision: 0.5714285714: recall: 0.0010518704: f1: 0.0020998753
3: 3232: loss: 0.5919223356:
3: 6432: loss: 0.5906322062:
3: 9632: loss: 0.5901912642:
3: 12832: loss: 0.5887943032:
3: 16032: loss: 0.5889716088:
3: 19232: loss: 0.5882246020:
3: 22432: loss: 0.5872546389:
3: 25632: loss: 0.5866006630:
3: 28832: loss: 0.5860771561:
3: 32032: loss: 0.5853229150:
3: 35232: loss: 0.5839913232:
3: 38432: loss: 0.5832345574:
3: 41632: loss: 0.5824362373:
3: 44832: loss: 0.5816861887:
3: 48032: loss: 0.5811702118:
3: 51232: loss: 0.5805551646:
3: 54432: loss: 0.5798514334:
3: 57632: loss: 0.5794229819:
3: 60832: loss: 0.5784994317:
3: 64032: loss: 0.5780408562:
3: 67232: loss: 0.5773249200:
3: 70432: loss: 0.5765240194:
3: 73632: loss: 0.5758193569:
3: 76832: loss: 0.5750378131:
3: 80032: loss: 0.5744309590:
3: 83232: loss: 0.5737039085:
3: 86432: loss: 0.5729426439:
3: 89632: loss: 0.5723238589:
3: 92832: loss: 0.5715682973:
3: 96032: loss: 0.5708107831:
3: 99232: loss: 0.5700114476:
3: 102432: loss: 0.5694245499:
3: 105632: loss: 0.5686540151:
3: 108832: loss: 0.5679788557:
3: 112032: loss: 0.5673472116:
3: 115232: loss: 0.5664553795:
3: 118432: loss: 0.5657696701:
3: 121632: loss: 0.5650742798:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.5389342606:
4: 6432: loss: 0.5384308842:
4: 9632: loss: 0.5384040185:
4: 12832: loss: 0.5377957755:
4: 16032: loss: 0.5373816696:
4: 19232: loss: 0.5361573017:
4: 22432: loss: 0.5355085296:
4: 25632: loss: 0.5349395465:
4: 28832: loss: 0.5346715769:
4: 32032: loss: 0.5340741404:
4: 35232: loss: 0.5335604199:
4: 38432: loss: 0.5330935421:
4: 41632: loss: 0.5320729828:
4: 44832: loss: 0.5314128020:
4: 48032: loss: 0.5307642541:
4: 51232: loss: 0.5303977743:
4: 54432: loss: 0.5296596745:
4: 57632: loss: 0.5288930633:
4: 60832: loss: 0.5281112128:
4: 64032: loss: 0.5275783276:
4: 67232: loss: 0.5269970969:
4: 70432: loss: 0.5264470255:
4: 73632: loss: 0.5255718872:
4: 76832: loss: 0.5251473885:
4: 80032: loss: 0.5244064310:
4: 83232: loss: 0.5239169492:
4: 86432: loss: 0.5233229065:
4: 89632: loss: 0.5228845621:
4: 92832: loss: 0.5222468071:
4: 96032: loss: 0.5216962431:
4: 99232: loss: 0.5212998397:
4: 102432: loss: 0.5206287394:
4: 105632: loss: 0.5198095949:
4: 108832: loss: 0.5193057729:
4: 112032: loss: 0.5186301988:
4: 115232: loss: 0.5178132380:
4: 118432: loss: 0.5173640272:
4: 121632: loss: 0.5168370241:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4901585808:
5: 6432: loss: 0.4907431524:
5: 9632: loss: 0.4900482334:
5: 12832: loss: 0.4903454501:
5: 16032: loss: 0.4903206078:
5: 19232: loss: 0.4906322821:
5: 22432: loss: 0.4898857161:
5: 25632: loss: 0.4892844004:
5: 28832: loss: 0.4888646905:
5: 32032: loss: 0.4880307095:
5: 35232: loss: 0.4877307829:
5: 38432: loss: 0.4877012652:
5: 41632: loss: 0.4865715052:
5: 44832: loss: 0.4858636678:
5: 48032: loss: 0.4850890471:
5: 51232: loss: 0.4848001316:
5: 54432: loss: 0.4842860305:
5: 57632: loss: 0.4838777146:
5: 60832: loss: 0.4839856291:
5: 64032: loss: 0.4835824872:
5: 67232: loss: 0.4830235298:
5: 70432: loss: 0.4826314449:
5: 73632: loss: 0.4820073505:
5: 76832: loss: 0.4817277675:
5: 80032: loss: 0.4810552130:
5: 83232: loss: 0.4803785485:
5: 86432: loss: 0.4797886616:
5: 89632: loss: 0.4791942942:
5: 92832: loss: 0.4789209899:
5: 96032: loss: 0.4781867953:
5: 99232: loss: 0.4777045058:
5: 102432: loss: 0.4772275785:
5: 105632: loss: 0.4767202053:
5: 108832: loss: 0.4762760631:
5: 112032: loss: 0.4759449431:
5: 115232: loss: 0.4753539274:
5: 118432: loss: 0.4748152762:
5: 121632: loss: 0.4741719353:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.4540618882:
6: 6432: loss: 0.4563801733:
6: 9632: loss: 0.4554558447:
6: 12832: loss: 0.4539342485:
6: 16032: loss: 0.4522488521:
6: 19232: loss: 0.4516222475:
6: 22432: loss: 0.4514449868:
6: 25632: loss: 0.4520095589:
6: 28832: loss: 0.4520726395:
6: 32032: loss: 0.4515964191:
6: 35232: loss: 0.4511907936:
6: 38432: loss: 0.4505480937:
6: 41632: loss: 0.4498722616:
6: 44832: loss: 0.4490376772:
6: 48032: loss: 0.4487299753:
6: 51232: loss: 0.4486807251:
6: 54432: loss: 0.4476938735:
6: 57632: loss: 0.4466140418:
6: 60832: loss: 0.4464074882:
6: 64032: loss: 0.4458594410:
6: 67232: loss: 0.4453578334:
6: 70432: loss: 0.4446981508:
6: 73632: loss: 0.4443510485:
6: 76832: loss: 0.4435668927:
6: 80032: loss: 0.4435999255:
6: 83232: loss: 0.4430372026:
6: 86432: loss: 0.4424501190:
6: 89632: loss: 0.4422559133:
6: 92832: loss: 0.4418882460:
6: 96032: loss: 0.4414921773:
6: 99232: loss: 0.4410814501:
6: 102432: loss: 0.4406961851:
6: 105632: loss: 0.4400605225:
6: 108832: loss: 0.4394731357:
6: 112032: loss: 0.4390506340:
6: 115232: loss: 0.4388170813:
6: 118432: loss: 0.4383033514:
6: 121632: loss: 0.4376617469:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.4295052409:
7: 6432: loss: 0.4200822094:
7: 9632: loss: 0.4202678182:
7: 12832: loss: 0.4202286468:
7: 16032: loss: 0.4217937572:
7: 19232: loss: 0.4206795978:
7: 22432: loss: 0.4203169084:
7: 25632: loss: 0.4189223327:
7: 28832: loss: 0.4188018019:
7: 32032: loss: 0.4189841911:
7: 35232: loss: 0.4178144082:
7: 38432: loss: 0.4173466175:
7: 41632: loss: 0.4176089932:
7: 44832: loss: 0.4171442757:
7: 48032: loss: 0.4174041473:
7: 51232: loss: 0.4170166005:
7: 54432: loss: 0.4162387788:
7: 57632: loss: 0.4159974662:
7: 60832: loss: 0.4152069580:
7: 64032: loss: 0.4149087791:
7: 67232: loss: 0.4146359218:
7: 70432: loss: 0.4143426044:
7: 73632: loss: 0.4136479749:
7: 76832: loss: 0.4134481724:
7: 80032: loss: 0.4130351822:
7: 83232: loss: 0.4131351505:
7: 86432: loss: 0.4123773947:
7: 89632: loss: 0.4120427093:
7: 92832: loss: 0.4117960226:
7: 96032: loss: 0.4115267464:
7: 99232: loss: 0.4110730858:
7: 102432: loss: 0.4107563372:
7: 105632: loss: 0.4103291272:
7: 108832: loss: 0.4097220231:
7: 112032: loss: 0.4088731094:
7: 115232: loss: 0.4085762002:
7: 118432: loss: 0.4080057704:
7: 121632: loss: 0.4077245351:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3946105313:
8: 6432: loss: 0.3952478020:
8: 9632: loss: 0.3960704246:
8: 12832: loss: 0.3957460684:
8: 16032: loss: 0.3963227669:
8: 19232: loss: 0.3968059579:
8: 22432: loss: 0.3953425110:
8: 25632: loss: 0.3948833522:
8: 28832: loss: 0.3954793825:
8: 32032: loss: 0.3943654175:
8: 35232: loss: 0.3935861989:
8: 38432: loss: 0.3934756477:
8: 41632: loss: 0.3921600468:
8: 44832: loss: 0.3921378158:
8: 48032: loss: 0.3922721891:
8: 51232: loss: 0.3916990770:
8: 54432: loss: 0.3916865798:
8: 57632: loss: 0.3902340029:
8: 60832: loss: 0.3901594407:
8: 64032: loss: 0.3896570365:
8: 67232: loss: 0.3885587264:
8: 70432: loss: 0.3886370307:
8: 73632: loss: 0.3881697573:
8: 76832: loss: 0.3874626481:
8: 80032: loss: 0.3874040832:
8: 83232: loss: 0.3871807319:
8: 86432: loss: 0.3864368941:
8: 89632: loss: 0.3858478993:
8: 92832: loss: 0.3859669892:
8: 96032: loss: 0.3856230652:
8: 99232: loss: 0.3853908583:
8: 102432: loss: 0.3853907275:
8: 105632: loss: 0.3849718093:
8: 108832: loss: 0.3844919544:
8: 112032: loss: 0.3843222541:
8: 115232: loss: 0.3841815038:
8: 118432: loss: 0.3840843385:
8: 121632: loss: 0.3837688714:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3753574267:
9: 6432: loss: 0.3689605255:
9: 9632: loss: 0.3691612800:
9: 12832: loss: 0.3711862521:
9: 16032: loss: 0.3698581912:
9: 19232: loss: 0.3686972847:
9: 22432: loss: 0.3690316430:
9: 25632: loss: 0.3695667160:
9: 28832: loss: 0.3705890111:
9: 32032: loss: 0.3696971924:
9: 35232: loss: 0.3686321690:
9: 38432: loss: 0.3694819427:
9: 41632: loss: 0.3700264665:
9: 44832: loss: 0.3697346271:
9: 48032: loss: 0.3688824517:
9: 51232: loss: 0.3690075025:
9: 54432: loss: 0.3687027453:
9: 57632: loss: 0.3689531727:
9: 60832: loss: 0.3686363139:
9: 64032: loss: 0.3694945756:
9: 67232: loss: 0.3691999844:
9: 70432: loss: 0.3689556549:
9: 73632: loss: 0.3689022854:
9: 76832: loss: 0.3687330513:
9: 80032: loss: 0.3682794227:
9: 83232: loss: 0.3677347153:
9: 86432: loss: 0.3678748302:
9: 89632: loss: 0.3670986781:
9: 92832: loss: 0.3671259404:
9: 96032: loss: 0.3665968422:
9: 99232: loss: 0.3660393458:
9: 102432: loss: 0.3656036743:
9: 105632: loss: 0.3653347609:
9: 108832: loss: 0.3651138890:
9: 112032: loss: 0.3649447069:
9: 115232: loss: 0.3650251713:
9: 118432: loss: 0.3645537251:
9: 121632: loss: 0.3643655774:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.3634402582:
10: 6432: loss: 0.3576652572:
10: 9632: loss: 0.3575812585:
10: 12832: loss: 0.3605551847:
10: 16032: loss: 0.3563275729:
10: 19232: loss: 0.3560284990:
10: 22432: loss: 0.3557252696:
10: 25632: loss: 0.3556058432:
10: 28832: loss: 0.3547158997:
10: 32032: loss: 0.3528661701:
10: 35232: loss: 0.3521163358:
10: 38432: loss: 0.3513694537:
10: 41632: loss: 0.3513933615:
10: 44832: loss: 0.3513719023:
10: 48032: loss: 0.3514851863:
10: 51232: loss: 0.3515920443:
10: 54432: loss: 0.3519105689:
10: 57632: loss: 0.3519651443:
10: 60832: loss: 0.3520554796:
10: 64032: loss: 0.3525137864:
10: 67232: loss: 0.3524580841:
10: 70432: loss: 0.3521712800:
10: 73632: loss: 0.3516514465:
10: 76832: loss: 0.3514935037:
10: 80032: loss: 0.3514934118:
10: 83232: loss: 0.3512641632:
10: 86432: loss: 0.3511239373:
10: 89632: loss: 0.3510931063:
10: 92832: loss: 0.3505683298:
10: 96032: loss: 0.3502105790:
10: 99232: loss: 0.3501069461:
10: 102432: loss: 0.3499913737:
10: 105632: loss: 0.3497900398:
10: 108832: loss: 0.3496747125:
10: 112032: loss: 0.3491450909:
10: 115232: loss: 0.3486225402:
10: 118432: loss: 0.3485685659:
10: 121632: loss: 0.3484064249:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.3487916562:
11: 6432: loss: 0.3401751373:
11: 9632: loss: 0.3410120304:
11: 12832: loss: 0.3402495674:
11: 16032: loss: 0.3414417868:
11: 19232: loss: 0.3409961755:
11: 22432: loss: 0.3398849793:
11: 25632: loss: 0.3393496269:
11: 28832: loss: 0.3396507546:
11: 32032: loss: 0.3380833188:
11: 35232: loss: 0.3380814228:
11: 38432: loss: 0.3386618058:
11: 41632: loss: 0.3385086316:
11: 44832: loss: 0.3383681150:
11: 48032: loss: 0.3382814345:
11: 51232: loss: 0.3392426165:
11: 54432: loss: 0.3391126963:
11: 57632: loss: 0.3395099328:
11: 60832: loss: 0.3388918294:
11: 64032: loss: 0.3387551390:
11: 67232: loss: 0.3390689982:
11: 70432: loss: 0.3389013586:
11: 73632: loss: 0.3384159512:
11: 76832: loss: 0.3379087834:
11: 80032: loss: 0.3373517190:
11: 83232: loss: 0.3372867140:
11: 86432: loss: 0.3370766437:
11: 89632: loss: 0.3364440858:
11: 92832: loss: 0.3363754574:
11: 96032: loss: 0.3364830153:
11: 99232: loss: 0.3365890810:
11: 102432: loss: 0.3359561158:
11: 105632: loss: 0.3357129452:
11: 108832: loss: 0.3356486376:
11: 112032: loss: 0.3354606115:
11: 115232: loss: 0.3350809338:
11: 118432: loss: 0.3353090203:
11: 121632: loss: 0.3352725995:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.3262486459:
12: 6432: loss: 0.3279587971:
12: 9632: loss: 0.3274004396:
12: 12832: loss: 0.3253888895:
12: 16032: loss: 0.3269026004:
12: 19232: loss: 0.3278357243:
12: 22432: loss: 0.3278577044:
12: 25632: loss: 0.3294121189:
12: 28832: loss: 0.3282982212:
12: 32032: loss: 0.3274778098:
12: 35232: loss: 0.3266425466:
12: 38432: loss: 0.3271447635:
12: 41632: loss: 0.3278491778:
12: 44832: loss: 0.3269988105:
12: 48032: loss: 0.3269185325:
12: 51232: loss: 0.3268483025:
12: 54432: loss: 0.3259216067:
12: 57632: loss: 0.3259454147:
12: 60832: loss: 0.3251636228:
12: 64032: loss: 0.3252680946:
12: 67232: loss: 0.3256877579:
12: 70432: loss: 0.3259179057:
12: 73632: loss: 0.3255282235:
12: 76832: loss: 0.3253384199:
12: 80032: loss: 0.3249407988:
12: 83232: loss: 0.3250254202:
12: 86432: loss: 0.3245690109:
12: 89632: loss: 0.3250268546:
12: 92832: loss: 0.3250775409:
12: 96032: loss: 0.3255335197:
12: 99232: loss: 0.3250001026:
12: 102432: loss: 0.3247236040:
12: 105632: loss: 0.3245888762:
12: 108832: loss: 0.3241790972:
12: 112032: loss: 0.3244421142:
12: 115232: loss: 0.3243550287:
12: 118432: loss: 0.3238369678:
12: 121632: loss: 0.3237368958:
Dev-Acc: 12: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 12: Accuracy: 0.8754684329: precision: 1.0000000000: recall: 0.0037472881: f1: 0.0074665968
13: 3232: loss: 0.3219692166:
13: 6432: loss: 0.3205835526:
13: 9632: loss: 0.3145606276:
13: 12832: loss: 0.3172058681:
13: 16032: loss: 0.3151795119:
13: 19232: loss: 0.3161612000:
13: 22432: loss: 0.3148513360:
13: 25632: loss: 0.3177522298:
13: 28832: loss: 0.3193050131:
13: 32032: loss: 0.3181037726:
13: 35232: loss: 0.3171031202:
13: 38432: loss: 0.3162963822:
13: 41632: loss: 0.3172068448:
13: 44832: loss: 0.3163064385:
13: 48032: loss: 0.3154989364:
13: 51232: loss: 0.3152517453:
13: 54432: loss: 0.3151500260:
13: 57632: loss: 0.3150203764:
13: 60832: loss: 0.3150792142:
13: 64032: loss: 0.3147154269:
13: 67232: loss: 0.3146367532:
13: 70432: loss: 0.3138993568:
13: 73632: loss: 0.3139291398:
13: 76832: loss: 0.3137632316:
13: 80032: loss: 0.3142725210:
13: 83232: loss: 0.3141527897:
13: 86432: loss: 0.3143897474:
13: 89632: loss: 0.3144184607:
13: 92832: loss: 0.3141757606:
13: 96032: loss: 0.3138534362:
13: 99232: loss: 0.3137133676:
13: 102432: loss: 0.3135404730:
13: 105632: loss: 0.3138458037:
13: 108832: loss: 0.3136426052:
13: 112032: loss: 0.3134763909:
13: 115232: loss: 0.3133444873:
13: 118432: loss: 0.3131879440:
13: 121632: loss: 0.3131408005:
Dev-Acc: 13: Accuracy: 0.9418657422: precision: 0.7500000000: recall: 0.0056112906: f1: 0.0111392405
Train-Acc: 13: Accuracy: 0.8773749471: precision: 0.8603491272: recall: 0.0226809546: f1: 0.0441967717
14: 3232: loss: 0.3017743935:
14: 6432: loss: 0.3013602079:
14: 9632: loss: 0.3032749109:
14: 12832: loss: 0.3038307241:
14: 16032: loss: 0.3044581651:
14: 19232: loss: 0.3059727436:
14: 22432: loss: 0.3066473323:
14: 25632: loss: 0.3083250824:
14: 28832: loss: 0.3076684568:
14: 32032: loss: 0.3073453749:
14: 35232: loss: 0.3071959758:
14: 38432: loss: 0.3078825748:
14: 41632: loss: 0.3067867791:
14: 44832: loss: 0.3075418675:
14: 48032: loss: 0.3079797624:
14: 51232: loss: 0.3072159266:
14: 54432: loss: 0.3072603020:
14: 57632: loss: 0.3066614319:
14: 60832: loss: 0.3066889640:
14: 64032: loss: 0.3067911674:
14: 67232: loss: 0.3061936795:
14: 70432: loss: 0.3065470489:
14: 73632: loss: 0.3072166002:
14: 76832: loss: 0.3069559918:
14: 80032: loss: 0.3065209363:
14: 83232: loss: 0.3062991095:
14: 86432: loss: 0.3058804699:
14: 89632: loss: 0.3056132081:
14: 92832: loss: 0.3054287908:
14: 96032: loss: 0.3052779596:
14: 99232: loss: 0.3052811969:
14: 102432: loss: 0.3051120141:
14: 105632: loss: 0.3052655503:
14: 108832: loss: 0.3046969974:
14: 112032: loss: 0.3050583296:
14: 115232: loss: 0.3046454467:
14: 118432: loss: 0.3044647001:
14: 121632: loss: 0.3040924760:
Dev-Acc: 14: Accuracy: 0.9424412251: precision: 0.6923076923: recall: 0.0244856317: f1: 0.0472984070
Train-Acc: 14: Accuracy: 0.8790020943: precision: 0.8349381018: recall: 0.0399053317: f1: 0.0761701594
15: 3232: loss: 0.3143420143:
15: 6432: loss: 0.3077201499:
15: 9632: loss: 0.3050002073:
15: 12832: loss: 0.3029038522:
15: 16032: loss: 0.3005155838:
15: 19232: loss: 0.3012401832:
15: 22432: loss: 0.3025157321:
15: 25632: loss: 0.3034646525:
15: 28832: loss: 0.3030240344:
15: 32032: loss: 0.3018017689:
15: 35232: loss: 0.3008441020:
15: 38432: loss: 0.3019787836:
15: 41632: loss: 0.3008125493:
15: 44832: loss: 0.3013128563:
15: 48032: loss: 0.3003302566:
15: 51232: loss: 0.3004291701:
15: 54432: loss: 0.3003219154:
15: 57632: loss: 0.3001036392:
15: 60832: loss: 0.2998425950:
15: 64032: loss: 0.2991195592:
15: 67232: loss: 0.2991522599:
15: 70432: loss: 0.2991538858:
15: 73632: loss: 0.2989229990:
15: 76832: loss: 0.2987190533:
15: 80032: loss: 0.2988504112:
15: 83232: loss: 0.2975647870:
15: 86432: loss: 0.2971953458:
15: 89632: loss: 0.2974584226:
15: 92832: loss: 0.2975996515:
15: 96032: loss: 0.2971795944:
15: 99232: loss: 0.2965468936:
15: 102432: loss: 0.2966833237:
15: 105632: loss: 0.2965268471:
15: 108832: loss: 0.2964682400:
15: 112032: loss: 0.2961743327:
15: 115232: loss: 0.2959476614:
15: 118432: loss: 0.2958538219:
15: 121632: loss: 0.2958763030:
Dev-Acc: 15: Accuracy: 0.9424213767: precision: 0.5773809524: recall: 0.0494813807: f1: 0.0911511355
Train-Acc: 15: Accuracy: 0.8825028539: precision: 0.8654923939: recall: 0.0710669910: f1: 0.1313487242
16: 3232: loss: 0.3100556035:
16: 6432: loss: 0.3023816193:
16: 9632: loss: 0.3000761569:
16: 12832: loss: 0.2982097609:
16: 16032: loss: 0.2988988147:
16: 19232: loss: 0.2974893708:
16: 22432: loss: 0.2929670154:
16: 25632: loss: 0.2920770011:
16: 28832: loss: 0.2919082931:
16: 32032: loss: 0.2923782274:
16: 35232: loss: 0.2929174492:
16: 38432: loss: 0.2915700839:
16: 41632: loss: 0.2929905724:
16: 44832: loss: 0.2927280316:
16: 48032: loss: 0.2924293664:
16: 51232: loss: 0.2921186445:
16: 54432: loss: 0.2921790560:
16: 57632: loss: 0.2919189014:
16: 60832: loss: 0.2918311006:
16: 64032: loss: 0.2924955423:
16: 67232: loss: 0.2922187523:
16: 70432: loss: 0.2916525231:
16: 73632: loss: 0.2914890750:
16: 76832: loss: 0.2919998909:
16: 80032: loss: 0.2914131448:
16: 83232: loss: 0.2913372837:
16: 86432: loss: 0.2909185172:
16: 89632: loss: 0.2906545364:
16: 92832: loss: 0.2904208587:
16: 96032: loss: 0.2902306864:
16: 99232: loss: 0.2901238302:
16: 102432: loss: 0.2903609728:
16: 105632: loss: 0.2901064465:
16: 108832: loss: 0.2897253788:
16: 112032: loss: 0.2895166864:
16: 115232: loss: 0.2892394714:
16: 118432: loss: 0.2892109929:
16: 121632: loss: 0.2888399849:
Dev-Acc: 16: Accuracy: 0.9422923923: precision: 0.5382803298: recall: 0.0777078728: f1: 0.1358098068
Train-Acc: 16: Accuracy: 0.8858885169: precision: 0.8707330722: recall: 0.1022943922: f1: 0.1830803624
17: 3232: loss: 0.2898175167:
17: 6432: loss: 0.2836246085:
17: 9632: loss: 0.2825704314:
17: 12832: loss: 0.2862634577:
17: 16032: loss: 0.2843173211:
17: 19232: loss: 0.2867774302:
17: 22432: loss: 0.2878108590:
17: 25632: loss: 0.2876498142:
17: 28832: loss: 0.2863001874:
17: 32032: loss: 0.2860029899:
17: 35232: loss: 0.2872274747:
17: 38432: loss: 0.2863915598:
17: 41632: loss: 0.2851110117:
17: 44832: loss: 0.2853473041:
17: 48032: loss: 0.2851520417:
17: 51232: loss: 0.2848463571:
17: 54432: loss: 0.2855200428:
17: 57632: loss: 0.2860326486:
17: 60832: loss: 0.2858741044:
17: 64032: loss: 0.2852260173:
17: 67232: loss: 0.2849519512:
17: 70432: loss: 0.2845645819:
17: 73632: loss: 0.2840085692:
17: 76832: loss: 0.2839176330:
17: 80032: loss: 0.2840256812:
17: 83232: loss: 0.2847435432:
17: 86432: loss: 0.2839021452:
17: 89632: loss: 0.2839735118:
17: 92832: loss: 0.2840098658:
17: 96032: loss: 0.2839892475:
17: 99232: loss: 0.2834752253:
17: 102432: loss: 0.2829998496:
17: 105632: loss: 0.2825854685:
17: 108832: loss: 0.2823497388:
17: 112032: loss: 0.2825765716:
17: 115232: loss: 0.2825046661:
17: 118432: loss: 0.2822228486:
17: 121632: loss: 0.2820266061:
Dev-Acc: 17: Accuracy: 0.9431754947: precision: 0.5614035088: recall: 0.1197075327: f1: 0.1973370708
Train-Acc: 17: Accuracy: 0.8896687031: precision: 0.8916191312: recall: 0.1335875353: f1: 0.2323613493
18: 3232: loss: 0.2922057629:
18: 6432: loss: 0.2822856268:
18: 9632: loss: 0.2757157454:
18: 12832: loss: 0.2806822780:
18: 16032: loss: 0.2780419847:
18: 19232: loss: 0.2785235374:
18: 22432: loss: 0.2784372709:
18: 25632: loss: 0.2791155502:
18: 28832: loss: 0.2791948165:
18: 32032: loss: 0.2801341335:
18: 35232: loss: 0.2805835854:
18: 38432: loss: 0.2806749867:
18: 41632: loss: 0.2807619106:
18: 44832: loss: 0.2805653478:
18: 48032: loss: 0.2804008658:
18: 51232: loss: 0.2795118681:
18: 54432: loss: 0.2787754847:
18: 57632: loss: 0.2792753007:
18: 60832: loss: 0.2782740184:
18: 64032: loss: 0.2776654406:
18: 67232: loss: 0.2779865503:
18: 70432: loss: 0.2778943253:
18: 73632: loss: 0.2779786761:
18: 76832: loss: 0.2781385813:
18: 80032: loss: 0.2779170716:
18: 83232: loss: 0.2774148583:
18: 86432: loss: 0.2771695620:
18: 89632: loss: 0.2771051689:
18: 92832: loss: 0.2763684182:
18: 96032: loss: 0.2758970025:
18: 99232: loss: 0.2759021765:
18: 102432: loss: 0.2756544716:
18: 105632: loss: 0.2756556898:
18: 108832: loss: 0.2755517696:
18: 112032: loss: 0.2758830469:
18: 115232: loss: 0.2762643269:
18: 118432: loss: 0.2763584089:
18: 121632: loss: 0.2760815128:
Dev-Acc: 18: Accuracy: 0.9433441758: precision: 0.5500878735: recall: 0.1596667233: f1: 0.2474960464
Train-Acc: 18: Accuracy: 0.8943445683: precision: 0.8899933731: recall: 0.1765827362: f1: 0.2946952658
19: 3232: loss: 0.2669502220:
19: 6432: loss: 0.2620651463:
19: 9632: loss: 0.2666626135:
19: 12832: loss: 0.2680337663:
19: 16032: loss: 0.2699017813:
19: 19232: loss: 0.2686955604:
19: 22432: loss: 0.2693426536:
19: 25632: loss: 0.2698576127:
19: 28832: loss: 0.2697904321:
19: 32032: loss: 0.2716910054:
19: 35232: loss: 0.2708724458:
19: 38432: loss: 0.2699988651:
19: 41632: loss: 0.2696403120:
19: 44832: loss: 0.2694841608:
19: 48032: loss: 0.2696933595:
19: 51232: loss: 0.2693047960:
19: 54432: loss: 0.2688322626:
19: 57632: loss: 0.2699705287:
19: 60832: loss: 0.2698244239:
19: 64032: loss: 0.2703324130:
19: 67232: loss: 0.2709625057:
19: 70432: loss: 0.2705142099:
19: 73632: loss: 0.2709278259:
19: 76832: loss: 0.2709284219:
19: 80032: loss: 0.2706119139:
19: 83232: loss: 0.2709479103:
19: 86432: loss: 0.2708542958:
19: 89632: loss: 0.2712674346:
19: 92832: loss: 0.2709576910:
19: 96032: loss: 0.2710260698:
19: 99232: loss: 0.2708974436:
19: 102432: loss: 0.2714868695:
19: 105632: loss: 0.2708761613:
19: 108832: loss: 0.2705095809:
19: 112032: loss: 0.2705374839:
19: 115232: loss: 0.2709324789:
19: 118432: loss: 0.2703640256:
19: 121632: loss: 0.2703787765:
Dev-Acc: 19: Accuracy: 0.9393355846: precision: 0.4633301857: recall: 0.2502975684: f1: 0.3250165599
Train-Acc: 19: Accuracy: 0.8990533352: precision: 0.8542725732: recall: 0.2320031556: f1: 0.3649053872
20: 3232: loss: 0.2769183212:
20: 6432: loss: 0.2764363506:
20: 9632: loss: 0.2771954071:
20: 12832: loss: 0.2750645665:
20: 16032: loss: 0.2717640114:
20: 19232: loss: 0.2723192565:
20: 22432: loss: 0.2715636180:
20: 25632: loss: 0.2711986889:
20: 28832: loss: 0.2725399067:
20: 32032: loss: 0.2715035017:
20: 35232: loss: 0.2721782904:
20: 38432: loss: 0.2703231788:
20: 41632: loss: 0.2694755626:
20: 44832: loss: 0.2686856085:
20: 48032: loss: 0.2688607339:
20: 51232: loss: 0.2683406639:
20: 54432: loss: 0.2688635645:
20: 57632: loss: 0.2694771805:
20: 60832: loss: 0.2692193859:
20: 64032: loss: 0.2689364020:
20: 67232: loss: 0.2693657456:
20: 70432: loss: 0.2692589091:
20: 73632: loss: 0.2695728648:
20: 76832: loss: 0.2696232739:
20: 80032: loss: 0.2689317568:
20: 83232: loss: 0.2682342472:
20: 86432: loss: 0.2682937552:
20: 89632: loss: 0.2684115147:
20: 92832: loss: 0.2680061019:
20: 96032: loss: 0.2678959345:
20: 99232: loss: 0.2680155592:
20: 102432: loss: 0.2675869493:
20: 105632: loss: 0.2674087576:
20: 108832: loss: 0.2667221280:
20: 112032: loss: 0.2666424008:
20: 115232: loss: 0.2661515070:
20: 118432: loss: 0.2652275892:
20: 121632: loss: 0.2654098819:
Dev-Acc: 20: Accuracy: 0.9299095273: precision: 0.3896249300: recall: 0.3550416596: f1: 0.3715302491
Train-Acc: 20: Accuracy: 0.9012885690: precision: 0.7722553191: recall: 0.2982709881: f1: 0.4303329223
21: 3232: loss: 0.2673207951:
21: 6432: loss: 0.2655354000:
21: 9632: loss: 0.2635472157:
21: 12832: loss: 0.2616767601:
21: 16032: loss: 0.2604714704:
21: 19232: loss: 0.2615067307:
21: 22432: loss: 0.2609400145:
21: 25632: loss: 0.2611534523:
21: 28832: loss: 0.2601983568:
21: 32032: loss: 0.2597528880:
21: 35232: loss: 0.2597430751:
21: 38432: loss: 0.2593882405:
21: 41632: loss: 0.2599074954:
21: 44832: loss: 0.2598175822:
21: 48032: loss: 0.2614573155:
21: 51232: loss: 0.2607091038:
21: 54432: loss: 0.2612095599:
21: 57632: loss: 0.2619525809:
21: 60832: loss: 0.2624595105:
21: 64032: loss: 0.2626748499:
21: 67232: loss: 0.2620101444:
21: 70432: loss: 0.2615997472:
21: 73632: loss: 0.2614581223:
21: 76832: loss: 0.2608711558:
21: 80032: loss: 0.2607654771:
21: 83232: loss: 0.2609101473:
21: 86432: loss: 0.2608092533:
21: 89632: loss: 0.2609878412:
21: 92832: loss: 0.2609671704:
21: 96032: loss: 0.2606460068:
21: 99232: loss: 0.2605528144:
21: 102432: loss: 0.2607627859:
21: 105632: loss: 0.2609681340:
21: 108832: loss: 0.2607345957:
21: 112032: loss: 0.2608470506:
21: 115232: loss: 0.2611340997:
21: 118432: loss: 0.2610551335:
21: 121632: loss: 0.2609808412:
Dev-Acc: 21: Accuracy: 0.9242935181: precision: 0.3635512560: recall: 0.3961911240: f1: 0.3791700570
Train-Acc: 21: Accuracy: 0.9020939469: precision: 0.7322158050: recall: 0.3417263822: f1: 0.4659793814
22: 3232: loss: 0.2386575301:
22: 6432: loss: 0.2499797860:
22: 9632: loss: 0.2541010093:
22: 12832: loss: 0.2547997075:
22: 16032: loss: 0.2577809635:
22: 19232: loss: 0.2540988508:
22: 22432: loss: 0.2557116564:
22: 25632: loss: 0.2550562346:
22: 28832: loss: 0.2554497594:
22: 32032: loss: 0.2550230227:
22: 35232: loss: 0.2558061459:
22: 38432: loss: 0.2560614562:
22: 41632: loss: 0.2554705521:
22: 44832: loss: 0.2563830192:
22: 48032: loss: 0.2561188759:
22: 51232: loss: 0.2568790454:
22: 54432: loss: 0.2571591940:
22: 57632: loss: 0.2574932714:
22: 60832: loss: 0.2571959251:
22: 64032: loss: 0.2576571701:
22: 67232: loss: 0.2573513303:
22: 70432: loss: 0.2576137628:
22: 73632: loss: 0.2576636672:
22: 76832: loss: 0.2570466266:
22: 80032: loss: 0.2564507830:
22: 83232: loss: 0.2561129057:
22: 86432: loss: 0.2564474271:
22: 89632: loss: 0.2560103325:
22: 92832: loss: 0.2561628417:
22: 96032: loss: 0.2558514543:
22: 99232: loss: 0.2552751075:
22: 102432: loss: 0.2553823611:
22: 105632: loss: 0.2554643692:
22: 108832: loss: 0.2560089150:
22: 112032: loss: 0.2563391285:
22: 115232: loss: 0.2562783078:
22: 118432: loss: 0.2565011744:
22: 121632: loss: 0.2565143882:
Dev-Acc: 22: Accuracy: 0.9222198129: precision: 0.3571637000: recall: 0.4162557388: f1: 0.3844522968
Train-Acc: 22: Accuracy: 0.9027431011: precision: 0.7182570468: recall: 0.3651962396: f1: 0.4842013511
23: 3232: loss: 0.2591481401:
23: 6432: loss: 0.2585022668:
23: 9632: loss: 0.2548113735:
23: 12832: loss: 0.2561858504:
23: 16032: loss: 0.2558171057:
23: 19232: loss: 0.2542337913:
23: 22432: loss: 0.2538184781:
23: 25632: loss: 0.2548785721:
23: 28832: loss: 0.2562048410:
23: 32032: loss: 0.2566784367:
23: 35232: loss: 0.2561565017:
23: 38432: loss: 0.2566439419:
23: 41632: loss: 0.2572145190:
23: 44832: loss: 0.2571487587:
23: 48032: loss: 0.2574433019:
23: 51232: loss: 0.2568447113:
23: 54432: loss: 0.2564070530:
23: 57632: loss: 0.2568284601:
23: 60832: loss: 0.2561524404:
23: 64032: loss: 0.2562348663:
23: 67232: loss: 0.2556505809:
23: 70432: loss: 0.2556208999:
23: 73632: loss: 0.2554483453:
23: 76832: loss: 0.2552410340:
23: 80032: loss: 0.2546969434:
23: 83232: loss: 0.2540562287:
23: 86432: loss: 0.2546590422:
23: 89632: loss: 0.2538661542:
23: 92832: loss: 0.2535950535:
23: 96032: loss: 0.2532642491:
23: 99232: loss: 0.2530822839:
23: 102432: loss: 0.2525035816:
23: 105632: loss: 0.2525265207:
23: 108832: loss: 0.2528289041:
23: 112032: loss: 0.2526260041:
23: 115232: loss: 0.2529593471:
23: 118432: loss: 0.2529913022:
23: 121632: loss: 0.2527064156:
Dev-Acc: 23: Accuracy: 0.9214061499: precision: 0.3559728890: recall: 0.4286685938: f1: 0.3889531744
Train-Acc: 23: Accuracy: 0.9043209553: precision: 0.7181462460: recall: 0.3861021629: f1: 0.5022018898
24: 3232: loss: 0.2476159775:
24: 6432: loss: 0.2474497468:
24: 9632: loss: 0.2470432087:
24: 12832: loss: 0.2472032461:
24: 16032: loss: 0.2467705760:
24: 19232: loss: 0.2480269095:
24: 22432: loss: 0.2477849755:
24: 25632: loss: 0.2478524761:
24: 28832: loss: 0.2492007640:
24: 32032: loss: 0.2493683144:
24: 35232: loss: 0.2491886942:
24: 38432: loss: 0.2499583540:
24: 41632: loss: 0.2495445069:
24: 44832: loss: 0.2503717586:
24: 48032: loss: 0.2502212931:
24: 51232: loss: 0.2501084114:
24: 54432: loss: 0.2503779313:
24: 57632: loss: 0.2510727597:
24: 60832: loss: 0.2511720497:
24: 64032: loss: 0.2510815514:
24: 67232: loss: 0.2510765916:
24: 70432: loss: 0.2514231243:
24: 73632: loss: 0.2503637594:
24: 76832: loss: 0.2504237444:
24: 80032: loss: 0.2496738152:
24: 83232: loss: 0.2498015842:
24: 86432: loss: 0.2495702225:
24: 89632: loss: 0.2495298346:
24: 92832: loss: 0.2497863902:
24: 96032: loss: 0.2499952483:
24: 99232: loss: 0.2501171512:
24: 102432: loss: 0.2496095576:
24: 105632: loss: 0.2495618042:
24: 108832: loss: 0.2494047577:
24: 112032: loss: 0.2493130959:
24: 115232: loss: 0.2496320127:
24: 118432: loss: 0.2493507658:
24: 121632: loss: 0.2490521291:
Dev-Acc: 24: Accuracy: 0.9207909703: precision: 0.3553935058: recall: 0.4392110185: f1: 0.3928815880
Train-Acc: 24: Accuracy: 0.9057837129: precision: 0.7204566855: recall: 0.4024061534: f1: 0.5163875649
25: 3232: loss: 0.2490297548:
25: 6432: loss: 0.2398689170:
25: 9632: loss: 0.2376957803:
25: 12832: loss: 0.2428426087:
25: 16032: loss: 0.2430598451:
25: 19232: loss: 0.2445086198:
25: 22432: loss: 0.2445682751:
25: 25632: loss: 0.2464507296:
25: 28832: loss: 0.2463915641:
25: 32032: loss: 0.2447109278:
25: 35232: loss: 0.2442379514:
25: 38432: loss: 0.2440718725:
25: 41632: loss: 0.2451180340:
25: 44832: loss: 0.2451152438:
25: 48032: loss: 0.2459334285:
25: 51232: loss: 0.2456073455:
25: 54432: loss: 0.2450499167:
25: 57632: loss: 0.2454543254:
25: 60832: loss: 0.2456813359:
25: 64032: loss: 0.2456445924:
25: 67232: loss: 0.2453623247:
25: 70432: loss: 0.2452708839:
25: 73632: loss: 0.2459955665:
25: 76832: loss: 0.2464230475:
25: 80032: loss: 0.2467532857:
25: 83232: loss: 0.2466679609:
25: 86432: loss: 0.2459690996:
25: 89632: loss: 0.2461772908:
25: 92832: loss: 0.2461062440:
25: 96032: loss: 0.2461914616:
25: 99232: loss: 0.2459778533:
25: 102432: loss: 0.2456425976:
25: 105632: loss: 0.2463155650:
25: 108832: loss: 0.2462303869:
25: 112032: loss: 0.2461251997:
25: 115232: loss: 0.2460550270:
25: 118432: loss: 0.2460391829:
25: 121632: loss: 0.2457877244:
Dev-Acc: 25: Accuracy: 0.9200964570: precision: 0.3546184739: recall: 0.4504335997: f1: 0.3968242079
Train-Acc: 25: Accuracy: 0.9067040682: precision: 0.7218771567: recall: 0.4125961475: f1: 0.5250784355
26: 3232: loss: 0.2321633569:
26: 6432: loss: 0.2347793026:
26: 9632: loss: 0.2346998515:
26: 12832: loss: 0.2349555647:
26: 16032: loss: 0.2372922369:
26: 19232: loss: 0.2386111338:
26: 22432: loss: 0.2380085217:
26: 25632: loss: 0.2389328714:
26: 28832: loss: 0.2398465496:
26: 32032: loss: 0.2402797064:
26: 35232: loss: 0.2414975885:
26: 38432: loss: 0.2431013883:
26: 41632: loss: 0.2426766917:
26: 44832: loss: 0.2424253599:
26: 48032: loss: 0.2418932772:
26: 51232: loss: 0.2409710013:
26: 54432: loss: 0.2414039010:
26: 57632: loss: 0.2424631334:
26: 60832: loss: 0.2427807632:
26: 64032: loss: 0.2427936971:
26: 67232: loss: 0.2426617973:
26: 70432: loss: 0.2426782002:
26: 73632: loss: 0.2425483788:
26: 76832: loss: 0.2421579537:
26: 80032: loss: 0.2417739000:
26: 83232: loss: 0.2414837559:
26: 86432: loss: 0.2415103632:
26: 89632: loss: 0.2419711777:
26: 92832: loss: 0.2416459584:
26: 96032: loss: 0.2418419298:
26: 99232: loss: 0.2417111731:
26: 102432: loss: 0.2416530183:
26: 105632: loss: 0.2418844620:
26: 108832: loss: 0.2420968523:
26: 112032: loss: 0.2422086776:
26: 115232: loss: 0.2422507311:
26: 118432: loss: 0.2426154383:
26: 121632: loss: 0.2423454190:
Dev-Acc: 26: Accuracy: 0.9190248251: precision: 0.3519480519: recall: 0.4608059854: f1: 0.3990869597
Train-Acc: 26: Accuracy: 0.9078463316: precision: 0.7241224627: recall: 0.4244954309: f1: 0.5352287798
27: 3232: loss: 0.2493209521:
27: 6432: loss: 0.2460073109:
27: 9632: loss: 0.2461551845:
27: 12832: loss: 0.2448526244:
27: 16032: loss: 0.2438752657:
27: 19232: loss: 0.2427818620:
27: 22432: loss: 0.2449379029:
27: 25632: loss: 0.2460186963:
27: 28832: loss: 0.2465157490:
27: 32032: loss: 0.2460259577:
27: 35232: loss: 0.2457962427:
27: 38432: loss: 0.2447249675:
27: 41632: loss: 0.2448207411:
27: 44832: loss: 0.2440044758:
27: 48032: loss: 0.2444160025:
27: 51232: loss: 0.2450544770:
27: 54432: loss: 0.2442918328:
27: 57632: loss: 0.2440219448:
27: 60832: loss: 0.2438846209:
27: 64032: loss: 0.2440236198:
27: 67232: loss: 0.2438497495:
27: 70432: loss: 0.2434260846:
27: 73632: loss: 0.2429555864:
27: 76832: loss: 0.2432060145:
27: 80032: loss: 0.2426953165:
27: 83232: loss: 0.2429608324:
27: 86432: loss: 0.2433112064:
27: 89632: loss: 0.2426904356:
27: 92832: loss: 0.2418446699:
27: 96032: loss: 0.2418828637:
27: 99232: loss: 0.2413951828:
27: 102432: loss: 0.2408686875:
27: 105632: loss: 0.2409269926:
27: 108832: loss: 0.2407315871:
27: 112032: loss: 0.2402827153:
27: 115232: loss: 0.2402813881:
27: 118432: loss: 0.2399625915:
27: 121632: loss: 0.2397870005:
Dev-Acc: 27: Accuracy: 0.9181020856: precision: 0.3502839117: recall: 0.4720285666: f1: 0.4021439954
Train-Acc: 27: Accuracy: 0.9089721441: precision: 0.7262478109: recall: 0.4361974887: f1: 0.5450363494
28: 3232: loss: 0.2372025179:
28: 6432: loss: 0.2315986182:
28: 9632: loss: 0.2314534799:
28: 12832: loss: 0.2373570448:
28: 16032: loss: 0.2385238886:
28: 19232: loss: 0.2406402275:
28: 22432: loss: 0.2388335352:
28: 25632: loss: 0.2386217412:
28: 28832: loss: 0.2394485003:
28: 32032: loss: 0.2395340169:
28: 35232: loss: 0.2409092408:
28: 38432: loss: 0.2412337227:
28: 41632: loss: 0.2408573437:
28: 44832: loss: 0.2415062996:
28: 48032: loss: 0.2410268487:
28: 51232: loss: 0.2402950912:
28: 54432: loss: 0.2404927407:
28: 57632: loss: 0.2396391337:
28: 60832: loss: 0.2397884377:
28: 64032: loss: 0.2397237989:
28: 67232: loss: 0.2399508851:
28: 70432: loss: 0.2397192841:
28: 73632: loss: 0.2393719006:
28: 76832: loss: 0.2391961994:
28: 80032: loss: 0.2391061675:
28: 83232: loss: 0.2386647907:
28: 86432: loss: 0.2383722041:
28: 89632: loss: 0.2385347106:
28: 92832: loss: 0.2381009607:
28: 96032: loss: 0.2377525481:
28: 99232: loss: 0.2378532403:
28: 102432: loss: 0.2376457056:
28: 105632: loss: 0.2376230987:
28: 108832: loss: 0.2377897027:
28: 112032: loss: 0.2371950467:
28: 115232: loss: 0.2369500083:
28: 118432: loss: 0.2370517337:
28: 121632: loss: 0.2369960469:
Dev-Acc: 28: Accuracy: 0.9176357388: precision: 0.3504326329: recall: 0.4820608740: f1: 0.4058406700
Train-Acc: 28: Accuracy: 0.9099254012: precision: 0.7283473028: recall: 0.4455985800: f1: 0.5529224620
29: 3232: loss: 0.2478263109:
29: 6432: loss: 0.2424452449:
29: 9632: loss: 0.2356525747:
29: 12832: loss: 0.2370498869:
29: 16032: loss: 0.2362915046:
29: 19232: loss: 0.2373417560:
29: 22432: loss: 0.2406391350:
29: 25632: loss: 0.2376564696:
29: 28832: loss: 0.2359489187:
29: 32032: loss: 0.2360504716:
29: 35232: loss: 0.2358999454:
29: 38432: loss: 0.2354609149:
29: 41632: loss: 0.2355690092:
29: 44832: loss: 0.2356709182:
29: 48032: loss: 0.2351524166:
29: 51232: loss: 0.2359739431:
29: 54432: loss: 0.2353859383:
29: 57632: loss: 0.2353157204:
29: 60832: loss: 0.2355076053:
29: 64032: loss: 0.2358673415:
29: 67232: loss: 0.2357257585:
29: 70432: loss: 0.2361620669:
29: 73632: loss: 0.2356320062:
29: 76832: loss: 0.2355280623:
29: 80032: loss: 0.2352118050:
29: 83232: loss: 0.2348773710:
29: 86432: loss: 0.2343946912:
29: 89632: loss: 0.2340446556:
29: 92832: loss: 0.2340130225:
29: 96032: loss: 0.2342052448:
29: 99232: loss: 0.2340911696:
29: 102432: loss: 0.2339639685:
29: 105632: loss: 0.2341732078:
29: 108832: loss: 0.2340330076:
29: 112032: loss: 0.2342710630:
29: 115232: loss: 0.2345970698:
29: 118432: loss: 0.2346571768:
29: 121632: loss: 0.2346103560:
Dev-Acc: 29: Accuracy: 0.9168915749: precision: 0.3488062053: recall: 0.4893725557: f1: 0.4073025757
Train-Acc: 29: Accuracy: 0.9106157422: precision: 0.7288762146: recall: 0.4536848333: f1: 0.5592609101
30: 3232: loss: 0.2401936591:
30: 6432: loss: 0.2341340218:
30: 9632: loss: 0.2345814011:
30: 12832: loss: 0.2331032470:
30: 16032: loss: 0.2326314840:
30: 19232: loss: 0.2326451081:
30: 22432: loss: 0.2330458323:
30: 25632: loss: 0.2314592531:
30: 28832: loss: 0.2328106520:
30: 32032: loss: 0.2346103247:
30: 35232: loss: 0.2335279577:
30: 38432: loss: 0.2331891003:
30: 41632: loss: 0.2340574125:
30: 44832: loss: 0.2337961713:
30: 48032: loss: 0.2340717652:
30: 51232: loss: 0.2333591516:
30: 54432: loss: 0.2333474585:
30: 57632: loss: 0.2330257189:
30: 60832: loss: 0.2329804856:
30: 64032: loss: 0.2323815264:
30: 67232: loss: 0.2324933153:
30: 70432: loss: 0.2320843987:
30: 73632: loss: 0.2315395360:
30: 76832: loss: 0.2308995679:
30: 80032: loss: 0.2315547465:
30: 83232: loss: 0.2317091888:
30: 86432: loss: 0.2320221857:
30: 89632: loss: 0.2320958256:
30: 92832: loss: 0.2322103205:
30: 96032: loss: 0.2317965178:
30: 99232: loss: 0.2318338803:
30: 102432: loss: 0.2321955627:
30: 105632: loss: 0.2323358800:
30: 108832: loss: 0.2322051130:
30: 112032: loss: 0.2319469577:
30: 115232: loss: 0.2321915743:
30: 118432: loss: 0.2323218968:
30: 121632: loss: 0.2320310531:
Dev-Acc: 30: Accuracy: 0.9162862897: precision: 0.3472388238: recall: 0.4939636116: f1: 0.4078051520
Train-Acc: 30: Accuracy: 0.9113799930: precision: 0.7305008851: recall: 0.4611794096: f1: 0.5654066253
31: 3232: loss: 0.2396761190:
31: 6432: loss: 0.2315014477:
31: 9632: loss: 0.2307775137:
31: 12832: loss: 0.2304229663:
31: 16032: loss: 0.2322435699:
31: 19232: loss: 0.2335526971:
31: 22432: loss: 0.2347800410:
31: 25632: loss: 0.2352119975:
31: 28832: loss: 0.2343288041:
31: 32032: loss: 0.2334248222:
31: 35232: loss: 0.2334914701:
31: 38432: loss: 0.2325359806:
31: 41632: loss: 0.2324298132:
31: 44832: loss: 0.2324996222:
31: 48032: loss: 0.2324538420:
31: 51232: loss: 0.2326320979:
31: 54432: loss: 0.2327887369:
31: 57632: loss: 0.2325055620:
31: 60832: loss: 0.2321077011:
31: 64032: loss: 0.2323614709:
31: 67232: loss: 0.2319532993:
31: 70432: loss: 0.2308880189:
31: 73632: loss: 0.2308057229:
31: 76832: loss: 0.2311774370:
31: 80032: loss: 0.2308727725:
31: 83232: loss: 0.2307710678:
31: 86432: loss: 0.2305416242:
31: 89632: loss: 0.2308716778:
31: 92832: loss: 0.2309373190:
31: 96032: loss: 0.2308940038:
31: 99232: loss: 0.2306341398:
31: 102432: loss: 0.2305104239:
31: 105632: loss: 0.2303808256:
31: 108832: loss: 0.2305478150:
31: 112032: loss: 0.2309109646:
31: 115232: loss: 0.2305688242:
31: 118432: loss: 0.2301630901:
31: 121632: loss: 0.2301409950:
Dev-Acc: 31: Accuracy: 0.9158000946: precision: 0.3461311282: recall: 0.4982145894: f1: 0.4084762303
Train-Acc: 31: Accuracy: 0.9118319154: precision: 0.7311262376: recall: 0.4660443100: f1: 0.5692375637
32: 3232: loss: 0.2231184667:
32: 6432: loss: 0.2405525924:
32: 9632: loss: 0.2363819047:
32: 12832: loss: 0.2344052340:
32: 16032: loss: 0.2349409472:
32: 19232: loss: 0.2320120269:
32: 22432: loss: 0.2327894556:
32: 25632: loss: 0.2330550607:
32: 28832: loss: 0.2321818528:
32: 32032: loss: 0.2316619717:
32: 35232: loss: 0.2331018841:
32: 38432: loss: 0.2320602592:
32: 41632: loss: 0.2310334133:
32: 44832: loss: 0.2304021392:
32: 48032: loss: 0.2308000162:
32: 51232: loss: 0.2304195827:
32: 54432: loss: 0.2294909529:
32: 57632: loss: 0.2286036500:
32: 60832: loss: 0.2296616604:
32: 64032: loss: 0.2295692706:
32: 67232: loss: 0.2290206789:
32: 70432: loss: 0.2287640804:
32: 73632: loss: 0.2289761137:
32: 76832: loss: 0.2295219470:
32: 80032: loss: 0.2291394501:
32: 83232: loss: 0.2289614148:
32: 86432: loss: 0.2288318888:
32: 89632: loss: 0.2283542897:
32: 92832: loss: 0.2277323906:
32: 96032: loss: 0.2277494967:
32: 99232: loss: 0.2275400386:
32: 102432: loss: 0.2281795660:
32: 105632: loss: 0.2279794333:
32: 108832: loss: 0.2279622731:
32: 112032: loss: 0.2280344823:
32: 115232: loss: 0.2279025944:
32: 118432: loss: 0.2280585252:
32: 121632: loss: 0.2281172449:
Dev-Acc: 32: Accuracy: 0.9157009125: precision: 0.3467354355: recall: 0.5029756844: f1: 0.4104912573
Train-Acc: 32: Accuracy: 0.9122921228: precision: 0.7311532192: recall: 0.4718295970: f1: 0.5735405762
33: 3232: loss: 0.2311870315:
33: 6432: loss: 0.2294318656:
33: 9632: loss: 0.2316767604:
33: 12832: loss: 0.2284465832:
33: 16032: loss: 0.2258983618:
33: 19232: loss: 0.2261527158:
33: 22432: loss: 0.2242010598:
33: 25632: loss: 0.2227373598:
33: 28832: loss: 0.2247958032:
33: 32032: loss: 0.2249656670:
33: 35232: loss: 0.2256501945:
33: 38432: loss: 0.2242973756:
33: 41632: loss: 0.2237144580:
33: 44832: loss: 0.2249513392:
33: 48032: loss: 0.2253135320:
33: 51232: loss: 0.2248627950:
33: 54432: loss: 0.2252998267:
33: 57632: loss: 0.2252434024:
33: 60832: loss: 0.2248016170:
33: 64032: loss: 0.2259819153:
33: 67232: loss: 0.2254913256:
33: 70432: loss: 0.2257167599:
33: 73632: loss: 0.2253654422:
33: 76832: loss: 0.2253908867:
33: 80032: loss: 0.2256252761:
33: 83232: loss: 0.2253473303:
33: 86432: loss: 0.2253462641:
33: 89632: loss: 0.2254126443:
33: 92832: loss: 0.2253667730:
33: 96032: loss: 0.2256162751:
33: 99232: loss: 0.2256131044:
33: 102432: loss: 0.2258021975:
33: 105632: loss: 0.2258456985:
33: 108832: loss: 0.2261829716:
33: 112032: loss: 0.2264499340:
33: 115232: loss: 0.2264089608:
33: 118432: loss: 0.2261453834:
33: 121632: loss: 0.2259211200:
Dev-Acc: 33: Accuracy: 0.9154230952: precision: 0.3463907939: recall: 0.5067165448: f1: 0.4114885391
Train-Acc: 33: Accuracy: 0.9125140309: precision: 0.7307653422: recall: 0.4751824338: f1: 0.5758903673
34: 3232: loss: 0.2291441069:
34: 6432: loss: 0.2285094697:
34: 9632: loss: 0.2273981917:
34: 12832: loss: 0.2253287152:
34: 16032: loss: 0.2220410770:
34: 19232: loss: 0.2223090712:
34: 22432: loss: 0.2215080096:
34: 25632: loss: 0.2237607550:
34: 28832: loss: 0.2233146202:
34: 32032: loss: 0.2240671015:
34: 35232: loss: 0.2256299042:
34: 38432: loss: 0.2246993428:
34: 41632: loss: 0.2244829322:
34: 44832: loss: 0.2240887956:
34: 48032: loss: 0.2246413404:
34: 51232: loss: 0.2250578704:
34: 54432: loss: 0.2254113063:
34: 57632: loss: 0.2249591196:
34: 60832: loss: 0.2250543933:
34: 64032: loss: 0.2244493372:
34: 67232: loss: 0.2244446153:
34: 70432: loss: 0.2245765286:
34: 73632: loss: 0.2248244015:
34: 76832: loss: 0.2245074283:
34: 80032: loss: 0.2245099688:
34: 83232: loss: 0.2251654364:
34: 86432: loss: 0.2248377992:
34: 89632: loss: 0.2249107815:
34: 92832: loss: 0.2247851605:
34: 96032: loss: 0.2246444118:
34: 99232: loss: 0.2248372377:
34: 102432: loss: 0.2251148099:
34: 105632: loss: 0.2247271232:
34: 108832: loss: 0.2247428583:
34: 112032: loss: 0.2247509951:
34: 115232: loss: 0.2243121094:
34: 118432: loss: 0.2244952976:
34: 121632: loss: 0.2240349316:
Dev-Acc: 34: Accuracy: 0.9149666429: precision: 0.3453353273: recall: 0.5104574052: f1: 0.4119665157
Train-Acc: 34: Accuracy: 0.9130892754: precision: 0.7323774190: recall: 0.4801788180: f1: 0.5800508259
35: 3232: loss: 0.2289396776:
35: 6432: loss: 0.2191829479:
35: 9632: loss: 0.2255084787:
35: 12832: loss: 0.2287860819:
35: 16032: loss: 0.2289179956:
35: 19232: loss: 0.2228471424:
35: 22432: loss: 0.2222269073:
35: 25632: loss: 0.2242241728:
35: 28832: loss: 0.2224494642:
35: 32032: loss: 0.2222957301:
35: 35232: loss: 0.2219008392:
35: 38432: loss: 0.2228332292:
35: 41632: loss: 0.2228263939:
35: 44832: loss: 0.2236675164:
35: 48032: loss: 0.2232422590:
35: 51232: loss: 0.2234776711:
35: 54432: loss: 0.2235278759:
35: 57632: loss: 0.2227681224:
35: 60832: loss: 0.2223949044:
35: 64032: loss: 0.2223193841:
35: 67232: loss: 0.2226655835:
35: 70432: loss: 0.2231973937:
35: 73632: loss: 0.2232685152:
35: 76832: loss: 0.2239397142:
35: 80032: loss: 0.2237461851:
35: 83232: loss: 0.2236568374:
35: 86432: loss: 0.2231507259:
35: 89632: loss: 0.2230904809:
35: 92832: loss: 0.2232627449:
35: 96032: loss: 0.2226855243:
35: 99232: loss: 0.2219262700:
35: 102432: loss: 0.2219783392:
35: 105632: loss: 0.2219427635:
35: 108832: loss: 0.2221745842:
35: 112032: loss: 0.2224573596:
35: 115232: loss: 0.2223113261:
35: 118432: loss: 0.2221481738:
35: 121632: loss: 0.2223824789:
Dev-Acc: 35: Accuracy: 0.9148376584: precision: 0.3452817224: recall: 0.5126679136: f1: 0.4126462739
Train-Acc: 35: Accuracy: 0.9138206244: precision: 0.7358226837: recall: 0.4845177832: f1: 0.5842946050
36: 3232: loss: 0.2284057020:
36: 6432: loss: 0.2277649285:
36: 9632: loss: 0.2235496586:
36: 12832: loss: 0.2232922642:
36: 16032: loss: 0.2229026496:
36: 19232: loss: 0.2223256180:
36: 22432: loss: 0.2239498030:
36: 25632: loss: 0.2253860563:
36: 28832: loss: 0.2260315151:
36: 32032: loss: 0.2250418911:
36: 35232: loss: 0.2258394420:
36: 38432: loss: 0.2266706596:
36: 41632: loss: 0.2255938003:
36: 44832: loss: 0.2254320133:
36: 48032: loss: 0.2252921177:
36: 51232: loss: 0.2249629730:
36: 54432: loss: 0.2238126818:
36: 57632: loss: 0.2231814167:
36: 60832: loss: 0.2226409652:
36: 64032: loss: 0.2225777020:
36: 67232: loss: 0.2219096140:
36: 70432: loss: 0.2221170214:
36: 73632: loss: 0.2217095801:
36: 76832: loss: 0.2217811022:
36: 80032: loss: 0.2219161782:
36: 83232: loss: 0.2216544310:
36: 86432: loss: 0.2212758808:
36: 89632: loss: 0.2209946888:
36: 92832: loss: 0.2203999569:
36: 96032: loss: 0.2205097897:
36: 99232: loss: 0.2206938251:
36: 102432: loss: 0.2205798991:
36: 105632: loss: 0.2210423591:
36: 108832: loss: 0.2214169178:
36: 112032: loss: 0.2211856961:
36: 115232: loss: 0.2210599570:
36: 118432: loss: 0.2205907532:
36: 121632: loss: 0.2205324824:
Dev-Acc: 36: Accuracy: 0.9147185683: precision: 0.3452326642: recall: 0.5147083829: f1: 0.4132705304
Train-Acc: 36: Accuracy: 0.9146095514: precision: 0.7398964762: recall: 0.4886595227: f1: 0.5885893020
37: 3232: loss: 0.2180482621:
37: 6432: loss: 0.2200872001:
37: 9632: loss: 0.2160859413:
37: 12832: loss: 0.2120800002:
37: 16032: loss: 0.2122250877:
37: 19232: loss: 0.2148289465:
37: 22432: loss: 0.2176008682:
37: 25632: loss: 0.2181738914:
37: 28832: loss: 0.2192336481:
37: 32032: loss: 0.2177097225:
37: 35232: loss: 0.2183874458:
37: 38432: loss: 0.2185982614:
37: 41632: loss: 0.2191982148:
37: 44832: loss: 0.2189662730:
37: 48032: loss: 0.2190194068:
37: 51232: loss: 0.2188714575:
37: 54432: loss: 0.2181299141:
37: 57632: loss: 0.2182611740:
37: 60832: loss: 0.2187683660:
37: 64032: loss: 0.2185673720:
37: 67232: loss: 0.2189211208:
37: 70432: loss: 0.2193641871:
37: 73632: loss: 0.2194649839:
37: 76832: loss: 0.2200396278:
37: 80032: loss: 0.2204714716:
37: 83232: loss: 0.2198942859:
37: 86432: loss: 0.2201140565:
37: 89632: loss: 0.2199124085:
37: 92832: loss: 0.2196909953:
37: 96032: loss: 0.2195114795:
37: 99232: loss: 0.2192639283:
37: 102432: loss: 0.2191809426:
37: 105632: loss: 0.2194972071:
37: 108832: loss: 0.2193245994:
37: 112032: loss: 0.2192380723:
37: 115232: loss: 0.2189280206:
37: 118432: loss: 0.2190945697:
37: 121632: loss: 0.2187451331:
Dev-Acc: 37: Accuracy: 0.9148277640: precision: 0.3463680800: recall: 0.5181091651: f1: 0.4151791797
Train-Acc: 37: Accuracy: 0.9153901935: precision: 0.7441144333: recall: 0.4924725528: f1: 0.5926892950
38: 3232: loss: 0.2065520671:
38: 6432: loss: 0.2139131407:
38: 9632: loss: 0.2124420563:
38: 12832: loss: 0.2140604030:
38: 16032: loss: 0.2143090971:
38: 19232: loss: 0.2188787197:
38: 22432: loss: 0.2159558857:
38: 25632: loss: 0.2159593016:
38: 28832: loss: 0.2160157867:
38: 32032: loss: 0.2161612658:
38: 35232: loss: 0.2164328328:
38: 38432: loss: 0.2154080102:
38: 41632: loss: 0.2155026384:
38: 44832: loss: 0.2158134696:
38: 48032: loss: 0.2156699833:
38: 51232: loss: 0.2147598455:
38: 54432: loss: 0.2147277996:
38: 57632: loss: 0.2144907008:
38: 60832: loss: 0.2145421742:
38: 64032: loss: 0.2146145976:
38: 67232: loss: 0.2144655749:
38: 70432: loss: 0.2142822646:
38: 73632: loss: 0.2142843342:
38: 76832: loss: 0.2142888655:
38: 80032: loss: 0.2151544823:
38: 83232: loss: 0.2152105583:
38: 86432: loss: 0.2155717246:
38: 89632: loss: 0.2158629793:
38: 92832: loss: 0.2165018036:
38: 96032: loss: 0.2164753906:
38: 99232: loss: 0.2162587219:
38: 102432: loss: 0.2166595094:
38: 105632: loss: 0.2163029844:
38: 108832: loss: 0.2162890043:
38: 112032: loss: 0.2165021248:
38: 115232: loss: 0.2168323300:
38: 118432: loss: 0.2172900916:
38: 121632: loss: 0.2175419625:
Dev-Acc: 38: Accuracy: 0.9151055813: precision: 0.3484075711: recall: 0.5227002211: f1: 0.4181175190
Train-Acc: 38: Accuracy: 0.9159325957: precision: 0.7471469683: recall: 0.4949707449: f1: 0.5954602974
39: 3232: loss: 0.2216982253:
39: 6432: loss: 0.2245434698:
39: 9632: loss: 0.2241850260:
39: 12832: loss: 0.2213488666:
39: 16032: loss: 0.2198207615:
39: 19232: loss: 0.2195206969:
39: 22432: loss: 0.2196346473:
39: 25632: loss: 0.2184118640:
39: 28832: loss: 0.2185320074:
39: 32032: loss: 0.2200629368:
39: 35232: loss: 0.2203283597:
39: 38432: loss: 0.2212715334:
39: 41632: loss: 0.2212450126:
39: 44832: loss: 0.2203231147:
39: 48032: loss: 0.2205604134:
39: 51232: loss: 0.2205493451:
39: 54432: loss: 0.2208014186:
39: 57632: loss: 0.2204891283:
39: 60832: loss: 0.2201230607:
39: 64032: loss: 0.2192516255:
39: 67232: loss: 0.2185173689:
39: 70432: loss: 0.2183432838:
39: 73632: loss: 0.2182890709:
39: 76832: loss: 0.2176152692:
39: 80032: loss: 0.2167432710:
39: 83232: loss: 0.2167755018:
39: 86432: loss: 0.2167420993:
39: 89632: loss: 0.2162927423:
39: 92832: loss: 0.2160691464:
39: 96032: loss: 0.2161029685:
39: 99232: loss: 0.2163941727:
39: 102432: loss: 0.2165990795:
39: 105632: loss: 0.2165324979:
39: 108832: loss: 0.2159309751:
39: 112032: loss: 0.2158828749:
39: 115232: loss: 0.2158929752:
39: 118432: loss: 0.2161829173:
39: 121632: loss: 0.2161306593:
Dev-Acc: 39: Accuracy: 0.9155917764: precision: 0.3510662432: recall: 0.5262710423: f1: 0.4211743893
Train-Acc: 39: Accuracy: 0.9165653586: precision: 0.7522441652: recall: 0.4958253895: f1: 0.5976938622
40: 3232: loss: 0.2228823416:
40: 6432: loss: 0.2202181784:
40: 9632: loss: 0.2139539416:
40: 12832: loss: 0.2116052528:
40: 16032: loss: 0.2113488389:
40: 19232: loss: 0.2143887529:
40: 22432: loss: 0.2150328085:
40: 25632: loss: 0.2158187674:
40: 28832: loss: 0.2146838160:
40: 32032: loss: 0.2144135970:
40: 35232: loss: 0.2139123687:
40: 38432: loss: 0.2141253453:
40: 41632: loss: 0.2144399543:
40: 44832: loss: 0.2150997513:
40: 48032: loss: 0.2165037959:
40: 51232: loss: 0.2155383184:
40: 54432: loss: 0.2149304132:
40: 57632: loss: 0.2147003659:
40: 60832: loss: 0.2147418652:
40: 64032: loss: 0.2151786904:
40: 67232: loss: 0.2142433264:
40: 70432: loss: 0.2148450976:
40: 73632: loss: 0.2145228511:
40: 76832: loss: 0.2144908717:
40: 80032: loss: 0.2141470385:
40: 83232: loss: 0.2136938693:
40: 86432: loss: 0.2143632928:
40: 89632: loss: 0.2146590458:
40: 92832: loss: 0.2146559875:
40: 96032: loss: 0.2145310525:
40: 99232: loss: 0.2143230843:
40: 102432: loss: 0.2141160724:
40: 105632: loss: 0.2140748269:
40: 108832: loss: 0.2140383377:
40: 112032: loss: 0.2137994300:
40: 115232: loss: 0.2138238794:
40: 118432: loss: 0.2138613983:
40: 121632: loss: 0.2143158004:
Dev-Acc: 40: Accuracy: 0.9158298969: precision: 0.3524274047: recall: 0.5283115116: f1: 0.4228073757
Train-Acc: 40: Accuracy: 0.9171816707: precision: 0.7556020317: recall: 0.4987837749: f1: 0.6009028988
41: 3232: loss: 0.2053007941:
41: 6432: loss: 0.2091232941:
41: 9632: loss: 0.2077907737:
41: 12832: loss: 0.2083311311:
41: 16032: loss: 0.2084604596:
41: 19232: loss: 0.2091606172:
41: 22432: loss: 0.2104616713:
41: 25632: loss: 0.2112058949:
41: 28832: loss: 0.2129839743:
41: 32032: loss: 0.2138834327:
41: 35232: loss: 0.2141968346:
41: 38432: loss: 0.2144817595:
41: 41632: loss: 0.2155534892:
41: 44832: loss: 0.2150268073:
41: 48032: loss: 0.2146790038:
41: 51232: loss: 0.2147129924:
41: 54432: loss: 0.2147497794:
41: 57632: loss: 0.2155394583:
41: 60832: loss: 0.2157326692:
41: 64032: loss: 0.2156579945:
41: 67232: loss: 0.2155351920:
41: 70432: loss: 0.2159004945:
41: 73632: loss: 0.2162528085:
41: 76832: loss: 0.2157406020:
41: 80032: loss: 0.2158403121:
41: 83232: loss: 0.2160101484:
41: 86432: loss: 0.2154700678:
41: 89632: loss: 0.2152963205:
41: 92832: loss: 0.2154429849:
41: 96032: loss: 0.2155088840:
41: 99232: loss: 0.2151565880:
41: 102432: loss: 0.2150900819:
41: 105632: loss: 0.2146767947:
41: 108832: loss: 0.2147718312:
41: 112032: loss: 0.2147438181:
41: 115232: loss: 0.2143425174:
41: 118432: loss: 0.2138908005:
41: 121632: loss: 0.2135278120:
Dev-Acc: 41: Accuracy: 0.9165641069: precision: 0.3553775744: recall: 0.5281414725: f1: 0.4248683401
Train-Acc: 41: Accuracy: 0.9180527925: precision: 0.7615576635: recall: 0.5014134508: f1: 0.6046935701
42: 3232: loss: 0.2241299266:
42: 6432: loss: 0.2228400591:
42: 9632: loss: 0.2200350316:
42: 12832: loss: 0.2172425987:
42: 16032: loss: 0.2135633882:
42: 19232: loss: 0.2123994566:
42: 22432: loss: 0.2106711301:
42: 25632: loss: 0.2111102796:
42: 28832: loss: 0.2115039850:
42: 32032: loss: 0.2108986028:
42: 35232: loss: 0.2122402445:
42: 38432: loss: 0.2125831897:
42: 41632: loss: 0.2125209827:
42: 44832: loss: 0.2123581708:
42: 48032: loss: 0.2137105156:
42: 51232: loss: 0.2133905260:
42: 54432: loss: 0.2134882193:
42: 57632: loss: 0.2133576674:
42: 60832: loss: 0.2129829862:
42: 64032: loss: 0.2130817797:
42: 67232: loss: 0.2125116860:
42: 70432: loss: 0.2128859318:
42: 73632: loss: 0.2130838746:
42: 76832: loss: 0.2132131016:
42: 80032: loss: 0.2131586697:
42: 83232: loss: 0.2128432308:
42: 86432: loss: 0.2127496044:
42: 89632: loss: 0.2125804127:
42: 92832: loss: 0.2126845430:
42: 96032: loss: 0.2124041424:
42: 99232: loss: 0.2125311673:
42: 102432: loss: 0.2118713184:
42: 105632: loss: 0.2115457732:
42: 108832: loss: 0.2114636049:
42: 112032: loss: 0.2117426019:
42: 115232: loss: 0.2120222162:
42: 118432: loss: 0.2119937226:
42: 121632: loss: 0.2118098953:
Dev-Acc: 42: Accuracy: 0.9174075127: precision: 0.3592255388: recall: 0.5300119027: f1: 0.4282181618
Train-Acc: 42: Accuracy: 0.9187348485: precision: 0.7671150371: recall: 0.5023995793: f1: 0.6071584634
43: 3232: loss: 0.2071673714:
43: 6432: loss: 0.2138934068:
43: 9632: loss: 0.2113797761:
43: 12832: loss: 0.2134340185:
43: 16032: loss: 0.2119657374:
43: 19232: loss: 0.2108104868:
43: 22432: loss: 0.2112746022:
43: 25632: loss: 0.2107773622:
43: 28832: loss: 0.2106040682:
43: 32032: loss: 0.2109913752:
43: 35232: loss: 0.2109451947:
43: 38432: loss: 0.2107345370:
43: 41632: loss: 0.2108049394:
43: 44832: loss: 0.2118505351:
43: 48032: loss: 0.2107535975:
43: 51232: loss: 0.2111187375:
43: 54432: loss: 0.2106774140:
43: 57632: loss: 0.2102375661:
43: 60832: loss: 0.2104170396:
43: 64032: loss: 0.2096855698:
43: 67232: loss: 0.2103319099:
43: 70432: loss: 0.2100914834:
43: 73632: loss: 0.2099930156:
43: 76832: loss: 0.2102648021:
43: 80032: loss: 0.2105512043:
43: 83232: loss: 0.2103975535:
43: 86432: loss: 0.2106752828:
43: 89632: loss: 0.2103145556:
43: 92832: loss: 0.2105616188:
43: 96032: loss: 0.2100944108:
43: 99232: loss: 0.2098099569:
43: 102432: loss: 0.2099912695:
43: 105632: loss: 0.2100571529:
43: 108832: loss: 0.2104040763:
43: 112032: loss: 0.2102917833:
43: 115232: loss: 0.2109644395:
43: 118432: loss: 0.2110172670:
43: 121632: loss: 0.2106657601:
Dev-Acc: 43: Accuracy: 0.9177051783: precision: 0.3604718399: recall: 0.5300119027: f1: 0.4291024229
Train-Acc: 43: Accuracy: 0.9192607999: precision: 0.7712530218: recall: 0.5033857077: f1: 0.6091729981
44: 3232: loss: 0.2135918298:
44: 6432: loss: 0.2198813086:
44: 9632: loss: 0.2145432925:
44: 12832: loss: 0.2144883955:
44: 16032: loss: 0.2137793104:
44: 19232: loss: 0.2138114459:
44: 22432: loss: 0.2135471623:
44: 25632: loss: 0.2139473778:
44: 28832: loss: 0.2121266468:
44: 32032: loss: 0.2122108694:
44: 35232: loss: 0.2113300072:
44: 38432: loss: 0.2108500010:
44: 41632: loss: 0.2111584631:
44: 44832: loss: 0.2108874647:
44: 48032: loss: 0.2105227370:
44: 51232: loss: 0.2103503762:
44: 54432: loss: 0.2107669018:
44: 57632: loss: 0.2099497922:
44: 60832: loss: 0.2097416426:
44: 64032: loss: 0.2093636761:
44: 67232: loss: 0.2090411367:
44: 70432: loss: 0.2090007602:
44: 73632: loss: 0.2089234094:
44: 76832: loss: 0.2090651604:
44: 80032: loss: 0.2090917202:
44: 83232: loss: 0.2095189298:
44: 86432: loss: 0.2090069662:
44: 89632: loss: 0.2089561712:
44: 92832: loss: 0.2086553784:
44: 96032: loss: 0.2092762047:
44: 99232: loss: 0.2093392223:
44: 102432: loss: 0.2092533588:
44: 105632: loss: 0.2091254876:
44: 108832: loss: 0.2091834431:
44: 112032: loss: 0.2092496343:
44: 115232: loss: 0.2093507802:
44: 118432: loss: 0.2094650473:
44: 121632: loss: 0.2095636482:
Dev-Acc: 44: Accuracy: 0.9181219339: precision: 0.3617492711: recall: 0.5274613161: f1: 0.4291643608
Train-Acc: 44: Accuracy: 0.9199510813: precision: 0.7779471545: recall: 0.5032542239: f1: 0.6111532474
45: 3232: loss: 0.2046555361:
45: 6432: loss: 0.2078233604:
45: 9632: loss: 0.2052113772:
45: 12832: loss: 0.2093991810:
45: 16032: loss: 0.2098750869:
45: 19232: loss: 0.2082631148:
45: 22432: loss: 0.2082234434:
45: 25632: loss: 0.2080114175:
45: 28832: loss: 0.2078139388:
45: 32032: loss: 0.2107670031:
45: 35232: loss: 0.2102570181:
45: 38432: loss: 0.2102734402:
45: 41632: loss: 0.2101550520:
45: 44832: loss: 0.2092482990:
45: 48032: loss: 0.2087173695:
45: 51232: loss: 0.2085460008:
45: 54432: loss: 0.2088928058:
45: 57632: loss: 0.2080683801:
45: 60832: loss: 0.2080122901:
45: 64032: loss: 0.2079041979:
45: 67232: loss: 0.2084536927:
45: 70432: loss: 0.2082832359:
45: 73632: loss: 0.2082751064:
45: 76832: loss: 0.2080842631:
45: 80032: loss: 0.2081619628:
45: 83232: loss: 0.2083487488:
45: 86432: loss: 0.2080980763:
45: 89632: loss: 0.2081757526:
45: 92832: loss: 0.2078626187:
45: 96032: loss: 0.2078746305:
45: 99232: loss: 0.2081555071:
45: 102432: loss: 0.2084265130:
45: 105632: loss: 0.2081764469:
45: 108832: loss: 0.2082197387:
45: 112032: loss: 0.2083012283:
45: 115232: loss: 0.2085311246:
45: 118432: loss: 0.2085827813:
45: 121632: loss: 0.2083766227:
Dev-Acc: 45: Accuracy: 0.9186378717: precision: 0.3628946435: recall: 0.5218500255: f1: 0.4280931790
Train-Acc: 45: Accuracy: 0.9201976061: precision: 0.7807841536: recall: 0.5027282887: f1: 0.6116376725
46: 3232: loss: 0.2113276639:
46: 6432: loss: 0.2094340463:
46: 9632: loss: 0.2106393233:
46: 12832: loss: 0.2096768662:
46: 16032: loss: 0.2137704075:
46: 19232: loss: 0.2116448270:
46: 22432: loss: 0.2099130833:
46: 25632: loss: 0.2099838355:
46: 28832: loss: 0.2092658830:
46: 32032: loss: 0.2089337513:
46: 35232: loss: 0.2092548688:
46: 38432: loss: 0.2077904323:
46: 41632: loss: 0.2083074558:
46: 44832: loss: 0.2078407205:
46: 48032: loss: 0.2074839095:
46: 51232: loss: 0.2078693330:
46: 54432: loss: 0.2076339325:
46: 57632: loss: 0.2079304835:
46: 60832: loss: 0.2076933825:
46: 64032: loss: 0.2077605143:
46: 67232: loss: 0.2072622543:
46: 70432: loss: 0.2066622536:
46: 73632: loss: 0.2071321847:
46: 76832: loss: 0.2069946269:
46: 80032: loss: 0.2065509007:
46: 83232: loss: 0.2067648753:
46: 86432: loss: 0.2065706950:
46: 89632: loss: 0.2067794331:
46: 92832: loss: 0.2069832329:
46: 96032: loss: 0.2068382147:
46: 99232: loss: 0.2070735032:
46: 102432: loss: 0.2067990599:
46: 105632: loss: 0.2071909187:
46: 108832: loss: 0.2073741046:
46: 112032: loss: 0.2074265626:
46: 115232: loss: 0.2071616191:
46: 118432: loss: 0.2072112946:
46: 121632: loss: 0.2073449503:
Dev-Acc: 46: Accuracy: 0.9191042185: precision: 0.3642771804: recall: 0.5184492433: f1: 0.4278997965
Train-Acc: 46: Accuracy: 0.9204276800: precision: 0.7837199754: recall: 0.5019393860: f1: 0.6119504669
47: 3232: loss: 0.2101402119:
47: 6432: loss: 0.2077914896:
47: 9632: loss: 0.2077263819:
47: 12832: loss: 0.2061414341:
47: 16032: loss: 0.2068603633:
47: 19232: loss: 0.2083723712:
47: 22432: loss: 0.2082733622:
47: 25632: loss: 0.2078442377:
47: 28832: loss: 0.2073621257:
47: 32032: loss: 0.2071526886:
47: 35232: loss: 0.2083001057:
47: 38432: loss: 0.2088249686:
47: 41632: loss: 0.2083457797:
47: 44832: loss: 0.2085369246:
47: 48032: loss: 0.2091639606:
47: 51232: loss: 0.2085045720:
47: 54432: loss: 0.2085849373:
47: 57632: loss: 0.2089941470:
47: 60832: loss: 0.2079139132:
47: 64032: loss: 0.2078534514:
47: 67232: loss: 0.2077800775:
47: 70432: loss: 0.2076564293:
47: 73632: loss: 0.2071272164:
47: 76832: loss: 0.2069700271:
47: 80032: loss: 0.2067793311:
47: 83232: loss: 0.2070770030:
47: 86432: loss: 0.2072925741:
47: 89632: loss: 0.2076536425:
47: 92832: loss: 0.2078068382:
47: 96032: loss: 0.2074029145:
47: 99232: loss: 0.2071858889:
47: 102432: loss: 0.2070212862:
47: 105632: loss: 0.2072196628:
47: 108832: loss: 0.2072754278:
47: 112032: loss: 0.2070874402:
47: 115232: loss: 0.2065796715:
47: 118432: loss: 0.2063839330:
47: 121632: loss: 0.2061102194:
Dev-Acc: 47: Accuracy: 0.9192629457: precision: 0.3647157592: recall: 0.5170889305: f1: 0.4277375343
Train-Acc: 47: Accuracy: 0.9206906557: precision: 0.7864207707: recall: 0.5018079022: f1: 0.6126740779
48: 3232: loss: 0.1965642485:
48: 6432: loss: 0.1961718215:
48: 9632: loss: 0.1971396326:
48: 12832: loss: 0.2030613468:
48: 16032: loss: 0.2029674885:
48: 19232: loss: 0.2033683699:
48: 22432: loss: 0.2026135665:
48: 25632: loss: 0.2037627355:
48: 28832: loss: 0.2036984894:
48: 32032: loss: 0.2039566514:
48: 35232: loss: 0.2042826522:
48: 38432: loss: 0.2045732388:
48: 41632: loss: 0.2043464491:
48: 44832: loss: 0.2038332339:
48: 48032: loss: 0.2025474616:
48: 51232: loss: 0.2026140031:
48: 54432: loss: 0.2028949303:
48: 57632: loss: 0.2033956480:
48: 60832: loss: 0.2039106975:
48: 64032: loss: 0.2038704447:
48: 67232: loss: 0.2043135326:
48: 70432: loss: 0.2046044145:
48: 73632: loss: 0.2048708424:
48: 76832: loss: 0.2057202077:
48: 80032: loss: 0.2056718670:
48: 83232: loss: 0.2056079275:
48: 86432: loss: 0.2056664694:
48: 89632: loss: 0.2056530646:
48: 92832: loss: 0.2059835985:
48: 96032: loss: 0.2060563218:
48: 99232: loss: 0.2058541038:
48: 102432: loss: 0.2053766219:
48: 105632: loss: 0.2052147541:
48: 108832: loss: 0.2055707014:
48: 112032: loss: 0.2055820369:
48: 115232: loss: 0.2053307441:
48: 118432: loss: 0.2052870218:
48: 121632: loss: 0.2053428200:
Dev-Acc: 48: Accuracy: 0.9199575186: precision: 0.3669345021: recall: 0.5124978745: f1: 0.4276693863
Train-Acc: 48: Accuracy: 0.9209125638: precision: 0.7909592751: recall: 0.4992439682: f1: 0.6121231662
49: 3232: loss: 0.1930320752:
49: 6432: loss: 0.1987613557:
49: 9632: loss: 0.1995295775:
49: 12832: loss: 0.2012406100:
49: 16032: loss: 0.2032143820:
49: 19232: loss: 0.2031067464:
49: 22432: loss: 0.2033902088:
49: 25632: loss: 0.2030907501:
49: 28832: loss: 0.2019777960:
49: 32032: loss: 0.2028747171:
49: 35232: loss: 0.2021866613:
49: 38432: loss: 0.2016296963:
49: 41632: loss: 0.2013849302:
49: 44832: loss: 0.2018193272:
49: 48032: loss: 0.2013702450:
49: 51232: loss: 0.2010087828:
49: 54432: loss: 0.2016704409:
49: 57632: loss: 0.2013855516:
49: 60832: loss: 0.2019612719:
49: 64032: loss: 0.2017440269:
49: 67232: loss: 0.2020900826:
49: 70432: loss: 0.2020218215:
49: 73632: loss: 0.2022077147:
49: 76832: loss: 0.2027634029:
49: 80032: loss: 0.2027866472:
49: 83232: loss: 0.2028813223:
49: 86432: loss: 0.2031702203:
49: 89632: loss: 0.2033253890:
49: 92832: loss: 0.2031572365:
49: 96032: loss: 0.2033654738:
49: 99232: loss: 0.2037092016:
49: 102432: loss: 0.2037848764:
49: 105632: loss: 0.2037226111:
49: 108832: loss: 0.2043821877:
49: 112032: loss: 0.2044087803:
49: 115232: loss: 0.2044957298:
49: 118432: loss: 0.2046353185:
49: 121632: loss: 0.2042559618:
Dev-Acc: 49: Accuracy: 0.9200865030: precision: 0.3667034720: recall: 0.5082468968: f1: 0.4260262258
Train-Acc: 49: Accuracy: 0.9213398695: precision: 0.7953902567: recall: 0.4991124844: f1: 0.6133462595
50: 3232: loss: 0.2074240675:
50: 6432: loss: 0.2128354197:
50: 9632: loss: 0.2056689980:
50: 12832: loss: 0.2087614270:
50: 16032: loss: 0.2090717845:
50: 19232: loss: 0.2056522693:
50: 22432: loss: 0.2070497591:
50: 25632: loss: 0.2063911101:
50: 28832: loss: 0.2058862215:
50: 32032: loss: 0.2053578853:
50: 35232: loss: 0.2052946324:
50: 38432: loss: 0.2061216400:
50: 41632: loss: 0.2058703727:
50: 44832: loss: 0.2059975775:
50: 48032: loss: 0.2059843573:
50: 51232: loss: 0.2064424209:
50: 54432: loss: 0.2058423048:
50: 57632: loss: 0.2059288012:
50: 60832: loss: 0.2065789151:
50: 64032: loss: 0.2060510019:
50: 67232: loss: 0.2063942553:
50: 70432: loss: 0.2061546685:
50: 73632: loss: 0.2062848604:
50: 76832: loss: 0.2055014558:
50: 80032: loss: 0.2046556980:
50: 83232: loss: 0.2048281989:
50: 86432: loss: 0.2047983081:
50: 89632: loss: 0.2047907034:
50: 92832: loss: 0.2046396239:
50: 96032: loss: 0.2039315080:
50: 99232: loss: 0.2038086160:
50: 102432: loss: 0.2035545012:
50: 105632: loss: 0.2037910745:
50: 108832: loss: 0.2036432313:
50: 112032: loss: 0.2033462346:
50: 115232: loss: 0.2034663925:
50: 118432: loss: 0.2032230137:
50: 121632: loss: 0.2030787777:
Dev-Acc: 50: Accuracy: 0.9209299088: precision: 0.3694347174: recall: 0.5022955280: f1: 0.4257404338
Train-Acc: 50: Accuracy: 0.9216274619: precision: 0.8000846203: recall: 0.4972717113: f1: 0.6133387391
