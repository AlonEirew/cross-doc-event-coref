1: 6464: loss: 0.6878753030:
1: 12864: loss: 0.6872294161:
1: 19264: loss: 0.6867685503:
1: 25664: loss: 0.6862092656:
Dev-Acc: 1: Accuracy: 0.2446420044: precision: 0.0646404799: recall: 0.8867539534: f1: 0.1204972388
Train-Acc: 1: Accuracy: 0.5763592124: precision: 0.5466597035: recall: 0.8946157386: f1: 0.6786355476
2: 6464: loss: 0.6827333891:
2: 12864: loss: 0.6824738479:
2: 19264: loss: 0.6819527759:
2: 25664: loss: 0.6814271353:
Dev-Acc: 2: Accuracy: 0.2355433404: precision: 0.0650228601: recall: 0.9044380207: f1: 0.1213234036
Train-Acc: 2: Accuracy: 0.5974295139: precision: 0.5596378270: recall: 0.9142725659: f1: 0.6942912059
3: 6464: loss: 0.6785977304:
3: 12864: loss: 0.6777270007:
3: 19264: loss: 0.6769247460:
3: 25664: loss: 0.6764989853:
Dev-Acc: 3: Accuracy: 0.2289351523: precision: 0.0656669488: recall: 0.9233123618: f1: 0.1226134965
Train-Acc: 3: Accuracy: 0.6187298894: precision: 0.5734923089: recall: 0.9265005588: f1: 0.7084579615
4: 6464: loss: 0.6740921062:
4: 12864: loss: 0.6736531949:
4: 19264: loss: 0.6732036789:
4: 25664: loss: 0.6724031015:
Dev-Acc: 4: Accuracy: 0.2239145041: precision: 0.0659874721: recall: 0.9350450604: f1: 0.1232752340
Train-Acc: 4: Accuracy: 0.6395043135: precision: 0.5875918435: recall: 0.9358359082: f1: 0.7219108959
5: 6464: loss: 0.6690998745:
5: 12864: loss: 0.6683953792:
5: 19264: loss: 0.6684316037:
5: 25664: loss: 0.6682710023:
Dev-Acc: 5: Accuracy: 0.2209874541: precision: 0.0660349175: recall: 0.9396361163: f1: 0.1233977938
Train-Acc: 5: Accuracy: 0.6624153852: precision: 0.6041131105: recall: 0.9424100980: f1: 0.7362609142
6: 6464: loss: 0.6647641301:
6: 12864: loss: 0.6650011697:
6: 19264: loss: 0.6644298418:
6: 25664: loss: 0.6638242669:
Dev-Acc: 6: Accuracy: 0.2191915363: precision: 0.0661924168: recall: 0.9445672505: f1: 0.1237152434
Train-Acc: 6: Accuracy: 0.6813161969: precision: 0.6186338610: recall: 0.9454999671: f1: 0.7479133623
7: 6464: loss: 0.6616480154:
7: 12864: loss: 0.6604905403:
7: 19264: loss: 0.6597194872:
7: 25664: loss: 0.6592288716:
Dev-Acc: 7: Accuracy: 0.2181397825: precision: 0.0662740899: recall: 0.9472878762: f1: 0.1238812110
Train-Acc: 7: Accuracy: 0.7009730339: precision: 0.6349192338: recall: 0.9457629347: f1: 0.7597771264
8: 6464: loss: 0.6545034462:
8: 12864: loss: 0.6549788165:
8: 19264: loss: 0.6551626001:
8: 25664: loss: 0.6546727911:
Dev-Acc: 8: Accuracy: 0.2181497067: precision: 0.0663780492: recall: 0.9489882673: f1: 0.1240773677
Train-Acc: 8: Accuracy: 0.7210571766: precision: 0.6522113078: recall: 0.9472092565: f1: 0.7725054957
9: 6464: loss: 0.6526702893:
9: 12864: loss: 0.6514886674:
9: 19264: loss: 0.6502934283:
9: 25664: loss: 0.6501262237:
Dev-Acc: 9: Accuracy: 0.2178520411: precision: 0.0664574641: recall: 0.9506886584: f1: 0.1242306410
Train-Acc: 9: Accuracy: 0.7391361594: precision: 0.6687856712: recall: 0.9475379659: f1: 0.7841249116
10: 6464: loss: 0.6487666404:
10: 12864: loss: 0.6471294841:
10: 19264: loss: 0.6463887781:
10: 25664: loss: 0.6460036950:
Dev-Acc: 10: Accuracy: 0.2189037949: precision: 0.0666238294: recall: 0.9520489713: f1: 0.1245329181
Train-Acc: 10: Accuracy: 0.7564263344: precision: 0.6859062962: recall: 0.9460916442: f1: 0.7952586207
11: 6464: loss: 0.6433471626:
11: 12864: loss: 0.6421066830:
11: 19264: loss: 0.6415647690:
11: 25664: loss: 0.6409669831:
Dev-Acc: 11: Accuracy: 0.2209874541: precision: 0.0668217332: recall: 0.9525590886: f1: 0.1248829640
Train-Acc: 11: Accuracy: 0.7727302909: precision: 0.7030095425: recall: 0.9444480968: f1: 0.8060371430
12: 6464: loss: 0.6384124303:
12: 12864: loss: 0.6367553565:
12: 19264: loss: 0.6367152301:
12: 25664: loss: 0.6364004588:
Dev-Acc: 12: Accuracy: 0.2227436900: precision: 0.0668699187: recall: 0.9510287366: f1: 0.1249539214
Train-Acc: 12: Accuracy: 0.7900533080: precision: 0.7222670025: recall: 0.9425415818: f1: 0.8178317789
13: 6464: loss: 0.6330923253:
13: 12864: loss: 0.6327992591:
13: 19264: loss: 0.6323118500:
13: 25664: loss: 0.6317198098:
Dev-Acc: 13: Accuracy: 0.2253036201: precision: 0.0670456726: recall: 0.9505186193: f1: 0.1252562824
Train-Acc: 13: Accuracy: 0.8050752878: precision: 0.7400548342: recall: 0.9405035829: f1: 0.8283249378
14: 6464: loss: 0.6273555762:
14: 12864: loss: 0.6288283736:
14: 19264: loss: 0.6283955042:
14: 25664: loss: 0.6270660636:
Dev-Acc: 14: Accuracy: 0.2287565470: precision: 0.0673058393: recall: 0.9501785411: f1: 0.1257072156
Train-Acc: 14: Accuracy: 0.8179935813: precision: 0.7565503341: recall: 0.9377424232: f1: 0.8374578013
15: 6464: loss: 0.6226306742:
15: 12864: loss: 0.6229954246:
15: 19264: loss: 0.6225778306:
15: 25664: loss: 0.6221137121:
Dev-Acc: 15: Accuracy: 0.2330330163: precision: 0.0676047128: recall: 0.9493283455: f1: 0.1262208356
Train-Acc: 15: Accuracy: 0.8317336440: precision: 0.7750163506: recall: 0.9348497798: f1: 0.8474626777
16: 6464: loss: 0.6193955004:
16: 12864: loss: 0.6183921072:
16: 19264: loss: 0.6170426887:
16: 25664: loss: 0.6171598025:
Dev-Acc: 16: Accuracy: 0.2374980152: precision: 0.0678909361: recall: 0.9479680326: f1: 0.1267074252
Train-Acc: 16: Accuracy: 0.8438630104: precision: 0.7921903804: recall: 0.9322858458: f1: 0.8565474752
17: 6464: loss: 0.6142445940:
17: 12864: loss: 0.6132952547:
17: 19264: loss: 0.6131110348:
17: 25664: loss: 0.6122465365:
Dev-Acc: 17: Accuracy: 0.2412783802: precision: 0.0681228585: recall: 0.9466077198: f1: 0.1270990057
Train-Acc: 17: Accuracy: 0.8535270691: precision: 0.8069874979: recall: 0.9293274604: f1: 0.8638474701
18: 6464: loss: 0.6118782061:
18: 12864: loss: 0.6084316760:
18: 19264: loss: 0.6075253346:
18: 25664: loss: 0.6066300228:
Dev-Acc: 18: Accuracy: 0.2464676946: precision: 0.0684659826: recall: 0.9450773678: f1: 0.1276820584
Train-Acc: 18: Accuracy: 0.8626980782: precision: 0.8213911220: recall: 0.9269607521: f1: 0.8709886648
19: 6464: loss: 0.6011432213:
19: 12864: loss: 0.6028633019:
19: 19264: loss: 0.6022555035:
19: 25664: loss: 0.6021630910:
Dev-Acc: 19: Accuracy: 0.2516967058: precision: 0.0686887483: recall: 0.9415065465: f1: 0.1280364431
Train-Acc: 19: Accuracy: 0.8711788058: precision: 0.8357916022: recall: 0.9238708829: f1: 0.8776268540
20: 6464: loss: 0.5998898149:
20: 12864: loss: 0.6001397470:
20: 19264: loss: 0.5989050506:
20: 25664: loss: 0.5974054311:
Dev-Acc: 20: Accuracy: 0.2572729886: precision: 0.0691387022: recall: 0.9409964292: f1: 0.1288130070
Train-Acc: 20: Accuracy: 0.8792321682: precision: 0.8501790809: recall: 0.9207152718: f1: 0.8840424189
