1: 3232: loss: 0.6963562405:
1: 6432: loss: 0.6897467867:
1: 9632: loss: 0.6848381740:
1: 12832: loss: 0.6790521614:
1: 16032: loss: 0.6739967823:
1: 19232: loss: 0.6686292443:
1: 22432: loss: 0.6631099854:
1: 25632: loss: 0.6580938253:
1: 28832: loss: 0.6529288024:
1: 32032: loss: 0.6477678945:
1: 35232: loss: 0.6425702032:
1: 38432: loss: 0.6372092249:
1: 41632: loss: 0.6316160838:
1: 44832: loss: 0.6260918358:
1: 48032: loss: 0.6206828889:
1: 51232: loss: 0.6152742219:
1: 54432: loss: 0.6103790273:
1: 57632: loss: 0.6051104747:
1: 60832: loss: 0.5999791748:
Dev-Acc: 1: Accuracy: 0.9289470315: precision: 0.3452611219: recall: 0.2428158476: f1: 0.2851153040
Train-Acc: 1: Accuracy: 0.8214450479: precision: 1.0000000000: recall: 0.2857800276: f1: 0.4445239800
2: 3232: loss: 0.5029435849:
2: 6432: loss: 0.4964610447:
2: 9632: loss: 0.4922560544:
2: 12832: loss: 0.4859802915:
2: 16032: loss: 0.4803040008:
2: 19232: loss: 0.4750236371:
2: 22432: loss: 0.4701026927:
2: 25632: loss: 0.4654038359:
2: 28832: loss: 0.4606453310:
2: 32032: loss: 0.4551925619:
2: 35232: loss: 0.4503361771:
2: 38432: loss: 0.4463083544:
2: 41632: loss: 0.4417778986:
2: 44832: loss: 0.4373866040:
2: 48032: loss: 0.4331525994:
2: 51232: loss: 0.4289151276:
2: 54432: loss: 0.4246987484:
2: 57632: loss: 0.4215356097:
2: 60832: loss: 0.4175447530:
Dev-Acc: 2: Accuracy: 0.8449853063: precision: 0.2034579325: recall: 0.5682707023: f1: 0.2996368853
Train-Acc: 2: Accuracy: 0.8807935715: precision: 0.9475815523: recall: 0.5538097429: f1: 0.6990581304
3: 3232: loss: 0.3509246531:
3: 6432: loss: 0.3381989355:
3: 9632: loss: 0.3333351185:
3: 12832: loss: 0.3300153701:
3: 16032: loss: 0.3291557332:
3: 19232: loss: 0.3284281734:
3: 22432: loss: 0.3266546321:
3: 25632: loss: 0.3246832753:
3: 28832: loss: 0.3217620364:
3: 32032: loss: 0.3203543491:
3: 35232: loss: 0.3174492212:
3: 38432: loss: 0.3151375932:
3: 41632: loss: 0.3129442177:
3: 44832: loss: 0.3107019215:
3: 48032: loss: 0.3077905725:
3: 51232: loss: 0.3059628142:
3: 54432: loss: 0.3029837072:
3: 57632: loss: 0.3011266020:
3: 60832: loss: 0.2985953562:
Dev-Acc: 3: Accuracy: 0.7509723902: precision: 0.1409782162: recall: 0.6415575582: f1: 0.2311603970
Train-Acc: 3: Accuracy: 0.9116429090: precision: 0.9490457492: recall: 0.6832555388: f1: 0.7945111230
4: 3232: loss: 0.2505451728:
4: 6432: loss: 0.2525375069:
4: 9632: loss: 0.2524940656:
4: 12832: loss: 0.2518454623:
4: 16032: loss: 0.2522043935:
4: 19232: loss: 0.2530176529:
4: 22432: loss: 0.2514880385:
4: 25632: loss: 0.2489274709:
4: 28832: loss: 0.2478922977:
4: 32032: loss: 0.2472015314:
4: 35232: loss: 0.2464848142:
4: 38432: loss: 0.2439993558:
4: 41632: loss: 0.2429921339:
4: 44832: loss: 0.2422925165:
4: 48032: loss: 0.2412829837:
4: 51232: loss: 0.2405330985:
4: 54432: loss: 0.2389164496:
4: 57632: loss: 0.2378035091:
4: 60832: loss: 0.2368270445:
Dev-Acc: 4: Accuracy: 0.6710191965: precision: 0.1146619197: recall: 0.6900187043: f1: 0.1966466369
Train-Acc: 4: Accuracy: 0.9317928553: precision: 0.9516537362: recall: 0.7660903294: f1: 0.8488490676
5: 3232: loss: 0.2121581079:
5: 6432: loss: 0.2230153913:
5: 9632: loss: 0.2127495557:
5: 12832: loss: 0.2100091624:
5: 16032: loss: 0.2104383581:
5: 19232: loss: 0.2107076651:
5: 22432: loss: 0.2082862918:
5: 25632: loss: 0.2077001720:
5: 28832: loss: 0.2071977018:
5: 32032: loss: 0.2072121978:
5: 35232: loss: 0.2061638523:
5: 38432: loss: 0.2054814549:
5: 41632: loss: 0.2048362093:
5: 44832: loss: 0.2043687923:
5: 48032: loss: 0.2039479132:
5: 51232: loss: 0.2036001232:
5: 54432: loss: 0.2025588403:
5: 57632: loss: 0.2018152241:
5: 60832: loss: 0.2011379868:
Dev-Acc: 5: Accuracy: 0.6114065647: precision: 0.1025910448: recall: 0.7304880122: f1: 0.1799145657
Train-Acc: 5: Accuracy: 0.9404214621: precision: 0.9537834874: recall: 0.8004733417: f1: 0.8704292812
6: 3232: loss: 0.1980880195:
6: 6432: loss: 0.1907759123:
6: 9632: loss: 0.1881210240:
6: 12832: loss: 0.1869267115:
6: 16032: loss: 0.1865940541:
6: 19232: loss: 0.1880832283:
6: 22432: loss: 0.1854321323:
6: 25632: loss: 0.1848183367:
6: 28832: loss: 0.1846887226:
6: 32032: loss: 0.1853148033:
6: 35232: loss: 0.1853573404:
6: 38432: loss: 0.1843797909:
6: 41632: loss: 0.1838960285:
6: 44832: loss: 0.1834038688:
6: 48032: loss: 0.1826992752:
6: 51232: loss: 0.1819529942:
6: 54432: loss: 0.1811894173:
6: 57632: loss: 0.1804554171:
6: 60832: loss: 0.1792297998:
Dev-Acc: 6: Accuracy: 0.5827512145: precision: 0.0979838620: recall: 0.7495323925: f1: 0.1733113156
Train-Acc: 6: Accuracy: 0.9443824291: precision: 0.9530376159: recall: 0.8178292026: f1: 0.8802717237
7: 3232: loss: 0.1737905037:
7: 6432: loss: 0.1685590793:
7: 9632: loss: 0.1677787289:
7: 12832: loss: 0.1684304172:
7: 16032: loss: 0.1658263559:
7: 19232: loss: 0.1669425424:
7: 22432: loss: 0.1667975058:
7: 25632: loss: 0.1662592550:
7: 28832: loss: 0.1670910382:
7: 32032: loss: 0.1670152773:
7: 35232: loss: 0.1666828450:
7: 38432: loss: 0.1665368248:
7: 41632: loss: 0.1660328438:
7: 44832: loss: 0.1662362816:
7: 48032: loss: 0.1653367044:
7: 51232: loss: 0.1655136241:
7: 54432: loss: 0.1657137179:
7: 57632: loss: 0.1642627793:
7: 60832: loss: 0.1637127183:
Dev-Acc: 7: Accuracy: 0.5621427894: precision: 0.0947618240: recall: 0.7604148954: f1: 0.1685226009
Train-Acc: 7: Accuracy: 0.9480968118: precision: 0.9513218003: recall: 0.8351193215: f1: 0.8894412547
8: 3232: loss: 0.1561398666:
8: 6432: loss: 0.1531348983:
8: 9632: loss: 0.1538967890:
8: 12832: loss: 0.1524392806:
8: 16032: loss: 0.1517351852:
8: 19232: loss: 0.1537038032:
8: 22432: loss: 0.1531619666:
8: 25632: loss: 0.1557725708:
8: 28832: loss: 0.1555404437:
8: 32032: loss: 0.1552672108:
8: 35232: loss: 0.1546045851:
8: 38432: loss: 0.1539608480:
8: 41632: loss: 0.1533790814:
8: 44832: loss: 0.1539266893:
8: 48032: loss: 0.1538237143:
8: 51232: loss: 0.1536985144:
8: 54432: loss: 0.1535426056:
8: 57632: loss: 0.1534815750:
8: 60832: loss: 0.1534765795:
Dev-Acc: 8: Accuracy: 0.5442034602: precision: 0.0921808186: recall: 0.7697670464: f1: 0.1646451237
Train-Acc: 8: Accuracy: 0.9519755840: precision: 0.9480131243: recall: 0.8547761488: f1: 0.8989836134
9: 3232: loss: 0.1666679384:
9: 6432: loss: 0.1557845893:
9: 9632: loss: 0.1521981587:
9: 12832: loss: 0.1492805876:
9: 16032: loss: 0.1487499538:
9: 19232: loss: 0.1475494345:
9: 22432: loss: 0.1487404162:
9: 25632: loss: 0.1492633468:
9: 28832: loss: 0.1502552492:
9: 32032: loss: 0.1486476918:
9: 35232: loss: 0.1482587386:
9: 38432: loss: 0.1485137784:
9: 41632: loss: 0.1475547406:
9: 44832: loss: 0.1481763692:
9: 48032: loss: 0.1470300755:
9: 51232: loss: 0.1462164528:
9: 54432: loss: 0.1458420037:
9: 57632: loss: 0.1454344401:
9: 60832: loss: 0.1451755983:
Dev-Acc: 9: Accuracy: 0.5321479440: precision: 0.0907522361: recall: 0.7780989628: f1: 0.1625461779
Train-Acc: 9: Accuracy: 0.9540300369: precision: 0.9490017361: recall: 0.8624679508: f1: 0.9036679869
10: 3232: loss: 0.1376799036:
10: 6432: loss: 0.1332402849:
10: 9632: loss: 0.1411706935:
10: 12832: loss: 0.1427760223:
10: 16032: loss: 0.1418690623:
10: 19232: loss: 0.1429003519:
10: 22432: loss: 0.1413739273:
10: 25632: loss: 0.1411672265:
10: 28832: loss: 0.1418791407:
10: 32032: loss: 0.1407219411:
10: 35232: loss: 0.1417285222:
10: 38432: loss: 0.1410041340:
10: 41632: loss: 0.1403646111:
10: 44832: loss: 0.1399050079:
10: 48032: loss: 0.1393910816:
10: 51232: loss: 0.1384666172:
10: 54432: loss: 0.1388320738:
10: 57632: loss: 0.1389203045:
10: 60832: loss: 0.1387821149:
Dev-Acc: 10: Accuracy: 0.5218883753: precision: 0.0892000544: recall: 0.7809896276: f1: 0.1601129471
Train-Acc: 10: Accuracy: 0.9556900263: precision: 0.9480844970: recall: 0.8704227204: f1: 0.9075952838
11: 3232: loss: 0.1242784095:
11: 6432: loss: 0.1276424801:
11: 9632: loss: 0.1259922862:
11: 12832: loss: 0.1308692621:
11: 16032: loss: 0.1315927360:
11: 19232: loss: 0.1330278951:
11: 22432: loss: 0.1350994590:
11: 25632: loss: 0.1332541002:
11: 28832: loss: 0.1334993401:
11: 32032: loss: 0.1330726358:
11: 35232: loss: 0.1336553796:
11: 38432: loss: 0.1332703590:
11: 41632: loss: 0.1333099997:
11: 44832: loss: 0.1326253649:
11: 48032: loss: 0.1325613991:
11: 51232: loss: 0.1323043184:
11: 54432: loss: 0.1323622532:
11: 57632: loss: 0.1324286976:
11: 60832: loss: 0.1325516428:
Dev-Acc: 11: Accuracy: 0.5102297664: precision: 0.0880391117: recall: 0.7900017004: f1: 0.1584232691
Train-Acc: 11: Accuracy: 0.9572842717: precision: 0.9460950764: recall: 0.8792321346: f1: 0.9114389887
12: 3232: loss: 0.1464058587:
12: 6432: loss: 0.1379847179:
12: 9632: loss: 0.1354961988:
12: 12832: loss: 0.1322079034:
12: 16032: loss: 0.1320507842:
12: 19232: loss: 0.1319110047:
12: 22432: loss: 0.1328729129:
12: 25632: loss: 0.1335497039:
12: 28832: loss: 0.1323880268:
12: 32032: loss: 0.1316053551:
12: 35232: loss: 0.1301200418:
12: 38432: loss: 0.1293471203:
12: 41632: loss: 0.1287346634:
12: 44832: loss: 0.1286391132:
12: 48032: loss: 0.1294428945:
12: 51232: loss: 0.1291125905:
12: 54432: loss: 0.1285688094:
12: 57632: loss: 0.1288657860:
12: 60832: loss: 0.1278974246:
Dev-Acc: 12: Accuracy: 0.5063899159: precision: 0.0874137055: recall: 0.7901717395: f1: 0.1574133668
Train-Acc: 12: Accuracy: 0.9588456154: precision: 0.9526897043: recall: 0.8790349089: f1: 0.9143814539
13: 3232: loss: 0.1302985149:
13: 6432: loss: 0.1234646215:
13: 9632: loss: 0.1240300981:
13: 12832: loss: 0.1229847099:
13: 16032: loss: 0.1207419843:
13: 19232: loss: 0.1220934891:
13: 22432: loss: 0.1228266366:
13: 25632: loss: 0.1235214470:
13: 28832: loss: 0.1229103995:
13: 32032: loss: 0.1234110407:
13: 35232: loss: 0.1243543436:
13: 38432: loss: 0.1241808541:
13: 41632: loss: 0.1246920104:
13: 44832: loss: 0.1250622531:
13: 48032: loss: 0.1244135047:
13: 51232: loss: 0.1245369629:
13: 54432: loss: 0.1247084144:
13: 57632: loss: 0.1239110840:
13: 60832: loss: 0.1233187636:
Dev-Acc: 13: Accuracy: 0.4985711873: precision: 0.0866272934: recall: 0.7956129910: f1: 0.1562426954
Train-Acc: 13: Accuracy: 0.9602426291: precision: 0.9536170213: recall: 0.8839655512: f1: 0.9174712565
14: 3232: loss: 0.1244171671:
14: 6432: loss: 0.1217263423:
14: 9632: loss: 0.1168818588:
14: 12832: loss: 0.1162132581:
14: 16032: loss: 0.1175116278:
14: 19232: loss: 0.1170146963:
14: 22432: loss: 0.1179426428:
14: 25632: loss: 0.1183795241:
14: 28832: loss: 0.1180496618:
14: 32032: loss: 0.1184117811:
14: 35232: loss: 0.1181109955:
14: 38432: loss: 0.1178412823:
14: 41632: loss: 0.1178345379:
14: 44832: loss: 0.1184025532:
14: 48032: loss: 0.1190413531:
14: 51232: loss: 0.1194592102:
14: 54432: loss: 0.1194600152:
14: 57632: loss: 0.1194826197:
14: 60832: loss: 0.1194557179:
Dev-Acc: 14: Accuracy: 0.4886489809: precision: 0.0855648953: recall: 0.8013943207: f1: 0.1546209114
Train-Acc: 14: Accuracy: 0.9612451792: precision: 0.9529851272: recall: 0.8888304516: f1: 0.9197904619
15: 3232: loss: 0.1280240479:
15: 6432: loss: 0.1254465182:
15: 9632: loss: 0.1224504034:
15: 12832: loss: 0.1224057478:
15: 16032: loss: 0.1212210534:
15: 19232: loss: 0.1205074501:
15: 22432: loss: 0.1201795957:
15: 25632: loss: 0.1206033364:
15: 28832: loss: 0.1205571553:
15: 32032: loss: 0.1199064726:
15: 35232: loss: 0.1199710209:
15: 38432: loss: 0.1185809575:
15: 41632: loss: 0.1180142233:
15: 44832: loss: 0.1174319858:
15: 48032: loss: 0.1159630954:
15: 51232: loss: 0.1156986274:
15: 54432: loss: 0.1153401944:
15: 57632: loss: 0.1156518896:
15: 60832: loss: 0.1155884887:
Dev-Acc: 15: Accuracy: 0.4798479974: precision: 0.0848319418: recall: 0.8085359633: f1: 0.1535530331
Train-Acc: 15: Accuracy: 0.9626093507: precision: 0.9528143377: recall: 0.8947472224: f1: 0.9228682828
16: 3232: loss: 0.1108585110:
16: 6432: loss: 0.1145267410:
16: 9632: loss: 0.1152092346:
16: 12832: loss: 0.1132541375:
16: 16032: loss: 0.1129727050:
16: 19232: loss: 0.1142541421:
16: 22432: loss: 0.1136647581:
16: 25632: loss: 0.1140242212:
16: 28832: loss: 0.1126994003:
16: 32032: loss: 0.1131721533:
16: 35232: loss: 0.1133561241:
16: 38432: loss: 0.1142960267:
16: 41632: loss: 0.1135072065:
16: 44832: loss: 0.1134036866:
16: 48032: loss: 0.1136568385:
16: 51232: loss: 0.1137791013:
16: 54432: loss: 0.1135001947:
16: 57632: loss: 0.1127409033:
16: 60832: loss: 0.1124203450:
Dev-Acc: 16: Accuracy: 0.4756508768: precision: 0.0841229080: recall: 0.8076857677: f1: 0.1523754531
Train-Acc: 16: Accuracy: 0.9634311199: precision: 0.9540559441: recall: 0.8969167050: f1: 0.9246043848
17: 3232: loss: 0.1186458003:
17: 6432: loss: 0.1123466557:
17: 9632: loss: 0.1076947699:
17: 12832: loss: 0.1075755324:
17: 16032: loss: 0.1077567119:
17: 19232: loss: 0.1107622732:
17: 22432: loss: 0.1103151865:
17: 25632: loss: 0.1106371376:
17: 28832: loss: 0.1095347345:
17: 32032: loss: 0.1095020834:
17: 35232: loss: 0.1100606179:
17: 38432: loss: 0.1097635446:
17: 41632: loss: 0.1098561888:
17: 44832: loss: 0.1090224868:
17: 48032: loss: 0.1096970618:
17: 51232: loss: 0.1098861316:
17: 54432: loss: 0.1096262490:
17: 57632: loss: 0.1095361465:
17: 60832: loss: 0.1098028184:
Dev-Acc: 17: Accuracy: 0.4672566950: precision: 0.0831720458: recall: 0.8110865499: f1: 0.1508729757
Train-Acc: 17: Accuracy: 0.9643021822: precision: 0.9535304348: recall: 0.9011241864: f1: 0.9265868992
18: 3232: loss: 0.1046302405:
18: 6432: loss: 0.1046473712:
18: 9632: loss: 0.1075217910:
18: 12832: loss: 0.1071429885:
18: 16032: loss: 0.1077149728:
18: 19232: loss: 0.1070272468:
18: 22432: loss: 0.1056278967:
18: 25632: loss: 0.1047141305:
18: 28832: loss: 0.1054146328:
18: 32032: loss: 0.1052472711:
18: 35232: loss: 0.1062090262:
18: 38432: loss: 0.1070466231:
18: 41632: loss: 0.1077762651:
18: 44832: loss: 0.1075123024:
18: 48032: loss: 0.1070215251:
18: 51232: loss: 0.1067118942:
18: 54432: loss: 0.1068578127:
18: 57632: loss: 0.1067820002:
18: 60832: loss: 0.1067885189:
Dev-Acc: 18: Accuracy: 0.4659370482: precision: 0.0825002612: recall: 0.8054752593: f1: 0.1496706109
Train-Acc: 18: Accuracy: 0.9651075602: precision: 0.9602616402: recall: 0.8975741240: f1: 0.9278602739
19: 3232: loss: 0.1057798095:
19: 6432: loss: 0.1045685049:
19: 9632: loss: 0.1033529630:
19: 12832: loss: 0.1025940118:
19: 16032: loss: 0.1028515680:
19: 19232: loss: 0.1033578849:
19: 22432: loss: 0.1044198769:
19: 25632: loss: 0.1036348376:
19: 28832: loss: 0.1031629404:
19: 32032: loss: 0.1030992076:
19: 35232: loss: 0.1030696617:
19: 38432: loss: 0.1031996496:
19: 41632: loss: 0.1030888700:
19: 44832: loss: 0.1037598804:
19: 48032: loss: 0.1037002259:
19: 51232: loss: 0.1036293761:
19: 54432: loss: 0.1040079221:
19: 57632: loss: 0.1038460198:
19: 60832: loss: 0.1038891038:
Dev-Acc: 19: Accuracy: 0.4606286585: precision: 0.0818829455: recall: 0.8071756504: f1: 0.1486829329
Train-Acc: 19: Accuracy: 0.9652389884: precision: 0.9599606631: recall: 0.8984287687: f1: 0.9281760451
20: 3232: loss: 0.0932938118:
20: 6432: loss: 0.0971196241:
20: 9632: loss: 0.0981350638:
20: 12832: loss: 0.0986138021:
20: 16032: loss: 0.1009384402:
20: 19232: loss: 0.0977854870:
20: 22432: loss: 0.0985106768:
20: 25632: loss: 0.0998505787:
20: 28832: loss: 0.1006681278:
20: 32032: loss: 0.1009671975:
20: 35232: loss: 0.1014040348:
20: 38432: loss: 0.1015288320:
20: 41632: loss: 0.1014416337:
20: 44832: loss: 0.1012405477:
20: 48032: loss: 0.1015674004:
20: 51232: loss: 0.1014915817:
20: 54432: loss: 0.1014256464:
20: 57632: loss: 0.1011957922:
20: 60832: loss: 0.1012077761:
Dev-Acc: 20: Accuracy: 0.4580985010: precision: 0.0810062590: recall: 0.8010542425: f1: 0.1471336883
Train-Acc: 20: Accuracy: 0.9658799767: precision: 0.9646268129: recall: 0.8963907698: f1: 0.9292578205
