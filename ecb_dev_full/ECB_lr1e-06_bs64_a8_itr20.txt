1: 6464: loss: 0.7004909098:
1: 12864: loss: 0.6904422802:
1: 19264: loss: 0.6810232377:
1: 25664: loss: 0.6715480287:
1: 32064: loss: 0.6627044841:
1: 38464: loss: 0.6540512166:
1: 44864: loss: 0.6453199182:
1: 51264: loss: 0.6365514424:
1: 57664: loss: 0.6284911435:
1: 64064: loss: 0.6203225978:
1: 70464: loss: 0.6121050933:
1: 76864: loss: 0.6037626421:
1: 83264: loss: 0.5958954223:
1: 89664: loss: 0.5873867558:
1: 96064: loss: 0.5796784142:
1: 102464: loss: 0.5722065610:
1: 108864: loss: 0.5647397279:
1: 115264: loss: 0.5574096042:
1: 121664: loss: 0.5506413628:
1: 128064: loss: 0.5440361129:
1: 134464: loss: 0.5375632936:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 6464: loss: 0.3962102222:
2: 12864: loss: 0.3888747926:
2: 19264: loss: 0.3837647968:
2: 25664: loss: 0.3798303657:
2: 32064: loss: 0.3761611242:
2: 38464: loss: 0.3734082194:
2: 44864: loss: 0.3694773994:
2: 51264: loss: 0.3660833357:
2: 57664: loss: 0.3645445418:
2: 64064: loss: 0.3607276857:
2: 70464: loss: 0.3583197486:
2: 76864: loss: 0.3547599087:
2: 83264: loss: 0.3525064830:
2: 89664: loss: 0.3505223065:
2: 96064: loss: 0.3485830180:
2: 102464: loss: 0.3462274607:
2: 108864: loss: 0.3440053972:
2: 115264: loss: 0.3418136441:
2: 121664: loss: 0.3403019693:
2: 128064: loss: 0.3379783792:
2: 134464: loss: 0.3360740170:
Dev-Acc: 2: Accuracy: 0.9416970611: precision: 0.8571428571: recall: 0.0010202347: f1: 0.0020380435
Train-Acc: 2: Accuracy: 0.8894951940: precision: 0.9662921348: recall: 0.0056538032: f1: 0.0112418301
3: 6464: loss: 0.2915733463:
3: 12864: loss: 0.2924413183:
3: 19264: loss: 0.2927751530:
3: 25664: loss: 0.2880954331:
3: 32064: loss: 0.2869297734:
3: 38464: loss: 0.2858364809:
3: 44864: loss: 0.2842447158:
3: 51264: loss: 0.2851318576:
3: 57664: loss: 0.2843192820:
3: 64064: loss: 0.2833290932:
3: 70464: loss: 0.2821062790:
3: 76864: loss: 0.2804019638:
3: 83264: loss: 0.2785006641:
3: 89664: loss: 0.2775747108:
3: 96064: loss: 0.2754617682:
3: 102464: loss: 0.2748852717:
3: 108864: loss: 0.2739076102:
3: 115264: loss: 0.2730176599:
3: 121664: loss: 0.2717704338:
3: 128064: loss: 0.2712918727:
3: 134464: loss: 0.2707664456:
Dev-Acc: 3: Accuracy: 0.9437509775: precision: 0.5645554202: recall: 0.1576262540: f1: 0.2464442377
Train-Acc: 3: Accuracy: 0.9064711928: precision: 0.8461317227: recall: 0.1934126619: f1: 0.3148544521
4: 6464: loss: 0.2461997081:
4: 12864: loss: 0.2513082624:
4: 19264: loss: 0.2476257768:
4: 25664: loss: 0.2479662095:
4: 32064: loss: 0.2487977065:
4: 38464: loss: 0.2469187698:
4: 44864: loss: 0.2460757020:
4: 51264: loss: 0.2455879298:
4: 57664: loss: 0.2441049560:
4: 64064: loss: 0.2435045794:
4: 70464: loss: 0.2436835752:
4: 76864: loss: 0.2435767133:
4: 83264: loss: 0.2432383159:
4: 89664: loss: 0.2429879201:
4: 96064: loss: 0.2425755215:
4: 102464: loss: 0.2413464414:
4: 108864: loss: 0.2406231561:
4: 115264: loss: 0.2396442910:
4: 121664: loss: 0.2391842457:
4: 128064: loss: 0.2388336180:
4: 134464: loss: 0.2387076282:
Dev-Acc: 4: Accuracy: 0.9376785755: precision: 0.4528524281: recall: 0.3266451284: f1: 0.3795317594
Train-Acc: 4: Accuracy: 0.9165297151: precision: 0.7978589421: recall: 0.3331799356: f1: 0.4700644623
5: 6464: loss: 0.2327753931:
5: 12864: loss: 0.2314773538:
5: 19264: loss: 0.2285089229:
5: 25664: loss: 0.2283554600:
5: 32064: loss: 0.2285430524:
5: 38464: loss: 0.2257944094:
5: 44864: loss: 0.2247976929:
5: 51264: loss: 0.2245018880:
5: 57664: loss: 0.2240741759:
5: 64064: loss: 0.2233554245:
5: 70464: loss: 0.2233722043:
5: 76864: loss: 0.2222589568:
5: 83264: loss: 0.2222661955:
5: 89664: loss: 0.2217983387:
5: 96064: loss: 0.2215206986:
5: 102464: loss: 0.2212109092:
5: 108864: loss: 0.2209569354:
5: 115264: loss: 0.2202618347:
5: 121664: loss: 0.2199396062:
5: 128064: loss: 0.2200807535:
5: 134464: loss: 0.2199242994:
Dev-Acc: 5: Accuracy: 0.9351385236: precision: 0.4334415584: recall: 0.3632035368: f1: 0.3952262004
Train-Acc: 5: Accuracy: 0.9216064811: precision: 0.7976079734: recall: 0.3945828677: f1: 0.5279732583
6: 6464: loss: 0.2178125848:
6: 12864: loss: 0.2126952155:
6: 19264: loss: 0.2137305916:
6: 25664: loss: 0.2114394927:
6: 32064: loss: 0.2113154575:
6: 38464: loss: 0.2106586345:
6: 44864: loss: 0.2108922843:
6: 51264: loss: 0.2101767875:
6: 57664: loss: 0.2105929929:
6: 64064: loss: 0.2093192302:
6: 70464: loss: 0.2091475885:
6: 76864: loss: 0.2088262279:
6: 83264: loss: 0.2090772723:
6: 89664: loss: 0.2092761154:
6: 96064: loss: 0.2091630690:
6: 102464: loss: 0.2083208531:
6: 108864: loss: 0.2084289239:
6: 115264: loss: 0.2083438189:
6: 121664: loss: 0.2082389510:
6: 128064: loss: 0.2084456253:
6: 134464: loss: 0.2081852460:
Dev-Acc: 6: Accuracy: 0.9334815145: precision: 0.4253040479: recall: 0.3984016324: f1: 0.4114135206
Train-Acc: 6: Accuracy: 0.9247035980: precision: 0.7998043292: recall: 0.4299520084: f1: 0.5592611596
7: 6464: loss: 0.2030063831:
7: 12864: loss: 0.2028711148:
7: 19264: loss: 0.2005954846:
7: 25664: loss: 0.1998754115:
7: 32064: loss: 0.2013863111:
7: 38464: loss: 0.2015551531:
7: 44864: loss: 0.2004651495:
7: 51264: loss: 0.2007467156:
7: 57664: loss: 0.1992991681:
7: 64064: loss: 0.1995772209:
7: 70464: loss: 0.2004850946:
7: 76864: loss: 0.2015440113:
7: 83264: loss: 0.2005719913:
7: 89664: loss: 0.2005137951:
7: 96064: loss: 0.2001634544:
7: 102464: loss: 0.1999981712:
7: 108864: loss: 0.2001507316:
7: 115264: loss: 0.1997694435:
7: 121664: loss: 0.2001475466:
7: 128064: loss: 0.1996139964:
7: 134464: loss: 0.1994488072:
Dev-Acc: 7: Accuracy: 0.9330151677: precision: 0.4241102582: recall: 0.4133650740: f1: 0.4186687333
Train-Acc: 7: Accuracy: 0.9261499643: precision: 0.8017271974: recall: 0.4455328381: f1: 0.5727687627
8: 6464: loss: 0.2036885362:
8: 12864: loss: 0.2005128078:
8: 19264: loss: 0.1985750871:
8: 25664: loss: 0.1982369095:
8: 32064: loss: 0.1979752253:
8: 38464: loss: 0.1979917860:
8: 44864: loss: 0.1968907986:
8: 51264: loss: 0.1958352034:
8: 57664: loss: 0.1956507297:
8: 64064: loss: 0.1950638342:
8: 70464: loss: 0.1956787187:
8: 76864: loss: 0.1953268514:
8: 83264: loss: 0.1948668197:
8: 89664: loss: 0.1946643967:
8: 96064: loss: 0.1942860375:
8: 102464: loss: 0.1935140501:
8: 108864: loss: 0.1930180640:
8: 115264: loss: 0.1926204985:
8: 121664: loss: 0.1925695283:
8: 128064: loss: 0.1923727018:
8: 134464: loss: 0.1922631401:
Dev-Acc: 8: Accuracy: 0.9312787652: precision: 0.4136791674: recall: 0.4257779289: f1: 0.4196413608
Train-Acc: 8: Accuracy: 0.9280564785: precision: 0.8021866546: recall: 0.4678850832: f1: 0.5910393223
9: 6464: loss: 0.1896492005:
9: 12864: loss: 0.1865735246:
9: 19264: loss: 0.1875174076:
9: 25664: loss: 0.1894314662:
9: 32064: loss: 0.1878143743:
9: 38464: loss: 0.1885035916:
9: 44864: loss: 0.1879512808:
9: 51264: loss: 0.1887569996:
9: 57664: loss: 0.1886668826:
9: 64064: loss: 0.1893595134:
9: 70464: loss: 0.1894036460:
9: 76864: loss: 0.1895007756:
9: 83264: loss: 0.1889580744:
9: 89664: loss: 0.1880827584:
9: 96064: loss: 0.1876220118:
9: 102464: loss: 0.1872281147:
9: 108864: loss: 0.1873559680:
9: 115264: loss: 0.1875450029:
9: 121664: loss: 0.1873019311:
9: 128064: loss: 0.1875209381:
9: 134464: loss: 0.1870247334:
Dev-Acc: 9: Accuracy: 0.9303560257: precision: 0.4085209003: recall: 0.4320693760: f1: 0.4199652921
Train-Acc: 9: Accuracy: 0.9291521907: precision: 0.8011363636: recall: 0.4820195911: f1: 0.6018963182
10: 6464: loss: 0.1896456857:
10: 12864: loss: 0.1827824184:
10: 19264: loss: 0.1780157431:
10: 25664: loss: 0.1761933727:
10: 32064: loss: 0.1784502264:
10: 38464: loss: 0.1785102953:
10: 44864: loss: 0.1781534102:
10: 51264: loss: 0.1792263717:
10: 57664: loss: 0.1785881383:
10: 64064: loss: 0.1802000726:
10: 70464: loss: 0.1797227793:
10: 76864: loss: 0.1801572535:
10: 83264: loss: 0.1797353515:
10: 89664: loss: 0.1800800785:
10: 96064: loss: 0.1796766143:
10: 102464: loss: 0.1797306787:
10: 108864: loss: 0.1798019975:
10: 115264: loss: 0.1806593115:
10: 121664: loss: 0.1804345203:
10: 128064: loss: 0.1810894733:
10: 134464: loss: 0.1817993404:
Dev-Acc: 10: Accuracy: 0.9281830192: precision: 0.3961744453: recall: 0.4402312532: f1: 0.4170425258
Train-Acc: 10: Accuracy: 0.9308760762: precision: 0.8006905210: recall: 0.5031227401: f1: 0.6179498567
11: 6464: loss: 0.1763280635:
11: 12864: loss: 0.1775894526:
11: 19264: loss: 0.1762127118:
11: 25664: loss: 0.1772534268:
11: 32064: loss: 0.1765486587:
11: 38464: loss: 0.1765756166:
11: 44864: loss: 0.1769556124:
11: 51264: loss: 0.1776287434:
11: 57664: loss: 0.1785969086:
11: 64064: loss: 0.1788412543:
11: 70464: loss: 0.1785773494:
11: 76864: loss: 0.1781299657:
11: 83264: loss: 0.1792982526:
11: 89664: loss: 0.1791230123:
11: 96064: loss: 0.1795441939:
11: 102464: loss: 0.1790890257:
11: 108864: loss: 0.1788205006:
11: 115264: loss: 0.1787576277:
11: 121664: loss: 0.1787381100:
11: 128064: loss: 0.1781927569:
11: 134464: loss: 0.1781263167:
Dev-Acc: 11: Accuracy: 0.9277861714: precision: 0.3930812797: recall: 0.4366604319: f1: 0.4137264379
Train-Acc: 11: Accuracy: 0.9321762919: precision: 0.8076204319: recall: 0.5114062192: f1: 0.6262528680
12: 6464: loss: 0.1885265716:
12: 12864: loss: 0.1859227755:
12: 19264: loss: 0.1814899987:
12: 25664: loss: 0.1787598661:
12: 32064: loss: 0.1790477803:
12: 38464: loss: 0.1779525744:
12: 44864: loss: 0.1775684402:
12: 51264: loss: 0.1773161656:
12: 57664: loss: 0.1763966137:
12: 64064: loss: 0.1759984934:
12: 70464: loss: 0.1758772788:
12: 76864: loss: 0.1755812625:
12: 83264: loss: 0.1748142216:
12: 89664: loss: 0.1746837106:
12: 96064: loss: 0.1744078608:
12: 102464: loss: 0.1743833366:
12: 108864: loss: 0.1742994511:
12: 115264: loss: 0.1746483196:
12: 121664: loss: 0.1750089264:
12: 128064: loss: 0.1749745025:
12: 134464: loss: 0.1747498932:
Dev-Acc: 12: Accuracy: 0.9256727099: precision: 0.3814782097: recall: 0.4405713314: f1: 0.4089008128
Train-Acc: 12: Accuracy: 0.9340608716: precision: 0.8105041173: recall: 0.5306028532: f1: 0.6413445111
13: 6464: loss: 0.1725927120:
13: 12864: loss: 0.1758250543:
13: 19264: loss: 0.1746849044:
13: 25664: loss: 0.1737874312:
13: 32064: loss: 0.1747325891:
13: 38464: loss: 0.1717250953:
13: 44864: loss: 0.1713761742:
13: 51264: loss: 0.1702603521:
13: 57664: loss: 0.1712776316:
13: 64064: loss: 0.1706074426:
13: 70464: loss: 0.1710027117:
13: 76864: loss: 0.1719209982:
13: 83264: loss: 0.1722943340:
13: 89664: loss: 0.1712531805:
13: 96064: loss: 0.1717585079:
13: 102464: loss: 0.1718527296:
13: 108864: loss: 0.1720163755:
13: 115264: loss: 0.1719596606:
13: 121664: loss: 0.1715403639:
13: 128064: loss: 0.1713681400:
13: 134464: loss: 0.1712295076:
Dev-Acc: 13: Accuracy: 0.9241546392: precision: 0.3731837146: recall: 0.4410814487: f1: 0.4043017456
Train-Acc: 13: Accuracy: 0.9358212948: precision: 0.8149201059: recall: 0.5465123923: f1: 0.6542578309
14: 6464: loss: 0.1802553377:
14: 12864: loss: 0.1743257327:
14: 19264: loss: 0.1726495911:
14: 25664: loss: 0.1710746006:
14: 32064: loss: 0.1697580740:
14: 38464: loss: 0.1689101779:
14: 44864: loss: 0.1692538131:
14: 51264: loss: 0.1685625424:
14: 57664: loss: 0.1681473097:
14: 64064: loss: 0.1681313834:
14: 70464: loss: 0.1687532523:
14: 76864: loss: 0.1682993778:
14: 83264: loss: 0.1683564170:
14: 89664: loss: 0.1682314958:
14: 96064: loss: 0.1680630624:
14: 102464: loss: 0.1687651553:
14: 108864: loss: 0.1689514068:
14: 115264: loss: 0.1690212782:
14: 121664: loss: 0.1690972523:
14: 128064: loss: 0.1692777974:
14: 134464: loss: 0.1687657708:
Dev-Acc: 14: Accuracy: 0.9228944778: precision: 0.3665254237: recall: 0.4412514878: f1: 0.4004320654
Train-Acc: 14: Accuracy: 0.9370411634: precision: 0.8206849582: recall: 0.5545329038: f1: 0.6618541332
15: 6464: loss: 0.1613997418:
15: 12864: loss: 0.1697469067:
15: 19264: loss: 0.1699008284:
15: 25664: loss: 0.1706721129:
15: 32064: loss: 0.1717113983:
15: 38464: loss: 0.1692682030:
15: 44864: loss: 0.1680668072:
15: 51264: loss: 0.1672763953:
15: 57664: loss: 0.1665322160:
15: 64064: loss: 0.1665645490:
15: 70464: loss: 0.1662709518:
15: 76864: loss: 0.1664211506:
15: 83264: loss: 0.1665401593:
15: 89664: loss: 0.1658515888:
15: 96064: loss: 0.1655943907:
15: 102464: loss: 0.1654405614:
15: 108864: loss: 0.1654075927:
15: 115264: loss: 0.1652260460:
15: 121664: loss: 0.1653012568:
15: 128064: loss: 0.1651851163:
15: 134464: loss: 0.1655216271:
Dev-Acc: 15: Accuracy: 0.9220213294: precision: 0.3613292204: recall: 0.4381907839: f1: 0.3960654730
Train-Acc: 15: Accuracy: 0.9384729266: precision: 0.8301556420: recall: 0.5610413517: f1: 0.6695696520
16: 6464: loss: 0.1561645263:
16: 12864: loss: 0.1589410366:
16: 19264: loss: 0.1645618229:
16: 25664: loss: 0.1643115028:
16: 32064: loss: 0.1639309912:
16: 38464: loss: 0.1637369081:
16: 44864: loss: 0.1633024705:
16: 51264: loss: 0.1621999838:
16: 57664: loss: 0.1630527579:
16: 64064: loss: 0.1634608841:
16: 70464: loss: 0.1635380019:
16: 76864: loss: 0.1640215068:
16: 83264: loss: 0.1638183281:
16: 89664: loss: 0.1644934886:
16: 96064: loss: 0.1641796519:
16: 102464: loss: 0.1645126624:
16: 108864: loss: 0.1642210734:
16: 115264: loss: 0.1638558000:
16: 121664: loss: 0.1631355646:
16: 128064: loss: 0.1633808711:
16: 134464: loss: 0.1628275476:
Dev-Acc: 16: Accuracy: 0.9209596515: precision: 0.3562663725: recall: 0.4393810576: f1: 0.3934825643
Train-Acc: 16: Accuracy: 0.9395905137: precision: 0.8351521004: recall: 0.5685359279: f1: 0.6765235078
17: 6464: loss: 0.1564304231:
17: 12864: loss: 0.1554508332:
17: 19264: loss: 0.1577855607:
17: 25664: loss: 0.1601550324:
17: 32064: loss: 0.1599602004:
17: 38464: loss: 0.1595243933:
17: 44864: loss: 0.1591367475:
17: 51264: loss: 0.1602606735:
17: 57664: loss: 0.1597907916:
17: 64064: loss: 0.1602088712:
17: 70464: loss: 0.1605635980:
17: 76864: loss: 0.1605208623:
17: 83264: loss: 0.1611232066:
17: 89664: loss: 0.1607851070:
17: 96064: loss: 0.1614136098:
17: 102464: loss: 0.1608535889:
17: 108864: loss: 0.1601893580:
17: 115264: loss: 0.1597294715:
17: 121664: loss: 0.1598001020:
17: 128064: loss: 0.1598219376:
17: 134464: loss: 0.1600809302:
Dev-Acc: 17: Accuracy: 0.9190049767: precision: 0.3472556894: recall: 0.4410814487: f1: 0.3885851247
Train-Acc: 17: Accuracy: 0.9404889941: precision: 0.8347867299: recall: 0.5789888896: f1: 0.6837467490
18: 6464: loss: 0.1581535874:
18: 12864: loss: 0.1605274128:
18: 19264: loss: 0.1632711846:
18: 25664: loss: 0.1625371988:
18: 32064: loss: 0.1619406644:
18: 38464: loss: 0.1606047948:
18: 44864: loss: 0.1595432678:
18: 51264: loss: 0.1593120779:
18: 57664: loss: 0.1587255840:
18: 64064: loss: 0.1598746364:
18: 70464: loss: 0.1606048697:
18: 76864: loss: 0.1601795984:
18: 83264: loss: 0.1598591788:
18: 89664: loss: 0.1594328297:
18: 96064: loss: 0.1596980180:
18: 102464: loss: 0.1598108943:
18: 108864: loss: 0.1597958301:
18: 115264: loss: 0.1592596408:
18: 121664: loss: 0.1589318194:
18: 128064: loss: 0.1583700471:
18: 134464: loss: 0.1579659160:
Dev-Acc: 18: Accuracy: 0.9179731011: precision: 0.3428194993: recall: 0.4424417616: f1: 0.3863113355
Train-Acc: 18: Accuracy: 0.9412997961: precision: 0.8365700347: recall: 0.5862204983: f1: 0.6893699266
19: 6464: loss: 0.1591177252:
19: 12864: loss: 0.1618099037:
19: 19264: loss: 0.1642436003:
19: 25664: loss: 0.1643040993:
19: 32064: loss: 0.1617705444:
19: 38464: loss: 0.1608826697:
19: 44864: loss: 0.1594540477:
19: 51264: loss: 0.1582998701:
19: 57664: loss: 0.1571252890:
19: 64064: loss: 0.1571681839:
19: 70464: loss: 0.1569690114:
19: 76864: loss: 0.1566369779:
19: 83264: loss: 0.1571776555:
19: 89664: loss: 0.1570699231:
19: 96064: loss: 0.1565798163:
19: 102464: loss: 0.1561643707:
19: 108864: loss: 0.1561122550:
19: 115264: loss: 0.1563218485:
19: 121664: loss: 0.1557866527:
19: 128064: loss: 0.1555552023:
19: 134464: loss: 0.1559448301:
Dev-Acc: 19: Accuracy: 0.9165839553: precision: 0.3368217054: recall: 0.4432919572: f1: 0.3827912782
Train-Acc: 19: Accuracy: 0.9419645071: precision: 0.8367630701: recall: 0.5934521070: f1: 0.6944113235
20: 6464: loss: 0.1592650832:
20: 12864: loss: 0.1529413767:
20: 19264: loss: 0.1494426470:
20: 25664: loss: 0.1502505436:
20: 32064: loss: 0.1502319179:
20: 38464: loss: 0.1487365264:
20: 44864: loss: 0.1501930885:
20: 51264: loss: 0.1499095105:
20: 57664: loss: 0.1503546161:
20: 64064: loss: 0.1509003852:
20: 70464: loss: 0.1521466625:
20: 76864: loss: 0.1524506053:
20: 83264: loss: 0.1522533886:
20: 89664: loss: 0.1527552112:
20: 96064: loss: 0.1530170504:
20: 102464: loss: 0.1529296481:
20: 108864: loss: 0.1532745758:
20: 115264: loss: 0.1536517410:
20: 121664: loss: 0.1538410490:
20: 128064: loss: 0.1535871626:
20: 134464: loss: 0.1536204965:
Dev-Acc: 20: Accuracy: 0.9163061380: precision: 0.3353108073: recall: 0.4421016834: f1: 0.3813714705
Train-Acc: 20: Accuracy: 0.9426803589: precision: 0.8443052179: recall: 0.5935835908: f1: 0.6970855047
