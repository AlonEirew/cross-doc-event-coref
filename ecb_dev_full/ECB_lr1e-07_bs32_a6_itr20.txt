1: 3232: loss: 0.7068718487:
1: 6432: loss: 0.7063348055:
1: 9632: loss: 0.7051870642:
1: 12832: loss: 0.7043769459:
1: 16032: loss: 0.7034020436:
1: 19232: loss: 0.7026152936:
1: 22432: loss: 0.7020176870:
1: 25632: loss: 0.7013365881:
1: 28832: loss: 0.7005833348:
1: 32032: loss: 0.6996372836:
1: 35232: loss: 0.6989054259:
1: 38432: loss: 0.6982270889:
1: 41632: loss: 0.6974227142:
1: 44832: loss: 0.6967066399:
1: 48032: loss: 0.6958791595:
1: 51232: loss: 0.6951980587:
1: 54432: loss: 0.6944344595:
1: 57632: loss: 0.6938123621:
1: 60832: loss: 0.6930908548:
1: 64032: loss: 0.6923712539:
1: 67232: loss: 0.6916680370:
1: 70432: loss: 0.6909044956:
1: 73632: loss: 0.6902155048:
1: 76832: loss: 0.6894534286:
1: 80032: loss: 0.6887057102:
1: 83232: loss: 0.6879535543:
1: 86432: loss: 0.6872804705:
1: 89632: loss: 0.6865139532:
1: 92832: loss: 0.6857803238:
1: 96032: loss: 0.6850452602:
1: 99232: loss: 0.6843352371:
1: 102432: loss: 0.6836367743:
1: 105632: loss: 0.6829412204:
Dev-Acc: 1: Accuracy: 0.8600472212: precision: 0.0878933654: recall: 0.1491242986: f1: 0.1105996595
Train-Acc: 1: Accuracy: 0.8272960186: precision: 0.3247684164: recall: 0.1936098876: f1: 0.2425964826
2: 3232: loss: 0.6587429065:
2: 6432: loss: 0.6576669326:
2: 9632: loss: 0.6572669635:
2: 12832: loss: 0.6563652489:
2: 16032: loss: 0.6555088596:
2: 19232: loss: 0.6549285064:
2: 22432: loss: 0.6541985283:
2: 25632: loss: 0.6536529309:
2: 28832: loss: 0.6527620879:
2: 32032: loss: 0.6521026273:
2: 35232: loss: 0.6515169558:
2: 38432: loss: 0.6508782626:
2: 41632: loss: 0.6502962634:
2: 44832: loss: 0.6496062143:
2: 48032: loss: 0.6489623110:
2: 51232: loss: 0.6484179424:
2: 54432: loss: 0.6477227955:
2: 57632: loss: 0.6470837800:
2: 60832: loss: 0.6465121919:
2: 64032: loss: 0.6458098465:
2: 67232: loss: 0.6451189385:
2: 70432: loss: 0.6445135020:
2: 73632: loss: 0.6438366368:
2: 76832: loss: 0.6432224937:
2: 80032: loss: 0.6425408731:
2: 83232: loss: 0.6418656338:
2: 86432: loss: 0.6412339460:
2: 89632: loss: 0.6406013743:
2: 92832: loss: 0.6399182392:
2: 96032: loss: 0.6393259588:
2: 99232: loss: 0.6385582751:
2: 102432: loss: 0.6379182788:
2: 105632: loss: 0.6372641832:
Dev-Acc: 2: Accuracy: 0.9410322905: precision: 0.1900000000: recall: 0.0032307431: f1: 0.0063534526
Train-Acc: 2: Accuracy: 0.8579787016: precision: 0.6630036630: recall: 0.0118992834: f1: 0.0233789718
3: 3232: loss: 0.6129398179:
3: 6432: loss: 0.6137961781:
3: 9632: loss: 0.6134730367:
3: 12832: loss: 0.6118154620:
3: 16032: loss: 0.6114015363:
3: 19232: loss: 0.6110175385:
3: 22432: loss: 0.6103998379:
3: 25632: loss: 0.6099538325:
3: 28832: loss: 0.6092370583:
3: 32032: loss: 0.6088357086:
3: 35232: loss: 0.6081095109:
3: 38432: loss: 0.6075546627:
3: 41632: loss: 0.6066881796:
3: 44832: loss: 0.6061751306:
3: 48032: loss: 0.6055881344:
3: 51232: loss: 0.6051155221:
3: 54432: loss: 0.6046955782:
3: 57632: loss: 0.6039704447:
3: 60832: loss: 0.6035856763:
3: 64032: loss: 0.6029493630:
3: 67232: loss: 0.6023550700:
3: 70432: loss: 0.6017623415:
3: 73632: loss: 0.6011719044:
3: 76832: loss: 0.6005154114:
3: 80032: loss: 0.5998519908:
3: 83232: loss: 0.5993177587:
3: 86432: loss: 0.5988095785:
3: 89632: loss: 0.5983046898:
3: 92832: loss: 0.5976726090:
3: 96032: loss: 0.5970312935:
3: 99232: loss: 0.5964674143:
3: 102432: loss: 0.5956604075:
3: 105632: loss: 0.5950957160:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8572273850: precision: 1.0000000000: recall: 0.0005916771: f1: 0.0011826544
4: 3232: loss: 0.5762108192:
4: 6432: loss: 0.5716735421:
4: 9632: loss: 0.5724200297:
4: 12832: loss: 0.5726130948:
4: 16032: loss: 0.5713787253:
4: 19232: loss: 0.5703684408:
4: 22432: loss: 0.5697026218:
4: 25632: loss: 0.5694229767:
4: 28832: loss: 0.5695345053:
4: 32032: loss: 0.5687375199:
4: 35232: loss: 0.5684481990:
4: 38432: loss: 0.5678690212:
4: 41632: loss: 0.5670865710:
4: 44832: loss: 0.5666102160:
4: 48032: loss: 0.5658600673:
4: 51232: loss: 0.5652914915:
4: 54432: loss: 0.5648695663:
4: 57632: loss: 0.5640836221:
4: 60832: loss: 0.5633107776:
4: 64032: loss: 0.5630218850:
4: 67232: loss: 0.5624409177:
4: 70432: loss: 0.5617714113:
4: 73632: loss: 0.5609945224:
4: 76832: loss: 0.5605263906:
4: 80032: loss: 0.5601058247:
4: 83232: loss: 0.5596435941:
4: 86432: loss: 0.5590088209:
4: 89632: loss: 0.5583871027:
4: 92832: loss: 0.5578169986:
4: 96032: loss: 0.5571121256:
4: 99232: loss: 0.5567250018:
4: 102432: loss: 0.5561283642:
4: 105632: loss: 0.5556418917:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.5370616111:
5: 6432: loss: 0.5362548937:
5: 9632: loss: 0.5346345790:
5: 12832: loss: 0.5345730250:
5: 16032: loss: 0.5345986465:
5: 19232: loss: 0.5342916504:
5: 22432: loss: 0.5333658345:
5: 25632: loss: 0.5327492965:
5: 28832: loss: 0.5324200148:
5: 32032: loss: 0.5315462230:
5: 35232: loss: 0.5313097426:
5: 38432: loss: 0.5308395737:
5: 41632: loss: 0.5300542141:
5: 44832: loss: 0.5295001494:
5: 48032: loss: 0.5287713779:
5: 51232: loss: 0.5283403089:
5: 54432: loss: 0.5278696858:
5: 57632: loss: 0.5276512081:
5: 60832: loss: 0.5271672494:
5: 64032: loss: 0.5263853446:
5: 67232: loss: 0.5260962964:
5: 70432: loss: 0.5256759098:
5: 73632: loss: 0.5252537145:
5: 76832: loss: 0.5245260332:
5: 80032: loss: 0.5238775268:
5: 83232: loss: 0.5234151664:
5: 86432: loss: 0.5230241466:
5: 89632: loss: 0.5224885605:
5: 92832: loss: 0.5220758053:
5: 96032: loss: 0.5212234771:
5: 99232: loss: 0.5205895146:
5: 102432: loss: 0.5200446737:
5: 105632: loss: 0.5195625265:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.5048070127:
6: 6432: loss: 0.5033156431:
6: 9632: loss: 0.5025130797:
6: 12832: loss: 0.5006808048:
6: 16032: loss: 0.5007488001:
6: 19232: loss: 0.4992941767:
6: 22432: loss: 0.4989577052:
6: 25632: loss: 0.4983061352:
6: 28832: loss: 0.4980483121:
6: 32032: loss: 0.4977427047:
6: 35232: loss: 0.4970505875:
6: 38432: loss: 0.4963972665:
6: 41632: loss: 0.4963018680:
6: 44832: loss: 0.4951643721:
6: 48032: loss: 0.4945136373:
6: 51232: loss: 0.4941141030:
6: 54432: loss: 0.4932189271:
6: 57632: loss: 0.4924624805:
6: 60832: loss: 0.4920531602:
6: 64032: loss: 0.4915924693:
6: 67232: loss: 0.4913432750:
6: 70432: loss: 0.4911633508:
6: 73632: loss: 0.4908202838:
6: 76832: loss: 0.4903897140:
6: 80032: loss: 0.4899198917:
6: 83232: loss: 0.4890731734:
6: 86432: loss: 0.4888189655:
6: 89632: loss: 0.4883733052:
6: 92832: loss: 0.4881476301:
6: 96032: loss: 0.4875461150:
6: 99232: loss: 0.4868289667:
6: 102432: loss: 0.4866341870:
6: 105632: loss: 0.4862828884:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.4720939782:
7: 6432: loss: 0.4686055495:
7: 9632: loss: 0.4721983637:
7: 12832: loss: 0.4704418100:
7: 16032: loss: 0.4687574446:
7: 19232: loss: 0.4692451298:
7: 22432: loss: 0.4695983091:
7: 25632: loss: 0.4685908314:
7: 28832: loss: 0.4678133228:
7: 32032: loss: 0.4676779708:
7: 35232: loss: 0.4680698134:
7: 38432: loss: 0.4669119481:
7: 41632: loss: 0.4660446716:
7: 44832: loss: 0.4658525017:
7: 48032: loss: 0.4651171208:
7: 51232: loss: 0.4646339244:
7: 54432: loss: 0.4643358862:
7: 57632: loss: 0.4639269865:
7: 60832: loss: 0.4632137231:
7: 64032: loss: 0.4631759924:
7: 67232: loss: 0.4628625713:
7: 70432: loss: 0.4623471721:
7: 73632: loss: 0.4615282708:
7: 76832: loss: 0.4612109643:
7: 80032: loss: 0.4604957135:
7: 83232: loss: 0.4602811543:
7: 86432: loss: 0.4596224919:
7: 89632: loss: 0.4590183559:
7: 92832: loss: 0.4584503940:
7: 96032: loss: 0.4579554511:
7: 99232: loss: 0.4577660041:
7: 102432: loss: 0.4574266123:
7: 105632: loss: 0.4570418854:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.4458869594:
8: 6432: loss: 0.4438831809:
8: 9632: loss: 0.4430794855:
8: 12832: loss: 0.4426062220:
8: 16032: loss: 0.4411635557:
8: 19232: loss: 0.4385517200:
8: 22432: loss: 0.4383957025:
8: 25632: loss: 0.4391334440:
8: 28832: loss: 0.4386181861:
8: 32032: loss: 0.4380065192:
8: 35232: loss: 0.4362627312:
8: 38432: loss: 0.4359399695:
8: 41632: loss: 0.4366537909:
8: 44832: loss: 0.4367519407:
8: 48032: loss: 0.4363661947:
8: 51232: loss: 0.4359027353:
8: 54432: loss: 0.4353007161:
8: 57632: loss: 0.4353725866:
8: 60832: loss: 0.4351777747:
8: 64032: loss: 0.4348722023:
8: 67232: loss: 0.4347781401:
8: 70432: loss: 0.4345148066:
8: 73632: loss: 0.4343228598:
8: 76832: loss: 0.4338135642:
8: 80032: loss: 0.4339900314:
8: 83232: loss: 0.4341797362:
8: 86432: loss: 0.4336397067:
8: 89632: loss: 0.4335788366:
8: 92832: loss: 0.4329023525:
8: 96032: loss: 0.4322171025:
8: 99232: loss: 0.4317986481:
8: 102432: loss: 0.4314457287:
8: 105632: loss: 0.4310399482:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.4125023463:
9: 6432: loss: 0.4134137392:
9: 9632: loss: 0.4130690074:
9: 12832: loss: 0.4131271937:
9: 16032: loss: 0.4142498986:
9: 19232: loss: 0.4139981640:
9: 22432: loss: 0.4141577742:
9: 25632: loss: 0.4147545270:
9: 28832: loss: 0.4145029554:
9: 32032: loss: 0.4146071126:
9: 35232: loss: 0.4139382806:
9: 38432: loss: 0.4145528535:
9: 41632: loss: 0.4138892632:
9: 44832: loss: 0.4142745963:
9: 48032: loss: 0.4139026278:
9: 51232: loss: 0.4140584313:
9: 54432: loss: 0.4139765518:
9: 57632: loss: 0.4133523202:
9: 60832: loss: 0.4135300099:
9: 64032: loss: 0.4126261076:
9: 67232: loss: 0.4125859968:
9: 70432: loss: 0.4119562164:
9: 73632: loss: 0.4107854915:
9: 76832: loss: 0.4108528607:
9: 80032: loss: 0.4103966280:
9: 83232: loss: 0.4103184692:
9: 86432: loss: 0.4105266112:
9: 89632: loss: 0.4102779482:
9: 92832: loss: 0.4100846276:
9: 96032: loss: 0.4096590402:
9: 99232: loss: 0.4093497475:
9: 102432: loss: 0.4092433450:
9: 105632: loss: 0.4090730626:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.3954370221:
10: 6432: loss: 0.3962918787:
10: 9632: loss: 0.3993481532:
10: 12832: loss: 0.3960104250:
10: 16032: loss: 0.3965488466:
10: 19232: loss: 0.3968276992:
10: 22432: loss: 0.3971728802:
10: 25632: loss: 0.3957767515:
10: 28832: loss: 0.3964426686:
10: 32032: loss: 0.3962003875:
10: 35232: loss: 0.3951460872:
10: 38432: loss: 0.3944982744:
10: 41632: loss: 0.3948833831:
10: 44832: loss: 0.3942188212:
10: 48032: loss: 0.3931482792:
10: 51232: loss: 0.3933742220:
10: 54432: loss: 0.3923484337:
10: 57632: loss: 0.3920259683:
10: 60832: loss: 0.3928849171:
10: 64032: loss: 0.3927434755:
10: 67232: loss: 0.3924847126:
10: 70432: loss: 0.3920842287:
10: 73632: loss: 0.3922508735:
10: 76832: loss: 0.3922432759:
10: 80032: loss: 0.3928352956:
10: 83232: loss: 0.3927235470:
10: 86432: loss: 0.3927157641:
10: 89632: loss: 0.3925734112:
10: 92832: loss: 0.3918367662:
10: 96032: loss: 0.3911300346:
10: 99232: loss: 0.3908810273:
10: 102432: loss: 0.3905175814:
10: 105632: loss: 0.3901350527:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.3791150874:
11: 6432: loss: 0.3726468806:
11: 9632: loss: 0.3793514772:
11: 12832: loss: 0.3791362965:
11: 16032: loss: 0.3783494027:
11: 19232: loss: 0.3772268900:
11: 22432: loss: 0.3785392705:
11: 25632: loss: 0.3804913141:
11: 28832: loss: 0.3789051932:
11: 32032: loss: 0.3789430653:
11: 35232: loss: 0.3793312378:
11: 38432: loss: 0.3792866423:
11: 41632: loss: 0.3791527481:
11: 44832: loss: 0.3785942515:
11: 48032: loss: 0.3782545004:
11: 51232: loss: 0.3777292441:
11: 54432: loss: 0.3779184096:
11: 57632: loss: 0.3776318126:
11: 60832: loss: 0.3778644403:
11: 64032: loss: 0.3774693794:
11: 67232: loss: 0.3775077777:
11: 70432: loss: 0.3772418466:
11: 73632: loss: 0.3764885063:
11: 76832: loss: 0.3763075636:
11: 80032: loss: 0.3760124375:
11: 83232: loss: 0.3760380511:
11: 86432: loss: 0.3758521493:
11: 89632: loss: 0.3755726853:
11: 92832: loss: 0.3755956789:
11: 96032: loss: 0.3749323709:
11: 99232: loss: 0.3745551779:
11: 102432: loss: 0.3742210398:
11: 105632: loss: 0.3738181892:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.3616600214:
12: 6432: loss: 0.3683223713:
12: 9632: loss: 0.3702149453:
12: 12832: loss: 0.3701344951:
12: 16032: loss: 0.3698357615:
12: 19232: loss: 0.3680825886:
12: 22432: loss: 0.3675816956:
12: 25632: loss: 0.3687078835:
12: 28832: loss: 0.3683222587:
12: 32032: loss: 0.3683137206:
12: 35232: loss: 0.3673785307:
12: 38432: loss: 0.3659015367:
12: 41632: loss: 0.3647887751:
12: 44832: loss: 0.3644245624:
12: 48032: loss: 0.3637810328:
12: 51232: loss: 0.3634509792:
12: 54432: loss: 0.3636243976:
12: 57632: loss: 0.3630087940:
12: 60832: loss: 0.3625466498:
12: 64032: loss: 0.3626116904:
12: 67232: loss: 0.3618977370:
12: 70432: loss: 0.3616300056:
12: 73632: loss: 0.3612098813:
12: 76832: loss: 0.3612888209:
12: 80032: loss: 0.3613874407:
12: 83232: loss: 0.3606104144:
12: 86432: loss: 0.3607275265:
12: 89632: loss: 0.3602755010:
12: 92832: loss: 0.3600719580:
12: 96032: loss: 0.3601284230:
12: 99232: loss: 0.3599874179:
12: 102432: loss: 0.3599352616:
12: 105632: loss: 0.3596666212:
Dev-Acc: 12: Accuracy: 0.9416673183: precision: 0.7500000000: recall: 0.0005101173: f1: 0.0010195412
Train-Acc: 12: Accuracy: 0.8575279117: precision: 1.0000000000: recall: 0.0026954178: f1: 0.0053763441
13: 3232: loss: 0.3529801969:
13: 6432: loss: 0.3581276298:
13: 9632: loss: 0.3596283262:
13: 12832: loss: 0.3596470957:
13: 16032: loss: 0.3563666192:
13: 19232: loss: 0.3576945848:
13: 22432: loss: 0.3546784393:
13: 25632: loss: 0.3544488467:
13: 28832: loss: 0.3552559782:
13: 32032: loss: 0.3547563821:
13: 35232: loss: 0.3542568535:
13: 38432: loss: 0.3525904048:
13: 41632: loss: 0.3517367848:
13: 44832: loss: 0.3516608336:
13: 48032: loss: 0.3514862722:
13: 51232: loss: 0.3510133918:
13: 54432: loss: 0.3494444365:
13: 57632: loss: 0.3490570237:
13: 60832: loss: 0.3484514053:
13: 64032: loss: 0.3482675679:
13: 67232: loss: 0.3475976124:
13: 70432: loss: 0.3481252004:
13: 73632: loss: 0.3478405242:
13: 76832: loss: 0.3481542699:
13: 80032: loss: 0.3478257153:
13: 83232: loss: 0.3481848650:
13: 86432: loss: 0.3480711346:
13: 89632: loss: 0.3481204114:
13: 92832: loss: 0.3479356896:
13: 96032: loss: 0.3478015720:
13: 99232: loss: 0.3477149648:
13: 102432: loss: 0.3475554775:
13: 105632: loss: 0.3472045203:
Dev-Acc: 13: Accuracy: 0.9417467117: precision: 0.8571428571: recall: 0.0020404693: f1: 0.0040712468
Train-Acc: 13: Accuracy: 0.8587676287: precision: 0.9576719577: recall: 0.0118992834: f1: 0.0235064935
14: 3232: loss: 0.3416733831:
14: 6432: loss: 0.3355677386:
14: 9632: loss: 0.3387625180:
14: 12832: loss: 0.3389138978:
14: 16032: loss: 0.3389830133:
14: 19232: loss: 0.3378914269:
14: 22432: loss: 0.3406840601:
14: 25632: loss: 0.3410488172:
14: 28832: loss: 0.3424471455:
14: 32032: loss: 0.3412608263:
14: 35232: loss: 0.3393933469:
14: 38432: loss: 0.3382811473:
14: 41632: loss: 0.3380043673:
14: 44832: loss: 0.3379901250:
14: 48032: loss: 0.3378327500:
14: 51232: loss: 0.3376004601:
14: 54432: loss: 0.3381384998:
14: 57632: loss: 0.3371679291:
14: 60832: loss: 0.3372471677:
14: 64032: loss: 0.3379790836:
14: 67232: loss: 0.3381687777:
14: 70432: loss: 0.3378212795:
14: 73632: loss: 0.3373627131:
14: 76832: loss: 0.3374716472:
14: 80032: loss: 0.3372581552:
14: 83232: loss: 0.3372409647:
14: 86432: loss: 0.3371862395:
14: 89632: loss: 0.3371054943:
14: 92832: loss: 0.3369852239:
14: 96032: loss: 0.3364091219:
14: 99232: loss: 0.3361712591:
14: 102432: loss: 0.3361553427:
14: 105632: loss: 0.3358900671:
Dev-Acc: 14: Accuracy: 0.9421932101: precision: 0.6410256410: recall: 0.0212548886: f1: 0.0411454905
Train-Acc: 14: Accuracy: 0.8638203740: precision: 0.8572864322: recall: 0.0560778384: f1: 0.1052696532
15: 3232: loss: 0.3366952530:
15: 6432: loss: 0.3354621612:
15: 9632: loss: 0.3374299560:
15: 12832: loss: 0.3341612522:
15: 16032: loss: 0.3293822232:
15: 19232: loss: 0.3295005216:
15: 22432: loss: 0.3288515174:
15: 25632: loss: 0.3284720914:
15: 28832: loss: 0.3275800668:
15: 32032: loss: 0.3302158930:
15: 35232: loss: 0.3313404738:
15: 38432: loss: 0.3314624571:
15: 41632: loss: 0.3308553534:
15: 44832: loss: 0.3305351469:
15: 48032: loss: 0.3301443696:
15: 51232: loss: 0.3294748332:
15: 54432: loss: 0.3291355537:
15: 57632: loss: 0.3289154886:
15: 60832: loss: 0.3286258862:
15: 64032: loss: 0.3284552591:
15: 67232: loss: 0.3277065894:
15: 70432: loss: 0.3273555649:
15: 73632: loss: 0.3275293509:
15: 76832: loss: 0.3272186284:
15: 80032: loss: 0.3273032151:
15: 83232: loss: 0.3275052329:
15: 86432: loss: 0.3273993161:
15: 89632: loss: 0.3273764794:
15: 92832: loss: 0.3271032599:
15: 96032: loss: 0.3267773893:
15: 99232: loss: 0.3265367542:
15: 102432: loss: 0.3266300528:
15: 105632: loss: 0.3264755801:
Dev-Acc: 15: Accuracy: 0.9423717856: precision: 0.5758835759: recall: 0.0471008332: f1: 0.0870795347
Train-Acc: 15: Accuracy: 0.8675112724: precision: 0.8694779116: recall: 0.0853987246: f1: 0.1555222987
16: 3232: loss: 0.3174660562:
16: 6432: loss: 0.3185499393:
16: 9632: loss: 0.3214318161:
16: 12832: loss: 0.3226884225:
16: 16032: loss: 0.3199383901:
16: 19232: loss: 0.3199236230:
16: 22432: loss: 0.3185417386:
16: 25632: loss: 0.3193551930:
16: 28832: loss: 0.3199515679:
16: 32032: loss: 0.3199565987:
16: 35232: loss: 0.3197965131:
16: 38432: loss: 0.3197773004:
16: 41632: loss: 0.3192436141:
16: 44832: loss: 0.3188764534:
16: 48032: loss: 0.3195810974:
16: 51232: loss: 0.3191868489:
16: 54432: loss: 0.3192571443:
16: 57632: loss: 0.3189495300:
16: 60832: loss: 0.3187450202:
16: 64032: loss: 0.3192458300:
16: 67232: loss: 0.3195079817:
16: 70432: loss: 0.3193474636:
16: 73632: loss: 0.3186255138:
16: 76832: loss: 0.3181113174:
16: 80032: loss: 0.3178804921:
16: 83232: loss: 0.3177902251:
16: 86432: loss: 0.3175887349:
16: 89632: loss: 0.3175807975:
16: 92832: loss: 0.3173319366:
16: 96032: loss: 0.3169525919:
16: 99232: loss: 0.3175507365:
16: 102432: loss: 0.3174248433:
16: 105632: loss: 0.3175698574:
Dev-Acc: 16: Accuracy: 0.9423122406: precision: 0.5435630689: recall: 0.0710763476: f1: 0.1257142857
Train-Acc: 16: Accuracy: 0.8721695542: precision: 0.8913894325: recall: 0.1197817369: f1: 0.2111851637
17: 3232: loss: 0.3136497158:
17: 6432: loss: 0.3129095512:
17: 9632: loss: 0.3119881186:
17: 12832: loss: 0.3133015312:
17: 16032: loss: 0.3129607967:
17: 19232: loss: 0.3129491006:
17: 22432: loss: 0.3125649874:
17: 25632: loss: 0.3141996825:
17: 28832: loss: 0.3128107193:
17: 32032: loss: 0.3130252381:
17: 35232: loss: 0.3113732232:
17: 38432: loss: 0.3108980211:
17: 41632: loss: 0.3112652310:
17: 44832: loss: 0.3105668728:
17: 48032: loss: 0.3103926098:
17: 51232: loss: 0.3107155835:
17: 54432: loss: 0.3110922729:
17: 57632: loss: 0.3115237642:
17: 60832: loss: 0.3114250486:
17: 64032: loss: 0.3108648527:
17: 67232: loss: 0.3104362210:
17: 70432: loss: 0.3100187675:
17: 73632: loss: 0.3096519987:
17: 76832: loss: 0.3102731273:
17: 80032: loss: 0.3100903445:
17: 83232: loss: 0.3097856921:
17: 86432: loss: 0.3096777478:
17: 89632: loss: 0.3096023606:
17: 92832: loss: 0.3095796553:
17: 96032: loss: 0.3093917478:
17: 99232: loss: 0.3094342483:
17: 102432: loss: 0.3091020417:
17: 105632: loss: 0.3091797997:
Dev-Acc: 17: Accuracy: 0.9424908757: precision: 0.5405921681: recall: 0.0962421357: f1: 0.1633949192
Train-Acc: 17: Accuracy: 0.8750716448: precision: 0.8916700862: recall: 0.1428571429: f1: 0.2462601995
18: 3232: loss: 0.3035776798:
18: 6432: loss: 0.3105614568:
18: 9632: loss: 0.3035307641:
18: 12832: loss: 0.3057743973:
18: 16032: loss: 0.3055526541:
18: 19232: loss: 0.3059320479:
18: 22432: loss: 0.3020726115:
18: 25632: loss: 0.3013589147:
18: 28832: loss: 0.3011574519:
18: 32032: loss: 0.3020250935:
18: 35232: loss: 0.3020256244:
18: 38432: loss: 0.3016613900:
18: 41632: loss: 0.3013731159:
18: 44832: loss: 0.3015466581:
18: 48032: loss: 0.3011211261:
18: 51232: loss: 0.3016388765:
18: 54432: loss: 0.3009410693:
18: 57632: loss: 0.3012763609:
18: 60832: loss: 0.3012074801:
18: 64032: loss: 0.3016104277:
18: 67232: loss: 0.3008831367:
18: 70432: loss: 0.3002530069:
18: 73632: loss: 0.3008151506:
18: 76832: loss: 0.3012967744:
18: 80032: loss: 0.3012996430:
18: 83232: loss: 0.3011485640:
18: 86432: loss: 0.3013513967:
18: 89632: loss: 0.3014902445:
18: 92832: loss: 0.3017075958:
18: 96032: loss: 0.3019911420:
18: 99232: loss: 0.3021948957:
18: 102432: loss: 0.3019133497:
18: 105632: loss: 0.3016085641:
Dev-Acc: 18: Accuracy: 0.9429075718: precision: 0.5415304120: recall: 0.1407923822: f1: 0.2234817814
Train-Acc: 18: Accuracy: 0.8800867796: precision: 0.9059488202: recall: 0.1792124121: f1: 0.2992316136
19: 3232: loss: 0.3117444220:
19: 6432: loss: 0.3031695249:
19: 9632: loss: 0.2998023659:
19: 12832: loss: 0.2999867692:
19: 16032: loss: 0.2968184960:
19: 19232: loss: 0.2953096919:
19: 22432: loss: 0.2963042997:
19: 25632: loss: 0.2966829587:
19: 28832: loss: 0.2969983296:
19: 32032: loss: 0.2976032434:
19: 35232: loss: 0.2961486796:
19: 38432: loss: 0.2962984459:
19: 41632: loss: 0.2965572184:
19: 44832: loss: 0.2970439855:
19: 48032: loss: 0.2965211212:
19: 51232: loss: 0.2963226392:
19: 54432: loss: 0.2953038514:
19: 57632: loss: 0.2949086158:
19: 60832: loss: 0.2950150820:
19: 64032: loss: 0.2944339136:
19: 67232: loss: 0.2940291003:
19: 70432: loss: 0.2944263883:
19: 73632: loss: 0.2948934961:
19: 76832: loss: 0.2950550079:
19: 80032: loss: 0.2954153599:
19: 83232: loss: 0.2954679269:
19: 86432: loss: 0.2959503291:
19: 89632: loss: 0.2954516687:
19: 92832: loss: 0.2949817692:
19: 96032: loss: 0.2952583587:
19: 99232: loss: 0.2951651265:
19: 102432: loss: 0.2945399753:
19: 105632: loss: 0.2949827243:
Dev-Acc: 19: Accuracy: 0.9413399100: precision: 0.4935064935: recall: 0.2003060704: f1: 0.2849540397
Train-Acc: 19: Accuracy: 0.8862665296: precision: 0.9118193891: recall: 0.2256919335: f1: 0.3618254637
20: 3232: loss: 0.2877588826:
20: 6432: loss: 0.2907059794:
20: 9632: loss: 0.2925425748:
20: 12832: loss: 0.2904188075:
20: 16032: loss: 0.2897097543:
20: 19232: loss: 0.2915883809:
20: 22432: loss: 0.2912698137:
20: 25632: loss: 0.2922395242:
20: 28832: loss: 0.2918346384:
20: 32032: loss: 0.2921363388:
20: 35232: loss: 0.2906789526:
20: 38432: loss: 0.2914284016:
20: 41632: loss: 0.2912421707:
20: 44832: loss: 0.2906289599:
20: 48032: loss: 0.2901042378:
20: 51232: loss: 0.2900031965:
20: 54432: loss: 0.2900859015:
20: 57632: loss: 0.2893951414:
20: 60832: loss: 0.2899105233:
20: 64032: loss: 0.2893081639:
20: 67232: loss: 0.2898777325:
20: 70432: loss: 0.2900762891:
20: 73632: loss: 0.2901205287:
20: 76832: loss: 0.2903768600:
20: 80032: loss: 0.2898350340:
20: 83232: loss: 0.2902129628:
20: 86432: loss: 0.2896352418:
20: 89632: loss: 0.2896164607:
20: 92832: loss: 0.2898064739:
20: 96032: loss: 0.2893135849:
20: 99232: loss: 0.2888837507:
20: 102432: loss: 0.2885663070:
20: 105632: loss: 0.2881655274:
Dev-Acc: 20: Accuracy: 0.9358925819: precision: 0.4312144213: recall: 0.3091311002: f1: 0.3601069625
Train-Acc: 20: Accuracy: 0.8908496499: precision: 0.8705347925: recall: 0.2771678391: f1: 0.4204647452
