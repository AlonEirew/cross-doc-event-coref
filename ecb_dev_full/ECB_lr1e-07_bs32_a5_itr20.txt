1: 3232: loss: 0.7057842040:
1: 6432: loss: 0.7043076333:
1: 9632: loss: 0.7036511594:
1: 12832: loss: 0.7033386086:
1: 16032: loss: 0.7025046688:
1: 19232: loss: 0.7016794334:
1: 22432: loss: 0.7008250430:
1: 25632: loss: 0.7001362472:
1: 28832: loss: 0.6994917807:
1: 32032: loss: 0.6985109109:
1: 35232: loss: 0.6977911420:
1: 38432: loss: 0.6972402188:
1: 41632: loss: 0.6965860430:
1: 44832: loss: 0.6958670280:
1: 48032: loss: 0.6952398328:
1: 51232: loss: 0.6945120179:
1: 54432: loss: 0.6937996821:
1: 57632: loss: 0.6931081373:
1: 60832: loss: 0.6924540873:
1: 64032: loss: 0.6917728174:
1: 67232: loss: 0.6911503521:
1: 70432: loss: 0.6905583465:
1: 73632: loss: 0.6898979481:
1: 76832: loss: 0.6892005379:
1: 80032: loss: 0.6885209924:
1: 83232: loss: 0.6878564452:
1: 86432: loss: 0.6871275057:
1: 89632: loss: 0.6864584122:
Dev-Acc: 1: Accuracy: 0.7959398031: precision: 0.0798764090: recall: 0.2373745962: f1: 0.1195307817
Train-Acc: 1: Accuracy: 0.7861415744: precision: 0.3378510654: recall: 0.2949838932: f1: 0.3149656044
2: 3232: loss: 0.6688944215:
2: 6432: loss: 0.6671119860:
2: 9632: loss: 0.6660527160:
2: 12832: loss: 0.6651758781:
2: 16032: loss: 0.6644013994:
2: 19232: loss: 0.6634101392:
2: 22432: loss: 0.6629451739:
2: 25632: loss: 0.6622439877:
2: 28832: loss: 0.6614982182:
2: 32032: loss: 0.6607381576:
2: 35232: loss: 0.6600985309:
2: 38432: loss: 0.6594538544:
2: 41632: loss: 0.6588619659:
2: 44832: loss: 0.6582417426:
2: 48032: loss: 0.6577481253:
2: 51232: loss: 0.6572296495:
2: 54432: loss: 0.6565951829:
2: 57632: loss: 0.6559072293:
2: 60832: loss: 0.6552156281:
2: 64032: loss: 0.6546180242:
2: 67232: loss: 0.6539507313:
2: 70432: loss: 0.6533098977:
2: 73632: loss: 0.6525911925:
2: 76832: loss: 0.6519656031:
2: 80032: loss: 0.6514195209:
2: 83232: loss: 0.6507912462:
2: 86432: loss: 0.6501487206:
2: 89632: loss: 0.6496566051:
Dev-Acc: 2: Accuracy: 0.9342058301: precision: 0.1687279152: recall: 0.0324774698: f1: 0.0544702695
Train-Acc: 2: Accuracy: 0.8393377662: precision: 0.6841397849: recall: 0.0669252515: f1: 0.1219234685
3: 3232: loss: 0.6293685031:
3: 6432: loss: 0.6295385480:
3: 9632: loss: 0.6293741796:
3: 12832: loss: 0.6286639565:
3: 16032: loss: 0.6286415706:
3: 19232: loss: 0.6281488298:
3: 22432: loss: 0.6277265997:
3: 25632: loss: 0.6271804912:
3: 28832: loss: 0.6268002048:
3: 32032: loss: 0.6262453102:
3: 35232: loss: 0.6257039958:
3: 38432: loss: 0.6251742196:
3: 41632: loss: 0.6245342910:
3: 44832: loss: 0.6238712287:
3: 48032: loss: 0.6232409333:
3: 51232: loss: 0.6226817180:
3: 54432: loss: 0.6220791301:
3: 57632: loss: 0.6214575022:
3: 60832: loss: 0.6208626964:
3: 64032: loss: 0.6202541217:
3: 67232: loss: 0.6196069610:
3: 70432: loss: 0.6189300850:
3: 73632: loss: 0.6183992766:
3: 76832: loss: 0.6178378687:
3: 80032: loss: 0.6172256137:
3: 83232: loss: 0.6165813898:
3: 86432: loss: 0.6159977490:
3: 89632: loss: 0.6153806442:
Dev-Acc: 3: Accuracy: 0.9415184855: precision: 0.2173913043: recall: 0.0008501955: f1: 0.0016937669
Train-Acc: 3: Accuracy: 0.8345167041: precision: 0.9354838710: recall: 0.0076260601: f1: 0.0151287903
4: 3232: loss: 0.5982389212:
4: 6432: loss: 0.5982936183:
4: 9632: loss: 0.5973033776:
4: 12832: loss: 0.5964277214:
4: 16032: loss: 0.5960408026:
4: 19232: loss: 0.5957728891:
4: 22432: loss: 0.5952613069:
4: 25632: loss: 0.5947806361:
4: 28832: loss: 0.5937822569:
4: 32032: loss: 0.5929926913:
4: 35232: loss: 0.5921903521:
4: 38432: loss: 0.5918160895:
4: 41632: loss: 0.5910888774:
4: 44832: loss: 0.5904809059:
4: 48032: loss: 0.5900961629:
4: 51232: loss: 0.5896294459:
4: 54432: loss: 0.5891189311:
4: 57632: loss: 0.5885701454:
4: 60832: loss: 0.5880829718:
4: 64032: loss: 0.5874104917:
4: 67232: loss: 0.5869979714:
4: 70432: loss: 0.5864064205:
4: 73632: loss: 0.5856911370:
4: 76832: loss: 0.5850773303:
4: 80032: loss: 0.5845811961:
4: 83232: loss: 0.5839373165:
4: 86432: loss: 0.5833374548:
4: 89632: loss: 0.5828123434:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8334757686: precision: 1.0000000000: recall: 0.0008546447: f1: 0.0017078297
5: 3232: loss: 0.5647611409:
5: 6432: loss: 0.5647628248:
5: 9632: loss: 0.5643628212:
5: 12832: loss: 0.5641697764:
5: 16032: loss: 0.5640128899:
5: 19232: loss: 0.5639778792:
5: 22432: loss: 0.5628990511:
5: 25632: loss: 0.5627633541:
5: 28832: loss: 0.5621112841:
5: 32032: loss: 0.5619837182:
5: 35232: loss: 0.5611522568:
5: 38432: loss: 0.5605965307:
5: 41632: loss: 0.5599218076:
5: 44832: loss: 0.5593572794:
5: 48032: loss: 0.5586355448:
5: 51232: loss: 0.5582848011:
5: 54432: loss: 0.5577438414:
5: 57632: loss: 0.5571493113:
5: 60832: loss: 0.5564154092:
5: 64032: loss: 0.5560263104:
5: 67232: loss: 0.5554008224:
5: 70432: loss: 0.5549507986:
5: 73632: loss: 0.5544941918:
5: 76832: loss: 0.5540142531:
5: 80032: loss: 0.5536988806:
5: 83232: loss: 0.5532539718:
5: 86432: loss: 0.5527830946:
5: 89632: loss: 0.5523434987:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.5404741374:
6: 6432: loss: 0.5391964673:
6: 9632: loss: 0.5381241044:
6: 12832: loss: 0.5359076155:
6: 16032: loss: 0.5348869755:
6: 19232: loss: 0.5339136941:
6: 22432: loss: 0.5335706996:
6: 25632: loss: 0.5336383215:
6: 28832: loss: 0.5326138707:
6: 32032: loss: 0.5319139607:
6: 35232: loss: 0.5313857396:
6: 38432: loss: 0.5306141801:
6: 41632: loss: 0.5310651784:
6: 44832: loss: 0.5301996100:
6: 48032: loss: 0.5299838808:
6: 51232: loss: 0.5297069040:
6: 54432: loss: 0.5289677365:
6: 57632: loss: 0.5286244009:
6: 60832: loss: 0.5282241409:
6: 64032: loss: 0.5275526288:
6: 67232: loss: 0.5268468014:
6: 70432: loss: 0.5266463574:
6: 73632: loss: 0.5263109500:
6: 76832: loss: 0.5259733907:
6: 80032: loss: 0.5256845057:
6: 83232: loss: 0.5252306476:
6: 86432: loss: 0.5245417493:
6: 89632: loss: 0.5237876146:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.5135271013:
7: 6432: loss: 0.5129389592:
7: 9632: loss: 0.5145278208:
7: 12832: loss: 0.5128032616:
7: 16032: loss: 0.5115547762:
7: 19232: loss: 0.5106246628:
7: 22432: loss: 0.5092289175:
7: 25632: loss: 0.5082095824:
7: 28832: loss: 0.5077229911:
7: 32032: loss: 0.5071654066:
7: 35232: loss: 0.5065088186:
7: 38432: loss: 0.5054506975:
7: 41632: loss: 0.5047156486:
7: 44832: loss: 0.5040195227:
7: 48032: loss: 0.5031692344:
7: 51232: loss: 0.5028915346:
7: 54432: loss: 0.5022314347:
7: 57632: loss: 0.5015225598:
7: 60832: loss: 0.5008120121:
7: 64032: loss: 0.5001780280:
7: 67232: loss: 0.4997464761:
7: 70432: loss: 0.4995734559:
7: 73632: loss: 0.4992392957:
7: 76832: loss: 0.4987797884:
7: 80032: loss: 0.4982400176:
7: 83232: loss: 0.4978183393:
7: 86432: loss: 0.4975876805:
7: 89632: loss: 0.4970444133:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.4753348020:
8: 6432: loss: 0.4786903267:
8: 9632: loss: 0.4804281357:
8: 12832: loss: 0.4800939190:
8: 16032: loss: 0.4796852309:
8: 19232: loss: 0.4799545097:
8: 22432: loss: 0.4792877989:
8: 25632: loss: 0.4798974258:
8: 28832: loss: 0.4799538567:
8: 32032: loss: 0.4795732526:
8: 35232: loss: 0.4794127636:
8: 38432: loss: 0.4788283430:
8: 41632: loss: 0.4788881394:
8: 44832: loss: 0.4781385205:
8: 48032: loss: 0.4778554067:
8: 51232: loss: 0.4777776070:
8: 54432: loss: 0.4774575814:
8: 57632: loss: 0.4771779904:
8: 60832: loss: 0.4764493498:
8: 64032: loss: 0.4762932351:
8: 67232: loss: 0.4757568564:
8: 70432: loss: 0.4753824681:
8: 73632: loss: 0.4754519775:
8: 76832: loss: 0.4751411272:
8: 80032: loss: 0.4741479420:
8: 83232: loss: 0.4738377384:
8: 86432: loss: 0.4735734983:
8: 89632: loss: 0.4729777572:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.4704354191:
9: 6432: loss: 0.4650535563:
9: 9632: loss: 0.4617267169:
9: 12832: loss: 0.4598503642:
9: 16032: loss: 0.4596204357:
9: 19232: loss: 0.4593071255:
9: 22432: loss: 0.4592677572:
9: 25632: loss: 0.4596939251:
9: 28832: loss: 0.4594717866:
9: 32032: loss: 0.4586980315:
9: 35232: loss: 0.4572761941:
9: 38432: loss: 0.4573860011:
9: 41632: loss: 0.4570566593:
9: 44832: loss: 0.4574356857:
9: 48032: loss: 0.4566084322:
9: 51232: loss: 0.4560049805:
9: 54432: loss: 0.4553238932:
9: 57632: loss: 0.4544721366:
9: 60832: loss: 0.4538506320:
9: 64032: loss: 0.4537360575:
9: 67232: loss: 0.4530323672:
9: 70432: loss: 0.4527428616:
9: 73632: loss: 0.4527307926:
9: 76832: loss: 0.4521578424:
9: 80032: loss: 0.4519542570:
9: 83232: loss: 0.4515290026:
9: 86432: loss: 0.4509416634:
9: 89632: loss: 0.4505991688:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.4297407085:
10: 6432: loss: 0.4310905473:
10: 9632: loss: 0.4340729338:
10: 12832: loss: 0.4364196571:
10: 16032: loss: 0.4381716099:
10: 19232: loss: 0.4355628335:
10: 22432: loss: 0.4345434523:
10: 25632: loss: 0.4346480843:
10: 28832: loss: 0.4345537532:
10: 32032: loss: 0.4340394955:
10: 35232: loss: 0.4343262735:
10: 38432: loss: 0.4339860937:
10: 41632: loss: 0.4337379260:
10: 44832: loss: 0.4335673484:
10: 48032: loss: 0.4337808892:
10: 51232: loss: 0.4334494209:
10: 54432: loss: 0.4328112387:
10: 57632: loss: 0.4330035921:
10: 60832: loss: 0.4329643579:
10: 64032: loss: 0.4330312875:
10: 67232: loss: 0.4332487196:
10: 70432: loss: 0.4334108195:
10: 73632: loss: 0.4328666873:
10: 76832: loss: 0.4326763803:
10: 80032: loss: 0.4321543598:
10: 83232: loss: 0.4317467618:
10: 86432: loss: 0.4307459238:
10: 89632: loss: 0.4306701152:
Dev-Acc: 10: Accuracy: 0.9416574240: precision: 0.6666666667: recall: 0.0003400782: f1: 0.0006798097
Train-Acc: 10: Accuracy: 0.8351193070: precision: 1.0000000000: recall: 0.0107159293: f1: 0.0212046312
11: 3232: loss: 0.4169706121:
11: 6432: loss: 0.4128347088:
11: 9632: loss: 0.4138658930:
11: 12832: loss: 0.4151492944:
11: 16032: loss: 0.4179579943:
11: 19232: loss: 0.4166486564:
11: 22432: loss: 0.4178318493:
11: 25632: loss: 0.4191927053:
11: 28832: loss: 0.4197493740:
11: 32032: loss: 0.4192964363:
11: 35232: loss: 0.4192227246:
11: 38432: loss: 0.4191198157:
11: 41632: loss: 0.4189352747:
11: 44832: loss: 0.4182907855:
11: 48032: loss: 0.4180034666:
11: 51232: loss: 0.4173394628:
11: 54432: loss: 0.4167296099:
11: 57632: loss: 0.4163357861:
11: 60832: loss: 0.4154388281:
11: 64032: loss: 0.4150531680:
11: 67232: loss: 0.4152415268:
11: 70432: loss: 0.4150606586:
11: 73632: loss: 0.4147682509:
11: 76832: loss: 0.4145484737:
11: 80032: loss: 0.4147045645:
11: 83232: loss: 0.4141594329:
11: 86432: loss: 0.4134474607:
11: 89632: loss: 0.4127687116:
Dev-Acc: 11: Accuracy: 0.9417169094: precision: 0.8888888889: recall: 0.0013603129: f1: 0.0027164686
Train-Acc: 11: Accuracy: 0.8381763101: precision: 1.0000000000: recall: 0.0290579186: f1: 0.0564747972
12: 3232: loss: 0.4150278947:
12: 6432: loss: 0.4080184807:
12: 9632: loss: 0.4078930479:
12: 12832: loss: 0.4061385092:
12: 16032: loss: 0.4022872401:
12: 19232: loss: 0.4015581746:
12: 22432: loss: 0.4036938877:
12: 25632: loss: 0.4020128426:
12: 28832: loss: 0.4024975190:
12: 32032: loss: 0.4028058108:
12: 35232: loss: 0.4032578797:
12: 38432: loss: 0.4020743800:
12: 41632: loss: 0.4012233380:
12: 44832: loss: 0.4016114649:
12: 48032: loss: 0.4013957079:
12: 51232: loss: 0.4009595313:
12: 54432: loss: 0.4004486263:
12: 57632: loss: 0.4000520356:
12: 60832: loss: 0.3994794101:
12: 64032: loss: 0.3989618230:
12: 67232: loss: 0.3985221680:
12: 70432: loss: 0.3984266994:
12: 73632: loss: 0.3983711355:
12: 76832: loss: 0.3982304119:
12: 80032: loss: 0.3980176197:
12: 83232: loss: 0.3976268552:
12: 86432: loss: 0.3973309580:
12: 89632: loss: 0.3971422470:
Dev-Acc: 12: Accuracy: 0.9418756962: precision: 0.6236559140: recall: 0.0098622683: f1: 0.0194174757
Train-Acc: 12: Accuracy: 0.8404992223: precision: 0.9698275862: recall: 0.0443757807: f1: 0.0848682970
13: 3232: loss: 0.3825259939:
13: 6432: loss: 0.3802911390:
13: 9632: loss: 0.3841789171:
13: 12832: loss: 0.3861343030:
13: 16032: loss: 0.3857849568:
13: 19232: loss: 0.3846411920:
13: 22432: loss: 0.3846249255:
13: 25632: loss: 0.3850647530:
13: 28832: loss: 0.3859910692:
13: 32032: loss: 0.3861075783:
13: 35232: loss: 0.3858323447:
13: 38432: loss: 0.3865046288:
13: 41632: loss: 0.3863356318:
13: 44832: loss: 0.3852379735:
13: 48032: loss: 0.3845512613:
13: 51232: loss: 0.3845228208:
13: 54432: loss: 0.3840484945:
13: 57632: loss: 0.3842280597:
13: 60832: loss: 0.3844177087:
13: 64032: loss: 0.3843071982:
13: 67232: loss: 0.3840018571:
13: 70432: loss: 0.3842283199:
13: 73632: loss: 0.3838810148:
13: 76832: loss: 0.3834390555:
13: 80032: loss: 0.3833067775:
13: 83232: loss: 0.3829509141:
13: 86432: loss: 0.3826964834:
13: 89632: loss: 0.3826592045:
Dev-Acc: 13: Accuracy: 0.9425107241: precision: 0.6407766990: recall: 0.0336677436: f1: 0.0639741519
Train-Acc: 13: Accuracy: 0.8450244069: precision: 0.9038607116: recall: 0.0784958254: f1: 0.1444471328
14: 3232: loss: 0.3535040693:
14: 6432: loss: 0.3638656590:
14: 9632: loss: 0.3651767365:
14: 12832: loss: 0.3670661250:
14: 16032: loss: 0.3697837704:
14: 19232: loss: 0.3684471103:
14: 22432: loss: 0.3701234414:
14: 25632: loss: 0.3704535744:
14: 28832: loss: 0.3716717133:
14: 32032: loss: 0.3714491373:
14: 35232: loss: 0.3710410086:
14: 38432: loss: 0.3719711040:
14: 41632: loss: 0.3716930645:
14: 44832: loss: 0.3717350359:
14: 48032: loss: 0.3707544450:
14: 51232: loss: 0.3710587272:
14: 54432: loss: 0.3707707826:
14: 57632: loss: 0.3708433105:
14: 60832: loss: 0.3718187740:
14: 64032: loss: 0.3718216158:
14: 67232: loss: 0.3708489054:
14: 70432: loss: 0.3701848135:
14: 73632: loss: 0.3701015049:
14: 76832: loss: 0.3706949021:
14: 80032: loss: 0.3704021467:
14: 83232: loss: 0.3700890881:
14: 86432: loss: 0.3695510200:
14: 89632: loss: 0.3692422548:
Dev-Acc: 14: Accuracy: 0.9423817396: precision: 0.5553892216: recall: 0.0630845094: f1: 0.1132997404
Train-Acc: 14: Accuracy: 0.8486950397: precision: 0.9157769870: recall: 0.1015054894: f1: 0.1827543351
15: 3232: loss: 0.3668524939:
15: 6432: loss: 0.3635802555:
15: 9632: loss: 0.3621157792:
15: 12832: loss: 0.3602773418:
15: 16032: loss: 0.3593185329:
15: 19232: loss: 0.3602760362:
15: 22432: loss: 0.3600886235:
15: 25632: loss: 0.3603434054:
15: 28832: loss: 0.3594108857:
15: 32032: loss: 0.3607275993:
15: 35232: loss: 0.3611670276:
15: 38432: loss: 0.3603504155:
15: 41632: loss: 0.3590280363:
15: 44832: loss: 0.3590944448:
15: 48032: loss: 0.3594697533:
15: 51232: loss: 0.3582939190:
15: 54432: loss: 0.3580882921:
15: 57632: loss: 0.3574006193:
15: 60832: loss: 0.3579323769:
15: 64032: loss: 0.3570144569:
15: 67232: loss: 0.3569515343:
15: 70432: loss: 0.3571914978:
15: 73632: loss: 0.3565913952:
15: 76832: loss: 0.3567357907:
15: 80032: loss: 0.3569704468:
15: 83232: loss: 0.3571612603:
15: 86432: loss: 0.3573273183:
15: 89632: loss: 0.3572588854:
Dev-Acc: 15: Accuracy: 0.9424412251: precision: 0.5478468900: recall: 0.0778779119: f1: 0.1363704035
Train-Acc: 15: Accuracy: 0.8537023664: precision: 0.9265718219: recall: 0.1327328907: f1: 0.2322024152
16: 3232: loss: 0.3590247019:
16: 6432: loss: 0.3567991886:
16: 9632: loss: 0.3546470870:
16: 12832: loss: 0.3541652779:
16: 16032: loss: 0.3532440339:
16: 19232: loss: 0.3525719545:
16: 22432: loss: 0.3525534222:
16: 25632: loss: 0.3531798752:
16: 28832: loss: 0.3518778493:
16: 32032: loss: 0.3513410264:
16: 35232: loss: 0.3505294007:
16: 38432: loss: 0.3491207788:
16: 41632: loss: 0.3491320171:
16: 44832: loss: 0.3488659192:
16: 48032: loss: 0.3490781750:
16: 51232: loss: 0.3486788905:
16: 54432: loss: 0.3489793639:
16: 57632: loss: 0.3480743603:
16: 60832: loss: 0.3469600150:
16: 64032: loss: 0.3471907054:
16: 67232: loss: 0.3477205848:
16: 70432: loss: 0.3475389146:
16: 73632: loss: 0.3474773253:
16: 76832: loss: 0.3476424231:
16: 80032: loss: 0.3469344074:
16: 83232: loss: 0.3470179684:
16: 86432: loss: 0.3471802022:
16: 89632: loss: 0.3467442205:
Dev-Acc: 16: Accuracy: 0.9424412251: precision: 0.5299401198: recall: 0.1203876892: f1: 0.1962034086
Train-Acc: 16: Accuracy: 0.8588631153: precision: 0.9276798825: recall: 0.1661297745: f1: 0.2817953722
17: 3232: loss: 0.3418999998:
17: 6432: loss: 0.3434117508:
17: 9632: loss: 0.3402497193:
17: 12832: loss: 0.3407205783:
17: 16032: loss: 0.3410687022:
17: 19232: loss: 0.3434617085:
17: 22432: loss: 0.3422969087:
17: 25632: loss: 0.3421114182:
17: 28832: loss: 0.3432806623:
17: 32032: loss: 0.3433490407:
17: 35232: loss: 0.3428953476:
17: 38432: loss: 0.3433786291:
17: 41632: loss: 0.3429451673:
17: 44832: loss: 0.3428939826:
17: 48032: loss: 0.3431700061:
17: 51232: loss: 0.3433391425:
17: 54432: loss: 0.3425908778:
17: 57632: loss: 0.3419274816:
17: 60832: loss: 0.3406177416:
17: 64032: loss: 0.3404817944:
17: 67232: loss: 0.3399767426:
17: 70432: loss: 0.3390899269:
17: 73632: loss: 0.3388048653:
17: 76832: loss: 0.3382688047:
17: 80032: loss: 0.3379214381:
17: 83232: loss: 0.3374556950:
17: 86432: loss: 0.3372305167:
17: 89632: loss: 0.3369839551:
Dev-Acc: 17: Accuracy: 0.9419947267: precision: 0.5090439276: recall: 0.1674885224: f1: 0.2520470829
Train-Acc: 17: Accuracy: 0.8645826578: precision: 0.9342265530: recall: 0.2016961410: f1: 0.3317653420
18: 3232: loss: 0.3339113940:
18: 6432: loss: 0.3341133172:
18: 9632: loss: 0.3315888701:
18: 12832: loss: 0.3299948919:
18: 16032: loss: 0.3296657521:
18: 19232: loss: 0.3295726692:
18: 22432: loss: 0.3285657245:
18: 25632: loss: 0.3289411912:
18: 28832: loss: 0.3269853827:
18: 32032: loss: 0.3283356906:
18: 35232: loss: 0.3283181168:
18: 38432: loss: 0.3288649431:
18: 41632: loss: 0.3278929753:
18: 44832: loss: 0.3279383064:
18: 48032: loss: 0.3279113809:
18: 51232: loss: 0.3279625831:
18: 54432: loss: 0.3277338132:
18: 57632: loss: 0.3283081155:
18: 60832: loss: 0.3283935795:
18: 64032: loss: 0.3288300512:
18: 67232: loss: 0.3287022204:
18: 70432: loss: 0.3284951024:
18: 73632: loss: 0.3282229216:
18: 76832: loss: 0.3278434867:
18: 80032: loss: 0.3277406294:
18: 83232: loss: 0.3274869494:
18: 86432: loss: 0.3273199943:
18: 89632: loss: 0.3274382960:
Dev-Acc: 18: Accuracy: 0.9397622347: precision: 0.4668296089: recall: 0.2273422887: f1: 0.3057747284
Train-Acc: 18: Accuracy: 0.8720443249: precision: 0.9354202613: recall: 0.2494905003: f1: 0.3939173760
19: 3232: loss: 0.3224736601:
19: 6432: loss: 0.3273394279:
19: 9632: loss: 0.3262325650:
19: 12832: loss: 0.3243885155:
19: 16032: loss: 0.3224570697:
19: 19232: loss: 0.3214860935:
19: 22432: loss: 0.3232400952:
19: 25632: loss: 0.3238832327:
19: 28832: loss: 0.3233079840:
19: 32032: loss: 0.3233485606:
19: 35232: loss: 0.3223410080:
19: 38432: loss: 0.3223507463:
19: 41632: loss: 0.3219406307:
19: 44832: loss: 0.3226063928:
19: 48032: loss: 0.3228583286:
19: 51232: loss: 0.3224901315:
19: 54432: loss: 0.3216456788:
19: 57632: loss: 0.3214172721:
19: 60832: loss: 0.3210339732:
19: 64032: loss: 0.3208208768:
19: 67232: loss: 0.3203364809:
19: 70432: loss: 0.3202731423:
19: 73632: loss: 0.3202662811:
19: 76832: loss: 0.3204163830:
19: 80032: loss: 0.3205821639:
19: 83232: loss: 0.3198913964:
19: 86432: loss: 0.3192738059:
19: 89632: loss: 0.3189214578:
Dev-Acc: 19: Accuracy: 0.9292446971: precision: 0.3893413598: recall: 0.3739160007: f1: 0.3814728077
Train-Acc: 19: Accuracy: 0.8804264665: precision: 0.8782118972: recall: 0.3280520676: f1: 0.4776719475
20: 3232: loss: 0.3302867962:
20: 6432: loss: 0.3223507615:
20: 9632: loss: 0.3219136280:
20: 12832: loss: 0.3182782128:
20: 16032: loss: 0.3192775540:
20: 19232: loss: 0.3170165250:
20: 22432: loss: 0.3185959043:
20: 25632: loss: 0.3179153452:
20: 28832: loss: 0.3169922495:
20: 32032: loss: 0.3177717928:
20: 35232: loss: 0.3162495949:
20: 38432: loss: 0.3133056434:
20: 41632: loss: 0.3140179205:
20: 44832: loss: 0.3130169301:
20: 48032: loss: 0.3117301688:
20: 51232: loss: 0.3123231988:
20: 54432: loss: 0.3117779489:
20: 57632: loss: 0.3115403999:
20: 60832: loss: 0.3121312736:
20: 64032: loss: 0.3117614871:
20: 67232: loss: 0.3112891789:
20: 70432: loss: 0.3118925483:
20: 73632: loss: 0.3116402789:
20: 76832: loss: 0.3120676911:
20: 80032: loss: 0.3114561905:
20: 83232: loss: 0.3114694703:
20: 86432: loss: 0.3113647400:
20: 89632: loss: 0.3110786400:
Dev-Acc: 20: Accuracy: 0.9214359522: precision: 0.3540203526: recall: 0.4199965992: f1: 0.3841966091
Train-Acc: 20: Accuracy: 0.8830999136: precision: 0.8394618834: recall: 0.3692064953: f1: 0.5128532944
