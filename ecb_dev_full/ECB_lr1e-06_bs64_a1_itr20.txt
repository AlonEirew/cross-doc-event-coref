1: 6464: loss: 0.6839064980:
1: 12864: loss: 0.6790273300:
1: 19264: loss: 0.6744539615:
1: 25664: loss: 0.6696989481:
Dev-Acc: 1: Accuracy: 0.2198662460: precision: 0.0666698437: recall: 0.9515388539: f1: 0.1246089270
Train-Acc: 1: Accuracy: 0.7571823597: precision: 0.6868730295: recall: 0.9453027414: f1: 0.7956287177
2: 6464: loss: 0.6386126220:
2: 12864: loss: 0.6331342074:
2: 19264: loss: 0.6279528775:
2: 25664: loss: 0.6225711645:
Dev-Acc: 2: Accuracy: 0.2574714124: precision: 0.0691236753: recall: 0.9404863119: f1: 0.1287821461
Train-Acc: 2: Accuracy: 0.8803498149: precision: 0.8525379319: recall: 0.9197948853: f1: 0.8848902663
3: 6464: loss: 0.5867044163:
3: 12864: loss: 0.5795893753:
3: 19264: loss: 0.5729756443:
3: 25664: loss: 0.5672256257:
Dev-Acc: 3: Accuracy: 0.3185525537: precision: 0.0736611992: recall: 0.9224621663: f1: 0.1364282212
Train-Acc: 3: Accuracy: 0.9115114212: precision: 0.9316598855: recall: 0.8881730327: f1: 0.9093968767
4: 6464: loss: 0.5270612994:
4: 12864: loss: 0.5205643134:
4: 19264: loss: 0.5152840802:
4: 25664: loss: 0.5087209786:
Dev-Acc: 4: Accuracy: 0.3735910356: precision: 0.0773648699: recall: 0.8910049311: f1: 0.1423680922
Train-Acc: 4: Accuracy: 0.9165735841: precision: 0.9580712788: recall: 0.8712773651: f1: 0.9126153422
5: 6464: loss: 0.4660075063:
5: 12864: loss: 0.4614463966:
5: 19264: loss: 0.4557464003:
5: 25664: loss: 0.4515094803:
Dev-Acc: 5: Accuracy: 0.4108489454: precision: 0.0801864582: recall: 0.8687298079: f1: 0.1468208923
Train-Acc: 5: Accuracy: 0.9165735841: precision: 0.9673279740: recall: 0.8622707251: f1: 0.9117831074
6: 6464: loss: 0.4124786741:
6: 12864: loss: 0.4104297632:
6: 19264: loss: 0.4060909769:
6: 25664: loss: 0.4007621341:
Dev-Acc: 6: Accuracy: 0.4267641604: precision: 0.0817050364: recall: 0.8617582044: f1: 0.1492585666
Train-Acc: 6: Accuracy: 0.9159161448: precision: 0.9700572108: recall: 0.8583262113: f1: 0.9107778165
7: 6464: loss: 0.3728843090:
7: 12864: loss: 0.3672449687:
7: 19264: loss: 0.3621491337:
7: 25664: loss: 0.3573830166:
Dev-Acc: 7: Accuracy: 0.4311100841: precision: 0.0820146223: recall: 0.8583574222: f1: 0.1497234210
Train-Acc: 7: Accuracy: 0.9170666337: precision: 0.9712524142: recall: 0.8595753073: f1: 0.9120078122
8: 6464: loss: 0.3314557752:
8: 12864: loss: 0.3281125615:
8: 19264: loss: 0.3254240279:
8: 25664: loss: 0.3228588814:
Dev-Acc: 8: Accuracy: 0.4263176620: precision: 0.0815649119: recall: 0.8607379697: f1: 0.1490094492
Train-Acc: 8: Accuracy: 0.9194004536: precision: 0.9716820702: recall: 0.8639800145: f1: 0.9146714922
9: 6464: loss: 0.3052371874:
9: 12864: loss: 0.3006368350:
9: 19264: loss: 0.2948762654:
9: 25664: loss: 0.2930211945:
Dev-Acc: 9: Accuracy: 0.4196499288: precision: 0.0809850742: recall: 0.8644788301: f1: 0.1480963617
Train-Acc: 9: Accuracy: 0.9222273827: precision: 0.9720690922: recall: 0.8694365919: f1: 0.9178928373
10: 6464: loss: 0.2816319202:
10: 12864: loss: 0.2759422867:
10: 19264: loss: 0.2725886720:
10: 25664: loss: 0.2701440889:
Dev-Acc: 10: Accuracy: 0.4122281373: precision: 0.0801108016: recall: 0.8654990648: f1: 0.1466478435
Train-Acc: 10: Accuracy: 0.9264348745: precision: 0.9722606480: recall: 0.8779172967: f1: 0.9226836178
11: 6464: loss: 0.2553703785:
11: 12864: loss: 0.2541152980:
11: 19264: loss: 0.2522691598:
11: 25664: loss: 0.2497794841:
Dev-Acc: 11: Accuracy: 0.4025638998: precision: 0.0790839647: recall: 0.8678796123: f1: 0.1449588185
Train-Acc: 11: Accuracy: 0.9296891093: precision: 0.9720496894: recall: 0.8848201959: f1: 0.9263860688
12: 6464: loss: 0.2383787987:
12: 12864: loss: 0.2368735591:
12: 19264: loss: 0.2371066481:
12: 25664: loss: 0.2340729605:
Dev-Acc: 12: Accuracy: 0.3940208852: precision: 0.0782233906: recall: 0.8702601598: f1: 0.1435442931
Train-Acc: 12: Accuracy: 0.9332720041: precision: 0.9720650383: recall: 0.8921832884: f1: 0.9304127245
13: 6464: loss: 0.2258688022:
13: 12864: loss: 0.2220094907:
13: 19264: loss: 0.2204799962:
13: 25664: loss: 0.2191349014:
Dev-Acc: 13: Accuracy: 0.3839795887: precision: 0.0775659912: recall: 0.8774018024: f1: 0.1425315931
Train-Acc: 13: Accuracy: 0.9364933968: precision: 0.9721254355: recall: 0.8987574781: f1: 0.9340028694
14: 6464: loss: 0.2153151084:
14: 12864: loss: 0.2147064988:
14: 19264: loss: 0.2121541516:
14: 25664: loss: 0.2081755963:
Dev-Acc: 14: Accuracy: 0.3729857802: precision: 0.0767333314: recall: 0.8833531712: f1: 0.1412010926
Train-Acc: 14: Accuracy: 0.9395832419: precision: 0.9710461430: recall: 0.9061863125: f1: 0.9374957492
15: 6464: loss: 0.2033226699:
15: 12864: loss: 0.2000268999:
15: 19264: loss: 0.1983935147:
15: 25664: loss: 0.1963930621:
Dev-Acc: 15: Accuracy: 0.3637878895: precision: 0.0760428041: recall: 0.8881142663: f1: 0.1400906579
Train-Acc: 15: Accuracy: 0.9413582683: precision: 0.9702977233: recall: 0.9105910197: f1: 0.9394967103
16: 6464: loss: 0.1899661516:
16: 12864: loss: 0.1944247593:
16: 19264: loss: 0.1907649770:
16: 25664: loss: 0.1884510620:
Dev-Acc: 16: Accuracy: 0.3521789014: precision: 0.0752677410: recall: 0.8950858698: f1: 0.1388588462
Train-Acc: 16: Accuracy: 0.9433962703: precision: 0.9695092238: recall: 0.9155874039: f1: 0.9417771166
17: 6464: loss: 0.1854409102:
17: 12864: loss: 0.1825284121:
17: 19264: loss: 0.1820802800:
17: 25664: loss: 0.1812102896:
Dev-Acc: 17: Accuracy: 0.3442411423: precision: 0.0746810585: recall: 0.8988267301: f1: 0.1379040463
Train-Acc: 17: Accuracy: 0.9447110891: precision: 0.9691379430: recall: 0.9186772730: f1: 0.9432332096
18: 6464: loss: 0.1834442195:
18: 12864: loss: 0.1769418130:
18: 19264: loss: 0.1748872682:
18: 25664: loss: 0.1733380998:
Dev-Acc: 18: Accuracy: 0.3363232315: precision: 0.0742291640: recall: 0.9042679816: f1: 0.1371962231
Train-Acc: 18: Accuracy: 0.9459273219: precision: 0.9690871369: recall: 0.9212412070: f1: 0.9445586600
19: 6464: loss: 0.1667176340:
19: 12864: loss: 0.1705398438:
19: 19264: loss: 0.1699761502:
19: 25664: loss: 0.1695400092:
Dev-Acc: 19: Accuracy: 0.3328901231: precision: 0.0739197467: recall: 0.9049481381: f1: 0.1366753127
Train-Acc: 19: Accuracy: 0.9471435547: precision: 0.9700089835: recall: 0.9228190126: f1: 0.9458257530
20: 6464: loss: 0.1700739165:
20: 12864: loss: 0.1672656030:
20: 19264: loss: 0.1646481217:
20: 25664: loss: 0.1625925463:
Dev-Acc: 20: Accuracy: 0.3248134553: precision: 0.0734916780: recall: 0.9107294678: f1: 0.1360081260
Train-Acc: 20: Accuracy: 0.9487542510: precision: 0.9698513216: recall: 0.9263033331: f1: 0.9475772555
