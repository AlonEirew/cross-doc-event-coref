1: 6464: loss: 0.6974594599:
1: 12864: loss: 0.6890340355:
1: 19264: loss: 0.6815674557:
1: 25664: loss: 0.6743949507:
1: 32064: loss: 0.6676728028:
1: 38464: loss: 0.6606476135:
1: 44864: loss: 0.6537155810:
1: 51264: loss: 0.6471158886:
1: 57664: loss: 0.6403260335:
1: 64064: loss: 0.6334972824:
1: 70464: loss: 0.6268871371:
Dev-Acc: 1: Accuracy: 0.9409727454: precision: 0.4227272727: recall: 0.0316272743: f1: 0.0588514476
Train-Acc: 1: Accuracy: 0.8166458607: precision: 0.9306122449: recall: 0.0899349155: f1: 0.1640189437
2: 6464: loss: 0.5387532365:
2: 12864: loss: 0.5308924896:
2: 19264: loss: 0.5233982152:
2: 25664: loss: 0.5183639371:
2: 32064: loss: 0.5131891350:
2: 38464: loss: 0.5063593821:
2: 44864: loss: 0.5017738794:
2: 51264: loss: 0.4962971305:
2: 57664: loss: 0.4905392212:
2: 64064: loss: 0.4853015181:
2: 70464: loss: 0.4805637363:
Dev-Acc: 2: Accuracy: 0.9391669035: precision: 0.4412593985: recall: 0.1596667233: f1: 0.2344862030
Train-Acc: 2: Accuracy: 0.8404312730: precision: 0.9674673153: recall: 0.2091907172: f1: 0.3440000000
3: 6464: loss: 0.4196927378:
3: 12864: loss: 0.4121393494:
3: 19264: loss: 0.4060054474:
3: 25664: loss: 0.4014679625:
3: 32064: loss: 0.3986304827:
3: 38464: loss: 0.3946490034:
3: 44864: loss: 0.3909870693:
3: 51264: loss: 0.3867541952:
3: 57664: loss: 0.3826761998:
3: 64064: loss: 0.3791903806:
3: 70464: loss: 0.3758391097:
Dev-Acc: 3: Accuracy: 0.9044689536: precision: 0.3042115164: recall: 0.4949838463: f1: 0.3768284790
Train-Acc: 3: Accuracy: 0.8830188513: precision: 0.9021656051: recall: 0.4655841168: f1: 0.6141971294
4: 6464: loss: 0.3156976523:
4: 12864: loss: 0.3208846029:
4: 19264: loss: 0.3207147638:
4: 25664: loss: 0.3188833405:
4: 32064: loss: 0.3167204310:
4: 38464: loss: 0.3162419322:
4: 44864: loss: 0.3121757868:
4: 51264: loss: 0.3104880613:
4: 57664: loss: 0.3093856086:
4: 64064: loss: 0.3082635219:
4: 70464: loss: 0.3060248522:
Dev-Acc: 4: Accuracy: 0.8663081527: precision: 0.2332232450: recall: 0.5643598028: f1: 0.3300517104
Train-Acc: 4: Accuracy: 0.8996121287: precision: 0.9038379531: recall: 0.5573598054: f1: 0.6895205563
5: 6464: loss: 0.2850886264:
5: 12864: loss: 0.2820491611:
5: 19264: loss: 0.2808484985:
5: 25664: loss: 0.2783712153:
5: 32064: loss: 0.2768443686:
5: 38464: loss: 0.2754464429:
5: 44864: loss: 0.2731894943:
5: 51264: loss: 0.2716398260:
5: 57664: loss: 0.2694341297:
5: 64064: loss: 0.2683830184:
5: 70464: loss: 0.2663958653:
Dev-Acc: 5: Accuracy: 0.8409370184: precision: 0.2040471192: recall: 0.5949668424: f1: 0.3038777194
Train-Acc: 5: Accuracy: 0.9071592689: precision: 0.9006095163: recall: 0.6022615213: f1: 0.7218216917
6: 6464: loss: 0.2528934093:
6: 12864: loss: 0.2471386248:
6: 19264: loss: 0.2455492090:
6: 25664: loss: 0.2444126252:
6: 32064: loss: 0.2424874530:
6: 38464: loss: 0.2424709002:
6: 44864: loss: 0.2424610445:
6: 51264: loss: 0.2420439330:
6: 57664: loss: 0.2409569960:
6: 64064: loss: 0.2410482008:
6: 70464: loss: 0.2385953979:
Dev-Acc: 6: Accuracy: 0.8168558478: precision: 0.1831830319: recall: 0.6182622003: f1: 0.2826272833
Train-Acc: 6: Accuracy: 0.9157057405: precision: 0.8995641119: recall: 0.6512392348: f1: 0.7555199634
7: 6464: loss: 0.2204111812:
7: 12864: loss: 0.2174402340:
7: 19264: loss: 0.2182817117:
7: 25664: loss: 0.2205729299:
7: 32064: loss: 0.2215611508:
7: 38464: loss: 0.2228732135:
7: 44864: loss: 0.2225476070:
7: 51264: loss: 0.2216879710:
7: 57664: loss: 0.2206515774:
7: 64064: loss: 0.2201197921:
7: 70464: loss: 0.2193518368:
Dev-Acc: 7: Accuracy: 0.7972495556: precision: 0.1690243348: recall: 0.6318653290: f1: 0.2667049451
Train-Acc: 7: Accuracy: 0.9235553145: precision: 0.9012039962: recall: 0.6938399842: f1: 0.7840427903
8: 6464: loss: 0.2079373705:
8: 12864: loss: 0.2091784596:
8: 19264: loss: 0.2081175221:
8: 25664: loss: 0.2094075674:
8: 32064: loss: 0.2081613665:
8: 38464: loss: 0.2087427192:
8: 44864: loss: 0.2077597939:
8: 51264: loss: 0.2066164207:
8: 57664: loss: 0.2055485842:
8: 64064: loss: 0.2051692196:
8: 70464: loss: 0.2055049222:
Dev-Acc: 8: Accuracy: 0.7763533592: precision: 0.1572081156: recall: 0.6495493964: f1: 0.2531477800
Train-Acc: 8: Accuracy: 0.9297350645: precision: 0.9019144603: recall: 0.7278285451: f1: 0.8055737466
9: 6464: loss: 0.2077657614:
9: 12864: loss: 0.1993270256:
9: 19264: loss: 0.1959426060:
9: 25664: loss: 0.1954273499:
9: 32064: loss: 0.1954864801:
9: 38464: loss: 0.1950775005:
9: 44864: loss: 0.1951688519:
9: 51264: loss: 0.1953786089:
9: 57664: loss: 0.1952891993:
9: 64064: loss: 0.1955417282:
9: 70464: loss: 0.1946868810:
Dev-Acc: 9: Accuracy: 0.7616784573: precision: 0.1498725967: recall: 0.6600918211: f1: 0.2442815342
Train-Acc: 9: Accuracy: 0.9340345860: precision: 0.9021619063: recall: 0.7516928539: f1: 0.8200824816
10: 6464: loss: 0.1848238005:
10: 12864: loss: 0.1892526378:
10: 19264: loss: 0.1903143933:
10: 25664: loss: 0.1893138105:
10: 32064: loss: 0.1885190978:
10: 38464: loss: 0.1884039528:
10: 44864: loss: 0.1874247177:
10: 51264: loss: 0.1859962079:
10: 57664: loss: 0.1858070504:
10: 64064: loss: 0.1854598349:
10: 70464: loss: 0.1852027940:
Dev-Acc: 10: Accuracy: 0.7506449223: precision: 0.1452789858: recall: 0.6702941677: f1: 0.2388005452
Train-Acc: 10: Accuracy: 0.9364801645: precision: 0.9028252096: recall: 0.7647097495: f1: 0.8280476953
11: 6464: loss: 0.1846308587:
11: 12864: loss: 0.1790500430:
11: 19264: loss: 0.1785122118:
11: 25664: loss: 0.1785103990:
11: 32064: loss: 0.1805184570:
11: 38464: loss: 0.1792113712:
11: 44864: loss: 0.1771506954:
11: 51264: loss: 0.1760723711:
11: 57664: loss: 0.1759104893:
11: 64064: loss: 0.1774456824:
11: 70464: loss: 0.1773864887:
Dev-Acc: 11: Accuracy: 0.7391252518: precision: 0.1411694385: recall: 0.6827070226: f1: 0.2339607249
Train-Acc: 11: Accuracy: 0.9390572309: precision: 0.9051486362: recall: 0.7766747748: f1: 0.8360046704
12: 6464: loss: 0.1669030149:
12: 12864: loss: 0.1654601089:
12: 19264: loss: 0.1665970782:
12: 25664: loss: 0.1685303496:
12: 32064: loss: 0.1697198849:
12: 38464: loss: 0.1693351471:
12: 44864: loss: 0.1710495468:
12: 51264: loss: 0.1696454090:
12: 57664: loss: 0.1702880694:
12: 64064: loss: 0.1694972393:
12: 70464: loss: 0.1708727300:
Dev-Acc: 12: Accuracy: 0.7319217324: precision: 0.1386466988: recall: 0.6895085870: f1: 0.2308699613
Train-Acc: 12: Accuracy: 0.9418841600: precision: 0.9125315391: recall: 0.7846295444: f1: 0.8437610463
13: 6464: loss: 0.1694458227:
13: 12864: loss: 0.1660368486:
13: 19264: loss: 0.1675165684:
13: 25664: loss: 0.1676044878:
13: 32064: loss: 0.1675695878:
13: 38464: loss: 0.1684263813:
13: 44864: loss: 0.1685782712:
13: 51264: loss: 0.1671910434:
13: 57664: loss: 0.1664315725:
13: 64064: loss: 0.1670000296:
13: 70464: loss: 0.1661482474:
Dev-Acc: 13: Accuracy: 0.7269804478: precision: 0.1367894436: recall: 0.6927393300: f1: 0.2284656797
Train-Acc: 13: Accuracy: 0.9436723590: precision: 0.9145610441: recall: 0.7923870883: f1: 0.8491017964
14: 6464: loss: 0.1601236237:
14: 12864: loss: 0.1572663892:
14: 19264: loss: 0.1593860654:
14: 25664: loss: 0.1626597971:
14: 32064: loss: 0.1625343619:
14: 38464: loss: 0.1616403861:
14: 44864: loss: 0.1612149533:
14: 51264: loss: 0.1619134538:
14: 57664: loss: 0.1624819958:
14: 64064: loss: 0.1611774398:
14: 70464: loss: 0.1610306281:
Dev-Acc: 14: Accuracy: 0.7233390212: precision: 0.1352453581: recall: 0.6935895256: f1: 0.2263533198
Train-Acc: 14: Accuracy: 0.9452632666: precision: 0.9187386295: recall: 0.7967917954: f1: 0.8534309756
15: 6464: loss: 0.1633504514:
15: 12864: loss: 0.1606755317:
15: 19264: loss: 0.1610817584:
15: 25664: loss: 0.1593659521:
15: 32064: loss: 0.1588498004:
15: 38464: loss: 0.1594722502:
15: 44864: loss: 0.1599694336:
15: 51264: loss: 0.1593492888:
15: 57664: loss: 0.1588066788:
15: 64064: loss: 0.1577476169:
15: 70464: loss: 0.1571079557:
Dev-Acc: 15: Accuracy: 0.7150440216: precision: 0.1323330543: recall: 0.6988607380: f1: 0.2225290344
Train-Acc: 15: Accuracy: 0.9467622042: precision: 0.9171150972: recall: 0.8067188219: f1: 0.8583820083
16: 6464: loss: 0.1516056047:
16: 12864: loss: 0.1578844208:
16: 19264: loss: 0.1543860898:
16: 25664: loss: 0.1563091140:
16: 32064: loss: 0.1553057978:
16: 38464: loss: 0.1555782998:
16: 44864: loss: 0.1541063471:
16: 51264: loss: 0.1544597242:
16: 57664: loss: 0.1538405811:
16: 64064: loss: 0.1523932314:
16: 70464: loss: 0.1525183278:
Dev-Acc: 16: Accuracy: 0.7072948217: precision: 0.1295291276: recall: 0.7020914810: f1: 0.2187086180
Train-Acc: 16: Accuracy: 0.9478009343: precision: 0.9174155217: recall: 0.8121096575: f1: 0.8615567025
17: 6464: loss: 0.1516345308:
17: 12864: loss: 0.1483535772:
17: 19264: loss: 0.1495397318:
17: 25664: loss: 0.1477264110:
17: 32064: loss: 0.1484486373:
17: 38464: loss: 0.1486248122:
17: 44864: loss: 0.1495101398:
17: 51264: loss: 0.1488336607:
17: 57664: loss: 0.1489341406:
17: 64064: loss: 0.1492670191:
17: 70464: loss: 0.1494464369:
Dev-Acc: 17: Accuracy: 0.6983945966: precision: 0.1266674788: recall: 0.7071926543: f1: 0.2148521245
Train-Acc: 17: Accuracy: 0.9491814971: precision: 0.9159090909: recall: 0.8213135231: f1: 0.8660358393
18: 6464: loss: 0.1537612204:
18: 12864: loss: 0.1528310365:
18: 19264: loss: 0.1487084624:
18: 25664: loss: 0.1477471911:
18: 32064: loss: 0.1473194375:
18: 38464: loss: 0.1474484835:
18: 44864: loss: 0.1472825799:
18: 51264: loss: 0.1473646820:
18: 57664: loss: 0.1481815296:
18: 64064: loss: 0.1468262342:
18: 70464: loss: 0.1469672490:
Dev-Acc: 18: Accuracy: 0.6917566061: precision: 0.1253458689: recall: 0.7163747662: f1: 0.2133596678
Train-Acc: 18: Accuracy: 0.9494444728: precision: 0.9109777264: recall: 0.8281506804: f1: 0.8675918592
19: 6464: loss: 0.1412522375:
19: 12864: loss: 0.1444770102:
19: 19264: loss: 0.1479065030:
19: 25664: loss: 0.1456887091:
19: 32064: loss: 0.1447845585:
19: 38464: loss: 0.1458739840:
19: 44864: loss: 0.1461381263:
19: 51264: loss: 0.1452658045:
19: 57664: loss: 0.1439213497:
19: 64064: loss: 0.1429669805:
19: 70464: loss: 0.1426667225:
Dev-Acc: 19: Accuracy: 0.6880258918: precision: 0.1240476541: recall: 0.7170549226: f1: 0.2115056676
Train-Acc: 19: Accuracy: 0.9509434104: precision: 0.9156408400: recall: 0.8313062915: f1: 0.8714379243
20: 6464: loss: 0.1382250500:
20: 12864: loss: 0.1421470053:
20: 19264: loss: 0.1406901448:
20: 25664: loss: 0.1411345963:
20: 32064: loss: 0.1395355293:
20: 38464: loss: 0.1397771957:
20: 44864: loss: 0.1403387484:
20: 51264: loss: 0.1406982219:
20: 57664: loss: 0.1409135613:
20: 64064: loss: 0.1410572345:
20: 70464: loss: 0.1403318020:
Dev-Acc: 20: Accuracy: 0.6879166961: precision: 0.1237641243: recall: 0.7151844924: f1: 0.2110121661
Train-Acc: 20: Accuracy: 0.9519952536: precision: 0.9210986449: recall: 0.8311748077: f1: 0.8738293534
