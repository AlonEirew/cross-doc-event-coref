1: 6464: loss: 0.7072848785:
1: 12864: loss: 0.7055666488:
1: 19264: loss: 0.7044935685:
1: 25664: loss: 0.7037413451:
1: 32064: loss: 0.7027362286:
1: 38464: loss: 0.7019035049:
1: 44864: loss: 0.7010689671:
1: 51264: loss: 0.7002632593:
1: 57664: loss: 0.6993413040:
1: 64064: loss: 0.6985086076:
1: 70464: loss: 0.6976889437:
1: 76864: loss: 0.6969306577:
1: 83264: loss: 0.6961164574:
1: 89664: loss: 0.6952191573:
1: 96064: loss: 0.6943643673:
1: 102464: loss: 0.6935082103:
Dev-Acc: 1: Accuracy: 0.6735196114: precision: 0.0674520601: recall: 0.3582724027: f1: 0.1135298238
Train-Acc: 1: Accuracy: 0.6861293912: precision: 0.2056322545: recall: 0.4181184669: f1: 0.2756827048
2: 6464: loss: 0.6773782724:
2: 12864: loss: 0.6769733104:
2: 19264: loss: 0.6760749942:
2: 25664: loss: 0.6751647556:
2: 32064: loss: 0.6742103280:
2: 38464: loss: 0.6735021681:
2: 44864: loss: 0.6727542144:
2: 51264: loss: 0.6719971333:
2: 57664: loss: 0.6712229421:
2: 64064: loss: 0.6704899045:
2: 70464: loss: 0.6697132296:
2: 76864: loss: 0.6689730772:
2: 83264: loss: 0.6681863492:
2: 89664: loss: 0.6673687880:
2: 96064: loss: 0.6665376659:
2: 102464: loss: 0.6657359240:
Dev-Acc: 2: Accuracy: 0.8906770945: precision: 0.1050284484: recall: 0.1161367114: f1: 0.1103036176
Train-Acc: 2: Accuracy: 0.8459103703: precision: 0.3959276018: recall: 0.1495628164: f1: 0.2171112278
3: 6464: loss: 0.6513409764:
3: 12864: loss: 0.6502641425:
3: 19264: loss: 0.6497303909:
3: 25664: loss: 0.6490370768:
3: 32064: loss: 0.6484498103:
3: 38464: loss: 0.6475432012:
3: 44864: loss: 0.6466460834:
3: 51264: loss: 0.6459660506:
3: 57664: loss: 0.6451322080:
3: 64064: loss: 0.6444584837:
3: 70464: loss: 0.6437060397:
3: 76864: loss: 0.6429138176:
3: 83264: loss: 0.6421688482:
3: 89664: loss: 0.6415147537:
3: 96064: loss: 0.6407599425:
3: 102464: loss: 0.6399382875:
Dev-Acc: 3: Accuracy: 0.9370236993: precision: 0.1064189189: recall: 0.0107124639: f1: 0.0194654720
Train-Acc: 3: Accuracy: 0.8587770462: precision: 0.5931477516: recall: 0.0364210111: f1: 0.0686280582
4: 6464: loss: 0.6256105757:
4: 12864: loss: 0.6257256061:
4: 19264: loss: 0.6242876707:
4: 25664: loss: 0.6235766953:
4: 32064: loss: 0.6231215330:
4: 38464: loss: 0.6226441164:
4: 44864: loss: 0.6218462738:
4: 51264: loss: 0.6209599063:
4: 57664: loss: 0.6201977948:
4: 64064: loss: 0.6194075673:
4: 70464: loss: 0.6185867536:
4: 76864: loss: 0.6178523494:
4: 83264: loss: 0.6173171696:
4: 89664: loss: 0.6165267000:
4: 96064: loss: 0.6157001629:
4: 102464: loss: 0.6150723029:
Dev-Acc: 4: Accuracy: 0.9414589405: precision: 0.0476190476: recall: 0.0001700391: f1: 0.0003388682
Train-Acc: 4: Accuracy: 0.8576500416: precision: 0.8139534884: recall: 0.0046019328: f1: 0.0091521213
5: 6464: loss: 0.6030048358:
5: 12864: loss: 0.6012245646:
5: 19264: loss: 0.6010731033:
5: 25664: loss: 0.5997183602:
5: 32064: loss: 0.5988802814:
5: 38464: loss: 0.5983970129:
5: 44864: loss: 0.5974002893:
5: 51264: loss: 0.5965442246:
5: 57664: loss: 0.5960280496:
5: 64064: loss: 0.5952061320:
5: 70464: loss: 0.5946600112:
5: 76864: loss: 0.5938792926:
5: 83264: loss: 0.5931279872:
5: 89664: loss: 0.5924685950:
5: 96064: loss: 0.5916447178:
5: 102464: loss: 0.5908912787:
Dev-Acc: 5: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8572649360: precision: 1.0000000000: recall: 0.0008546447: f1: 0.0017078297
6: 6464: loss: 0.5796650368:
6: 12864: loss: 0.5778663838:
6: 19264: loss: 0.5767064059:
6: 25664: loss: 0.5760368846:
6: 32064: loss: 0.5756147480:
6: 38464: loss: 0.5747712609:
6: 44864: loss: 0.5739948870:
6: 51264: loss: 0.5732851022:
6: 57664: loss: 0.5722002712:
6: 64064: loss: 0.5716000137:
6: 70464: loss: 0.5712660124:
6: 76864: loss: 0.5707352363:
6: 83264: loss: 0.5698018587:
6: 89664: loss: 0.5692293680:
6: 96064: loss: 0.5686495427:
6: 102464: loss: 0.5679849324:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8571710587: precision: 1.0000000000: recall: 0.0001972257: f1: 0.0003943736
7: 6464: loss: 0.5559725285:
7: 12864: loss: 0.5562995410:
7: 19264: loss: 0.5556193038:
7: 25664: loss: 0.5550720854:
7: 32064: loss: 0.5541450079:
7: 38464: loss: 0.5534334770:
7: 44864: loss: 0.5525102870:
7: 51264: loss: 0.5516399408:
7: 57664: loss: 0.5510867088:
7: 64064: loss: 0.5504429629:
7: 70464: loss: 0.5497882006:
7: 76864: loss: 0.5490075078:
7: 83264: loss: 0.5482538279:
7: 89664: loss: 0.5473345175:
7: 96064: loss: 0.5465789222:
7: 102464: loss: 0.5460673371:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.5342527145:
8: 12864: loss: 0.5331588630:
8: 19264: loss: 0.5307103073:
8: 25664: loss: 0.5309906475:
8: 32064: loss: 0.5303507943:
8: 38464: loss: 0.5289479680:
8: 44864: loss: 0.5292329316:
8: 51264: loss: 0.5285554170:
8: 57664: loss: 0.5280345240:
8: 64064: loss: 0.5275784633:
8: 70464: loss: 0.5271639381:
8: 76864: loss: 0.5267117485:
8: 83264: loss: 0.5267328165:
8: 89664: loss: 0.5261634288:
8: 96064: loss: 0.5252178310:
8: 102464: loss: 0.5245559490:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.5107899728:
9: 12864: loss: 0.5102227530:
9: 19264: loss: 0.5105773336:
9: 25664: loss: 0.5102922938:
9: 32064: loss: 0.5099715084:
9: 38464: loss: 0.5098163147:
9: 44864: loss: 0.5095119017:
9: 51264: loss: 0.5090860121:
9: 57664: loss: 0.5083910062:
9: 64064: loss: 0.5076764106:
9: 70464: loss: 0.5070401433:
9: 76864: loss: 0.5062625882:
9: 83264: loss: 0.5057613564:
9: 89664: loss: 0.5055254106:
9: 96064: loss: 0.5049715205:
9: 102464: loss: 0.5044912121:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.4928883368:
10: 12864: loss: 0.4922211945:
10: 19264: loss: 0.4923930618:
10: 25664: loss: 0.4914933444:
10: 32064: loss: 0.4916141096:
10: 38464: loss: 0.4904121261:
10: 44864: loss: 0.4898875272:
10: 51264: loss: 0.4892106175:
10: 57664: loss: 0.4881665381:
10: 64064: loss: 0.4883410317:
10: 70464: loss: 0.4878154940:
10: 76864: loss: 0.4876322151:
10: 83264: loss: 0.4877468547:
10: 89664: loss: 0.4874497560:
10: 96064: loss: 0.4863265306:
10: 102464: loss: 0.4856810961:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.4702488890:
11: 12864: loss: 0.4748450580:
11: 19264: loss: 0.4738234718:
11: 25664: loss: 0.4752926409:
11: 32064: loss: 0.4740353629:
11: 38464: loss: 0.4739726879:
11: 44864: loss: 0.4732094016:
11: 51264: loss: 0.4724725418:
11: 57664: loss: 0.4720522120:
11: 64064: loss: 0.4716117114:
11: 70464: loss: 0.4712811903:
11: 76864: loss: 0.4703105922:
11: 83264: loss: 0.4698148554:
11: 89664: loss: 0.4691547190:
11: 96064: loss: 0.4684980812:
11: 102464: loss: 0.4678382745:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.4586854398:
12: 12864: loss: 0.4594404922:
12: 19264: loss: 0.4577037085:
12: 25664: loss: 0.4581952737:
12: 32064: loss: 0.4581592855:
12: 38464: loss: 0.4566386526:
12: 44864: loss: 0.4555900872:
12: 51264: loss: 0.4549041333:
12: 57664: loss: 0.4545895088:
12: 64064: loss: 0.4541812408:
12: 70464: loss: 0.4533187968:
12: 76864: loss: 0.4528614144:
12: 83264: loss: 0.4523222565:
12: 89664: loss: 0.4519246677:
12: 96064: loss: 0.4514560717:
12: 102464: loss: 0.4511082595:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.4455851376:
13: 12864: loss: 0.4455176467:
13: 19264: loss: 0.4444881544:
13: 25664: loss: 0.4424860652:
13: 32064: loss: 0.4429903523:
13: 38464: loss: 0.4412994953:
13: 44864: loss: 0.4405778128:
13: 51264: loss: 0.4399640762:
13: 57664: loss: 0.4382994483:
13: 64064: loss: 0.4376467755:
13: 70464: loss: 0.4374002822:
13: 76864: loss: 0.4371763427:
13: 83264: loss: 0.4370985092:
13: 89664: loss: 0.4367649356:
13: 96064: loss: 0.4362753190:
13: 102464: loss: 0.4358691071:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.4237418088:
14: 12864: loss: 0.4262690234:
14: 19264: loss: 0.4250667570:
14: 25664: loss: 0.4276812731:
14: 32064: loss: 0.4271400380:
14: 38464: loss: 0.4247611983:
14: 44864: loss: 0.4243845430:
14: 51264: loss: 0.4238767047:
14: 57664: loss: 0.4234073029:
14: 64064: loss: 0.4238334416:
14: 70464: loss: 0.4235165300:
14: 76864: loss: 0.4230129801:
14: 83264: loss: 0.4226584870:
14: 89664: loss: 0.4222479516:
14: 96064: loss: 0.4214693304:
14: 102464: loss: 0.4211493785:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.4165909553:
15: 12864: loss: 0.4169144367:
15: 19264: loss: 0.4128130445:
15: 25664: loss: 0.4117717411:
15: 32064: loss: 0.4124035287:
15: 38464: loss: 0.4133547883:
15: 44864: loss: 0.4124139399:
15: 51264: loss: 0.4113179126:
15: 57664: loss: 0.4107269297:
15: 64064: loss: 0.4101583934:
15: 70464: loss: 0.4093590419:
15: 76864: loss: 0.4091431700:
15: 83264: loss: 0.4090438906:
15: 89664: loss: 0.4086743439:
15: 96064: loss: 0.4080682454:
15: 102464: loss: 0.4078485253:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.3981312394:
16: 12864: loss: 0.4013133253:
16: 19264: loss: 0.3997023066:
16: 25664: loss: 0.3992322963:
16: 32064: loss: 0.3992847797:
16: 38464: loss: 0.3992288137:
16: 44864: loss: 0.3982540547:
16: 51264: loss: 0.3979451681:
16: 57664: loss: 0.3977050202:
16: 64064: loss: 0.3977699005:
16: 70464: loss: 0.3977455303:
16: 76864: loss: 0.3966759027:
16: 83264: loss: 0.3963498853:
16: 89664: loss: 0.3959784792:
16: 96064: loss: 0.3954612878:
16: 102464: loss: 0.3957318414:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.3896611682:
17: 12864: loss: 0.3889089002:
17: 19264: loss: 0.3883260899:
17: 25664: loss: 0.3898813960:
17: 32064: loss: 0.3892641711:
17: 38464: loss: 0.3868996020:
17: 44864: loss: 0.3866981243:
17: 51264: loss: 0.3866066286:
17: 57664: loss: 0.3867384701:
17: 64064: loss: 0.3863171767:
17: 70464: loss: 0.3854717252:
17: 76864: loss: 0.3857490925:
17: 83264: loss: 0.3853524925:
17: 89664: loss: 0.3850300190:
17: 96064: loss: 0.3847386282:
17: 102464: loss: 0.3843114294:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.3828252080:
18: 12864: loss: 0.3789777721:
18: 19264: loss: 0.3795084813:
18: 25664: loss: 0.3751913368:
18: 32064: loss: 0.3755873269:
18: 38464: loss: 0.3755188290:
18: 44864: loss: 0.3755878737:
18: 51264: loss: 0.3755665998:
18: 57664: loss: 0.3751601271:
18: 64064: loss: 0.3751992667:
18: 70464: loss: 0.3739950755:
18: 76864: loss: 0.3746945220:
18: 83264: loss: 0.3743522534:
18: 89664: loss: 0.3745154170:
18: 96064: loss: 0.3748071969:
18: 102464: loss: 0.3745526919:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.5000000000: recall: 0.0001700391: f1: 0.0003399626
Train-Acc: 18: Accuracy: 0.8572086096: precision: 1.0000000000: recall: 0.0004601933: f1: 0.0009199632
19: 6464: loss: 0.3713379353:
19: 12864: loss: 0.3693924706:
19: 19264: loss: 0.3656673159:
19: 25664: loss: 0.3670089068:
19: 32064: loss: 0.3674741758:
19: 38464: loss: 0.3668335643:
19: 44864: loss: 0.3674711247:
19: 51264: loss: 0.3669643899:
19: 57664: loss: 0.3657820766:
19: 64064: loss: 0.3650383505:
19: 70464: loss: 0.3649423044:
19: 76864: loss: 0.3654426863:
19: 83264: loss: 0.3655207291:
19: 89664: loss: 0.3654335861:
19: 96064: loss: 0.3653365926:
19: 102464: loss: 0.3645494694:
Dev-Acc: 19: Accuracy: 0.9416871667: precision: 0.8333333333: recall: 0.0008501955: f1: 0.0016986581
Train-Acc: 19: Accuracy: 0.8577721119: precision: 1.0000000000: recall: 0.0044047071: f1: 0.0087707815
20: 6464: loss: 0.3605850366:
20: 12864: loss: 0.3597892482:
20: 19264: loss: 0.3590190725:
20: 25664: loss: 0.3603112843:
20: 32064: loss: 0.3598645476:
20: 38464: loss: 0.3594538559:
20: 44864: loss: 0.3586281732:
20: 51264: loss: 0.3581003692:
20: 57664: loss: 0.3576255069:
20: 64064: loss: 0.3575079100:
20: 70464: loss: 0.3580507363:
20: 76864: loss: 0.3581493050:
20: 83264: loss: 0.3578533979:
20: 89664: loss: 0.3571566691:
20: 96064: loss: 0.3567242802:
20: 102464: loss: 0.3559629294:
Dev-Acc: 20: Accuracy: 0.9417367578: precision: 0.9090909091: recall: 0.0017003911: f1: 0.0033944331
Train-Acc: 20: Accuracy: 0.8584858775: precision: 0.9798657718: recall: 0.0095983170: f1: 0.0190104167
21: 6464: loss: 0.3513823877:
21: 12864: loss: 0.3498861729:
21: 19264: loss: 0.3499738174:
21: 25664: loss: 0.3511742352:
21: 32064: loss: 0.3503909646:
21: 38464: loss: 0.3520230842:
21: 44864: loss: 0.3514936181:
21: 51264: loss: 0.3512750692:
21: 57664: loss: 0.3509363176:
21: 64064: loss: 0.3507563916:
21: 70464: loss: 0.3492921963:
21: 76864: loss: 0.3486622685:
21: 83264: loss: 0.3488663343:
21: 89664: loss: 0.3483670937:
21: 96064: loss: 0.3484343443:
21: 102464: loss: 0.3477671501:
Dev-Acc: 21: Accuracy: 0.9420344234: precision: 0.7600000000: recall: 0.0096922292: f1: 0.0191403627
Train-Acc: 21: Accuracy: 0.8611906767: precision: 0.8998144712: recall: 0.0318848202: f1: 0.0615873016
22: 6464: loss: 0.3438061735:
22: 12864: loss: 0.3468451144:
22: 19264: loss: 0.3471532285:
22: 25664: loss: 0.3467113281:
22: 32064: loss: 0.3450414905:
22: 38464: loss: 0.3444312353:
22: 44864: loss: 0.3446936802:
22: 51264: loss: 0.3442670146:
22: 57664: loss: 0.3424541438:
22: 64064: loss: 0.3422267264:
22: 70464: loss: 0.3422273930:
22: 76864: loss: 0.3422507496:
22: 83264: loss: 0.3418628083:
22: 89664: loss: 0.3417309826:
22: 96064: loss: 0.3403348716:
22: 102464: loss: 0.3401109008:
Dev-Acc: 22: Accuracy: 0.9423618913: precision: 0.6592920354: recall: 0.0253358272: f1: 0.0487964631
Train-Acc: 22: Accuracy: 0.8643744588: precision: 0.8564814815: recall: 0.0608112550: f1: 0.1135596342
23: 6464: loss: 0.3420296025:
23: 12864: loss: 0.3377358609:
23: 19264: loss: 0.3356510602:
23: 25664: loss: 0.3333290515:
23: 32064: loss: 0.3329397674:
23: 38464: loss: 0.3316654691:
23: 44864: loss: 0.3311736271:
23: 51264: loss: 0.3319288609:
23: 57664: loss: 0.3318337932:
23: 64064: loss: 0.3321908723:
23: 70464: loss: 0.3321689609:
23: 76864: loss: 0.3323650715:
23: 83264: loss: 0.3320275448:
23: 89664: loss: 0.3323209417:
23: 96064: loss: 0.3326250042:
23: 102464: loss: 0.3327130551:
Dev-Acc: 23: Accuracy: 0.9426000118: precision: 0.6420118343: recall: 0.0368984867: f1: 0.0697861393
Train-Acc: 23: Accuracy: 0.8665345907: precision: 0.8644314869: recall: 0.0779698902: f1: 0.1430380510
24: 6464: loss: 0.3321214470:
24: 12864: loss: 0.3254738054:
24: 19264: loss: 0.3266987737:
24: 25664: loss: 0.3281955795:
24: 32064: loss: 0.3286999438:
24: 38464: loss: 0.3289301535:
24: 44864: loss: 0.3286118011:
24: 51264: loss: 0.3296326863:
24: 57664: loss: 0.3272243338:
24: 64064: loss: 0.3269461269:
24: 70464: loss: 0.3269913691:
24: 76864: loss: 0.3271482922:
24: 83264: loss: 0.3269708528:
24: 89664: loss: 0.3267049631:
24: 96064: loss: 0.3266437281:
24: 102464: loss: 0.3264419967:
Dev-Acc: 24: Accuracy: 0.9422824979: precision: 0.5490797546: recall: 0.0608740010: f1: 0.1095974284
Train-Acc: 24: Accuracy: 0.8692487478: precision: 0.8802359882: recall: 0.0980869108: f1: 0.1765053827
25: 6464: loss: 0.3216945575:
25: 12864: loss: 0.3220202877:
25: 19264: loss: 0.3234723581:
25: 25664: loss: 0.3215896404:
25: 32064: loss: 0.3198663956:
25: 38464: loss: 0.3189311140:
25: 44864: loss: 0.3193138059:
25: 51264: loss: 0.3193128027:
25: 57664: loss: 0.3205856561:
25: 64064: loss: 0.3202674696:
25: 70464: loss: 0.3204930560:
25: 76864: loss: 0.3205693970:
25: 83264: loss: 0.3200550894:
25: 89664: loss: 0.3207775267:
25: 96064: loss: 0.3203267042:
25: 102464: loss: 0.3208127063:
Dev-Acc: 25: Accuracy: 0.9422527552: precision: 0.5389527458: recall: 0.0717565040: f1: 0.1266506603
Train-Acc: 25: Accuracy: 0.8723762035: precision: 0.8925459826: recall: 0.1212280586: f1: 0.2134629855
26: 6464: loss: 0.3159525086:
26: 12864: loss: 0.3165802513:
26: 19264: loss: 0.3143863826:
26: 25664: loss: 0.3156039578:
26: 32064: loss: 0.3185097628:
26: 38464: loss: 0.3186966920:
26: 44864: loss: 0.3177399028:
26: 51264: loss: 0.3183101935:
26: 57664: loss: 0.3179789662:
26: 64064: loss: 0.3175276209:
26: 70464: loss: 0.3165940253:
26: 76864: loss: 0.3156559458:
26: 83264: loss: 0.3149267729:
26: 89664: loss: 0.3144143459:
26: 96064: loss: 0.3140054218:
26: 102464: loss: 0.3138832320:
Dev-Acc: 26: Accuracy: 0.9425007701: precision: 0.5461373391: recall: 0.0865499065: f1: 0.1494202260
Train-Acc: 26: Accuracy: 0.8743578196: precision: 0.8928418345: recall: 0.1369403721: f1: 0.2374601003
27: 6464: loss: 0.3110177922:
27: 12864: loss: 0.3132825767:
27: 19264: loss: 0.3116272747:
27: 25664: loss: 0.3108466522:
27: 32064: loss: 0.3108693025:
27: 38464: loss: 0.3105931475:
27: 44864: loss: 0.3115850621:
27: 51264: loss: 0.3121495249:
27: 57664: loss: 0.3120571703:
27: 64064: loss: 0.3105034990:
27: 70464: loss: 0.3095071950:
27: 76864: loss: 0.3095369319:
27: 83264: loss: 0.3086752744:
27: 89664: loss: 0.3085742479:
27: 96064: loss: 0.3084322326:
27: 102464: loss: 0.3084151187:
Dev-Acc: 27: Accuracy: 0.9429572225: precision: 0.5536585366: recall: 0.1157966332: f1: 0.1915342427
Train-Acc: 27: Accuracy: 0.8766964078: precision: 0.8976317800: recall: 0.1544934587: f1: 0.2636154580
28: 6464: loss: 0.3065509221:
28: 12864: loss: 0.3038610407:
28: 19264: loss: 0.3024014717:
28: 25664: loss: 0.3040459421:
28: 32064: loss: 0.3047999913:
28: 38464: loss: 0.3047883300:
28: 44864: loss: 0.3052071056:
28: 51264: loss: 0.3056517420:
28: 57664: loss: 0.3053421828:
28: 64064: loss: 0.3057274633:
28: 70464: loss: 0.3049689668:
28: 76864: loss: 0.3042643526:
28: 83264: loss: 0.3038445211:
28: 89664: loss: 0.3033787095:
28: 96064: loss: 0.3036874880:
28: 102464: loss: 0.3034356574:
Dev-Acc: 28: Accuracy: 0.9430068135: precision: 0.5443365696: recall: 0.1430028907: f1: 0.2265014813
Train-Acc: 28: Accuracy: 0.8797768354: precision: 0.9059973046: recall: 0.1767799619: f1: 0.2958358546
29: 6464: loss: 0.2972647220:
29: 12864: loss: 0.2964614353:
29: 19264: loss: 0.2981202286:
29: 25664: loss: 0.2996917316:
29: 32064: loss: 0.3002811165:
29: 38464: loss: 0.3010526397:
29: 44864: loss: 0.3003767497:
29: 51264: loss: 0.2997756881:
29: 57664: loss: 0.2999686971:
29: 64064: loss: 0.2998387652:
29: 70464: loss: 0.2993052033:
29: 76864: loss: 0.2984502997:
29: 83264: loss: 0.2984965183:
29: 89664: loss: 0.2988122568:
29: 96064: loss: 0.2990266425:
29: 102464: loss: 0.2989248802:
Dev-Acc: 29: Accuracy: 0.9423023462: precision: 0.5174234424: recall: 0.1666383268: f1: 0.2520900322
Train-Acc: 29: Accuracy: 0.8837777376: precision: 0.9122093023: recall: 0.2062980738: f1: 0.3364967026
30: 6464: loss: 0.2827063778:
30: 12864: loss: 0.2816174433:
30: 19264: loss: 0.2853334432:
30: 25664: loss: 0.2876094662:
30: 32064: loss: 0.2894998298:
30: 38464: loss: 0.2898491211:
30: 44864: loss: 0.2916969399:
30: 51264: loss: 0.2936717597:
30: 57664: loss: 0.2948721311:
30: 64064: loss: 0.2951076123:
30: 70464: loss: 0.2951108160:
30: 76864: loss: 0.2943335636:
30: 83264: loss: 0.2935330139:
30: 89664: loss: 0.2938932037:
30: 96064: loss: 0.2941363535:
30: 102464: loss: 0.2940561801:
Dev-Acc: 30: Accuracy: 0.9409132004: precision: 0.4866232827: recall: 0.2288726407: f1: 0.3113218457
Train-Acc: 30: Accuracy: 0.8877973557: precision: 0.9045612295: recall: 0.2398921833: f1: 0.3792153806
31: 6464: loss: 0.2818935296:
31: 12864: loss: 0.2879714393:
31: 19264: loss: 0.2888722937:
31: 25664: loss: 0.2900420044:
31: 32064: loss: 0.2882384968:
31: 38464: loss: 0.2892226625:
31: 44864: loss: 0.2881220391:
31: 51264: loss: 0.2884486883:
31: 57664: loss: 0.2881674986:
31: 64064: loss: 0.2883326604:
31: 70464: loss: 0.2877999084:
31: 76864: loss: 0.2886232069:
31: 83264: loss: 0.2888716565:
31: 89664: loss: 0.2891923671:
31: 96064: loss: 0.2889145414:
31: 102464: loss: 0.2895079660:
Dev-Acc: 31: Accuracy: 0.9358131886: precision: 0.4295639674: recall: 0.3048801224: f1: 0.3566384883
Train-Acc: 31: Accuracy: 0.8909717798: precision: 0.8742726517: recall: 0.2765761620: f1: 0.4202167507
32: 6464: loss: 0.2922074549:
32: 12864: loss: 0.2924407457:
32: 19264: loss: 0.2876621889:
32: 25664: loss: 0.2892107115:
32: 32064: loss: 0.2866297381:
32: 38464: loss: 0.2851023410:
32: 44864: loss: 0.2863837533:
32: 51264: loss: 0.2874221478:
32: 57664: loss: 0.2868440280:
32: 64064: loss: 0.2857309674:
32: 70464: loss: 0.2850272303:
32: 76864: loss: 0.2851734820:
32: 83264: loss: 0.2861318805:
32: 89664: loss: 0.2857000734:
32: 96064: loss: 0.2862519842:
32: 102464: loss: 0.2853711449:
Dev-Acc: 32: Accuracy: 0.9302269816: precision: 0.3954586739: recall: 0.3701751403: f1: 0.3823994379
Train-Acc: 32: Accuracy: 0.8929721713: precision: 0.8303030303: recall: 0.3152323976: f1: 0.4569713142
33: 6464: loss: 0.2817487286:
33: 12864: loss: 0.2867810380:
33: 19264: loss: 0.2843495362:
33: 25664: loss: 0.2828868527:
33: 32064: loss: 0.2812396549:
33: 38464: loss: 0.2805184456:
33: 44864: loss: 0.2796146812:
33: 51264: loss: 0.2795831570:
33: 57664: loss: 0.2810481554:
33: 64064: loss: 0.2808084783:
33: 70464: loss: 0.2805972301:
33: 76864: loss: 0.2815299745:
33: 83264: loss: 0.2816110239:
33: 89664: loss: 0.2822401889:
33: 96064: loss: 0.2827425462:
33: 102464: loss: 0.2821270578:
Dev-Acc: 33: Accuracy: 0.9247698188: precision: 0.3684454756: recall: 0.4050331576: f1: 0.3858739673
Train-Acc: 33: Accuracy: 0.8937423229: precision: 0.7983463482: recall: 0.3427782526: f1: 0.4796246895
34: 6464: loss: 0.2839349756:
34: 12864: loss: 0.2830838002:
34: 19264: loss: 0.2805993196:
34: 25664: loss: 0.2812595326:
34: 32064: loss: 0.2821203543:
34: 38464: loss: 0.2826473641:
34: 44864: loss: 0.2823842505:
34: 51264: loss: 0.2821758318:
34: 57664: loss: 0.2813536640:
34: 64064: loss: 0.2807467910:
34: 70464: loss: 0.2792070162:
34: 76864: loss: 0.2794756268:
34: 83264: loss: 0.2792565690:
34: 89664: loss: 0.2793561292:
34: 96064: loss: 0.2786119495:
34: 102464: loss: 0.2787781194:
Dev-Acc: 34: Accuracy: 0.9222694039: precision: 0.3584577475: recall: 0.4205067165: f1: 0.3870109546
Train-Acc: 34: Accuracy: 0.8955079317: precision: 0.7900042595: recall: 0.3657879166: f1: 0.5000449357
35: 6464: loss: 0.2696442546:
35: 12864: loss: 0.2685592322:
35: 19264: loss: 0.2722808520:
35: 25664: loss: 0.2745574903:
35: 32064: loss: 0.2776044356:
35: 38464: loss: 0.2760047059:
35: 44864: loss: 0.2751667661:
35: 51264: loss: 0.2749021298:
35: 57664: loss: 0.2749993268:
35: 64064: loss: 0.2750114621:
35: 70464: loss: 0.2750753696:
35: 76864: loss: 0.2753189145:
35: 83264: loss: 0.2745710611:
35: 89664: loss: 0.2751882417:
35: 96064: loss: 0.2754951495:
35: 102464: loss: 0.2746423772:
Dev-Acc: 35: Accuracy: 0.9205925465: precision: 0.3528432732: recall: 0.4325794933: f1: 0.3886639676
Train-Acc: 35: Accuracy: 0.8974896073: precision: 0.7890204521: recall: 0.3855104858: f1: 0.5179525681
36: 6464: loss: 0.2697924024:
36: 12864: loss: 0.2670425673:
36: 19264: loss: 0.2690418640:
36: 25664: loss: 0.2694365777:
36: 32064: loss: 0.2694660825:
36: 38464: loss: 0.2704914786:
36: 44864: loss: 0.2709594528:
36: 51264: loss: 0.2726301857:
36: 57664: loss: 0.2721994576:
36: 64064: loss: 0.2725447894:
36: 70464: loss: 0.2717493612:
36: 76864: loss: 0.2719063975:
36: 83264: loss: 0.2715365761:
36: 89664: loss: 0.2714390315:
36: 96064: loss: 0.2717592545:
36: 102464: loss: 0.2713884993:
Dev-Acc: 36: Accuracy: 0.9195308685: precision: 0.3498989899: recall: 0.4417616052: f1: 0.3905005261
Train-Acc: 36: Accuracy: 0.8990392089: precision: 0.7890371906: recall: 0.4003024127: f1: 0.5311409630
37: 6464: loss: 0.2775552830:
37: 12864: loss: 0.2746612508:
37: 19264: loss: 0.2742627158:
37: 25664: loss: 0.2730062310:
37: 32064: loss: 0.2734599637:
37: 38464: loss: 0.2734896601:
37: 44864: loss: 0.2729528573:
37: 51264: loss: 0.2715912341:
37: 57664: loss: 0.2715131790:
37: 64064: loss: 0.2705402598:
37: 70464: loss: 0.2704701195:
37: 76864: loss: 0.2696658184:
37: 83264: loss: 0.2698591220:
37: 89664: loss: 0.2695313112:
37: 96064: loss: 0.2689114623:
37: 102464: loss: 0.2683712019:
Dev-Acc: 37: Accuracy: 0.9186874628: precision: 0.3476027397: recall: 0.4487332086: f1: 0.3917464559
Train-Acc: 37: Accuracy: 0.8997154236: precision: 0.7839158211: recall: 0.4114127934: f1: 0.5396223161
38: 6464: loss: 0.2666521581:
38: 12864: loss: 0.2673218718:
38: 19264: loss: 0.2672840310:
38: 25664: loss: 0.2689117964:
38: 32064: loss: 0.2678811591:
38: 38464: loss: 0.2672876296:
38: 44864: loss: 0.2682128236:
38: 51264: loss: 0.2686398647:
38: 57664: loss: 0.2677467800:
38: 64064: loss: 0.2676767448:
38: 70464: loss: 0.2675279615:
38: 76864: loss: 0.2673106649:
38: 83264: loss: 0.2660081640:
38: 89664: loss: 0.2657888325:
38: 96064: loss: 0.2651615838:
38: 102464: loss: 0.2652088326:
Dev-Acc: 38: Accuracy: 0.9180226922: precision: 0.3462879277: recall: 0.4560448903: f1: 0.3936591810
Train-Acc: 38: Accuracy: 0.9006076455: precision: 0.7830927331: recall: 0.4208138847: f1: 0.5474449433
39: 6464: loss: 0.2551443844:
39: 12864: loss: 0.2553218084:
39: 19264: loss: 0.2586447796:
39: 25664: loss: 0.2590502785:
39: 32064: loss: 0.2596237442:
39: 38464: loss: 0.2589155859:
39: 44864: loss: 0.2589443630:
39: 51264: loss: 0.2593210804:
39: 57664: loss: 0.2601960827:
39: 64064: loss: 0.2610693581:
39: 70464: loss: 0.2620716797:
39: 76864: loss: 0.2617442953:
39: 83264: loss: 0.2616996480:
39: 89664: loss: 0.2618222868:
39: 96064: loss: 0.2624389721:
39: 102464: loss: 0.2623720471:
Dev-Acc: 39: Accuracy: 0.9167923331: precision: 0.3421152149: recall: 0.4614861418: f1: 0.3929347039
Train-Acc: 39: Accuracy: 0.9014904499: precision: 0.7820114668: recall: 0.4304122017: f1: 0.5552304626
40: 6464: loss: 0.2557813719:
40: 12864: loss: 0.2580289014:
40: 19264: loss: 0.2613791008:
40: 25664: loss: 0.2594316730:
40: 32064: loss: 0.2583249646:
40: 38464: loss: 0.2581584545:
40: 44864: loss: 0.2581556692:
40: 51264: loss: 0.2573626679:
40: 57664: loss: 0.2590108794:
40: 64064: loss: 0.2584912407:
40: 70464: loss: 0.2588666984:
40: 76864: loss: 0.2591046048:
40: 83264: loss: 0.2596451661:
40: 89664: loss: 0.2590166277:
40: 96064: loss: 0.2589214974:
40: 102464: loss: 0.2593069627:
Dev-Acc: 40: Accuracy: 0.9160184264: precision: 0.3406932281: recall: 0.4696480190: f1: 0.3949099228
Train-Acc: 40: Accuracy: 0.9022699594: precision: 0.7817520816: recall: 0.4382354875: f1: 0.5616311399
41: 6464: loss: 0.2720447041:
41: 12864: loss: 0.2685111355:
41: 19264: loss: 0.2678554465:
41: 25664: loss: 0.2649554683:
41: 32064: loss: 0.2638917348:
41: 38464: loss: 0.2631219147:
41: 44864: loss: 0.2607231299:
41: 51264: loss: 0.2594069743:
41: 57664: loss: 0.2587497641:
41: 64064: loss: 0.2585216972:
41: 70464: loss: 0.2588689926:
41: 76864: loss: 0.2586135570:
41: 83264: loss: 0.2577603421:
41: 89664: loss: 0.2568899212:
41: 96064: loss: 0.2569672739:
41: 102464: loss: 0.2566023828:
Dev-Acc: 41: Accuracy: 0.9151254296: precision: 0.3395365590: recall: 0.4808706002: f1: 0.3980295567
Train-Acc: 41: Accuracy: 0.9034251571: precision: 0.7835443038: recall: 0.4476365788: f1: 0.5697669554
42: 6464: loss: 0.2553081258:
42: 12864: loss: 0.2545376087:
42: 19264: loss: 0.2544969056:
42: 25664: loss: 0.2543599118:
42: 32064: loss: 0.2548077392:
42: 38464: loss: 0.2555190692:
42: 44864: loss: 0.2549281434:
42: 51264: loss: 0.2543753366:
42: 57664: loss: 0.2546343412:
42: 64064: loss: 0.2541189675:
42: 70464: loss: 0.2549478395:
42: 76864: loss: 0.2558040509:
42: 83264: loss: 0.2559457370:
42: 89664: loss: 0.2556866816:
42: 96064: loss: 0.2554459988:
42: 102464: loss: 0.2549342010:
Dev-Acc: 42: Accuracy: 0.9142720699: precision: 0.3374955825: recall: 0.4871620473: f1: 0.3987473904
Train-Acc: 42: Accuracy: 0.9042046666: precision: 0.7840380909: recall: 0.4546709618: f1: 0.5755659121
43: 6464: loss: 0.2444624306:
43: 12864: loss: 0.2468792238:
43: 19264: loss: 0.2512616806:
43: 25664: loss: 0.2505304668:
43: 32064: loss: 0.2518120760:
43: 38464: loss: 0.2526595808:
43: 44864: loss: 0.2522779381:
43: 51264: loss: 0.2516937804:
43: 57664: loss: 0.2520463102:
43: 64064: loss: 0.2514592196:
43: 70464: loss: 0.2518664510:
43: 76864: loss: 0.2525353712:
43: 83264: loss: 0.2513015964:
43: 89664: loss: 0.2521886961:
43: 96064: loss: 0.2522967125:
43: 102464: loss: 0.2520339838:
Dev-Acc: 43: Accuracy: 0.9134882689: precision: 0.3358018977: recall: 0.4934534943: f1: 0.3996419473
Train-Acc: 43: Accuracy: 0.9052001834: precision: 0.7851331773: recall: 0.4631516666: f1: 0.5826166060
44: 6464: loss: 0.2697002841:
44: 12864: loss: 0.2586325709:
44: 19264: loss: 0.2566852403:
44: 25664: loss: 0.2559078073:
44: 32064: loss: 0.2540155222:
44: 38464: loss: 0.2529907809:
44: 44864: loss: 0.2521580917:
44: 51264: loss: 0.2520925242:
44: 57664: loss: 0.2526750820:
44: 64064: loss: 0.2525687264:
44: 70464: loss: 0.2520534334:
44: 76864: loss: 0.2511400954:
44: 83264: loss: 0.2515829724:
44: 89664: loss: 0.2512672694:
44: 96064: loss: 0.2504285250:
44: 102464: loss: 0.2499859028:
Dev-Acc: 44: Accuracy: 0.9127242565: precision: 0.3338273857: recall: 0.4978745111: f1: 0.3996723997
Train-Acc: 44: Accuracy: 0.9062426686: precision: 0.7866228070: recall: 0.4716323713: f1: 0.5897003822
45: 6464: loss: 0.2455660377:
45: 12864: loss: 0.2455027229:
45: 19264: loss: 0.2487928142:
45: 25664: loss: 0.2466560342:
45: 32064: loss: 0.2456704524:
45: 38464: loss: 0.2453251023:
45: 44864: loss: 0.2461388933:
45: 51264: loss: 0.2458660076:
45: 57664: loss: 0.2463600860:
45: 64064: loss: 0.2464062995:
45: 70464: loss: 0.2462526651:
45: 76864: loss: 0.2466405360:
45: 83264: loss: 0.2468244467:
45: 89664: loss: 0.2465047484:
45: 96064: loss: 0.2469336459:
45: 102464: loss: 0.2475748284:
Dev-Acc: 45: Accuracy: 0.9123570919: precision: 0.3336713996: recall: 0.5034858017: f1: 0.4013554727
Train-Acc: 45: Accuracy: 0.9071066976: precision: 0.7878787879: recall: 0.4786010124: f1: 0.5954766676
46: 6464: loss: 0.2451912053:
46: 12864: loss: 0.2457313955:
46: 19264: loss: 0.2453248271:
46: 25664: loss: 0.2467198939:
46: 32064: loss: 0.2454405501:
46: 38464: loss: 0.2440257898:
46: 44864: loss: 0.2446978940:
46: 51264: loss: 0.2460006700:
46: 57664: loss: 0.2462840779:
46: 64064: loss: 0.2455808325:
46: 70464: loss: 0.2453571467:
46: 76864: loss: 0.2442643780:
46: 83264: loss: 0.2443242213:
46: 89664: loss: 0.2446222869:
46: 96064: loss: 0.2451005427:
46: 102464: loss: 0.2454045747:
Dev-Acc: 46: Accuracy: 0.9115236402: precision: 0.3315205327: recall: 0.5079068186: f1: 0.4011819220
Train-Acc: 46: Accuracy: 0.9079425931: precision: 0.7886031373: recall: 0.4858326211: f1: 0.6012529493
47: 6464: loss: 0.2401612009:
47: 12864: loss: 0.2438236232:
47: 19264: loss: 0.2443985496:
47: 25664: loss: 0.2436146226:
47: 32064: loss: 0.2437755729:
47: 38464: loss: 0.2411341312:
47: 44864: loss: 0.2416178475:
47: 51264: loss: 0.2417214031:
47: 57664: loss: 0.2427816841:
47: 64064: loss: 0.2429021516:
47: 70464: loss: 0.2440249718:
47: 76864: loss: 0.2437976126:
47: 83264: loss: 0.2435953729:
47: 89664: loss: 0.2436025646:
47: 96064: loss: 0.2436978654:
47: 102464: loss: 0.2434834510:
Dev-Acc: 47: Accuracy: 0.9105909467: precision: 0.3288495188: recall: 0.5113076007: f1: 0.4002662230
Train-Acc: 47: Accuracy: 0.9084966779: precision: 0.7890063425: recall: 0.4906975215: f1: 0.6050828908
48: 6464: loss: 0.2366588771:
48: 12864: loss: 0.2399393040:
48: 19264: loss: 0.2401746798:
48: 25664: loss: 0.2431730500:
48: 32064: loss: 0.2428897326:
48: 38464: loss: 0.2424417888:
48: 44864: loss: 0.2425893573:
48: 51264: loss: 0.2411985159:
48: 57664: loss: 0.2412713482:
48: 64064: loss: 0.2423141362:
48: 70464: loss: 0.2428447675:
48: 76864: loss: 0.2420005913:
48: 83264: loss: 0.2416578829:
48: 89664: loss: 0.2416871676:
48: 96064: loss: 0.2418111965:
48: 102464: loss: 0.2414691868:
Dev-Acc: 48: Accuracy: 0.9099460244: precision: 0.3273159658: recall: 0.5148784220: f1: 0.4002114724
Train-Acc: 48: Accuracy: 0.9089568853: precision: 0.7886366014: recall: 0.4954966800: f1: 0.6086078811
49: 6464: loss: 0.2453311022:
49: 12864: loss: 0.2407704350:
49: 19264: loss: 0.2442617733:
49: 25664: loss: 0.2426852240:
49: 32064: loss: 0.2417184708:
49: 38464: loss: 0.2432628913:
49: 44864: loss: 0.2437616613:
49: 51264: loss: 0.2432646025:
49: 57664: loss: 0.2418829418:
49: 64064: loss: 0.2411640763:
49: 70464: loss: 0.2413101613:
49: 76864: loss: 0.2410530324:
49: 83264: loss: 0.2404940384:
49: 89664: loss: 0.2401065466:
49: 96064: loss: 0.2398005612:
49: 102464: loss: 0.2394975847:
Dev-Acc: 49: Accuracy: 0.9092316031: precision: 0.3254994125: recall: 0.5181091651: f1: 0.3998162971
Train-Acc: 49: Accuracy: 0.9095016122: precision: 0.7890098497: recall: 0.5002958385: f1: 0.6123270035
50: 6464: loss: 0.2365877923:
50: 12864: loss: 0.2327461928:
50: 19264: loss: 0.2335669136:
50: 25664: loss: 0.2339503773:
50: 32064: loss: 0.2337785294:
50: 38464: loss: 0.2342549901:
50: 44864: loss: 0.2359461301:
50: 51264: loss: 0.2349761360:
50: 57664: loss: 0.2355649063:
50: 64064: loss: 0.2378651472:
50: 70464: loss: 0.2371535640:
50: 76864: loss: 0.2367805031:
50: 83264: loss: 0.2372199129:
50: 89664: loss: 0.2373575022:
50: 96064: loss: 0.2380108056:
50: 102464: loss: 0.2382550690:
Dev-Acc: 50: Accuracy: 0.9085568786: precision: 0.3238618359: recall: 0.5213399082: f1: 0.3995308835
Train-Acc: 50: Accuracy: 0.9101684093: precision: 0.7901336074: recall: 0.5054237065: f1: 0.6164949280
