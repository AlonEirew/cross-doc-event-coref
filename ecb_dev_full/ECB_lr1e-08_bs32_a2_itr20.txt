1: 3232: loss: 0.6953572464:
1: 6432: loss: 0.6952517945:
1: 9632: loss: 0.6956570005:
1: 12832: loss: 0.6955428961:
1: 16032: loss: 0.6952070143:
1: 19232: loss: 0.6952225120:
1: 22432: loss: 0.6953576769:
1: 25632: loss: 0.6953769936:
1: 28832: loss: 0.6954216168:
1: 32032: loss: 0.6953301646:
1: 35232: loss: 0.6952166683:
1: 38432: loss: 0.6953387487:
1: 41632: loss: 0.6952563529:
1: 44832: loss: 0.6952417803:
Dev-Acc: 1: Accuracy: 0.2661831081: precision: 0.0634474798: recall: 0.8411834722: f1: 0.1179950150
Train-Acc: 1: Accuracy: 0.4602371156: precision: 0.3662083854: recall: 0.8475445401: f1: 0.5114350888
2: 3232: loss: 0.6946594709:
2: 6432: loss: 0.6952993128:
2: 9632: loss: 0.6952205676:
2: 12832: loss: 0.6953769189:
2: 16032: loss: 0.6949776167:
2: 19232: loss: 0.6946845177:
2: 22432: loss: 0.6946285874:
2: 25632: loss: 0.6946005385:
2: 28832: loss: 0.6944394124:
2: 32032: loss: 0.6943238094:
2: 35232: loss: 0.6943411747:
2: 38432: loss: 0.6943136059:
2: 41632: loss: 0.6942735498:
2: 44832: loss: 0.6941742794:
Dev-Acc: 2: Accuracy: 0.2788141072: precision: 0.0641027314: recall: 0.8352321034: f1: 0.1190672420
Train-Acc: 2: Accuracy: 0.4718514979: precision: 0.3714203066: recall: 0.8441259615: f1: 0.5158594645
3: 3232: loss: 0.6922685063:
3: 6432: loss: 0.6926713848:
3: 9632: loss: 0.6926499579:
3: 12832: loss: 0.6929384856:
3: 16032: loss: 0.6931760627:
3: 19232: loss: 0.6932026498:
3: 22432: loss: 0.6931425210:
3: 25632: loss: 0.6932269467:
3: 28832: loss: 0.6932291522:
3: 32032: loss: 0.6931527250:
3: 35232: loss: 0.6931480122:
3: 38432: loss: 0.6929760192:
3: 41632: loss: 0.6929080303:
3: 44832: loss: 0.6927734223:
Dev-Acc: 3: Accuracy: 0.2910878658: precision: 0.0646579199: recall: 0.8279204217: f1: 0.1199482663
Train-Acc: 3: Accuracy: 0.4851313829: precision: 0.3774918663: recall: 0.8390638354: f1: 0.5207156117
4: 3232: loss: 0.6928190613:
4: 6432: loss: 0.6927501819:
4: 9632: loss: 0.6921173910:
4: 12832: loss: 0.6922702350:
4: 16032: loss: 0.6923072202:
4: 19232: loss: 0.6923171829:
4: 22432: loss: 0.6921551091:
4: 25632: loss: 0.6919576984:
4: 28832: loss: 0.6918076183:
4: 32032: loss: 0.6917246690:
4: 35232: loss: 0.6916742300:
4: 38432: loss: 0.6915551312:
4: 41632: loss: 0.6916658628:
4: 44832: loss: 0.6916836954:
Dev-Acc: 4: Accuracy: 0.3044233322: precision: 0.0654098094: recall: 0.8217990138: f1: 0.1211748925
Train-Acc: 4: Accuracy: 0.4998575449: precision: 0.3847714199: recall: 0.8355137729: f1: 0.5268962086
5: 3232: loss: 0.6901994723:
5: 6432: loss: 0.6901271775:
5: 9632: loss: 0.6904154058:
5: 12832: loss: 0.6906125019:
5: 16032: loss: 0.6910062182:
5: 19232: loss: 0.6909906738:
5: 22432: loss: 0.6909046016:
5: 25632: loss: 0.6907979831:
5: 28832: loss: 0.6907539372:
5: 32032: loss: 0.6907118720:
5: 35232: loss: 0.6906624161:
5: 38432: loss: 0.6905580042:
5: 41632: loss: 0.6904996097:
5: 44832: loss: 0.6904593772:
Dev-Acc: 5: Accuracy: 0.3181953430: precision: 0.0659796093: recall: 0.8121067846: f1: 0.1220437732
Train-Acc: 5: Accuracy: 0.5143865347: precision: 0.3920526921: recall: 0.8295970022: f1: 0.5324697245
6: 3232: loss: 0.6893810695:
6: 6432: loss: 0.6894440135:
6: 9632: loss: 0.6897807411:
6: 12832: loss: 0.6897801982:
6: 16032: loss: 0.6898611319:
6: 19232: loss: 0.6895310113:
6: 22432: loss: 0.6894047444:
6: 25632: loss: 0.6895344419:
6: 28832: loss: 0.6895239209:
6: 32032: loss: 0.6893356459:
6: 35232: loss: 0.6890963893:
6: 38432: loss: 0.6892095590:
6: 41632: loss: 0.6891798785:
6: 44832: loss: 0.6892384426:
Dev-Acc: 6: Accuracy: 0.3313819468: precision: 0.0666647879: recall: 0.8044550247: f1: 0.1231261711
Train-Acc: 6: Accuracy: 0.5291565061: precision: 0.3998019992: recall: 0.8230228124: f1: 0.5381738458
7: 3232: loss: 0.6884231067:
7: 6432: loss: 0.6889694878:
7: 9632: loss: 0.6886844434:
7: 12832: loss: 0.6887962328:
7: 16032: loss: 0.6887092340:
7: 19232: loss: 0.6888481233:
7: 22432: loss: 0.6886619442:
7: 25632: loss: 0.6886334383:
7: 28832: loss: 0.6887110239:
7: 32032: loss: 0.6886580096:
7: 35232: loss: 0.6885570802:
7: 38432: loss: 0.6884235126:
7: 41632: loss: 0.6883284588:
7: 44832: loss: 0.6882531369:
Dev-Acc: 7: Accuracy: 0.3449654579: precision: 0.0674664825: recall: 0.7974834212: f1: 0.1244081329
Train-Acc: 7: Accuracy: 0.5446715951: precision: 0.4084465349: recall: 0.8163828808: f1: 0.5444819573
8: 3232: loss: 0.6868783617:
8: 6432: loss: 0.6871578041:
8: 9632: loss: 0.6871015590:
8: 12832: loss: 0.6865641560:
8: 16032: loss: 0.6864820648:
8: 19232: loss: 0.6864712200:
8: 22432: loss: 0.6869372419:
8: 25632: loss: 0.6869205551:
8: 28832: loss: 0.6867320806:
8: 32032: loss: 0.6866309936:
8: 35232: loss: 0.6866145666:
8: 38432: loss: 0.6866028621:
8: 41632: loss: 0.6865989607:
8: 44832: loss: 0.6866114447:
Dev-Acc: 8: Accuracy: 0.3592236936: precision: 0.0682182631: recall: 0.7884713484: f1: 0.1255720746
Train-Acc: 8: Accuracy: 0.5621151328: precision: 0.4189074345: recall: 0.8101374006: f1: 0.5522541902
9: 3232: loss: 0.6867027247:
9: 6432: loss: 0.6865190282:
9: 9632: loss: 0.6864411130:
9: 12832: loss: 0.6864179480:
9: 16032: loss: 0.6862620797:
9: 19232: loss: 0.6861093881:
9: 22432: loss: 0.6861600711:
9: 25632: loss: 0.6860864667:
9: 28832: loss: 0.6859924130:
9: 32032: loss: 0.6858427147:
9: 35232: loss: 0.6857382649:
9: 38432: loss: 0.6856081304:
9: 41632: loss: 0.6855343615:
9: 44832: loss: 0.6855782132:
Dev-Acc: 9: Accuracy: 0.3739879429: precision: 0.0693693829: recall: 0.7835402142: f1: 0.1274547768
Train-Acc: 9: Accuracy: 0.5809830427: precision: 0.4309626386: recall: 0.8023141148: f1: 0.5607296285
10: 3232: loss: 0.6862540936:
10: 6432: loss: 0.6859606385:
10: 9632: loss: 0.6854922849:
10: 12832: loss: 0.6848950000:
10: 16032: loss: 0.6846443167:
10: 19232: loss: 0.6845743469:
10: 22432: loss: 0.6845639479:
10: 25632: loss: 0.6845679112:
10: 28832: loss: 0.6843238537:
10: 32032: loss: 0.6843255943:
10: 35232: loss: 0.6843213731:
10: 38432: loss: 0.6842569315:
10: 41632: loss: 0.6842018569:
10: 44832: loss: 0.6842031320:
Dev-Acc: 10: Accuracy: 0.3883652091: precision: 0.0701489316: recall: 0.7736779459: f1: 0.1286346352
Train-Acc: 10: Accuracy: 0.5995441675: precision: 0.4438125986: recall: 0.7952797318: f1: 0.5696995385
11: 3232: loss: 0.6850005603:
11: 6432: loss: 0.6846052068:
11: 9632: loss: 0.6843117374:
11: 12832: loss: 0.6840431847:
11: 16032: loss: 0.6839662834:
11: 19232: loss: 0.6837406102:
11: 22432: loss: 0.6835920798:
11: 25632: loss: 0.6834997743:
11: 28832: loss: 0.6833301355:
11: 32032: loss: 0.6832311884:
11: 35232: loss: 0.6832358643:
11: 38432: loss: 0.6831363560:
11: 41632: loss: 0.6830415074:
11: 44832: loss: 0.6830396664:
Dev-Acc: 11: Accuracy: 0.4022364616: precision: 0.0711671347: recall: 0.7670464207: f1: 0.1302496138
Train-Acc: 11: Accuracy: 0.6177546978: precision: 0.4574240806: recall: 0.7882453488: f1: 0.5789054390
12: 3232: loss: 0.6835223955:
12: 6432: loss: 0.6827812254:
12: 9632: loss: 0.6825304455:
12: 12832: loss: 0.6825472470:
12: 16032: loss: 0.6826760967:
12: 19232: loss: 0.6825686076:
12: 22432: loss: 0.6824122155:
12: 25632: loss: 0.6822993735:
12: 28832: loss: 0.6823713201:
12: 32032: loss: 0.6823400684:
12: 35232: loss: 0.6822245087:
12: 38432: loss: 0.6819972879:
12: 41632: loss: 0.6819049390:
12: 44832: loss: 0.6817981639:
Dev-Acc: 12: Accuracy: 0.4164748192: precision: 0.0721111093: recall: 0.7583744261: f1: 0.1316993947
Train-Acc: 12: Accuracy: 0.6370828152: precision: 0.4730904162: recall: 0.7801590954: f1: 0.5890060802
13: 3232: loss: 0.6821299201:
13: 6432: loss: 0.6816765648:
13: 9632: loss: 0.6812253433:
13: 12832: loss: 0.6809659843:
13: 16032: loss: 0.6807004658:
13: 19232: loss: 0.6806998003:
13: 22432: loss: 0.6808456056:
13: 25632: loss: 0.6807662931:
13: 28832: loss: 0.6808464855:
13: 32032: loss: 0.6807743034:
13: 35232: loss: 0.6807587203:
13: 38432: loss: 0.6807018836:
13: 41632: loss: 0.6806728192:
13: 44832: loss: 0.6806737895:
Dev-Acc: 13: Accuracy: 0.4314772189: precision: 0.0731541284: recall: 0.7491923142: f1: 0.1332929965
Train-Acc: 13: Accuracy: 0.6556877494: precision: 0.4895507446: recall: 0.7715469068: f1: 0.5990200082
14: 3232: loss: 0.6791062343:
14: 6432: loss: 0.6791662368:
14: 9632: loss: 0.6795265830:
14: 12832: loss: 0.6796496443:
14: 16032: loss: 0.6797341316:
14: 19232: loss: 0.6798342094:
14: 22432: loss: 0.6797170956:
14: 25632: loss: 0.6796390301:
14: 28832: loss: 0.6795862270:
14: 32032: loss: 0.6795696378:
14: 35232: loss: 0.6796320935:
14: 38432: loss: 0.6795867218:
14: 41632: loss: 0.6794813286:
14: 44832: loss: 0.6794225623:
Dev-Acc: 14: Accuracy: 0.4449317157: precision: 0.0740576874: recall: 0.7400102023: f1: 0.1346409677
Train-Acc: 14: Accuracy: 0.6729340553: precision: 0.5062293082: recall: 0.7639865887: f1: 0.6089553803
15: 3232: loss: 0.6793549478:
15: 6432: loss: 0.6789116442:
15: 9632: loss: 0.6788592941:
15: 12832: loss: 0.6790498336:
15: 16032: loss: 0.6790985538:
15: 19232: loss: 0.6789883883:
15: 22432: loss: 0.6787629558:
15: 25632: loss: 0.6787091428:
15: 28832: loss: 0.6784665944:
15: 32032: loss: 0.6785032293:
15: 35232: loss: 0.6785521017:
15: 38432: loss: 0.6784929963:
15: 41632: loss: 0.6783710261:
15: 44832: loss: 0.6783283375:
Dev-Acc: 15: Accuracy: 0.4584557116: precision: 0.0747790856: recall: 0.7281074647: f1: 0.1356286524
Train-Acc: 15: Accuracy: 0.6899611950: precision: 0.5242638667: recall: 0.7549799487: f1: 0.6188166828
16: 3232: loss: 0.6781112868:
16: 6432: loss: 0.6786856699:
16: 9632: loss: 0.6784668136:
16: 12832: loss: 0.6780580108:
16: 16032: loss: 0.6778897949:
16: 19232: loss: 0.6777724103:
16: 22432: loss: 0.6776996443:
16: 25632: loss: 0.6776327623:
16: 28832: loss: 0.6775860872:
16: 32032: loss: 0.6775392728:
16: 35232: loss: 0.6775075254:
16: 38432: loss: 0.6775589430:
16: 41632: loss: 0.6774664918:
16: 44832: loss: 0.6773947666:
Dev-Acc: 16: Accuracy: 0.4720491171: precision: 0.0757162836: recall: 0.7180751573: f1: 0.1369880788
Train-Acc: 16: Accuracy: 0.7061775327: precision: 0.5432187545: recall: 0.7449214384: f1: 0.6282783477
17: 3232: loss: 0.6755949122:
17: 6432: loss: 0.6753911099:
17: 9632: loss: 0.6754369609:
17: 12832: loss: 0.6754540226:
17: 16032: loss: 0.6755130544:
17: 19232: loss: 0.6754430168:
17: 22432: loss: 0.6756680554:
17: 25632: loss: 0.6754923424:
17: 28832: loss: 0.6755971151:
17: 32032: loss: 0.6755414241:
17: 35232: loss: 0.6755731534:
17: 38432: loss: 0.6755540663:
17: 41632: loss: 0.6756297792:
17: 44832: loss: 0.6755162962:
Dev-Acc: 17: Accuracy: 0.4863470197: precision: 0.0768521422: recall: 0.7085529672: f1: 0.1386642707
Train-Acc: 17: Accuracy: 0.7207503319: precision: 0.5619042841: recall: 0.7363749918: f1: 0.6374164177
18: 3232: loss: 0.6743824303:
18: 6432: loss: 0.6746519339:
18: 9632: loss: 0.6747377678:
18: 12832: loss: 0.6751932272:
18: 16032: loss: 0.6752617255:
18: 19232: loss: 0.6752928473:
18: 22432: loss: 0.6749371668:
18: 25632: loss: 0.6748052994:
18: 28832: loss: 0.6747362285:
18: 32032: loss: 0.6748164976:
18: 35232: loss: 0.6748378070:
18: 38432: loss: 0.6748035190:
18: 41632: loss: 0.6747694453:
18: 44832: loss: 0.6747173118:
Dev-Acc: 18: Accuracy: 0.5008235574: precision: 0.0782899233: recall: 0.7012412855: f1: 0.1408542104
Train-Acc: 18: Accuracy: 0.7338767648: precision: 0.5806808018: recall: 0.7255933206: f1: 0.6450990707
19: 3232: loss: 0.6738790292:
19: 6432: loss: 0.6747089341:
19: 9632: loss: 0.6749749174:
19: 12832: loss: 0.6746273480:
19: 16032: loss: 0.6742089736:
19: 19232: loss: 0.6741317763:
19: 22432: loss: 0.6739641316:
19: 25632: loss: 0.6738729994:
19: 28832: loss: 0.6738147484:
19: 32032: loss: 0.6737954525:
19: 35232: loss: 0.6737355137:
19: 38432: loss: 0.6736740687:
19: 41632: loss: 0.6735550835:
19: 44832: loss: 0.6735838262:
Dev-Acc: 19: Accuracy: 0.5139704943: precision: 0.0798682184: recall: 0.6966502296: f1: 0.1433068663
Train-Acc: 19: Accuracy: 0.7454254627: precision: 0.5988666373: recall: 0.7156005522: f1: 0.6520501992
20: 3232: loss: 0.6743968099:
20: 6432: loss: 0.6738173351:
20: 9632: loss: 0.6732712326:
20: 12832: loss: 0.6728201541:
20: 16032: loss: 0.6726537493:
20: 19232: loss: 0.6725975709:
20: 22432: loss: 0.6725080264:
20: 25632: loss: 0.6723245696:
20: 28832: loss: 0.6724872126:
20: 32032: loss: 0.6723941894:
20: 35232: loss: 0.6723164115:
20: 38432: loss: 0.6723132496:
20: 41632: loss: 0.6723303232:
20: 44832: loss: 0.6722698761:
Dev-Acc: 20: Accuracy: 0.5269288421: precision: 0.0812007775: recall: 0.6889984696: f1: 0.1452798394
Train-Acc: 20: Accuracy: 0.7566892505: precision: 0.6179917279: recall: 0.7072513313: f1: 0.6596155615
