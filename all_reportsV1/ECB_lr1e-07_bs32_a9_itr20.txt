1: 3232: loss: 0.7111244577:
1: 6432: loss: 0.7094125569:
1: 9632: loss: 0.7083820947:
1: 12832: loss: 0.7074148309:
1: 16032: loss: 0.7065620910:
1: 19232: loss: 0.7057542194:
1: 22432: loss: 0.7047478926:
1: 25632: loss: 0.7037866612:
1: 28832: loss: 0.7030865491:
1: 32032: loss: 0.7021649307:
1: 35232: loss: 0.7013409737:
1: 38432: loss: 0.7003973394:
1: 41632: loss: 0.6993758652:
1: 44832: loss: 0.6984379728:
1: 48032: loss: 0.6975267936:
1: 51232: loss: 0.6966352424:
1: 54432: loss: 0.6956952740:
1: 57632: loss: 0.6948251848:
1: 60832: loss: 0.6939174083:
1: 64032: loss: 0.6929972117:
1: 67232: loss: 0.6920330500:
1: 70432: loss: 0.6911362050:
1: 73632: loss: 0.6903389028:
1: 76832: loss: 0.6894300652:
1: 80032: loss: 0.6885198372:
1: 83232: loss: 0.6876142357:
1: 86432: loss: 0.6867421676:
1: 89632: loss: 0.6858704972:
1: 92832: loss: 0.6850314956:
1: 96032: loss: 0.6841437631:
1: 99232: loss: 0.6832517824:
1: 102432: loss: 0.6823782315:
1: 105632: loss: 0.6815177701:
1: 108832: loss: 0.6806982373:
1: 112032: loss: 0.6798611891:
1: 115232: loss: 0.6789700724:
1: 118432: loss: 0.6781358388:
1: 121632: loss: 0.6772824614:
1: 124832: loss: 0.6764577491:
1: 128032: loss: 0.6756301627:
1: 131232: loss: 0.6748143931:
1: 134432: loss: 0.6739744677:
1: 137632: loss: 0.6731360874:
1: 140832: loss: 0.6722692980:
1: 144032: loss: 0.6714017851:
1: 147232: loss: 0.6705698666:
1: 150432: loss: 0.6697083469:
Dev-Acc: 1: Accuracy: 0.9352873564: precision: 0.0709504685: recall: 0.0090120728: f1: 0.0159927580
Train-Acc: 1: Accuracy: 0.8952468634: precision: 0.2578700603: recall: 0.0253106305: f1: 0.0460967433
2: 3232: loss: 0.6287059605:
2: 6432: loss: 0.6281528375:
2: 9632: loss: 0.6269738352:
2: 12832: loss: 0.6262593848:
2: 16032: loss: 0.6258687074:
2: 19232: loss: 0.6248125014:
2: 22432: loss: 0.6238944697:
2: 25632: loss: 0.6227080821:
2: 28832: loss: 0.6222386575:
2: 32032: loss: 0.6213797609:
2: 35232: loss: 0.6208388061:
2: 38432: loss: 0.6202462053:
2: 41632: loss: 0.6197291995:
2: 44832: loss: 0.6189679228:
2: 48032: loss: 0.6182607930:
2: 51232: loss: 0.6175890736:
2: 54432: loss: 0.6167038232:
2: 57632: loss: 0.6157203862:
2: 60832: loss: 0.6149861505:
2: 64032: loss: 0.6141333520:
2: 67232: loss: 0.6133992926:
2: 70432: loss: 0.6127012249:
2: 73632: loss: 0.6119510170:
2: 76832: loss: 0.6112364980:
2: 80032: loss: 0.6103247424:
2: 83232: loss: 0.6094769344:
2: 86432: loss: 0.6086784151:
2: 89632: loss: 0.6079074214:
2: 92832: loss: 0.6071721792:
2: 96032: loss: 0.6064485412:
2: 99232: loss: 0.6056695262:
2: 102432: loss: 0.6049351234:
2: 105632: loss: 0.6040841557:
2: 108832: loss: 0.6032260604:
2: 112032: loss: 0.6023864001:
2: 115232: loss: 0.6016858016:
2: 118432: loss: 0.6009249317:
2: 121632: loss: 0.6001332131:
2: 124832: loss: 0.5992123716:
2: 128032: loss: 0.5984274664:
2: 131232: loss: 0.5976618746:
2: 134432: loss: 0.5968648896:
2: 137632: loss: 0.5960876519:
2: 140832: loss: 0.5952446495:
2: 144032: loss: 0.5944984645:
2: 147232: loss: 0.5937823924:
2: 150432: loss: 0.5930545751:
Dev-Acc: 2: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 2: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
3: 3232: loss: 0.5563245171:
3: 6432: loss: 0.5567201337:
3: 9632: loss: 0.5554723357:
3: 12832: loss: 0.5534426728:
3: 16032: loss: 0.5527211322:
3: 19232: loss: 0.5524475500:
3: 22432: loss: 0.5516392888:
3: 25632: loss: 0.5507649744:
3: 28832: loss: 0.5502144073:
3: 32032: loss: 0.5498154551:
3: 35232: loss: 0.5491912520:
3: 38432: loss: 0.5482335689:
3: 41632: loss: 0.5474092528:
3: 44832: loss: 0.5464804520:
3: 48032: loss: 0.5455288795:
3: 51232: loss: 0.5450033374:
3: 54432: loss: 0.5444944425:
3: 57632: loss: 0.5439750802:
3: 60832: loss: 0.5430826939:
3: 64032: loss: 0.5424505071:
3: 67232: loss: 0.5417839313:
3: 70432: loss: 0.5408909609:
3: 73632: loss: 0.5404338519:
3: 76832: loss: 0.5397704324:
3: 80032: loss: 0.5391170383:
3: 83232: loss: 0.5385127973:
3: 86432: loss: 0.5377712782:
3: 89632: loss: 0.5370539374:
3: 92832: loss: 0.5362980399:
3: 96032: loss: 0.5356231990:
3: 99232: loss: 0.5347942539:
3: 102432: loss: 0.5341206611:
3: 105632: loss: 0.5334577984:
3: 108832: loss: 0.5327515791:
3: 112032: loss: 0.5322165992:
3: 115232: loss: 0.5314452467:
3: 118432: loss: 0.5309004305:
3: 121632: loss: 0.5302616488:
3: 124832: loss: 0.5295418924:
3: 128032: loss: 0.5287446705:
3: 131232: loss: 0.5279492861:
3: 134432: loss: 0.5273709668:
3: 137632: loss: 0.5267129966:
3: 140832: loss: 0.5261198953:
3: 144032: loss: 0.5255486537:
3: 147232: loss: 0.5249408234:
3: 150432: loss: 0.5242064102:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.4913266388:
4: 6432: loss: 0.4904819937:
4: 9632: loss: 0.4900741268:
4: 12832: loss: 0.4892859878:
4: 16032: loss: 0.4889517115:
4: 19232: loss: 0.4879994324:
4: 22432: loss: 0.4882463528:
4: 25632: loss: 0.4873012979:
4: 28832: loss: 0.4876868527:
4: 32032: loss: 0.4868017220:
4: 35232: loss: 0.4863861971:
4: 38432: loss: 0.4856660393:
4: 41632: loss: 0.4851257729:
4: 44832: loss: 0.4840123196:
4: 48032: loss: 0.4833452420:
4: 51232: loss: 0.4825029410:
4: 54432: loss: 0.4816464439:
4: 57632: loss: 0.4810352353:
4: 60832: loss: 0.4802902945:
4: 64032: loss: 0.4799512610:
4: 67232: loss: 0.4794841904:
4: 70432: loss: 0.4790989842:
4: 73632: loss: 0.4784338160:
4: 76832: loss: 0.4775538240:
4: 80032: loss: 0.4767078689:
4: 83232: loss: 0.4759508113:
4: 86432: loss: 0.4752266710:
4: 89632: loss: 0.4748427138:
4: 92832: loss: 0.4741563484:
4: 96032: loss: 0.4736528850:
4: 99232: loss: 0.4728215964:
4: 102432: loss: 0.4723767934:
4: 105632: loss: 0.4716076901:
4: 108832: loss: 0.4710934661:
4: 112032: loss: 0.4703090848:
4: 115232: loss: 0.4697288640:
4: 118432: loss: 0.4689530546:
4: 121632: loss: 0.4683378826:
4: 124832: loss: 0.4677239979:
4: 128032: loss: 0.4670731129:
4: 131232: loss: 0.4667640255:
4: 134432: loss: 0.4662299574:
4: 137632: loss: 0.4657622840:
4: 140832: loss: 0.4653106958:
4: 144032: loss: 0.4648016637:
4: 147232: loss: 0.4641017948:
4: 150432: loss: 0.4636714168:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4355729342:
5: 6432: loss: 0.4349409121:
5: 9632: loss: 0.4336597661:
5: 12832: loss: 0.4356011024:
5: 16032: loss: 0.4338588110:
5: 19232: loss: 0.4332160010:
5: 22432: loss: 0.4335530419:
5: 25632: loss: 0.4322853152:
5: 28832: loss: 0.4315384238:
5: 32032: loss: 0.4306270387:
5: 35232: loss: 0.4316708881:
5: 38432: loss: 0.4306295384:
5: 41632: loss: 0.4306276228:
5: 44832: loss: 0.4299086999:
5: 48032: loss: 0.4301309132:
5: 51232: loss: 0.4295664569:
5: 54432: loss: 0.4296146717:
5: 57632: loss: 0.4290034321:
5: 60832: loss: 0.4285042084:
5: 64032: loss: 0.4280729602:
5: 67232: loss: 0.4276458137:
5: 70432: loss: 0.4270683568:
5: 73632: loss: 0.4263049096:
5: 76832: loss: 0.4257095594:
5: 80032: loss: 0.4253398481:
5: 83232: loss: 0.4248018526:
5: 86432: loss: 0.4242744426:
5: 89632: loss: 0.4240041372:
5: 92832: loss: 0.4234627310:
5: 96032: loss: 0.4232178595:
5: 99232: loss: 0.4227228286:
5: 102432: loss: 0.4224543314:
5: 105632: loss: 0.4219554033:
5: 108832: loss: 0.4211990911:
5: 112032: loss: 0.4206348663:
5: 115232: loss: 0.4201463330:
5: 118432: loss: 0.4195248246:
5: 121632: loss: 0.4188151951:
5: 124832: loss: 0.4180038733:
5: 128032: loss: 0.4174258727:
5: 131232: loss: 0.4169535864:
5: 134432: loss: 0.4164034331:
5: 137632: loss: 0.4157973167:
5: 140832: loss: 0.4153193439:
5: 144032: loss: 0.4150163963:
5: 147232: loss: 0.4143594290:
5: 150432: loss: 0.4139559579:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.3870534724:
6: 6432: loss: 0.3858283243:
6: 9632: loss: 0.3880040434:
6: 12832: loss: 0.3911829364:
6: 16032: loss: 0.3904379705:
6: 19232: loss: 0.3904791996:
6: 22432: loss: 0.3912221542:
6: 25632: loss: 0.3906769877:
6: 28832: loss: 0.3893098807:
6: 32032: loss: 0.3887798075:
6: 35232: loss: 0.3888279248:
6: 38432: loss: 0.3882102260:
6: 41632: loss: 0.3874578896:
6: 44832: loss: 0.3873762195:
6: 48032: loss: 0.3877667080:
6: 51232: loss: 0.3875775048:
6: 54432: loss: 0.3875433847:
6: 57632: loss: 0.3875900075:
6: 60832: loss: 0.3868994794:
6: 64032: loss: 0.3863669859:
6: 67232: loss: 0.3856100938:
6: 70432: loss: 0.3846605896:
6: 73632: loss: 0.3841123354:
6: 76832: loss: 0.3839903802:
6: 80032: loss: 0.3837259664:
6: 83232: loss: 0.3833767582:
6: 86432: loss: 0.3831556223:
6: 89632: loss: 0.3826309029:
6: 92832: loss: 0.3820850027:
6: 96032: loss: 0.3818646604:
6: 99232: loss: 0.3813393564:
6: 102432: loss: 0.3803738394:
6: 105632: loss: 0.3802909010:
6: 108832: loss: 0.3797729670:
6: 112032: loss: 0.3794262857:
6: 115232: loss: 0.3790689380:
6: 118432: loss: 0.3789025076:
6: 121632: loss: 0.3784306674:
6: 124832: loss: 0.3782167321:
6: 128032: loss: 0.3776292483:
6: 131232: loss: 0.3773067410:
6: 134432: loss: 0.3766483202:
6: 137632: loss: 0.3762518825:
6: 140832: loss: 0.3758140733:
6: 144032: loss: 0.3754691248:
6: 147232: loss: 0.3751705095:
6: 150432: loss: 0.3748053116:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.3438250795:
7: 6432: loss: 0.3606323738:
7: 9632: loss: 0.3607528919:
7: 12832: loss: 0.3604182174:
7: 16032: loss: 0.3608775521:
7: 19232: loss: 0.3588995270:
7: 22432: loss: 0.3585622026:
7: 25632: loss: 0.3575903645:
7: 28832: loss: 0.3583907807:
7: 32032: loss: 0.3590011830:
7: 35232: loss: 0.3579084848:
7: 38432: loss: 0.3559903879:
7: 41632: loss: 0.3558013327:
7: 44832: loss: 0.3548727339:
7: 48032: loss: 0.3548455405:
7: 51232: loss: 0.3549205866:
7: 54432: loss: 0.3546092441:
7: 57632: loss: 0.3544438936:
7: 60832: loss: 0.3541005430:
7: 64032: loss: 0.3538793912:
7: 67232: loss: 0.3532390817:
7: 70432: loss: 0.3529138730:
7: 73632: loss: 0.3528403012:
7: 76832: loss: 0.3519573421:
7: 80032: loss: 0.3520395543:
7: 83232: loss: 0.3519882879:
7: 86432: loss: 0.3521106657:
7: 89632: loss: 0.3513621932:
7: 92832: loss: 0.3512728999:
7: 96032: loss: 0.3507967698:
7: 99232: loss: 0.3502152769:
7: 102432: loss: 0.3499930207:
7: 105632: loss: 0.3495499822:
7: 108832: loss: 0.3492310659:
7: 112032: loss: 0.3492743715:
7: 115232: loss: 0.3489429471:
7: 118432: loss: 0.3487784253:
7: 121632: loss: 0.3485139410:
7: 124832: loss: 0.3486293694:
7: 128032: loss: 0.3482315072:
7: 131232: loss: 0.3479570371:
7: 134432: loss: 0.3474480155:
7: 137632: loss: 0.3470585133:
7: 140832: loss: 0.3469573817:
7: 144032: loss: 0.3465027452:
7: 147232: loss: 0.3464266725:
7: 150432: loss: 0.3463399001:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3402914742:
8: 6432: loss: 0.3408429311:
8: 9632: loss: 0.3381149892:
8: 12832: loss: 0.3368816010:
8: 16032: loss: 0.3382071004:
8: 19232: loss: 0.3354449291:
8: 22432: loss: 0.3348411033:
8: 25632: loss: 0.3341500592:
8: 28832: loss: 0.3345893618:
8: 32032: loss: 0.3348601366:
8: 35232: loss: 0.3356442361:
8: 38432: loss: 0.3356340561:
8: 41632: loss: 0.3360277096:
8: 44832: loss: 0.3359685918:
8: 48032: loss: 0.3356729834:
8: 51232: loss: 0.3348315252:
8: 54432: loss: 0.3335688860:
8: 57632: loss: 0.3328852866:
8: 60832: loss: 0.3324579335:
8: 64032: loss: 0.3321752055:
8: 67232: loss: 0.3315366790:
8: 70432: loss: 0.3305003279:
8: 73632: loss: 0.3303691184:
8: 76832: loss: 0.3302712013:
8: 80032: loss: 0.3300826797:
8: 83232: loss: 0.3298316133:
8: 86432: loss: 0.3295049081:
8: 89632: loss: 0.3287838688:
8: 92832: loss: 0.3285679857:
8: 96032: loss: 0.3283303474:
8: 99232: loss: 0.3278768979:
8: 102432: loss: 0.3272710729:
8: 105632: loss: 0.3274599965:
8: 108832: loss: 0.3275729912:
8: 112032: loss: 0.3272896419:
8: 115232: loss: 0.3272543317:
8: 118432: loss: 0.3269494619:
8: 121632: loss: 0.3270012538:
8: 124832: loss: 0.3265824091:
8: 128032: loss: 0.3261817861:
8: 131232: loss: 0.3259275464:
8: 134432: loss: 0.3257951654:
8: 137632: loss: 0.3251410086:
8: 140832: loss: 0.3253900287:
8: 144032: loss: 0.3254039521:
8: 147232: loss: 0.3255554894:
8: 150432: loss: 0.3251731733:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3124183159:
9: 6432: loss: 0.3144356333:
9: 9632: loss: 0.3176171371:
9: 12832: loss: 0.3157209527:
9: 16032: loss: 0.3164894571:
9: 19232: loss: 0.3131324935:
9: 22432: loss: 0.3132864449:
9: 25632: loss: 0.3128764573:
9: 28832: loss: 0.3115157354:
9: 32032: loss: 0.3119829738:
9: 35232: loss: 0.3113781272:
9: 38432: loss: 0.3102617054:
9: 41632: loss: 0.3098571982:
9: 44832: loss: 0.3101988715:
9: 48032: loss: 0.3104433051:
9: 51232: loss: 0.3100427888:
9: 54432: loss: 0.3105441177:
9: 57632: loss: 0.3103633581:
9: 60832: loss: 0.3101933297:
9: 64032: loss: 0.3104872546:
9: 67232: loss: 0.3111016038:
9: 70432: loss: 0.3108378686:
9: 73632: loss: 0.3112826637:
9: 76832: loss: 0.3118280027:
9: 80032: loss: 0.3112811057:
9: 83232: loss: 0.3114561357:
9: 86432: loss: 0.3111984405:
9: 89632: loss: 0.3109063735:
9: 92832: loss: 0.3106059638:
9: 96032: loss: 0.3107021359:
9: 99232: loss: 0.3113881483:
9: 102432: loss: 0.3114812552:
9: 105632: loss: 0.3113340013:
9: 108832: loss: 0.3114450738:
9: 112032: loss: 0.3111627518:
9: 115232: loss: 0.3114373798:
9: 118432: loss: 0.3114046421:
9: 121632: loss: 0.3111898903:
9: 124832: loss: 0.3111552517:
9: 128032: loss: 0.3110230071:
9: 131232: loss: 0.3107881522:
9: 134432: loss: 0.3107739929:
9: 137632: loss: 0.3101921454:
9: 140832: loss: 0.3100451589:
9: 144032: loss: 0.3096932708:
9: 147232: loss: 0.3096000831:
9: 150432: loss: 0.3094847892:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.2917311358:
10: 6432: loss: 0.3051430292:
10: 9632: loss: 0.3029045116:
10: 12832: loss: 0.2988844875:
10: 16032: loss: 0.2956851780:
10: 19232: loss: 0.2985717031:
10: 22432: loss: 0.3000133289:
10: 25632: loss: 0.3012894201:
10: 28832: loss: 0.3013947346:
10: 32032: loss: 0.3022518711:
10: 35232: loss: 0.3008248667:
10: 38432: loss: 0.2995748415:
10: 41632: loss: 0.2983818144:
10: 44832: loss: 0.2988742240:
10: 48032: loss: 0.2987071459:
10: 51232: loss: 0.2989301325:
10: 54432: loss: 0.2990327109:
10: 57632: loss: 0.2990271806:
10: 60832: loss: 0.2990651213:
10: 64032: loss: 0.2989332847:
10: 67232: loss: 0.2989874891:
10: 70432: loss: 0.2990688933:
10: 73632: loss: 0.2983475343:
10: 76832: loss: 0.2976803058:
10: 80032: loss: 0.2975186271:
10: 83232: loss: 0.2982874227:
10: 86432: loss: 0.2986150682:
10: 89632: loss: 0.2983210550:
10: 92832: loss: 0.2987906282:
10: 96032: loss: 0.2986786515:
10: 99232: loss: 0.2983758808:
10: 102432: loss: 0.2979320936:
10: 105632: loss: 0.2980471565:
10: 108832: loss: 0.2978801825:
10: 112032: loss: 0.2973965167:
10: 115232: loss: 0.2975021831:
10: 118432: loss: 0.2972448163:
10: 121632: loss: 0.2972783582:
10: 124832: loss: 0.2971802640:
10: 128032: loss: 0.2970843292:
10: 131232: loss: 0.2969066683:
10: 134432: loss: 0.2969188223:
10: 137632: loss: 0.2967435673:
10: 140832: loss: 0.2963233908:
10: 144032: loss: 0.2964463405:
10: 147232: loss: 0.2965499415:
10: 150432: loss: 0.2966580023:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.2922160549:
11: 6432: loss: 0.2978521585:
11: 9632: loss: 0.2941921922:
11: 12832: loss: 0.2929243619:
11: 16032: loss: 0.2946414612:
11: 19232: loss: 0.2929871072:
11: 22432: loss: 0.2914759812:
11: 25632: loss: 0.2910312415:
11: 28832: loss: 0.2905834883:
11: 32032: loss: 0.2901246857:
11: 35232: loss: 0.2890931998:
11: 38432: loss: 0.2889766393:
11: 41632: loss: 0.2882968954:
11: 44832: loss: 0.2872743818:
11: 48032: loss: 0.2877662631:
11: 51232: loss: 0.2877825084:
11: 54432: loss: 0.2869865765:
11: 57632: loss: 0.2867231728:
11: 60832: loss: 0.2877226775:
11: 64032: loss: 0.2865390585:
11: 67232: loss: 0.2865505718:
11: 70432: loss: 0.2870612016:
11: 73632: loss: 0.2875339773:
11: 76832: loss: 0.2878269403:
11: 80032: loss: 0.2873952661:
11: 83232: loss: 0.2878718765:
11: 86432: loss: 0.2875314206:
11: 89632: loss: 0.2871905246:
11: 92832: loss: 0.2868564941:
11: 96032: loss: 0.2871262832:
11: 99232: loss: 0.2869141660:
11: 102432: loss: 0.2868509102:
11: 105632: loss: 0.2865797515:
11: 108832: loss: 0.2871461891:
11: 112032: loss: 0.2871668354:
11: 115232: loss: 0.2871721850:
11: 118432: loss: 0.2873976743:
11: 121632: loss: 0.2867607239:
11: 124832: loss: 0.2864696809:
11: 128032: loss: 0.2867967029:
11: 131232: loss: 0.2867141461:
11: 134432: loss: 0.2863841391:
11: 137632: loss: 0.2862115715:
11: 140832: loss: 0.2859508022:
11: 144032: loss: 0.2860572803:
11: 147232: loss: 0.2864137045:
11: 150432: loss: 0.2863606606:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.2812590869:
12: 6432: loss: 0.2895238905:
12: 9632: loss: 0.2829718986:
12: 12832: loss: 0.2821504930:
12: 16032: loss: 0.2818073618:
12: 19232: loss: 0.2826074572:
12: 22432: loss: 0.2827553565:
12: 25632: loss: 0.2823754240:
12: 28832: loss: 0.2825083807:
12: 32032: loss: 0.2821871806:
12: 35232: loss: 0.2828284562:
12: 38432: loss: 0.2822660418:
12: 41632: loss: 0.2819120634:
12: 44832: loss: 0.2820847602:
12: 48032: loss: 0.2816012771:
12: 51232: loss: 0.2806598491:
12: 54432: loss: 0.2805626340:
12: 57632: loss: 0.2804288840:
12: 60832: loss: 0.2807847059:
12: 64032: loss: 0.2806010726:
12: 67232: loss: 0.2797511138:
12: 70432: loss: 0.2783706083:
12: 73632: loss: 0.2786692158:
12: 76832: loss: 0.2792389571:
12: 80032: loss: 0.2794075184:
12: 83232: loss: 0.2796942107:
12: 86432: loss: 0.2794067545:
12: 89632: loss: 0.2790451973:
12: 92832: loss: 0.2785094886:
12: 96032: loss: 0.2788597408:
12: 99232: loss: 0.2791206823:
12: 102432: loss: 0.2789817606:
12: 105632: loss: 0.2788084020:
12: 108832: loss: 0.2788361523:
12: 112032: loss: 0.2782023850:
12: 115232: loss: 0.2782025643:
12: 118432: loss: 0.2785239330:
12: 121632: loss: 0.2783665680:
12: 124832: loss: 0.2782860792:
12: 128032: loss: 0.2782535201:
12: 131232: loss: 0.2781060658:
12: 134432: loss: 0.2779388142:
12: 137632: loss: 0.2779476300:
12: 140832: loss: 0.2781899295:
12: 144032: loss: 0.2779295231:
12: 147232: loss: 0.2778389598:
12: 150432: loss: 0.2776198754:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2909319723:
13: 6432: loss: 0.2832344667:
13: 9632: loss: 0.2725583443:
13: 12832: loss: 0.2763910281:
13: 16032: loss: 0.2778423490:
13: 19232: loss: 0.2774065713:
13: 22432: loss: 0.2787256910:
13: 25632: loss: 0.2770683816:
13: 28832: loss: 0.2743068188:
13: 32032: loss: 0.2740117814:
13: 35232: loss: 0.2732500665:
13: 38432: loss: 0.2731575733:
13: 41632: loss: 0.2740110856:
13: 44832: loss: 0.2730424206:
13: 48032: loss: 0.2733571729:
13: 51232: loss: 0.2728871004:
13: 54432: loss: 0.2728543167:
13: 57632: loss: 0.2720750458:
13: 60832: loss: 0.2720237409:
13: 64032: loss: 0.2716620059:
13: 67232: loss: 0.2715882533:
13: 70432: loss: 0.2721655395:
13: 73632: loss: 0.2728692305:
13: 76832: loss: 0.2724331759:
13: 80032: loss: 0.2724994168:
13: 83232: loss: 0.2723037643:
13: 86432: loss: 0.2716219535:
13: 89632: loss: 0.2715420941:
13: 92832: loss: 0.2716445954:
13: 96032: loss: 0.2718004201:
13: 99232: loss: 0.2718710832:
13: 102432: loss: 0.2717380231:
13: 105632: loss: 0.2716678897:
13: 108832: loss: 0.2717803086:
13: 112032: loss: 0.2714744500:
13: 115232: loss: 0.2715163305:
13: 118432: loss: 0.2712046785:
13: 121632: loss: 0.2709387110:
13: 124832: loss: 0.2710109989:
13: 128032: loss: 0.2711268447:
13: 131232: loss: 0.2712587748:
13: 134432: loss: 0.2710896598:
13: 137632: loss: 0.2710006970:
13: 140832: loss: 0.2708458445:
13: 144032: loss: 0.2703629536:
13: 147232: loss: 0.2700486588:
13: 150432: loss: 0.2696808907:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.9003484249: precision: 1.0000000000: recall: 0.0034843206: f1: 0.0069444444
14: 3232: loss: 0.2697176825:
14: 6432: loss: 0.2739817545:
14: 9632: loss: 0.2705094926:
14: 12832: loss: 0.2704290625:
14: 16032: loss: 0.2664338295:
14: 19232: loss: 0.2677733593:
14: 22432: loss: 0.2670274546:
14: 25632: loss: 0.2667878072:
14: 28832: loss: 0.2667911119:
14: 32032: loss: 0.2680442902:
14: 35232: loss: 0.2673563329:
14: 38432: loss: 0.2672762421:
14: 41632: loss: 0.2669564128:
14: 44832: loss: 0.2670316639:
14: 48032: loss: 0.2660517046:
14: 51232: loss: 0.2670853156:
14: 54432: loss: 0.2671736895:
14: 57632: loss: 0.2657793370:
14: 60832: loss: 0.2665261244:
14: 64032: loss: 0.2664557096:
14: 67232: loss: 0.2661879096:
14: 70432: loss: 0.2654436451:
14: 73632: loss: 0.2659946834:
14: 76832: loss: 0.2650793081:
14: 80032: loss: 0.2650875855:
14: 83232: loss: 0.2656816511:
14: 86432: loss: 0.2656412710:
14: 89632: loss: 0.2653627500:
14: 92832: loss: 0.2654077407:
14: 96032: loss: 0.2655167488:
14: 99232: loss: 0.2658105388:
14: 102432: loss: 0.2649934244:
14: 105632: loss: 0.2646921381:
14: 108832: loss: 0.2648121517:
14: 112032: loss: 0.2647695847:
14: 115232: loss: 0.2648145519:
14: 118432: loss: 0.2643823522:
14: 121632: loss: 0.2641784138:
14: 124832: loss: 0.2637773205:
14: 128032: loss: 0.2636116885:
14: 131232: loss: 0.2633464267:
14: 134432: loss: 0.2634513824:
14: 137632: loss: 0.2634355675:
14: 140832: loss: 0.2631667561:
14: 144032: loss: 0.2629625356:
14: 147232: loss: 0.2625727073:
14: 150432: loss: 0.2623206675:
Dev-Acc: 14: Accuracy: 0.9418558478: precision: 0.6567164179: recall: 0.0074817208: f1: 0.0147948890
Train-Acc: 14: Accuracy: 0.9020182490: precision: 0.8139059305: recall: 0.0261652751: f1: 0.0507006369
15: 3232: loss: 0.2594810870:
15: 6432: loss: 0.2510575066:
15: 9632: loss: 0.2530396612:
15: 12832: loss: 0.2552531694:
15: 16032: loss: 0.2545790986:
15: 19232: loss: 0.2544009738:
15: 22432: loss: 0.2561039498:
15: 25632: loss: 0.2551279851:
15: 28832: loss: 0.2575299777:
15: 32032: loss: 0.2582708917:
15: 35232: loss: 0.2596429226:
15: 38432: loss: 0.2586580187:
15: 41632: loss: 0.2583548555:
15: 44832: loss: 0.2589657326:
15: 48032: loss: 0.2589357654:
15: 51232: loss: 0.2579927937:
15: 54432: loss: 0.2572903228:
15: 57632: loss: 0.2578773790:
15: 60832: loss: 0.2579754520:
15: 64032: loss: 0.2575893012:
15: 67232: loss: 0.2580820408:
15: 70432: loss: 0.2580382390:
15: 73632: loss: 0.2590609183:
15: 76832: loss: 0.2592365622:
15: 80032: loss: 0.2588956299:
15: 83232: loss: 0.2591817931:
15: 86432: loss: 0.2586780882:
15: 89632: loss: 0.2588391223:
15: 92832: loss: 0.2589744520:
15: 96032: loss: 0.2591239099:
15: 99232: loss: 0.2589316152:
15: 102432: loss: 0.2585411014:
15: 105632: loss: 0.2581902920:
15: 108832: loss: 0.2582112697:
15: 112032: loss: 0.2582262077:
15: 115232: loss: 0.2583135128:
15: 118432: loss: 0.2580276846:
15: 121632: loss: 0.2578004889:
15: 124832: loss: 0.2573052674:
15: 128032: loss: 0.2569088003:
15: 131232: loss: 0.2570705500:
15: 134432: loss: 0.2569800381:
15: 137632: loss: 0.2567535461:
15: 140832: loss: 0.2564772693:
15: 144032: loss: 0.2566966482:
15: 147232: loss: 0.2565856385:
15: 150432: loss: 0.2565672802:
Dev-Acc: 15: Accuracy: 0.9420741200: precision: 0.7009345794: recall: 0.0127529332: f1: 0.0250501002
Train-Acc: 15: Accuracy: 0.9028203487: precision: 0.8206278027: recall: 0.0360923016: f1: 0.0691435768
16: 3232: loss: 0.2523232310:
16: 6432: loss: 0.2522386058:
16: 9632: loss: 0.2486077096:
16: 12832: loss: 0.2491137001:
16: 16032: loss: 0.2490023393:
16: 19232: loss: 0.2479461659:
16: 22432: loss: 0.2492817701:
16: 25632: loss: 0.2484754295:
16: 28832: loss: 0.2507528292:
16: 32032: loss: 0.2513178902:
16: 35232: loss: 0.2505156306:
16: 38432: loss: 0.2513670192:
16: 41632: loss: 0.2521956561:
16: 44832: loss: 0.2525751979:
16: 48032: loss: 0.2517845232:
16: 51232: loss: 0.2514789244:
16: 54432: loss: 0.2510242340:
16: 57632: loss: 0.2506247085:
16: 60832: loss: 0.2499596029:
16: 64032: loss: 0.2508140456:
16: 67232: loss: 0.2507086190:
16: 70432: loss: 0.2508920058:
16: 73632: loss: 0.2516105115:
16: 76832: loss: 0.2514538363:
16: 80032: loss: 0.2516343612:
16: 83232: loss: 0.2519279603:
16: 86432: loss: 0.2521578342:
16: 89632: loss: 0.2528449661:
16: 92832: loss: 0.2526993234:
16: 96032: loss: 0.2526160915:
16: 99232: loss: 0.2522835991:
16: 102432: loss: 0.2519301801:
16: 105632: loss: 0.2519105758:
16: 108832: loss: 0.2519718049:
16: 112032: loss: 0.2517259055:
16: 115232: loss: 0.2515460238:
16: 118432: loss: 0.2515918528:
16: 121632: loss: 0.2514174878:
16: 124832: loss: 0.2514412645:
16: 128032: loss: 0.2514906759:
16: 131232: loss: 0.2509563351:
16: 134432: loss: 0.2509592686:
16: 137632: loss: 0.2508930373:
16: 140832: loss: 0.2504399951:
16: 144032: loss: 0.2505601649:
16: 147232: loss: 0.2506794846:
16: 150432: loss: 0.2506071977:
Dev-Acc: 16: Accuracy: 0.9422923923: precision: 0.5970149254: recall: 0.0340078218: f1: 0.0643500644
Train-Acc: 16: Accuracy: 0.9045953751: precision: 0.8569969356: recall: 0.0551574518: f1: 0.1036442248
17: 3232: loss: 0.2518353560:
17: 6432: loss: 0.2524212846:
17: 9632: loss: 0.2508920569:
17: 12832: loss: 0.2484515785:
17: 16032: loss: 0.2499036130:
17: 19232: loss: 0.2487451746:
17: 22432: loss: 0.2496983988:
17: 25632: loss: 0.2479685703:
17: 28832: loss: 0.2471754253:
17: 32032: loss: 0.2484202965:
17: 35232: loss: 0.2468294185:
17: 38432: loss: 0.2457813538:
17: 41632: loss: 0.2461675572:
17: 44832: loss: 0.2455284523:
17: 48032: loss: 0.2454770509:
17: 51232: loss: 0.2462221212:
17: 54432: loss: 0.2468618605:
17: 57632: loss: 0.2474507519:
17: 60832: loss: 0.2478288778:
17: 64032: loss: 0.2482047310:
17: 67232: loss: 0.2484852635:
17: 70432: loss: 0.2481756819:
17: 73632: loss: 0.2476629306:
17: 76832: loss: 0.2475460264:
17: 80032: loss: 0.2461989290:
17: 83232: loss: 0.2464599544:
17: 86432: loss: 0.2467847385:
17: 89632: loss: 0.2474100672:
17: 92832: loss: 0.2470671075:
17: 96032: loss: 0.2473674220:
17: 99232: loss: 0.2479342819:
17: 102432: loss: 0.2481103216:
17: 105632: loss: 0.2480761395:
17: 108832: loss: 0.2477330156:
17: 112032: loss: 0.2479787717:
17: 115232: loss: 0.2477434036:
17: 118432: loss: 0.2474929478:
17: 121632: loss: 0.2471311101:
17: 124832: loss: 0.2470343641:
17: 128032: loss: 0.2469932622:
17: 131232: loss: 0.2469379872:
17: 134432: loss: 0.2468033833:
17: 137632: loss: 0.2468747073:
17: 140832: loss: 0.2468230925:
17: 144032: loss: 0.2466577175:
17: 147232: loss: 0.2465293279:
17: 150432: loss: 0.2461082627:
Dev-Acc: 17: Accuracy: 0.9419848323: precision: 0.5371179039: recall: 0.0418296208: f1: 0.0776147657
Train-Acc: 17: Accuracy: 0.9059101939: precision: 0.8514464425: recall: 0.0715929262: f1: 0.1320800485
18: 3232: loss: 0.2556764055:
18: 6432: loss: 0.2430247768:
18: 9632: loss: 0.2396186005:
18: 12832: loss: 0.2352557714:
18: 16032: loss: 0.2401929872:
18: 19232: loss: 0.2381765901:
18: 22432: loss: 0.2400578482:
18: 25632: loss: 0.2405303601:
18: 28832: loss: 0.2403014672:
18: 32032: loss: 0.2411397774:
18: 35232: loss: 0.2412607509:
18: 38432: loss: 0.2407487559:
18: 41632: loss: 0.2413086864:
18: 44832: loss: 0.2412965959:
18: 48032: loss: 0.2416851109:
18: 51232: loss: 0.2411335787:
18: 54432: loss: 0.2418917503:
18: 57632: loss: 0.2417414208:
18: 60832: loss: 0.2423974004:
18: 64032: loss: 0.2417145046:
18: 67232: loss: 0.2410927887:
18: 70432: loss: 0.2419087831:
18: 73632: loss: 0.2418711420:
18: 76832: loss: 0.2421712684:
18: 80032: loss: 0.2419380983:
18: 83232: loss: 0.2421546735:
18: 86432: loss: 0.2426706665:
18: 89632: loss: 0.2424697876:
18: 92832: loss: 0.2429126461:
18: 96032: loss: 0.2431033647:
18: 99232: loss: 0.2428401479:
18: 102432: loss: 0.2427251213:
18: 105632: loss: 0.2422817656:
18: 108832: loss: 0.2423992179:
18: 112032: loss: 0.2420694744:
18: 115232: loss: 0.2416635099:
18: 118432: loss: 0.2418145469:
18: 121632: loss: 0.2419424525:
18: 124832: loss: 0.2419931901:
18: 128032: loss: 0.2418233935:
18: 131232: loss: 0.2418308215:
18: 134432: loss: 0.2415402450:
18: 137632: loss: 0.2413283495:
18: 140832: loss: 0.2409586606:
18: 144032: loss: 0.2410324361:
18: 147232: loss: 0.2413944574:
18: 150432: loss: 0.2415013423:
Dev-Acc: 18: Accuracy: 0.9430862069: precision: 0.5898389095: recall: 0.0809386159: f1: 0.1423444976
Train-Acc: 18: Accuracy: 0.9087173939: precision: 0.8583783784: recall: 0.1043981329: f1: 0.1861555595
19: 3232: loss: 0.2443658295:
19: 6432: loss: 0.2379428275:
19: 9632: loss: 0.2365239518:
19: 12832: loss: 0.2377484675:
19: 16032: loss: 0.2418329543:
19: 19232: loss: 0.2419608575:
19: 22432: loss: 0.2413305771:
19: 25632: loss: 0.2424351204:
19: 28832: loss: 0.2413084421:
19: 32032: loss: 0.2414234180:
19: 35232: loss: 0.2401091357:
19: 38432: loss: 0.2388867005:
19: 41632: loss: 0.2385704071:
19: 44832: loss: 0.2389484771:
19: 48032: loss: 0.2388330087:
19: 51232: loss: 0.2374571939:
19: 54432: loss: 0.2371117725:
19: 57632: loss: 0.2365431027:
19: 60832: loss: 0.2359350691:
19: 64032: loss: 0.2361636182:
19: 67232: loss: 0.2363567098:
19: 70432: loss: 0.2366809310:
19: 73632: loss: 0.2368930860:
19: 76832: loss: 0.2365998347:
19: 80032: loss: 0.2358980570:
19: 83232: loss: 0.2355716429:
19: 86432: loss: 0.2350489998:
19: 89632: loss: 0.2359366425:
19: 92832: loss: 0.2361904230:
19: 96032: loss: 0.2366994740:
19: 99232: loss: 0.2370055208:
19: 102432: loss: 0.2370133404:
19: 105632: loss: 0.2373117386:
19: 108832: loss: 0.2372716539:
19: 112032: loss: 0.2370222655:
19: 115232: loss: 0.2367729879:
19: 118432: loss: 0.2370251445:
19: 121632: loss: 0.2371466317:
19: 124832: loss: 0.2371660220:
19: 128032: loss: 0.2373325485:
19: 131232: loss: 0.2369716098:
19: 134432: loss: 0.2369426646:
19: 137632: loss: 0.2370964875:
19: 140832: loss: 0.2371597150:
19: 144032: loss: 0.2372268943:
19: 147232: loss: 0.2375020901:
19: 150432: loss: 0.2374059909:
Dev-Acc: 19: Accuracy: 0.9430762529: precision: 0.5637168142: recall: 0.1083149124: f1: 0.1817144487
Train-Acc: 19: Accuracy: 0.9115048051: precision: 0.8574346405: recall: 0.1379922425: f1: 0.2377258055
20: 3232: loss: 0.2360446853:
20: 6432: loss: 0.2356314659:
20: 9632: loss: 0.2317123758:
20: 12832: loss: 0.2334427912:
20: 16032: loss: 0.2327332486:
20: 19232: loss: 0.2340714235:
20: 22432: loss: 0.2335612131:
20: 25632: loss: 0.2323141309:
20: 28832: loss: 0.2327255907:
20: 32032: loss: 0.2329321877:
20: 35232: loss: 0.2331036033:
20: 38432: loss: 0.2328951588:
20: 41632: loss: 0.2339588816:
20: 44832: loss: 0.2326705435:
20: 48032: loss: 0.2336924844:
20: 51232: loss: 0.2340369934:
20: 54432: loss: 0.2344456003:
20: 57632: loss: 0.2350512889:
20: 60832: loss: 0.2347388039:
20: 64032: loss: 0.2345511156:
20: 67232: loss: 0.2342578355:
20: 70432: loss: 0.2352101915:
20: 73632: loss: 0.2355708827:
20: 76832: loss: 0.2353801104:
20: 80032: loss: 0.2351529284:
20: 83232: loss: 0.2350463314:
20: 86432: loss: 0.2345985378:
20: 89632: loss: 0.2348766422:
20: 92832: loss: 0.2348452570:
20: 96032: loss: 0.2346791057:
20: 99232: loss: 0.2343021296:
20: 102432: loss: 0.2343178464:
20: 105632: loss: 0.2348689141:
20: 108832: loss: 0.2350326315:
20: 112032: loss: 0.2344650692:
20: 115232: loss: 0.2343818508:
20: 118432: loss: 0.2338998561:
20: 121632: loss: 0.2335588371:
20: 124832: loss: 0.2334396631:
20: 128032: loss: 0.2332142760:
20: 131232: loss: 0.2331228605:
20: 134432: loss: 0.2331898331:
20: 137632: loss: 0.2332851036:
20: 140832: loss: 0.2327303476:
20: 144032: loss: 0.2325601405:
20: 147232: loss: 0.2329575371:
20: 150432: loss: 0.2329301085:
Dev-Acc: 20: Accuracy: 0.9433640242: precision: 0.5657794677: recall: 0.1265090971: f1: 0.2067815453
Train-Acc: 20: Accuracy: 0.9138189554: precision: 0.8401294498: recall: 0.1706659654: f1: 0.2837003442
