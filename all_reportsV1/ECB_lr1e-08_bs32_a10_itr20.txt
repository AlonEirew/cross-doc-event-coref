1: 3232: loss: 0.7121883756:
1: 6432: loss: 0.7115585303:
1: 9632: loss: 0.7111709042:
1: 12832: loss: 0.7106558175:
1: 16032: loss: 0.7106216770:
1: 19232: loss: 0.7106449808:
1: 22432: loss: 0.7105010948:
1: 25632: loss: 0.7105572160:
1: 28832: loss: 0.7104638298:
1: 32032: loss: 0.7102468278:
1: 35232: loss: 0.7101374405:
1: 38432: loss: 0.7101098149:
1: 41632: loss: 0.7100550120:
1: 44832: loss: 0.7099185769:
1: 48032: loss: 0.7098241185:
1: 51232: loss: 0.7097074496:
1: 54432: loss: 0.7096144045:
1: 57632: loss: 0.7094981379:
1: 60832: loss: 0.7094350394:
1: 64032: loss: 0.7092860062:
1: 67232: loss: 0.7091670835:
1: 70432: loss: 0.7090366340:
1: 73632: loss: 0.7089608951:
1: 76832: loss: 0.7088779802:
1: 80032: loss: 0.7087835666:
1: 83232: loss: 0.7087031949:
1: 86432: loss: 0.7085814162:
1: 89632: loss: 0.7084764574:
1: 92832: loss: 0.7083814751:
1: 96032: loss: 0.7083195897:
1: 99232: loss: 0.7082151822:
1: 102432: loss: 0.7081137706:
1: 105632: loss: 0.7079837253:
1: 108832: loss: 0.7078688925:
1: 112032: loss: 0.7077768902:
1: 115232: loss: 0.7076727683:
1: 118432: loss: 0.7076136846:
1: 121632: loss: 0.7075199599:
1: 124832: loss: 0.7074379504:
1: 128032: loss: 0.7073573714:
1: 131232: loss: 0.7072654534:
1: 134432: loss: 0.7071517403:
1: 137632: loss: 0.7070517621:
1: 140832: loss: 0.7069741039:
1: 144032: loss: 0.7069058556:
1: 147232: loss: 0.7067841483:
1: 150432: loss: 0.7067026679:
1: 153632: loss: 0.7066171248:
1: 156832: loss: 0.7065335790:
1: 160032: loss: 0.7064277058:
1: 163232: loss: 0.7063259065:
1: 166432: loss: 0.7062655701:
Dev-Acc: 1: Accuracy: 0.3742558360: precision: 0.0651539117: recall: 0.7284475429: f1: 0.1196096771
Train-Acc: 1: Accuracy: 0.3921444416: precision: 0.1024342263: recall: 0.7325619617: f1: 0.1797359528
2: 3232: loss: 0.7012756211:
2: 6432: loss: 0.7009230396:
2: 9632: loss: 0.7006937271:
2: 12832: loss: 0.7007472460:
2: 16032: loss: 0.7008820683:
2: 19232: loss: 0.7006599905:
2: 22432: loss: 0.7003713718:
2: 25632: loss: 0.7001624623:
2: 28832: loss: 0.7001970058:
2: 32032: loss: 0.7001057080:
2: 35232: loss: 0.6999946771:
2: 38432: loss: 0.6999542022:
2: 41632: loss: 0.6998797867:
2: 44832: loss: 0.6996545986:
2: 48032: loss: 0.6996092195:
2: 51232: loss: 0.6994302946:
2: 54432: loss: 0.6993180993:
2: 57632: loss: 0.6992564189:
2: 60832: loss: 0.6991596980:
2: 64032: loss: 0.6991036235:
2: 67232: loss: 0.6989702475:
2: 70432: loss: 0.6988379897:
2: 73632: loss: 0.6987327138:
2: 76832: loss: 0.6986189680:
2: 80032: loss: 0.6985145270:
2: 83232: loss: 0.6984459752:
2: 86432: loss: 0.6984149507:
2: 89632: loss: 0.6982894445:
2: 92832: loss: 0.6982094284:
2: 96032: loss: 0.6980975364:
2: 99232: loss: 0.6980167436:
2: 102432: loss: 0.6979076733:
2: 105632: loss: 0.6978350223:
2: 108832: loss: 0.6977493497:
2: 112032: loss: 0.6976848530:
2: 115232: loss: 0.6975968094:
2: 118432: loss: 0.6975194762:
2: 121632: loss: 0.6973862976:
2: 124832: loss: 0.6972951624:
2: 128032: loss: 0.6972283165:
2: 131232: loss: 0.6971309025:
2: 134432: loss: 0.6970600249:
2: 137632: loss: 0.6969671022:
2: 140832: loss: 0.6968571211:
2: 144032: loss: 0.6967683991:
2: 147232: loss: 0.6966786675:
2: 150432: loss: 0.6965637448:
2: 153632: loss: 0.6964744113:
2: 156832: loss: 0.6963531570:
2: 160032: loss: 0.6962679459:
2: 163232: loss: 0.6961834401:
2: 166432: loss: 0.6960939506:
Dev-Acc: 2: Accuracy: 0.5184453726: precision: 0.0602342558: recall: 0.4966842374: f1: 0.1074390805
Train-Acc: 2: Accuracy: 0.5315770507: precision: 0.1058136342: recall: 0.5573598054: f1: 0.1778607618
3: 3232: loss: 0.6896281868:
3: 6432: loss: 0.6896496436:
3: 9632: loss: 0.6898905661:
3: 12832: loss: 0.6899058467:
3: 16032: loss: 0.6899963753:
3: 19232: loss: 0.6899476648:
3: 22432: loss: 0.6898273050:
3: 25632: loss: 0.6897330150:
3: 28832: loss: 0.6897895182:
3: 32032: loss: 0.6898306181:
3: 35232: loss: 0.6898331517:
3: 38432: loss: 0.6897309545:
3: 41632: loss: 0.6895501615:
3: 44832: loss: 0.6895645432:
3: 48032: loss: 0.6894920639:
3: 51232: loss: 0.6893938871:
3: 54432: loss: 0.6893428492:
3: 57632: loss: 0.6892246870:
3: 60832: loss: 0.6891581656:
3: 64032: loss: 0.6890496893:
3: 67232: loss: 0.6889845505:
3: 70432: loss: 0.6889200462:
3: 73632: loss: 0.6887457439:
3: 76832: loss: 0.6887130750:
3: 80032: loss: 0.6886698014:
3: 83232: loss: 0.6886257767:
3: 86432: loss: 0.6885521935:
3: 89632: loss: 0.6884605453:
3: 92832: loss: 0.6883972262:
3: 96032: loss: 0.6883181400:
3: 99232: loss: 0.6882134057:
3: 102432: loss: 0.6881359023:
3: 105632: loss: 0.6880777317:
3: 108832: loss: 0.6880029899:
3: 112032: loss: 0.6878909304:
3: 115232: loss: 0.6877785655:
3: 118432: loss: 0.6876992175:
3: 121632: loss: 0.6875908274:
3: 124832: loss: 0.6874926358:
3: 128032: loss: 0.6874037709:
3: 131232: loss: 0.6872904012:
3: 134432: loss: 0.6871977490:
3: 137632: loss: 0.6871016660:
3: 140832: loss: 0.6870190873:
3: 144032: loss: 0.6869365170:
3: 147232: loss: 0.6868505449:
3: 150432: loss: 0.6867571748:
3: 153632: loss: 0.6866634842:
3: 156832: loss: 0.6865711934:
3: 160032: loss: 0.6864974426:
3: 163232: loss: 0.6863845645:
3: 166432: loss: 0.6862752223:
Dev-Acc: 3: Accuracy: 0.6546872258: precision: 0.0617897512: recall: 0.3467097432: f1: 0.1048868313
Train-Acc: 3: Accuracy: 0.6593971848: precision: 0.1168400007: recall: 0.4187758859: f1: 0.1827047182
4: 3232: loss: 0.6807798517:
4: 6432: loss: 0.6808691740:
4: 9632: loss: 0.6807946557:
4: 12832: loss: 0.6808768253:
4: 16032: loss: 0.6806660125:
4: 19232: loss: 0.6807158715:
4: 22432: loss: 0.6806831685:
4: 25632: loss: 0.6804964813:
4: 28832: loss: 0.6804153594:
4: 32032: loss: 0.6804200848:
4: 35232: loss: 0.6802248348:
4: 38432: loss: 0.6800485072:
4: 41632: loss: 0.6799143969:
4: 44832: loss: 0.6798797906:
4: 48032: loss: 0.6799017887:
4: 51232: loss: 0.6797776405:
4: 54432: loss: 0.6796957400:
4: 57632: loss: 0.6796486575:
4: 60832: loss: 0.6796504647:
4: 64032: loss: 0.6795487550:
4: 67232: loss: 0.6795106322:
4: 70432: loss: 0.6794146364:
4: 73632: loss: 0.6793087039:
4: 76832: loss: 0.6792038475:
4: 80032: loss: 0.6791563632:
4: 83232: loss: 0.6790503688:
4: 86432: loss: 0.6789452842:
4: 89632: loss: 0.6788480507:
4: 92832: loss: 0.6787236040:
4: 96032: loss: 0.6786372008:
4: 99232: loss: 0.6785109155:
4: 102432: loss: 0.6784236907:
4: 105632: loss: 0.6783421479:
4: 108832: loss: 0.6782894809:
4: 112032: loss: 0.6781574729:
4: 115232: loss: 0.6780513163:
4: 118432: loss: 0.6779580553:
4: 121632: loss: 0.6778415880:
4: 124832: loss: 0.6777222849:
4: 128032: loss: 0.6776335700:
4: 131232: loss: 0.6775560706:
4: 134432: loss: 0.6774388935:
4: 137632: loss: 0.6773351361:
4: 140832: loss: 0.6772503833:
4: 144032: loss: 0.6771645858:
4: 147232: loss: 0.6770743938:
4: 150432: loss: 0.6769827547:
4: 153632: loss: 0.6768822940:
4: 156832: loss: 0.6767728742:
4: 160032: loss: 0.6766721303:
4: 163232: loss: 0.6765798041:
4: 166432: loss: 0.6764721269:
Dev-Acc: 4: Accuracy: 0.7624722123: precision: 0.0649513347: recall: 0.2292127189: f1: 0.1012201990
Train-Acc: 4: Accuracy: 0.7589663267: precision: 0.1316970177: recall: 0.2952468608: f1: 0.1821463335
5: 3232: loss: 0.6726713151:
5: 6432: loss: 0.6723751760:
5: 9632: loss: 0.6721071398:
5: 12832: loss: 0.6719424906:
5: 16032: loss: 0.6717360321:
5: 19232: loss: 0.6716310356:
5: 22432: loss: 0.6712977960:
5: 25632: loss: 0.6710860757:
5: 28832: loss: 0.6708696110:
5: 32032: loss: 0.6707872906:
5: 35232: loss: 0.6706004112:
5: 38432: loss: 0.6705748813:
5: 41632: loss: 0.6704741268:
5: 44832: loss: 0.6702396194:
5: 48032: loss: 0.6702585405:
5: 51232: loss: 0.6701258788:
5: 54432: loss: 0.6699752961:
5: 57632: loss: 0.6698545435:
5: 60832: loss: 0.6697406984:
5: 64032: loss: 0.6696446075:
5: 67232: loss: 0.6695665460:
5: 70432: loss: 0.6695237110:
5: 73632: loss: 0.6694698255:
5: 76832: loss: 0.6694360228:
5: 80032: loss: 0.6693348588:
5: 83232: loss: 0.6692479828:
5: 86432: loss: 0.6691386060:
5: 89632: loss: 0.6691026134:
5: 92832: loss: 0.6689998268:
5: 96032: loss: 0.6689136213:
5: 99232: loss: 0.6688266592:
5: 102432: loss: 0.6687614734:
5: 105632: loss: 0.6686876366:
5: 108832: loss: 0.6685863164:
5: 112032: loss: 0.6685306162:
5: 115232: loss: 0.6684325414:
5: 118432: loss: 0.6683648594:
5: 121632: loss: 0.6682098255:
5: 124832: loss: 0.6681353347:
5: 128032: loss: 0.6680600656:
5: 131232: loss: 0.6680082542:
5: 134432: loss: 0.6679310809:
5: 137632: loss: 0.6678289908:
5: 140832: loss: 0.6677051065:
5: 144032: loss: 0.6676018636:
5: 147232: loss: 0.6675215657:
5: 150432: loss: 0.6674371272:
5: 153632: loss: 0.6673565962:
5: 156832: loss: 0.6672873263:
5: 160032: loss: 0.6672037042:
5: 163232: loss: 0.6671235264:
5: 166432: loss: 0.6670271854:
Dev-Acc: 5: Accuracy: 0.8319574594: precision: 0.0643864765: recall: 0.1389219520: f1: 0.0879913840
Train-Acc: 5: Accuracy: 0.8275171518: precision: 0.1527855508: recall: 0.1974229176: f1: 0.1722595078
6: 3232: loss: 0.6630959553:
6: 6432: loss: 0.6626323113:
6: 9632: loss: 0.6631289955:
6: 12832: loss: 0.6628737462:
6: 16032: loss: 0.6626678189:
6: 19232: loss: 0.6626148880:
6: 22432: loss: 0.6623691442:
6: 25632: loss: 0.6620550427:
6: 28832: loss: 0.6619522309:
6: 32032: loss: 0.6617556376:
6: 35232: loss: 0.6617310036:
6: 38432: loss: 0.6615879179:
6: 41632: loss: 0.6614321907:
6: 44832: loss: 0.6612962308:
6: 48032: loss: 0.6612710401:
6: 51232: loss: 0.6611070762:
6: 54432: loss: 0.6609660215:
6: 57632: loss: 0.6608941406:
6: 60832: loss: 0.6607838334:
6: 64032: loss: 0.6607517284:
6: 67232: loss: 0.6607052688:
6: 70432: loss: 0.6605634464:
6: 73632: loss: 0.6604657143:
6: 76832: loss: 0.6604177970:
6: 80032: loss: 0.6603310593:
6: 83232: loss: 0.6601825188:
6: 86432: loss: 0.6601185174:
6: 89632: loss: 0.6599921738:
6: 92832: loss: 0.6598901822:
6: 96032: loss: 0.6598265212:
6: 99232: loss: 0.6597224707:
6: 102432: loss: 0.6595985298:
6: 105632: loss: 0.6594807617:
6: 108832: loss: 0.6594095534:
6: 112032: loss: 0.6593095807:
6: 115232: loss: 0.6591722942:
6: 118432: loss: 0.6590767173:
6: 121632: loss: 0.6589973536:
6: 124832: loss: 0.6589119805:
6: 128032: loss: 0.6588358221:
6: 131232: loss: 0.6587499948:
6: 134432: loss: 0.6586761798:
6: 137632: loss: 0.6586007922:
6: 140832: loss: 0.6584951362:
6: 144032: loss: 0.6583575972:
6: 147232: loss: 0.6582720197:
6: 150432: loss: 0.6581600617:
6: 153632: loss: 0.6580720230:
6: 156832: loss: 0.6579932490:
6: 160032: loss: 0.6579033267:
6: 163232: loss: 0.6577993828:
6: 166432: loss: 0.6577084871:
Dev-Acc: 6: Accuracy: 0.8804075718: precision: 0.0729310822: recall: 0.0896106104: f1: 0.0804150454
Train-Acc: 6: Accuracy: 0.8678109646: precision: 0.1759406963: recall: 0.1232660575: f1: 0.1449667543
7: 3232: loss: 0.6533292961:
7: 6432: loss: 0.6525424010:
7: 9632: loss: 0.6523607059:
7: 12832: loss: 0.6523459418:
7: 16032: loss: 0.6523358730:
7: 19232: loss: 0.6528177340:
7: 22432: loss: 0.6526481483:
7: 25632: loss: 0.6526840937:
7: 28832: loss: 0.6525826916:
7: 32032: loss: 0.6524553121:
7: 35232: loss: 0.6523500239:
7: 38432: loss: 0.6522317471:
7: 41632: loss: 0.6520799195:
7: 44832: loss: 0.6519462847:
7: 48032: loss: 0.6518986704:
7: 51232: loss: 0.6517674962:
7: 54432: loss: 0.6516540209:
7: 57632: loss: 0.6515966877:
7: 60832: loss: 0.6514643761:
7: 64032: loss: 0.6514117542:
7: 67232: loss: 0.6512998468:
7: 70432: loss: 0.6512012838:
7: 73632: loss: 0.6510842205:
7: 76832: loss: 0.6509357853:
7: 80032: loss: 0.6508545805:
7: 83232: loss: 0.6508191054:
7: 86432: loss: 0.6507089044:
7: 89632: loss: 0.6506416331:
7: 92832: loss: 0.6505991232:
7: 96032: loss: 0.6505073260:
7: 99232: loss: 0.6504282896:
7: 102432: loss: 0.6503552879:
7: 105632: loss: 0.6501902593:
7: 108832: loss: 0.6501040856:
7: 112032: loss: 0.6499868892:
7: 115232: loss: 0.6499273892:
7: 118432: loss: 0.6498378807:
7: 121632: loss: 0.6497263672:
7: 124832: loss: 0.6496431693:
7: 128032: loss: 0.6495555458:
7: 131232: loss: 0.6494560500:
7: 134432: loss: 0.6494028521:
7: 137632: loss: 0.6492820986:
7: 140832: loss: 0.6491925368:
7: 144032: loss: 0.6490938461:
7: 147232: loss: 0.6490300062:
7: 150432: loss: 0.6489270267:
7: 153632: loss: 0.6488391494:
7: 156832: loss: 0.6487849648:
7: 160032: loss: 0.6487216690:
7: 163232: loss: 0.6486562361:
7: 166432: loss: 0.6485852428:
Dev-Acc: 7: Accuracy: 0.9125059247: precision: 0.0699853587: recall: 0.0406393470: f1: 0.0514199656
Train-Acc: 7: Accuracy: 0.8891173601: precision: 0.1926048565: recall: 0.0688317665: f1: 0.1014190924
8: 3232: loss: 0.6433025384:
8: 6432: loss: 0.6433919361:
8: 9632: loss: 0.6434244188:
8: 12832: loss: 0.6430637869:
8: 16032: loss: 0.6430457942:
8: 19232: loss: 0.6432291963:
8: 22432: loss: 0.6429510432:
8: 25632: loss: 0.6428765310:
8: 28832: loss: 0.6428377466:
8: 32032: loss: 0.6428513977:
8: 35232: loss: 0.6426462462:
8: 38432: loss: 0.6425968487:
8: 41632: loss: 0.6424699025:
8: 44832: loss: 0.6423849270:
8: 48032: loss: 0.6423241008:
8: 51232: loss: 0.6423236444:
8: 54432: loss: 0.6421861591:
8: 57632: loss: 0.6421433892:
8: 60832: loss: 0.6420964648:
8: 64032: loss: 0.6419779777:
8: 67232: loss: 0.6419363202:
8: 70432: loss: 0.6418350982:
8: 73632: loss: 0.6417409205:
8: 76832: loss: 0.6417137007:
8: 80032: loss: 0.6417142253:
8: 83232: loss: 0.6416033127:
8: 86432: loss: 0.6415461223:
8: 89632: loss: 0.6414185471:
8: 92832: loss: 0.6413145576:
8: 96032: loss: 0.6412428582:
8: 99232: loss: 0.6411638973:
8: 102432: loss: 0.6410445509:
8: 105632: loss: 0.6409935594:
8: 108832: loss: 0.6409107219:
8: 112032: loss: 0.6408092152:
8: 115232: loss: 0.6407125628:
8: 118432: loss: 0.6406946432:
8: 121632: loss: 0.6405682527:
8: 124832: loss: 0.6404776248:
8: 128032: loss: 0.6404090404:
8: 131232: loss: 0.6403355293:
8: 134432: loss: 0.6402377193:
8: 137632: loss: 0.6401357714:
8: 140832: loss: 0.6400443952:
8: 144032: loss: 0.6399929960:
8: 147232: loss: 0.6398967901:
8: 150432: loss: 0.6398123469:
8: 153632: loss: 0.6397566002:
8: 156832: loss: 0.6396789982:
8: 160032: loss: 0.6396100352:
8: 163232: loss: 0.6395444664:
8: 166432: loss: 0.6394721826:
Dev-Acc: 8: Accuracy: 0.9282525182: precision: 0.0535714286: recall: 0.0137731678: f1: 0.0219126200
Train-Acc: 8: Accuracy: 0.9000245333: precision: 0.2119255602: recall: 0.0366839787: f1: 0.0625420309
9: 3232: loss: 0.6324096078:
9: 6432: loss: 0.6327760124:
9: 9632: loss: 0.6334262757:
9: 12832: loss: 0.6343874002:
9: 16032: loss: 0.6341082516:
9: 19232: loss: 0.6340220368:
9: 22432: loss: 0.6340528002:
9: 25632: loss: 0.6340626806:
9: 28832: loss: 0.6338990278:
9: 32032: loss: 0.6338413633:
9: 35232: loss: 0.6336913336:
9: 38432: loss: 0.6337387034:
9: 41632: loss: 0.6337112114:
9: 44832: loss: 0.6337570182:
9: 48032: loss: 0.6335517132:
9: 51232: loss: 0.6335758622:
9: 54432: loss: 0.6335286442:
9: 57632: loss: 0.6335969996:
9: 60832: loss: 0.6335029577:
9: 64032: loss: 0.6334667569:
9: 67232: loss: 0.6333025785:
9: 70432: loss: 0.6331991622:
9: 73632: loss: 0.6331700407:
9: 76832: loss: 0.6330015354:
9: 80032: loss: 0.6329215007:
9: 83232: loss: 0.6327861039:
9: 86432: loss: 0.6327436202:
9: 89632: loss: 0.6326420704:
9: 92832: loss: 0.6325319542:
9: 96032: loss: 0.6324557847:
9: 99232: loss: 0.6323850133:
9: 102432: loss: 0.6322618948:
9: 105632: loss: 0.6321736694:
9: 108832: loss: 0.6320346525:
9: 112032: loss: 0.6319596898:
9: 115232: loss: 0.6319053485:
9: 118432: loss: 0.6318011308:
9: 121632: loss: 0.6317800827:
9: 124832: loss: 0.6317177803:
9: 128032: loss: 0.6316707318:
9: 131232: loss: 0.6315792315:
9: 134432: loss: 0.6314478858:
9: 137632: loss: 0.6313694354:
9: 140832: loss: 0.6313066726:
9: 144032: loss: 0.6312214329:
9: 147232: loss: 0.6311375857:
9: 150432: loss: 0.6310347006:
9: 153632: loss: 0.6309244481:
9: 156832: loss: 0.6308518999:
9: 160032: loss: 0.6307869904:
9: 163232: loss: 0.6307154946:
9: 166432: loss: 0.6306077406:
Dev-Acc: 9: Accuracy: 0.9360116720: precision: 0.0748502994: recall: 0.0085019554: f1: 0.0152695068
Train-Acc: 9: Accuracy: 0.9045487642: precision: 0.2218155198: recall: 0.0199197949: f1: 0.0365566749
10: 3232: loss: 0.6229019308:
10: 6432: loss: 0.6240486339:
10: 9632: loss: 0.6251616398:
10: 12832: loss: 0.6249042985:
10: 16032: loss: 0.6252123615:
10: 19232: loss: 0.6250533706:
10: 22432: loss: 0.6248666582:
10: 25632: loss: 0.6246127198:
10: 28832: loss: 0.6248361182:
10: 32032: loss: 0.6247995626:
10: 35232: loss: 0.6246645486:
10: 38432: loss: 0.6244735733:
10: 41632: loss: 0.6245226033:
10: 44832: loss: 0.6244760487:
10: 48032: loss: 0.6245072518:
10: 51232: loss: 0.6245663816:
10: 54432: loss: 0.6244358181:
10: 57632: loss: 0.6242648169:
10: 60832: loss: 0.6243341250:
10: 64032: loss: 0.6242693678:
10: 67232: loss: 0.6242101002:
10: 70432: loss: 0.6240731141:
10: 73632: loss: 0.6239940806:
10: 76832: loss: 0.6239200405:
10: 80032: loss: 0.6238350594:
10: 83232: loss: 0.6237548749:
10: 86432: loss: 0.6236946902:
10: 89632: loss: 0.6235466703:
10: 92832: loss: 0.6235332233:
10: 96032: loss: 0.6235011351:
10: 99232: loss: 0.6234085978:
10: 102432: loss: 0.6233434360:
10: 105632: loss: 0.6234035214:
10: 108832: loss: 0.6233462393:
10: 112032: loss: 0.6233158719:
10: 115232: loss: 0.6232367716:
10: 118432: loss: 0.6231408110:
10: 121632: loss: 0.6230257690:
10: 124832: loss: 0.6229197949:
10: 128032: loss: 0.6228054068:
10: 131232: loss: 0.6227203162:
10: 134432: loss: 0.6226607875:
10: 137632: loss: 0.6226102007:
10: 140832: loss: 0.6225025315:
10: 144032: loss: 0.6224281046:
10: 147232: loss: 0.6223308251:
10: 150432: loss: 0.6222384314:
10: 153632: loss: 0.6221454243:
10: 156832: loss: 0.6220863595:
10: 160032: loss: 0.6220158584:
10: 163232: loss: 0.6219554884:
10: 166432: loss: 0.6219049015:
Dev-Acc: 10: Accuracy: 0.9394248724: precision: 0.0882352941: recall: 0.0040809386: f1: 0.0078010726
Train-Acc: 10: Accuracy: 0.9071903825: precision: 0.2249134948: recall: 0.0085464467: f1: 0.0164671607
11: 3232: loss: 0.6167073458:
11: 6432: loss: 0.6169333950:
11: 9632: loss: 0.6176310698:
11: 12832: loss: 0.6172991340:
11: 16032: loss: 0.6173745024:
11: 19232: loss: 0.6167537044:
11: 22432: loss: 0.6169964256:
11: 25632: loss: 0.6167193212:
11: 28832: loss: 0.6164642659:
11: 32032: loss: 0.6163104072:
11: 35232: loss: 0.6165391856:
11: 38432: loss: 0.6164252468:
11: 41632: loss: 0.6165050137:
11: 44832: loss: 0.6165719035:
11: 48032: loss: 0.6163560077:
11: 51232: loss: 0.6162805347:
11: 54432: loss: 0.6161476339:
11: 57632: loss: 0.6160943014:
11: 60832: loss: 0.6160320537:
11: 64032: loss: 0.6159831633:
11: 67232: loss: 0.6159070061:
11: 70432: loss: 0.6158926907:
11: 73632: loss: 0.6157666349:
11: 76832: loss: 0.6157481086:
11: 80032: loss: 0.6156721476:
11: 83232: loss: 0.6155906880:
11: 86432: loss: 0.6155716325:
11: 89632: loss: 0.6155317219:
11: 92832: loss: 0.6154602213:
11: 96032: loss: 0.6153567607:
11: 99232: loss: 0.6153410614:
11: 102432: loss: 0.6152854743:
11: 105632: loss: 0.6152224450:
11: 108832: loss: 0.6151304064:
11: 112032: loss: 0.6150277011:
11: 115232: loss: 0.6148789170:
11: 118432: loss: 0.6148261792:
11: 121632: loss: 0.6146612659:
11: 124832: loss: 0.6146195699:
11: 128032: loss: 0.6145068085:
11: 131232: loss: 0.6143989843:
11: 134432: loss: 0.6143502235:
11: 137632: loss: 0.6141958198:
11: 140832: loss: 0.6140978285:
11: 144032: loss: 0.6139575606:
11: 147232: loss: 0.6138733247:
11: 150432: loss: 0.6137858292:
11: 153632: loss: 0.6136680965:
11: 156832: loss: 0.6135772243:
11: 160032: loss: 0.6135346859:
11: 163232: loss: 0.6134103291:
11: 166432: loss: 0.6132796290:
Dev-Acc: 11: Accuracy: 0.9408140182: precision: 0.0882352941: recall: 0.0015303520: f1: 0.0030085242
Train-Acc: 11: Accuracy: 0.9082840681: precision: 0.2103004292: recall: 0.0032213530: f1: 0.0063455063
12: 3232: loss: 0.6089492106:
12: 6432: loss: 0.6081389269:
12: 9632: loss: 0.6083311172:
12: 12832: loss: 0.6082506390:
12: 16032: loss: 0.6081751311:
12: 19232: loss: 0.6082720371:
12: 22432: loss: 0.6084095603:
12: 25632: loss: 0.6081114384:
12: 28832: loss: 0.6077094093:
12: 32032: loss: 0.6077489387:
12: 35232: loss: 0.6078809452:
12: 38432: loss: 0.6075971060:
12: 41632: loss: 0.6076232857:
12: 44832: loss: 0.6073370655:
12: 48032: loss: 0.6073056966:
12: 51232: loss: 0.6073298199:
12: 54432: loss: 0.6071761791:
12: 57632: loss: 0.6071395281:
12: 60832: loss: 0.6069645494:
12: 64032: loss: 0.6067822297:
12: 67232: loss: 0.6066899394:
12: 70432: loss: 0.6066572573:
12: 73632: loss: 0.6066002858:
12: 76832: loss: 0.6065812309:
12: 80032: loss: 0.6064255973:
12: 83232: loss: 0.6064554473:
12: 86432: loss: 0.6063572763:
12: 89632: loss: 0.6062592437:
12: 92832: loss: 0.6061370691:
12: 96032: loss: 0.6060784680:
12: 99232: loss: 0.6060405820:
12: 102432: loss: 0.6059262366:
12: 105632: loss: 0.6058901294:
12: 108832: loss: 0.6058560431:
12: 112032: loss: 0.6058681214:
12: 115232: loss: 0.6059148201:
12: 118432: loss: 0.6058038476:
12: 121632: loss: 0.6056476101:
12: 124832: loss: 0.6055569668:
12: 128032: loss: 0.6054641686:
12: 131232: loss: 0.6054414778:
12: 134432: loss: 0.6053771425:
12: 137632: loss: 0.6052780362:
12: 140832: loss: 0.6052952220:
12: 144032: loss: 0.6051860975:
12: 147232: loss: 0.6050728572:
12: 150432: loss: 0.6050001809:
12: 153632: loss: 0.6049393698:
12: 156832: loss: 0.6048576868:
12: 160032: loss: 0.6048014183:
12: 163232: loss: 0.6046933758:
12: 166432: loss: 0.6046411513:
Dev-Acc: 12: Accuracy: 0.9413002133: precision: 0.1111111111: recall: 0.0008501955: f1: 0.0016874789
Train-Acc: 12: Accuracy: 0.9087263346: precision: 0.2038834951: recall: 0.0013805798: f1: 0.0027425885
13: 3232: loss: 0.6017605102:
13: 6432: loss: 0.6020600951:
13: 9632: loss: 0.6024096938:
13: 12832: loss: 0.6013630776:
13: 16032: loss: 0.6005735022:
13: 19232: loss: 0.6000780798:
13: 22432: loss: 0.6000001518:
13: 25632: loss: 0.5995865972:
13: 28832: loss: 0.5995267112:
13: 32032: loss: 0.5995468482:
13: 35232: loss: 0.5995296786:
13: 38432: loss: 0.5995804468:
13: 41632: loss: 0.5994372648:
13: 44832: loss: 0.5993035875:
13: 48032: loss: 0.5991497272:
13: 51232: loss: 0.5991064268:
13: 54432: loss: 0.5990053763:
13: 57632: loss: 0.5988905097:
13: 60832: loss: 0.5988478930:
13: 64032: loss: 0.5986172590:
13: 67232: loss: 0.5984695487:
13: 70432: loss: 0.5983278620:
13: 73632: loss: 0.5982092802:
13: 76832: loss: 0.5981020011:
13: 80032: loss: 0.5979861520:
13: 83232: loss: 0.5978481731:
13: 86432: loss: 0.5978609389:
13: 89632: loss: 0.5978042080:
13: 92832: loss: 0.5977834034:
13: 96032: loss: 0.5977575504:
13: 99232: loss: 0.5977585873:
13: 102432: loss: 0.5977688464:
13: 105632: loss: 0.5976605448:
13: 108832: loss: 0.5975683748:
13: 112032: loss: 0.5974093444:
13: 115232: loss: 0.5974190536:
13: 118432: loss: 0.5973666519:
13: 121632: loss: 0.5972593331:
13: 124832: loss: 0.5972385104:
13: 128032: loss: 0.5971228804:
13: 131232: loss: 0.5971465629:
13: 134432: loss: 0.5970632617:
13: 137632: loss: 0.5970424607:
13: 140832: loss: 0.5969927069:
13: 144032: loss: 0.5968198033:
13: 147232: loss: 0.5967141756:
13: 150432: loss: 0.5966078549:
13: 153632: loss: 0.5965646716:
13: 156832: loss: 0.5965174828:
13: 160032: loss: 0.5964948373:
13: 163232: loss: 0.5964287586:
13: 166432: loss: 0.5963597426:
Dev-Acc: 13: Accuracy: 0.9415184855: precision: 0.0666666667: recall: 0.0001700391: f1: 0.0003392130
Train-Acc: 13: Accuracy: 0.9090132117: precision: 0.3333333333: recall: 0.0008546447: f1: 0.0017049180
14: 3232: loss: 0.5891441625:
14: 6432: loss: 0.5909954208:
14: 9632: loss: 0.5923692509:
14: 12832: loss: 0.5918924102:
14: 16032: loss: 0.5915902070:
14: 19232: loss: 0.5914152721:
14: 22432: loss: 0.5911139041:
14: 25632: loss: 0.5914225816:
14: 28832: loss: 0.5914045635:
14: 32032: loss: 0.5913919535:
14: 35232: loss: 0.5912117487:
14: 38432: loss: 0.5911809029:
14: 41632: loss: 0.5910630898:
14: 44832: loss: 0.5912248898:
14: 48032: loss: 0.5912863702:
14: 51232: loss: 0.5910903354:
14: 54432: loss: 0.5910620998:
14: 57632: loss: 0.5909067842:
14: 60832: loss: 0.5908683917:
14: 64032: loss: 0.5906720521:
14: 67232: loss: 0.5905166677:
14: 70432: loss: 0.5904543401:
14: 73632: loss: 0.5904602676:
14: 76832: loss: 0.5903705962:
14: 80032: loss: 0.5901701436:
14: 83232: loss: 0.5900918581:
14: 86432: loss: 0.5900428621:
14: 89632: loss: 0.5899869766:
14: 92832: loss: 0.5900165191:
14: 96032: loss: 0.5898890244:
14: 99232: loss: 0.5897327876:
14: 102432: loss: 0.5896449951:
14: 105632: loss: 0.5895497398:
14: 108832: loss: 0.5893933572:
14: 112032: loss: 0.5893097466:
14: 115232: loss: 0.5892719930:
14: 118432: loss: 0.5892261221:
14: 121632: loss: 0.5891509578:
14: 124832: loss: 0.5890884679:
14: 128032: loss: 0.5890182531:
14: 131232: loss: 0.5889238804:
14: 134432: loss: 0.5888415191:
14: 137632: loss: 0.5887709114:
14: 140832: loss: 0.5886836237:
14: 144032: loss: 0.5885662308:
14: 147232: loss: 0.5884970494:
14: 150432: loss: 0.5884517667:
14: 153632: loss: 0.5883856865:
14: 156832: loss: 0.5882780824:
14: 160032: loss: 0.5881897096:
14: 163232: loss: 0.5881067658:
14: 166432: loss: 0.5880339634:
Dev-Acc: 14: Accuracy: 0.9416077733: precision: 0.1666666667: recall: 0.0001700391: f1: 0.0003397316
Train-Acc: 14: Accuracy: 0.9090430737: precision: 0.3000000000: recall: 0.0003944514: f1: 0.0007878669
15: 3232: loss: 0.5850055081:
15: 6432: loss: 0.5844152153:
15: 9632: loss: 0.5838196729:
15: 12832: loss: 0.5836576244:
15: 16032: loss: 0.5831883891:
15: 19232: loss: 0.5833914691:
15: 22432: loss: 0.5837283301:
15: 25632: loss: 0.5837875272:
15: 28832: loss: 0.5835558091:
15: 32032: loss: 0.5832380781:
15: 35232: loss: 0.5832155687:
15: 38432: loss: 0.5832096589:
15: 41632: loss: 0.5830119092:
15: 44832: loss: 0.5829050675:
15: 48032: loss: 0.5827395792:
15: 51232: loss: 0.5824705424:
15: 54432: loss: 0.5826397958:
15: 57632: loss: 0.5825327945:
15: 60832: loss: 0.5824115667:
15: 64032: loss: 0.5823561295:
15: 67232: loss: 0.5821967312:
15: 70432: loss: 0.5820500671:
15: 73632: loss: 0.5820117189:
15: 76832: loss: 0.5819082516:
15: 80032: loss: 0.5818502964:
15: 83232: loss: 0.5818715532:
15: 86432: loss: 0.5817307480:
15: 89632: loss: 0.5816660809:
15: 92832: loss: 0.5815771437:
15: 96032: loss: 0.5814579877:
15: 99232: loss: 0.5814234611:
15: 102432: loss: 0.5813472050:
15: 105632: loss: 0.5812155017:
15: 108832: loss: 0.5811848308:
15: 112032: loss: 0.5811376293:
15: 115232: loss: 0.5810127184:
15: 118432: loss: 0.5809963691:
15: 121632: loss: 0.5809738535:
15: 124832: loss: 0.5808824586:
15: 128032: loss: 0.5808058177:
15: 131232: loss: 0.5806979681:
15: 134432: loss: 0.5806261279:
15: 137632: loss: 0.5805563810:
15: 140832: loss: 0.5805312519:
15: 144032: loss: 0.5804679695:
15: 147232: loss: 0.5804371379:
15: 150432: loss: 0.5803272894:
15: 153632: loss: 0.5802013590:
15: 156832: loss: 0.5801683364:
15: 160032: loss: 0.5801292598:
15: 163232: loss: 0.5799913064:
15: 166432: loss: 0.5799403407:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.5000000000: recall: 0.0001700391: f1: 0.0003399626
Train-Acc: 15: Accuracy: 0.9090909362: precision: 0.5000000000: recall: 0.0003287095: f1: 0.0006569871
16: 3232: loss: 0.5776796806:
16: 6432: loss: 0.5761793754:
16: 9632: loss: 0.5756240374:
16: 12832: loss: 0.5759925820:
16: 16032: loss: 0.5757991108:
16: 19232: loss: 0.5760200765:
16: 22432: loss: 0.5756099166:
16: 25632: loss: 0.5754955811:
16: 28832: loss: 0.5751877736:
16: 32032: loss: 0.5753748371:
16: 35232: loss: 0.5753742300:
16: 38432: loss: 0.5749510075:
16: 41632: loss: 0.5750242603:
16: 44832: loss: 0.5750739310:
16: 48032: loss: 0.5746382122:
16: 51232: loss: 0.5745095349:
16: 54432: loss: 0.5743863930:
16: 57632: loss: 0.5743140384:
16: 60832: loss: 0.5740762285:
16: 64032: loss: 0.5741182714:
16: 67232: loss: 0.5739717848:
16: 70432: loss: 0.5738782394:
16: 73632: loss: 0.5738296378:
16: 76832: loss: 0.5736459811:
16: 80032: loss: 0.5735089244:
16: 83232: loss: 0.5735135741:
16: 86432: loss: 0.5734520136:
16: 89632: loss: 0.5733872654:
16: 92832: loss: 0.5733031836:
16: 96032: loss: 0.5731962560:
16: 99232: loss: 0.5731512534:
16: 102432: loss: 0.5730211130:
16: 105632: loss: 0.5728491444:
16: 108832: loss: 0.5726914047:
16: 112032: loss: 0.5726080768:
16: 115232: loss: 0.5725062566:
16: 118432: loss: 0.5725094135:
16: 121632: loss: 0.5724485865:
16: 124832: loss: 0.5723502801:
16: 128032: loss: 0.5722968443:
16: 131232: loss: 0.5721974749:
16: 134432: loss: 0.5721268880:
16: 137632: loss: 0.5720945264:
16: 140832: loss: 0.5720222594:
16: 144032: loss: 0.5720356679:
16: 147232: loss: 0.5719576831:
16: 150432: loss: 0.5718856940:
16: 153632: loss: 0.5718256643:
16: 156832: loss: 0.5717858632:
16: 160032: loss: 0.5717995529:
16: 163232: loss: 0.5717809989:
16: 166432: loss: 0.5717137689:
Dev-Acc: 16: Accuracy: 0.9416574240: precision: 1.0000000000: recall: 0.0001700391: f1: 0.0003400204
Train-Acc: 16: Accuracy: 0.9091028571: precision: 1.0000000000: recall: 0.0001314838: f1: 0.0002629330
17: 3232: loss: 0.5660956335:
17: 6432: loss: 0.5678670382:
17: 9632: loss: 0.5683728045:
17: 12832: loss: 0.5676706317:
17: 16032: loss: 0.5683071785:
17: 19232: loss: 0.5681652311:
17: 22432: loss: 0.5676642754:
17: 25632: loss: 0.5675800762:
17: 28832: loss: 0.5675059005:
17: 32032: loss: 0.5674956331:
17: 35232: loss: 0.5674679316:
17: 38432: loss: 0.5671947687:
17: 41632: loss: 0.5671757231:
17: 44832: loss: 0.5671771428:
17: 48032: loss: 0.5670494245:
17: 51232: loss: 0.5670167654:
17: 54432: loss: 0.5667335648:
17: 57632: loss: 0.5667907610:
17: 60832: loss: 0.5667297873:
17: 64032: loss: 0.5664988358:
17: 67232: loss: 0.5663091561:
17: 70432: loss: 0.5662530342:
17: 73632: loss: 0.5661432726:
17: 76832: loss: 0.5660178073:
17: 80032: loss: 0.5659180569:
17: 83232: loss: 0.5658710420:
17: 86432: loss: 0.5657527332:
17: 89632: loss: 0.5656413837:
17: 92832: loss: 0.5655926711:
17: 96032: loss: 0.5653814617:
17: 99232: loss: 0.5653783504:
17: 102432: loss: 0.5652660064:
17: 105632: loss: 0.5652216740:
17: 108832: loss: 0.5652327252:
17: 112032: loss: 0.5651274360:
17: 115232: loss: 0.5650843596:
17: 118432: loss: 0.5650601789:
17: 121632: loss: 0.5650363235:
17: 124832: loss: 0.5650525648:
17: 128032: loss: 0.5649486631:
17: 131232: loss: 0.5647854529:
17: 134432: loss: 0.5645614275:
17: 137632: loss: 0.5644966130:
17: 140832: loss: 0.5644135977:
17: 144032: loss: 0.5643217265:
17: 147232: loss: 0.5641950538:
17: 150432: loss: 0.5641421150:
17: 153632: loss: 0.5639964217:
17: 156832: loss: 0.5638458983:
17: 160032: loss: 0.5637997643:
17: 163232: loss: 0.5637053145:
17: 166432: loss: 0.5636454355:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 3232: loss: 0.5554239920:
18: 6432: loss: 0.5585616203:
18: 9632: loss: 0.5590636179:
18: 12832: loss: 0.5593204819:
18: 16032: loss: 0.5593141295:
18: 19232: loss: 0.5591183858:
18: 22432: loss: 0.5589067464:
18: 25632: loss: 0.5587450138:
18: 28832: loss: 0.5586962333:
18: 32032: loss: 0.5582155486:
18: 35232: loss: 0.5581648140:
18: 38432: loss: 0.5581298883:
18: 41632: loss: 0.5581850950:
18: 44832: loss: 0.5583438388:
18: 48032: loss: 0.5581019404:
18: 51232: loss: 0.5579621560:
18: 54432: loss: 0.5578875865:
18: 57632: loss: 0.5580703127:
18: 60832: loss: 0.5578782411:
18: 64032: loss: 0.5577418023:
18: 67232: loss: 0.5576982280:
18: 70432: loss: 0.5576593104:
18: 73632: loss: 0.5575145122:
18: 76832: loss: 0.5573964753:
18: 80032: loss: 0.5574206909:
18: 83232: loss: 0.5573145166:
18: 86432: loss: 0.5571831298:
18: 89632: loss: 0.5572217554:
18: 92832: loss: 0.5574036498:
18: 96032: loss: 0.5573917062:
18: 99232: loss: 0.5573934019:
18: 102432: loss: 0.5573021651:
18: 105632: loss: 0.5572855957:
18: 108832: loss: 0.5571189380:
18: 112032: loss: 0.5570613530:
18: 115232: loss: 0.5569669202:
18: 118432: loss: 0.5568292013:
18: 121632: loss: 0.5566695671:
18: 124832: loss: 0.5566004860:
18: 128032: loss: 0.5565150473:
18: 131232: loss: 0.5565104733:
18: 134432: loss: 0.5564688327:
18: 137632: loss: 0.5562957786:
18: 140832: loss: 0.5562908907:
18: 144032: loss: 0.5561929023:
18: 147232: loss: 0.5560619692:
18: 150432: loss: 0.5560140658:
18: 153632: loss: 0.5560124253:
18: 156832: loss: 0.5559675210:
18: 160032: loss: 0.5558693338:
18: 163232: loss: 0.5558475503:
18: 166432: loss: 0.5558384357:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 3232: loss: 0.5541344577:
19: 6432: loss: 0.5518482596:
19: 9632: loss: 0.5519979561:
19: 12832: loss: 0.5521939950:
19: 16032: loss: 0.5515203620:
19: 19232: loss: 0.5513415892:
19: 22432: loss: 0.5515170446:
19: 25632: loss: 0.5512204033:
19: 28832: loss: 0.5511077695:
19: 32032: loss: 0.5506289932:
19: 35232: loss: 0.5504743178:
19: 38432: loss: 0.5505025471:
19: 41632: loss: 0.5504268083:
19: 44832: loss: 0.5505179974:
19: 48032: loss: 0.5505395685:
19: 51232: loss: 0.5503749071:
19: 54432: loss: 0.5505529103:
19: 57632: loss: 0.5503374489:
19: 60832: loss: 0.5501761240:
19: 64032: loss: 0.5499913548:
19: 67232: loss: 0.5499996079:
19: 70432: loss: 0.5500585844:
19: 73632: loss: 0.5499051558:
19: 76832: loss: 0.5498500859:
19: 80032: loss: 0.5498338693:
19: 83232: loss: 0.5498446606:
19: 86432: loss: 0.5499711752:
19: 89632: loss: 0.5498777281:
19: 92832: loss: 0.5496744135:
19: 96032: loss: 0.5496860841:
19: 99232: loss: 0.5495452553:
19: 102432: loss: 0.5495478943:
19: 105632: loss: 0.5494693240:
19: 108832: loss: 0.5494531339:
19: 112032: loss: 0.5493282099:
19: 115232: loss: 0.5493109445:
19: 118432: loss: 0.5492456826:
19: 121632: loss: 0.5491810023:
19: 124832: loss: 0.5491069671:
19: 128032: loss: 0.5490470054:
19: 131232: loss: 0.5489366465:
19: 134432: loss: 0.5488008627:
19: 137632: loss: 0.5488073971:
19: 140832: loss: 0.5487888268:
19: 144032: loss: 0.5486536030:
19: 147232: loss: 0.5484924859:
19: 150432: loss: 0.5484291677:
19: 153632: loss: 0.5483386340:
19: 156832: loss: 0.5482813929:
19: 160032: loss: 0.5481716962:
19: 163232: loss: 0.5480948463:
19: 166432: loss: 0.5479956592:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
20: 3232: loss: 0.5484994939:
20: 6432: loss: 0.5470568082:
20: 9632: loss: 0.5454976579:
20: 12832: loss: 0.5461000051:
20: 16032: loss: 0.5457463763:
20: 19232: loss: 0.5447067044:
20: 22432: loss: 0.5438684449:
20: 25632: loss: 0.5437663497:
20: 28832: loss: 0.5437900346:
20: 32032: loss: 0.5437032970:
20: 35232: loss: 0.5436534680:
20: 38432: loss: 0.5435831857:
20: 41632: loss: 0.5435591893:
20: 44832: loss: 0.5435975464:
20: 48032: loss: 0.5436248477:
20: 51232: loss: 0.5433502339:
20: 54432: loss: 0.5430581074:
20: 57632: loss: 0.5431595017:
20: 60832: loss: 0.5431119691:
20: 64032: loss: 0.5428849550:
20: 67232: loss: 0.5426847405:
20: 70432: loss: 0.5424947120:
20: 73632: loss: 0.5423120386:
20: 76832: loss: 0.5422917611:
20: 80032: loss: 0.5421769910:
20: 83232: loss: 0.5420621392:
20: 86432: loss: 0.5418144786:
20: 89632: loss: 0.5417840773:
20: 92832: loss: 0.5417783372:
20: 96032: loss: 0.5418009370:
20: 99232: loss: 0.5417159939:
20: 102432: loss: 0.5417356804:
20: 105632: loss: 0.5416603177:
20: 108832: loss: 0.5416965610:
20: 112032: loss: 0.5416552178:
20: 115232: loss: 0.5416102839:
20: 118432: loss: 0.5415381603:
20: 121632: loss: 0.5414404472:
20: 124832: loss: 0.5413872506:
20: 128032: loss: 0.5413008239:
20: 131232: loss: 0.5411803735:
20: 134432: loss: 0.5410389500:
20: 137632: loss: 0.5408651945:
20: 140832: loss: 0.5408447225:
20: 144032: loss: 0.5408832233:
20: 147232: loss: 0.5408294063:
20: 150432: loss: 0.5408570535:
20: 153632: loss: 0.5407229289:
20: 156832: loss: 0.5406771446:
20: 160032: loss: 0.5405183335:
20: 163232: loss: 0.5403480906:
20: 166432: loss: 0.5403425497:
Dev-Acc: 20: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 20: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
