1: 3232: loss: 0.7087713969:
1: 6432: loss: 0.7082527632:
1: 9632: loss: 0.7082307704:
1: 12832: loss: 0.7084003232:
1: 16032: loss: 0.7078882247:
1: 19232: loss: 0.7080037140:
1: 22432: loss: 0.7080380763:
1: 25632: loss: 0.7078296416:
1: 28832: loss: 0.7078165657:
1: 32032: loss: 0.7076571478:
1: 35232: loss: 0.7074894935:
1: 38432: loss: 0.7074500404:
1: 41632: loss: 0.7074395939:
1: 44832: loss: 0.7073723809:
1: 48032: loss: 0.7072561318:
1: 51232: loss: 0.7072753979:
1: 54432: loss: 0.7072039071:
1: 57632: loss: 0.7071372161:
1: 60832: loss: 0.7070740364:
1: 64032: loss: 0.7069767078:
1: 67232: loss: 0.7068758785:
1: 70432: loss: 0.7068175923:
1: 73632: loss: 0.7067857847:
1: 76832: loss: 0.7066745610:
1: 80032: loss: 0.7065533475:
1: 83232: loss: 0.7065000218:
1: 86432: loss: 0.7064027660:
1: 89632: loss: 0.7063556380:
1: 92832: loss: 0.7062847057:
1: 96032: loss: 0.7062531765:
1: 99232: loss: 0.7061702069:
1: 102432: loss: 0.7060588455:
1: 105632: loss: 0.7059760207:
1: 108832: loss: 0.7059402549:
1: 112032: loss: 0.7058656815:
1: 115232: loss: 0.7057995954:
1: 118432: loss: 0.7057203588:
1: 121632: loss: 0.7056461501:
Dev-Acc: 1: Accuracy: 0.3270459473: precision: 0.0648429158: recall: 0.7847304880: f1: 0.1197876786
Train-Acc: 1: Accuracy: 0.3760108054: precision: 0.1408529012: recall: 0.7827887713: f1: 0.2387464159
2: 3232: loss: 0.7024248159:
2: 6432: loss: 0.7021491468:
2: 9632: loss: 0.7021363076:
2: 12832: loss: 0.7019497657:
2: 16032: loss: 0.7018374465:
2: 19232: loss: 0.7019008894:
2: 22432: loss: 0.7018577859:
2: 25632: loss: 0.7017142945:
2: 28832: loss: 0.7015220044:
2: 32032: loss: 0.7014524979:
2: 35232: loss: 0.7014260375:
2: 38432: loss: 0.7013211768:
2: 41632: loss: 0.7013278040:
2: 44832: loss: 0.7012285318:
2: 48032: loss: 0.7012604689:
2: 51232: loss: 0.7011126849:
2: 54432: loss: 0.7009729296:
2: 57632: loss: 0.7008123682:
2: 60832: loss: 0.7007597602:
2: 64032: loss: 0.7006172006:
2: 67232: loss: 0.7005235714:
2: 70432: loss: 0.7003989757:
2: 73632: loss: 0.7003480850:
2: 76832: loss: 0.7002950301:
2: 80032: loss: 0.7002619758:
2: 83232: loss: 0.7002282844:
2: 86432: loss: 0.7001637965:
2: 89632: loss: 0.7001210738:
2: 92832: loss: 0.7000106167:
2: 96032: loss: 0.6999313379:
2: 99232: loss: 0.6998320837:
2: 102432: loss: 0.6997733684:
2: 105632: loss: 0.6996879830:
2: 108832: loss: 0.6995733249:
2: 112032: loss: 0.6994990123:
2: 115232: loss: 0.6994196882:
2: 118432: loss: 0.6993438610:
2: 121632: loss: 0.6992662311:
Dev-Acc: 2: Accuracy: 0.4184691906: precision: 0.0655670171: recall: 0.6765856147: f1: 0.1195487253
Train-Acc: 2: Accuracy: 0.4527069330: precision: 0.1450810841: recall: 0.6904871475: f1: 0.2397808344
3: 3232: loss: 0.6955337620:
3: 6432: loss: 0.6956499568:
3: 9632: loss: 0.6955061450:
3: 12832: loss: 0.6954869150:
3: 16032: loss: 0.6952600267:
3: 19232: loss: 0.6952809050:
3: 22432: loss: 0.6953355989:
3: 25632: loss: 0.6954107701:
3: 28832: loss: 0.6955248597:
3: 32032: loss: 0.6954999456:
3: 35232: loss: 0.6952757792:
3: 38432: loss: 0.6952421735:
3: 41632: loss: 0.6952161265:
3: 44832: loss: 0.6951221378:
3: 48032: loss: 0.6950603735:
3: 51232: loss: 0.6949702933:
3: 54432: loss: 0.6949766507:
3: 57632: loss: 0.6949046128:
3: 60832: loss: 0.6948382749:
3: 64032: loss: 0.6947197184:
3: 67232: loss: 0.6945912494:
3: 70432: loss: 0.6944844481:
3: 73632: loss: 0.6943966401:
3: 76832: loss: 0.6942920044:
3: 80032: loss: 0.6942458874:
3: 83232: loss: 0.6941765833:
3: 86432: loss: 0.6941121608:
3: 89632: loss: 0.6940291582:
3: 92832: loss: 0.6939521244:
3: 96032: loss: 0.6938622609:
3: 99232: loss: 0.6937668545:
3: 102432: loss: 0.6936677563:
3: 105632: loss: 0.6935493407:
3: 108832: loss: 0.6934597129:
3: 112032: loss: 0.6933667884:
3: 115232: loss: 0.6932688525:
3: 118432: loss: 0.6931514636:
3: 121632: loss: 0.6930604618:
Dev-Acc: 3: Accuracy: 0.5148336887: precision: 0.0628277572: recall: 0.5255908859: f1: 0.1122387843
Train-Acc: 3: Accuracy: 0.5371441841: precision: 0.1491046891: recall: 0.5742554730: f1: 0.2367401144
4: 3232: loss: 0.6894271457:
4: 6432: loss: 0.6894313365:
4: 9632: loss: 0.6895363422:
4: 12832: loss: 0.6896957125:
4: 16032: loss: 0.6897058502:
4: 19232: loss: 0.6896663887:
4: 22432: loss: 0.6896766831:
4: 25632: loss: 0.6896402024:
4: 28832: loss: 0.6895729960:
4: 32032: loss: 0.6894754236:
4: 35232: loss: 0.6893930799:
4: 38432: loss: 0.6893608882:
4: 41632: loss: 0.6892602873:
4: 44832: loss: 0.6890958734:
4: 48032: loss: 0.6889855781:
4: 51232: loss: 0.6888504571:
4: 54432: loss: 0.6887815640:
4: 57632: loss: 0.6886767166:
4: 60832: loss: 0.6886125772:
4: 64032: loss: 0.6885930491:
4: 67232: loss: 0.6885196056:
4: 70432: loss: 0.6884099527:
4: 73632: loss: 0.6883188183:
4: 76832: loss: 0.6882538080:
4: 80032: loss: 0.6881623975:
4: 83232: loss: 0.6881093953:
4: 86432: loss: 0.6880370464:
4: 89632: loss: 0.6879722959:
4: 92832: loss: 0.6878827649:
4: 96032: loss: 0.6878040535:
4: 99232: loss: 0.6877340056:
4: 102432: loss: 0.6876145453:
4: 105632: loss: 0.6875012105:
4: 108832: loss: 0.6873967257:
4: 112032: loss: 0.6873330736:
4: 115232: loss: 0.6872510436:
4: 118432: loss: 0.6872056482:
4: 121632: loss: 0.6871523751:
Dev-Acc: 4: Accuracy: 0.6111882925: precision: 0.0630297305: recall: 0.4084339398: f1: 0.1092066379
Train-Acc: 4: Accuracy: 0.6194283962: precision: 0.1581666300: recall: 0.4730129512: f1: 0.2370636398
5: 3232: loss: 0.6841207987:
5: 6432: loss: 0.6843239209:
5: 9632: loss: 0.6841186005:
5: 12832: loss: 0.6838665870:
5: 16032: loss: 0.6838126409:
5: 19232: loss: 0.6839766553:
5: 22432: loss: 0.6839848776:
5: 25632: loss: 0.6837600587:
5: 28832: loss: 0.6836034732:
5: 32032: loss: 0.6834482119:
5: 35232: loss: 0.6833919943:
5: 38432: loss: 0.6833995109:
5: 41632: loss: 0.6833309755:
5: 44832: loss: 0.6831958519:
5: 48032: loss: 0.6830804784:
5: 51232: loss: 0.6829716536:
5: 54432: loss: 0.6828786365:
5: 57632: loss: 0.6827631611:
5: 60832: loss: 0.6827241955:
5: 64032: loss: 0.6826513073:
5: 67232: loss: 0.6825821149:
5: 70432: loss: 0.6825019622:
5: 73632: loss: 0.6824229274:
5: 76832: loss: 0.6823626554:
5: 80032: loss: 0.6822958480:
5: 83232: loss: 0.6822729887:
5: 86432: loss: 0.6821855428:
5: 89632: loss: 0.6821123989:
5: 92832: loss: 0.6819846787:
5: 96032: loss: 0.6818592199:
5: 99232: loss: 0.6818083844:
5: 102432: loss: 0.6817702095:
5: 105632: loss: 0.6816798272:
5: 108832: loss: 0.6815811109:
5: 112032: loss: 0.6814928732:
5: 115232: loss: 0.6814322760:
5: 118432: loss: 0.6813769466:
5: 121632: loss: 0.6813090027:
Dev-Acc: 5: Accuracy: 0.6943562627: precision: 0.0654531506: recall: 0.3191634076: f1: 0.1086289716
Train-Acc: 5: Accuracy: 0.6931332946: precision: 0.1732081155: recall: 0.3855762277: f1: 0.2390365178
6: 3232: loss: 0.6780373621:
6: 6432: loss: 0.6784690672:
6: 9632: loss: 0.6778328107:
6: 12832: loss: 0.6777316476:
6: 16032: loss: 0.6776440122:
6: 19232: loss: 0.6776209465:
6: 22432: loss: 0.6775703234:
6: 25632: loss: 0.6774943284:
6: 28832: loss: 0.6774757489:
6: 32032: loss: 0.6773191703:
6: 35232: loss: 0.6774727458:
6: 38432: loss: 0.6772181967:
6: 41632: loss: 0.6772606468:
6: 44832: loss: 0.6771615209:
6: 48032: loss: 0.6771071411:
6: 51232: loss: 0.6770689974:
6: 54432: loss: 0.6769942780:
6: 57632: loss: 0.6768612798:
6: 60832: loss: 0.6768295045:
6: 64032: loss: 0.6768058963:
6: 67232: loss: 0.6767012329:
6: 70432: loss: 0.6766040787:
6: 73632: loss: 0.6765271325:
6: 76832: loss: 0.6764223194:
6: 80032: loss: 0.6763828138:
6: 83232: loss: 0.6763165617:
6: 86432: loss: 0.6761888228:
6: 89632: loss: 0.6761481089:
6: 92832: loss: 0.6760488845:
6: 96032: loss: 0.6759737031:
6: 99232: loss: 0.6759479065:
6: 102432: loss: 0.6758583718:
6: 105632: loss: 0.6757335749:
6: 108832: loss: 0.6756789896:
6: 112032: loss: 0.6756023932:
6: 115232: loss: 0.6755369135:
6: 118432: loss: 0.6754710265:
6: 121632: loss: 0.6753603084:
Dev-Acc: 6: Accuracy: 0.7641887665: precision: 0.0687659739: recall: 0.2424757694: f1: 0.1071455406
Train-Acc: 6: Accuracy: 0.7522763610: precision: 0.1930192403: recall: 0.3086582079: f1: 0.2375110661
7: 3232: loss: 0.6719896156:
7: 6432: loss: 0.6726152548:
7: 9632: loss: 0.6727847538:
7: 12832: loss: 0.6727042937:
7: 16032: loss: 0.6726323414:
7: 19232: loss: 0.6723692597:
7: 22432: loss: 0.6721179103:
7: 25632: loss: 0.6719303416:
7: 28832: loss: 0.6717119926:
7: 32032: loss: 0.6717189016:
7: 35232: loss: 0.6716567513:
7: 38432: loss: 0.6714572332:
7: 41632: loss: 0.6713285439:
7: 44832: loss: 0.6711971567:
7: 48032: loss: 0.6711772044:
7: 51232: loss: 0.6711397846:
7: 54432: loss: 0.6710062520:
7: 57632: loss: 0.6709596624:
7: 60832: loss: 0.6709266275:
7: 64032: loss: 0.6708463414:
7: 67232: loss: 0.6707483290:
7: 70432: loss: 0.6707297926:
7: 73632: loss: 0.6706749780:
7: 76832: loss: 0.6705837943:
7: 80032: loss: 0.6704948051:
7: 83232: loss: 0.6704382069:
7: 86432: loss: 0.6702986644:
7: 89632: loss: 0.6702345120:
7: 92832: loss: 0.6701838917:
7: 96032: loss: 0.6701026193:
7: 99232: loss: 0.6699477284:
7: 102432: loss: 0.6698831221:
7: 105632: loss: 0.6698244401:
7: 108832: loss: 0.6697473503:
7: 112032: loss: 0.6696454427:
7: 115232: loss: 0.6695484324:
7: 118432: loss: 0.6695024240:
7: 121632: loss: 0.6694397423:
Dev-Acc: 7: Accuracy: 0.8142264485: precision: 0.0730718085: recall: 0.1868729808: f1: 0.1050618995
Train-Acc: 7: Accuracy: 0.7973753214: precision: 0.2193368196: recall: 0.2426533430: f1: 0.2304066918
8: 3232: loss: 0.6662255502:
8: 6432: loss: 0.6660575131:
8: 9632: loss: 0.6661236225:
8: 12832: loss: 0.6658173566:
8: 16032: loss: 0.6657489324:
8: 19232: loss: 0.6657470567:
8: 22432: loss: 0.6657555471:
8: 25632: loss: 0.6657195538:
8: 28832: loss: 0.6656284571:
8: 32032: loss: 0.6655982509:
8: 35232: loss: 0.6656186193:
8: 38432: loss: 0.6655553235:
8: 41632: loss: 0.6654949649:
8: 44832: loss: 0.6654175943:
8: 48032: loss: 0.6654120026:
8: 51232: loss: 0.6653628200:
8: 54432: loss: 0.6653063625:
8: 57632: loss: 0.6651907153:
8: 60832: loss: 0.6652285600:
8: 64032: loss: 0.6651027936:
8: 67232: loss: 0.6649565489:
8: 70432: loss: 0.6648566829:
8: 73632: loss: 0.6647783499:
8: 76832: loss: 0.6646830252:
8: 80032: loss: 0.6646287856:
8: 83232: loss: 0.6645492183:
8: 86432: loss: 0.6644530677:
8: 89632: loss: 0.6643447990:
8: 92832: loss: 0.6643061742:
8: 96032: loss: 0.6642100079:
8: 99232: loss: 0.6641335196:
8: 102432: loss: 0.6641096674:
8: 105632: loss: 0.6640078319:
8: 108832: loss: 0.6639777846:
8: 112032: loss: 0.6639601865:
8: 115232: loss: 0.6639059923:
8: 118432: loss: 0.6638183147:
8: 121632: loss: 0.6637557184:
Dev-Acc: 8: Accuracy: 0.8538359404: precision: 0.0771215596: recall: 0.1372215610: f1: 0.0987457938
Train-Acc: 8: Accuracy: 0.8272221088: precision: 0.2476124327: recall: 0.1874958911: f1: 0.2134011748
9: 3232: loss: 0.6611621720:
9: 6432: loss: 0.6611072734:
9: 9632: loss: 0.6611346155:
9: 12832: loss: 0.6610840181:
9: 16032: loss: 0.6609681185:
9: 19232: loss: 0.6608022148:
9: 22432: loss: 0.6607773272:
9: 25632: loss: 0.6606327884:
9: 28832: loss: 0.6605739713:
9: 32032: loss: 0.6603001947:
9: 35232: loss: 0.6600516695:
9: 38432: loss: 0.6599969938:
9: 41632: loss: 0.6599606186:
9: 44832: loss: 0.6599193308:
9: 48032: loss: 0.6597819620:
9: 51232: loss: 0.6596861999:
9: 54432: loss: 0.6595330587:
9: 57632: loss: 0.6594305488:
9: 60832: loss: 0.6593607587:
9: 64032: loss: 0.6593397560:
9: 67232: loss: 0.6593270812:
9: 70432: loss: 0.6592295107:
9: 73632: loss: 0.6591546842:
9: 76832: loss: 0.6590502760:
9: 80032: loss: 0.6589619414:
9: 83232: loss: 0.6589033591:
9: 86432: loss: 0.6588469857:
9: 89632: loss: 0.6586810763:
9: 92832: loss: 0.6586067174:
9: 96032: loss: 0.6585430016:
9: 99232: loss: 0.6584134611:
9: 102432: loss: 0.6583055598:
9: 105632: loss: 0.6582315744:
9: 108832: loss: 0.6581696602:
9: 112032: loss: 0.6581061939:
9: 115232: loss: 0.6580678126:
9: 118432: loss: 0.6579688767:
9: 121632: loss: 0.6578905494:
Dev-Acc: 9: Accuracy: 0.8834140301: precision: 0.0854640486: recall: 0.1028736609: f1: 0.0933641975
Train-Acc: 9: Accuracy: 0.8473391533: precision: 0.2806308655: recall: 0.1415423049: f1: 0.1881746275
10: 3232: loss: 0.6561953837:
10: 6432: loss: 0.6559719965:
10: 9632: loss: 0.6562015331:
10: 12832: loss: 0.6561631666:
10: 16032: loss: 0.6555391415:
10: 19232: loss: 0.6555601350:
10: 22432: loss: 0.6554454637:
10: 25632: loss: 0.6553657281:
10: 28832: loss: 0.6552344098:
10: 32032: loss: 0.6549106729:
10: 35232: loss: 0.6547208429:
10: 38432: loss: 0.6545212942:
10: 41632: loss: 0.6544200256:
10: 44832: loss: 0.6543632250:
10: 48032: loss: 0.6543165769:
10: 51232: loss: 0.6543160570:
10: 54432: loss: 0.6542624559:
10: 57632: loss: 0.6542042851:
10: 60832: loss: 0.6541851381:
10: 64032: loss: 0.6541667976:
10: 67232: loss: 0.6541538507:
10: 70432: loss: 0.6540291414:
10: 73632: loss: 0.6538996801:
10: 76832: loss: 0.6537809157:
10: 80032: loss: 0.6537243126:
10: 83232: loss: 0.6536821801:
10: 86432: loss: 0.6535627519:
10: 89632: loss: 0.6534710591:
10: 92832: loss: 0.6534015346:
10: 96032: loss: 0.6533131109:
10: 99232: loss: 0.6532343684:
10: 102432: loss: 0.6531195218:
10: 105632: loss: 0.6530457489:
10: 108832: loss: 0.6529804670:
10: 112032: loss: 0.6528828198:
10: 115232: loss: 0.6528002984:
10: 118432: loss: 0.6527175176:
10: 121632: loss: 0.6526289954:
Dev-Acc: 10: Accuracy: 0.9060465693: precision: 0.0924579736: recall: 0.0692059174: f1: 0.0791597783
Train-Acc: 10: Accuracy: 0.8597972393: precision: 0.3126771972: recall: 0.1015054894: f1: 0.1532582262
11: 3232: loss: 0.6512789071:
11: 6432: loss: 0.6500715566:
11: 9632: loss: 0.6497231209:
11: 12832: loss: 0.6496965830:
11: 16032: loss: 0.6498359897:
11: 19232: loss: 0.6495431562:
11: 22432: loss: 0.6495244381:
11: 25632: loss: 0.6494073718:
11: 28832: loss: 0.6494129009:
11: 32032: loss: 0.6492210029:
11: 35232: loss: 0.6490949522:
11: 38432: loss: 0.6490227953:
11: 41632: loss: 0.6489103057:
11: 44832: loss: 0.6487896639:
11: 48032: loss: 0.6486390047:
11: 51232: loss: 0.6487322535:
11: 54432: loss: 0.6486667531:
11: 57632: loss: 0.6486097193:
11: 60832: loss: 0.6484362529:
11: 64032: loss: 0.6483446798:
11: 67232: loss: 0.6483242975:
11: 70432: loss: 0.6482392280:
11: 73632: loss: 0.6481255031:
11: 76832: loss: 0.6480178641:
11: 80032: loss: 0.6478795673:
11: 83232: loss: 0.6478013262:
11: 86432: loss: 0.6476902463:
11: 89632: loss: 0.6476291804:
11: 92832: loss: 0.6475740267:
11: 96032: loss: 0.6475397512:
11: 99232: loss: 0.6474513358:
11: 102432: loss: 0.6473463283:
11: 105632: loss: 0.6472312340:
11: 108832: loss: 0.6471869157:
11: 112032: loss: 0.6471214516:
11: 115232: loss: 0.6470629244:
11: 118432: loss: 0.6470127359:
11: 121632: loss: 0.6469831204:
Dev-Acc: 11: Accuracy: 0.9212474227: precision: 0.0965463108: recall: 0.0418296208: f1: 0.0583699134
Train-Acc: 11: Accuracy: 0.8667001128: precision: 0.3347513089: recall: 0.0672539609: f1: 0.1120052554
12: 3232: loss: 0.6440718216:
12: 6432: loss: 0.6436718729:
12: 9632: loss: 0.6435022197:
12: 12832: loss: 0.6430310944:
12: 16032: loss: 0.6430885673:
12: 19232: loss: 0.6431728813:
12: 22432: loss: 0.6431499343:
12: 25632: loss: 0.6434833077:
12: 28832: loss: 0.6432506941:
12: 32032: loss: 0.6430901582:
12: 35232: loss: 0.6430185944:
12: 38432: loss: 0.6430459749:
12: 41632: loss: 0.6430542674:
12: 44832: loss: 0.6429569229:
12: 48032: loss: 0.6428212092:
12: 51232: loss: 0.6428347639:
12: 54432: loss: 0.6427707658:
12: 57632: loss: 0.6426902136:
12: 60832: loss: 0.6425815483:
12: 64032: loss: 0.6425405014:
12: 67232: loss: 0.6424309676:
12: 70432: loss: 0.6424005884:
12: 73632: loss: 0.6423736202:
12: 76832: loss: 0.6422997378:
12: 80032: loss: 0.6421798099:
12: 83232: loss: 0.6421494760:
12: 86432: loss: 0.6420466089:
12: 89632: loss: 0.6419995954:
12: 92832: loss: 0.6419484524:
12: 96032: loss: 0.6419518832:
12: 99232: loss: 0.6418658446:
12: 102432: loss: 0.6418414476:
12: 105632: loss: 0.6417474082:
12: 108832: loss: 0.6416624868:
12: 112032: loss: 0.6416737898:
12: 115232: loss: 0.6415979024:
12: 118432: loss: 0.6414648955:
12: 121632: loss: 0.6413922969:
Dev-Acc: 12: Accuracy: 0.9294729233: precision: 0.0712788260: recall: 0.0173439891: f1: 0.0278993435
Train-Acc: 12: Accuracy: 0.8701680303: precision: 0.3547430830: recall: 0.0472026823: f1: 0.0833188280
13: 3232: loss: 0.6399045426:
13: 6432: loss: 0.6389700007:
13: 9632: loss: 0.6382167542:
13: 12832: loss: 0.6385610192:
13: 16032: loss: 0.6385916753:
13: 19232: loss: 0.6385549102:
13: 22432: loss: 0.6382554780:
13: 25632: loss: 0.6385448763:
13: 28832: loss: 0.6388215802:
13: 32032: loss: 0.6386394162:
13: 35232: loss: 0.6383758022:
13: 38432: loss: 0.6382324367:
13: 41632: loss: 0.6382055013:
13: 44832: loss: 0.6379042523:
13: 48032: loss: 0.6377568427:
13: 51232: loss: 0.6376423996:
13: 54432: loss: 0.6376047800:
13: 57632: loss: 0.6374736231:
13: 60832: loss: 0.6374121489:
13: 64032: loss: 0.6372334242:
13: 67232: loss: 0.6371309874:
13: 70432: loss: 0.6370111561:
13: 73632: loss: 0.6369579164:
13: 76832: loss: 0.6368324278:
13: 80032: loss: 0.6368483365:
13: 83232: loss: 0.6368013423:
13: 86432: loss: 0.6367940787:
13: 89632: loss: 0.6367323487:
13: 92832: loss: 0.6366987974:
13: 96032: loss: 0.6365963335:
13: 99232: loss: 0.6364753378:
13: 102432: loss: 0.6364446033:
13: 105632: loss: 0.6363955000:
13: 108832: loss: 0.6363336015:
13: 112032: loss: 0.6362626561:
13: 115232: loss: 0.6361933068:
13: 118432: loss: 0.6361049670:
13: 121632: loss: 0.6360334914:
Dev-Acc: 13: Accuracy: 0.9348210096: precision: 0.0753086420: recall: 0.0103723856: f1: 0.0182334479
Train-Acc: 13: Accuracy: 0.8726004362: precision: 0.3866459627: recall: 0.0327394649: f1: 0.0603672950
14: 3232: loss: 0.6326524335:
14: 6432: loss: 0.6331266338:
14: 9632: loss: 0.6331833947:
14: 12832: loss: 0.6328818020:
14: 16032: loss: 0.6329906117:
14: 19232: loss: 0.6330313757:
14: 22432: loss: 0.6329932091:
14: 25632: loss: 0.6330552249:
14: 28832: loss: 0.6329400292:
14: 32032: loss: 0.6329928378:
14: 35232: loss: 0.6329193223:
14: 38432: loss: 0.6329044303:
14: 41632: loss: 0.6327666919:
14: 44832: loss: 0.6327681447:
14: 48032: loss: 0.6327945253:
14: 51232: loss: 0.6327338664:
14: 54432: loss: 0.6326286456:
14: 57632: loss: 0.6324449280:
14: 60832: loss: 0.6323252245:
14: 64032: loss: 0.6322166374:
14: 67232: loss: 0.6320874690:
14: 70432: loss: 0.6321115186:
14: 73632: loss: 0.6321607948:
14: 76832: loss: 0.6320373532:
14: 80032: loss: 0.6319811554:
14: 83232: loss: 0.6319171776:
14: 86432: loss: 0.6318619064:
14: 89632: loss: 0.6317187504:
14: 92832: loss: 0.6316570305:
14: 96032: loss: 0.6316095612:
14: 99232: loss: 0.6315942701:
14: 102432: loss: 0.6314826614:
14: 105632: loss: 0.6314238053:
14: 108832: loss: 0.6313141592:
14: 112032: loss: 0.6312985477:
14: 115232: loss: 0.6311693361:
14: 118432: loss: 0.6310449835:
14: 121632: loss: 0.6309010209:
Dev-Acc: 14: Accuracy: 0.9380357862: precision: 0.1008771930: recall: 0.0078217990: f1: 0.0145179107
Train-Acc: 14: Accuracy: 0.8742768764: precision: 0.4458128079: recall: 0.0237985668: f1: 0.0451850465
15: 3232: loss: 0.6298279393:
15: 6432: loss: 0.6289123917:
15: 9632: loss: 0.6285655155:
15: 12832: loss: 0.6284598628:
15: 16032: loss: 0.6279528538:
15: 19232: loss: 0.6276945213:
15: 22432: loss: 0.6278214593:
15: 25632: loss: 0.6279237832:
15: 28832: loss: 0.6278055639:
15: 32032: loss: 0.6276860647:
15: 35232: loss: 0.6274162381:
15: 38432: loss: 0.6275650243:
15: 41632: loss: 0.6274619674:
15: 44832: loss: 0.6274208845:
15: 48032: loss: 0.6271789078:
15: 51232: loss: 0.6271250635:
15: 54432: loss: 0.6269964305:
15: 57632: loss: 0.6269262525:
15: 60832: loss: 0.6267899553:
15: 64032: loss: 0.6266800043:
15: 67232: loss: 0.6267171238:
15: 70432: loss: 0.6266453333:
15: 73632: loss: 0.6265272569:
15: 76832: loss: 0.6264918781:
15: 80032: loss: 0.6264321264:
15: 83232: loss: 0.6261990550:
15: 86432: loss: 0.6261086181:
15: 89632: loss: 0.6260637865:
15: 92832: loss: 0.6260645598:
15: 96032: loss: 0.6259980844:
15: 99232: loss: 0.6258793634:
15: 102432: loss: 0.6258659091:
15: 105632: loss: 0.6257703615:
15: 108832: loss: 0.6257205472:
15: 112032: loss: 0.6256512012:
15: 115232: loss: 0.6255411744:
15: 118432: loss: 0.6254798335:
15: 121632: loss: 0.6254519890:
Dev-Acc: 15: Accuracy: 0.9398416281: precision: 0.1472868217: recall: 0.0064614861: f1: 0.0123798664
Train-Acc: 15: Accuracy: 0.8747535348: precision: 0.4651162791: recall: 0.0131483795: f1: 0.0255738124
16: 3232: loss: 0.6224105591:
16: 6432: loss: 0.6231938401:
16: 9632: loss: 0.6233747401:
16: 12832: loss: 0.6234928308:
16: 16032: loss: 0.6237119710:
16: 19232: loss: 0.6235837039:
16: 22432: loss: 0.6228701241:
16: 25632: loss: 0.6226467693:
16: 28832: loss: 0.6223998671:
16: 32032: loss: 0.6223211603:
16: 35232: loss: 0.6223055073:
16: 38432: loss: 0.6220616494:
16: 41632: loss: 0.6221666455:
16: 44832: loss: 0.6220206812:
16: 48032: loss: 0.6220355176:
16: 51232: loss: 0.6218531810:
16: 54432: loss: 0.6217229686:
16: 57632: loss: 0.6216040216:
16: 60832: loss: 0.6215685711:
16: 64032: loss: 0.6215447041:
16: 67232: loss: 0.6214616399:
16: 70432: loss: 0.6213282226:
16: 73632: loss: 0.6212693191:
16: 76832: loss: 0.6212996874:
16: 80032: loss: 0.6211959372:
16: 83232: loss: 0.6211286265:
16: 86432: loss: 0.6211044981:
16: 89632: loss: 0.6209709199:
16: 92832: loss: 0.6209234150:
16: 96032: loss: 0.6208373407:
16: 99232: loss: 0.6207364120:
16: 102432: loss: 0.6207277927:
16: 105632: loss: 0.6206361843:
16: 108832: loss: 0.6204963518:
16: 112032: loss: 0.6204028452:
16: 115232: loss: 0.6203261645:
16: 118432: loss: 0.6202624140:
16: 121632: loss: 0.6201441902:
Dev-Acc: 16: Accuracy: 0.9407842159: precision: 0.0934579439: recall: 0.0017003911: f1: 0.0033400134
Train-Acc: 16: Accuracy: 0.8748356700: precision: 0.4576271186: recall: 0.0071001249: f1: 0.0139832977
17: 3232: loss: 0.6186429209:
17: 6432: loss: 0.6174697924:
17: 9632: loss: 0.6170654494:
17: 12832: loss: 0.6175471154:
17: 16032: loss: 0.6172877598:
17: 19232: loss: 0.6175234731:
17: 22432: loss: 0.6173849585:
17: 25632: loss: 0.6173401981:
17: 28832: loss: 0.6171578046:
17: 32032: loss: 0.6171261919:
17: 35232: loss: 0.6172290430:
17: 38432: loss: 0.6171228766:
17: 41632: loss: 0.6169913406:
17: 44832: loss: 0.6170832457:
17: 48032: loss: 0.6169904667:
17: 51232: loss: 0.6168298849:
17: 54432: loss: 0.6168890438:
17: 57632: loss: 0.6168986332:
17: 60832: loss: 0.6167532393:
17: 64032: loss: 0.6165558267:
17: 67232: loss: 0.6164195705:
17: 70432: loss: 0.6163359947:
17: 73632: loss: 0.6162865208:
17: 76832: loss: 0.6161574970:
17: 80032: loss: 0.6160570747:
17: 83232: loss: 0.6161096153:
17: 86432: loss: 0.6159554126:
17: 89632: loss: 0.6159222793:
17: 92832: loss: 0.6158613720:
17: 96032: loss: 0.6158479996:
17: 99232: loss: 0.6156776581:
17: 102432: loss: 0.6155801933:
17: 105632: loss: 0.6154940195:
17: 108832: loss: 0.6153742038:
17: 112032: loss: 0.6153674813:
17: 115232: loss: 0.6152963695:
17: 118432: loss: 0.6151923764:
17: 121632: loss: 0.6150837590:
Dev-Acc: 17: Accuracy: 0.9413002133: precision: 0.1111111111: recall: 0.0008501955: f1: 0.0016874789
Train-Acc: 17: Accuracy: 0.8749671578: precision: 0.4846153846: recall: 0.0041417395: f1: 0.0082132847
18: 3232: loss: 0.6121323580:
18: 6432: loss: 0.6114236078:
18: 9632: loss: 0.6106481691:
18: 12832: loss: 0.6117864566:
18: 16032: loss: 0.6117334844:
18: 19232: loss: 0.6118826019:
18: 22432: loss: 0.6117733365:
18: 25632: loss: 0.6119747391:
18: 28832: loss: 0.6119018710:
18: 32032: loss: 0.6119652404:
18: 35232: loss: 0.6119798836:
18: 38432: loss: 0.6120244095:
18: 41632: loss: 0.6118852428:
18: 44832: loss: 0.6117478716:
18: 48032: loss: 0.6116734692:
18: 51232: loss: 0.6115626885:
18: 54432: loss: 0.6113923280:
18: 57632: loss: 0.6113554825:
18: 60832: loss: 0.6112819813:
18: 64032: loss: 0.6111131985:
18: 67232: loss: 0.6111704305:
18: 70432: loss: 0.6110480239:
18: 73632: loss: 0.6110708494:
18: 76832: loss: 0.6110503124:
18: 80032: loss: 0.6109761176:
18: 83232: loss: 0.6108007564:
18: 86432: loss: 0.6106330633:
18: 89632: loss: 0.6105387111:
18: 92832: loss: 0.6104058756:
18: 96032: loss: 0.6103133568:
18: 99232: loss: 0.6101949939:
18: 102432: loss: 0.6101109233:
18: 105632: loss: 0.6100773794:
18: 108832: loss: 0.6100068146:
18: 112032: loss: 0.6099940908:
18: 115232: loss: 0.6100011281:
18: 118432: loss: 0.6099716309:
18: 121632: loss: 0.6098550879:
Dev-Acc: 18: Accuracy: 0.9414589405: precision: 0.0869565217: recall: 0.0003400782: f1: 0.0006775068
Train-Acc: 18: Accuracy: 0.8751150966: precision: 0.5972222222: recall: 0.0028269016: f1: 0.0056271674
19: 3232: loss: 0.6051696897:
19: 6432: loss: 0.6048482972:
19: 9632: loss: 0.6061596187:
19: 12832: loss: 0.6061795264:
19: 16032: loss: 0.6065501788:
19: 19232: loss: 0.6065933455:
19: 22432: loss: 0.6067106721:
19: 25632: loss: 0.6066130706:
19: 28832: loss: 0.6065202371:
19: 32032: loss: 0.6067878438:
19: 35232: loss: 0.6064651049:
19: 38432: loss: 0.6062763326:
19: 41632: loss: 0.6060112634:
19: 44832: loss: 0.6059481737:
19: 48032: loss: 0.6058786856:
19: 51232: loss: 0.6057129517:
19: 54432: loss: 0.6055158165:
19: 57632: loss: 0.6055702171:
19: 60832: loss: 0.6054782191:
19: 64032: loss: 0.6054984758:
19: 67232: loss: 0.6055720060:
19: 70432: loss: 0.6053700492:
19: 73632: loss: 0.6053561399:
19: 76832: loss: 0.6053295376:
19: 80032: loss: 0.6052420688:
19: 83232: loss: 0.6051630738:
19: 86432: loss: 0.6050898541:
19: 89632: loss: 0.6050990295:
19: 92832: loss: 0.6050155511:
19: 96032: loss: 0.6049770757:
19: 99232: loss: 0.6049109589:
19: 102432: loss: 0.6049841174:
19: 105632: loss: 0.6048950888:
19: 108832: loss: 0.6047597322:
19: 112032: loss: 0.6047286250:
19: 115232: loss: 0.6048035851:
19: 118432: loss: 0.6046911635:
19: 121632: loss: 0.6046116875:
Dev-Acc: 19: Accuracy: 0.9415184855: precision: 0.0666666667: recall: 0.0001700391: f1: 0.0003392130
Train-Acc: 19: Accuracy: 0.8750986457: precision: 0.6304347826: recall: 0.0019065150: f1: 0.0038015337
20: 3232: loss: 0.6021120352:
20: 6432: loss: 0.6028764415:
20: 9632: loss: 0.6029136352:
20: 12832: loss: 0.6025690520:
20: 16032: loss: 0.6021314968:
20: 19232: loss: 0.6022734854:
20: 22432: loss: 0.6019719345:
20: 25632: loss: 0.6019635489:
20: 28832: loss: 0.6021428952:
20: 32032: loss: 0.6019899538:
20: 35232: loss: 0.6021061535:
20: 38432: loss: 0.6019242221:
20: 41632: loss: 0.6016692495:
20: 44832: loss: 0.6013730051:
20: 48032: loss: 0.6012140640:
20: 51232: loss: 0.6010323190:
20: 54432: loss: 0.6011416084:
20: 57632: loss: 0.6013100589:
20: 60832: loss: 0.6012154008:
20: 64032: loss: 0.6012304560:
20: 67232: loss: 0.6012831482:
20: 70432: loss: 0.6012343051:
20: 73632: loss: 0.6012660782:
20: 76832: loss: 0.6012126473:
20: 80032: loss: 0.6010615537:
20: 83232: loss: 0.6008768924:
20: 86432: loss: 0.6007671777:
20: 89632: loss: 0.6007953129:
20: 92832: loss: 0.6006585597:
20: 96032: loss: 0.6006222845:
20: 99232: loss: 0.6005906198:
20: 102432: loss: 0.6004470472:
20: 105632: loss: 0.6003133353:
20: 108832: loss: 0.6001047986:
20: 112032: loss: 0.6000005819:
20: 115232: loss: 0.5998697366:
20: 118432: loss: 0.5996866841:
20: 121632: loss: 0.5996667179:
Dev-Acc: 20: Accuracy: 0.9416077733: precision: 0.1666666667: recall: 0.0001700391: f1: 0.0003397316
Train-Acc: 20: Accuracy: 0.8750329018: precision: 0.5625000000: recall: 0.0011833542: f1: 0.0023617398
