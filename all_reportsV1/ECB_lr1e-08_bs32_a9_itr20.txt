1: 3232: loss: 0.7119818372:
1: 6432: loss: 0.7110550737:
1: 9632: loss: 0.7108775930:
1: 12832: loss: 0.7107939804:
1: 16032: loss: 0.7107618213:
1: 19232: loss: 0.7107776475:
1: 22432: loss: 0.7106151745:
1: 25632: loss: 0.7104954293:
1: 28832: loss: 0.7106185755:
1: 32032: loss: 0.7105133579:
1: 35232: loss: 0.7105344890:
1: 38432: loss: 0.7104119836:
1: 41632: loss: 0.7101924813:
1: 44832: loss: 0.7100448501:
1: 48032: loss: 0.7099674704:
1: 51232: loss: 0.7098908626:
1: 54432: loss: 0.7097859125:
1: 57632: loss: 0.7097314584:
1: 60832: loss: 0.7096534450:
1: 64032: loss: 0.7095806661:
1: 67232: loss: 0.7094016629:
1: 70432: loss: 0.7093033995:
1: 73632: loss: 0.7092883723:
1: 76832: loss: 0.7092043834:
1: 80032: loss: 0.7090276596:
1: 83232: loss: 0.7088759396:
1: 86432: loss: 0.7088221910:
1: 89632: loss: 0.7087342505:
1: 92832: loss: 0.7086423340:
1: 96032: loss: 0.7085613896:
1: 99232: loss: 0.7084528635:
1: 102432: loss: 0.7083388029:
1: 105632: loss: 0.7082829961:
1: 108832: loss: 0.7081971197:
1: 112032: loss: 0.7081196671:
1: 115232: loss: 0.7080227950:
1: 118432: loss: 0.7079276421:
1: 121632: loss: 0.7078357075:
1: 124832: loss: 0.7077690962:
1: 128032: loss: 0.7077158945:
1: 131232: loss: 0.7076522792:
1: 134432: loss: 0.7075186049:
1: 137632: loss: 0.7074304391:
1: 140832: loss: 0.7073381834:
1: 144032: loss: 0.7072326829:
1: 147232: loss: 0.7071671522:
1: 150432: loss: 0.7070529115:
Dev-Acc: 1: Accuracy: 0.3585787416: precision: 0.0651105651: recall: 0.7480020405: f1: 0.1197935814
Train-Acc: 1: Accuracy: 0.3760962486: precision: 0.1114415829: recall: 0.7512984025: f1: 0.1940929703
2: 3232: loss: 0.7022988987:
2: 6432: loss: 0.7026520926:
2: 9632: loss: 0.7022517053:
2: 12832: loss: 0.7023894031:
2: 16032: loss: 0.7025688689:
2: 19232: loss: 0.7024195553:
2: 22432: loss: 0.7022695060:
2: 25632: loss: 0.7019895303:
2: 28832: loss: 0.7018417826:
2: 32032: loss: 0.7017383854:
2: 35232: loss: 0.7016246971:
2: 38432: loss: 0.7015741592:
2: 41632: loss: 0.7015709453:
2: 44832: loss: 0.7015329775:
2: 48032: loss: 0.7014982406:
2: 51232: loss: 0.7014016334:
2: 54432: loss: 0.7012260256:
2: 57632: loss: 0.7010785273:
2: 60832: loss: 0.7010239668:
2: 64032: loss: 0.7009313518:
2: 67232: loss: 0.7008504456:
2: 70432: loss: 0.7007680512:
2: 73632: loss: 0.7006514014:
2: 76832: loss: 0.7005885332:
2: 80032: loss: 0.7004718506:
2: 83232: loss: 0.7003249770:
2: 86432: loss: 0.7002267924:
2: 89632: loss: 0.7000682064:
2: 92832: loss: 0.6999710326:
2: 96032: loss: 0.6998773118:
2: 99232: loss: 0.6997931117:
2: 102432: loss: 0.6997304164:
2: 105632: loss: 0.6995954900:
2: 108832: loss: 0.6995176910:
2: 112032: loss: 0.6994036017:
2: 115232: loss: 0.6993445067:
2: 118432: loss: 0.6992497080:
2: 121632: loss: 0.6991571466:
2: 124832: loss: 0.6990067742:
2: 128032: loss: 0.6989058188:
2: 131232: loss: 0.6988057234:
2: 134432: loss: 0.6987117575:
2: 137632: loss: 0.6986164357:
2: 140832: loss: 0.6984919880:
2: 144032: loss: 0.6984028460:
2: 147232: loss: 0.6983203485:
2: 150432: loss: 0.6982295490:
Dev-Acc: 2: Accuracy: 0.4857913852: precision: 0.0611184349: recall: 0.5439551097: f1: 0.1098897324
Train-Acc: 2: Accuracy: 0.4931233823: precision: 0.1136607656: recall: 0.5985142331: f1: 0.1910417694
3: 3232: loss: 0.6930222464:
3: 6432: loss: 0.6927090999:
3: 9632: loss: 0.6927214179:
3: 12832: loss: 0.6927010205:
3: 16032: loss: 0.6925512522:
3: 19232: loss: 0.6925633473:
3: 22432: loss: 0.6925635222:
3: 25632: loss: 0.6924828542:
3: 28832: loss: 0.6924144169:
3: 32032: loss: 0.6923915452:
3: 35232: loss: 0.6922821817:
3: 38432: loss: 0.6922601979:
3: 41632: loss: 0.6922683367:
3: 44832: loss: 0.6921346928:
3: 48032: loss: 0.6919357531:
3: 51232: loss: 0.6919081500:
3: 54432: loss: 0.6918968541:
3: 57632: loss: 0.6918445608:
3: 60832: loss: 0.6917687430:
3: 64032: loss: 0.6916934605:
3: 67232: loss: 0.6915650812:
3: 70432: loss: 0.6914359999:
3: 73632: loss: 0.6913310575:
3: 76832: loss: 0.6912916630:
3: 80032: loss: 0.6912402723:
3: 83232: loss: 0.6911455937:
3: 86432: loss: 0.6910751310:
3: 89632: loss: 0.6909592345:
3: 92832: loss: 0.6908992732:
3: 96032: loss: 0.6908371037:
3: 99232: loss: 0.6907537795:
3: 102432: loss: 0.6906582732:
3: 105632: loss: 0.6905510650:
3: 108832: loss: 0.6904906999:
3: 112032: loss: 0.6904166337:
3: 115232: loss: 0.6903144396:
3: 118432: loss: 0.6902234887:
3: 121632: loss: 0.6901224156:
3: 124832: loss: 0.6900641339:
3: 128032: loss: 0.6899921723:
3: 131232: loss: 0.6899099431:
3: 134432: loss: 0.6898557100:
3: 137632: loss: 0.6897597125:
3: 140832: loss: 0.6896635880:
3: 144032: loss: 0.6895963527:
3: 147232: loss: 0.6895234516:
3: 150432: loss: 0.6894209496:
Dev-Acc: 3: Accuracy: 0.6144328117: precision: 0.0617890932: recall: 0.3953409284: f1: 0.1068744398
Train-Acc: 3: Accuracy: 0.6109197140: precision: 0.1213097248: recall: 0.4630201828: f1: 0.1922504743
4: 3232: loss: 0.6845163906:
4: 6432: loss: 0.6845809060:
4: 9632: loss: 0.6847927157:
4: 12832: loss: 0.6844950879:
4: 16032: loss: 0.6846033626:
4: 19232: loss: 0.6844662794:
4: 22432: loss: 0.6845679296:
4: 25632: loss: 0.6843058167:
4: 28832: loss: 0.6843403924:
4: 32032: loss: 0.6843750966:
4: 35232: loss: 0.6843409581:
4: 38432: loss: 0.6842945583:
4: 41632: loss: 0.6841711560:
4: 44832: loss: 0.6840134910:
4: 48032: loss: 0.6838972993:
4: 51232: loss: 0.6838307088:
4: 54432: loss: 0.6837092359:
4: 57632: loss: 0.6836118037:
4: 60832: loss: 0.6834078646:
4: 64032: loss: 0.6833439100:
4: 67232: loss: 0.6832463793:
4: 70432: loss: 0.6832100981:
4: 73632: loss: 0.6831169291:
4: 76832: loss: 0.6830067478:
4: 80032: loss: 0.6828665626:
4: 83232: loss: 0.6827573252:
4: 86432: loss: 0.6826531606:
4: 89632: loss: 0.6825537346:
4: 92832: loss: 0.6824807792:
4: 96032: loss: 0.6823642083:
4: 99232: loss: 0.6822342039:
4: 102432: loss: 0.6821643473:
4: 105632: loss: 0.6820988184:
4: 108832: loss: 0.6820280424:
4: 112032: loss: 0.6819255571:
4: 115232: loss: 0.6818570835:
4: 118432: loss: 0.6817426511:
4: 121632: loss: 0.6816129728:
4: 124832: loss: 0.6815405965:
4: 128032: loss: 0.6814589496:
4: 131232: loss: 0.6813761540:
4: 134432: loss: 0.6812915091:
4: 137632: loss: 0.6811914335:
4: 140832: loss: 0.6811055513:
4: 144032: loss: 0.6810331291:
4: 147232: loss: 0.6809709549:
4: 150432: loss: 0.6808895711:
Dev-Acc: 4: Accuracy: 0.7231901884: precision: 0.0645888542: recall: 0.2776738650: f1: 0.1048004107
Train-Acc: 4: Accuracy: 0.7110249400: precision: 0.1354100606: recall: 0.3509302478: f1: 0.1954166057
5: 3232: loss: 0.6772134924:
5: 6432: loss: 0.6771178454:
5: 9632: loss: 0.6769250093:
5: 12832: loss: 0.6770460673:
5: 16032: loss: 0.6769022114:
5: 19232: loss: 0.6767445964:
5: 22432: loss: 0.6768087751:
5: 25632: loss: 0.6766263810:
5: 28832: loss: 0.6763577835:
5: 32032: loss: 0.6762532996:
5: 35232: loss: 0.6762207528:
5: 38432: loss: 0.6760215968:
5: 41632: loss: 0.6759629440:
5: 44832: loss: 0.6757677659:
5: 48032: loss: 0.6757223859:
5: 51232: loss: 0.6756609105:
5: 54432: loss: 0.6756153592:
5: 57632: loss: 0.6754883752:
5: 60832: loss: 0.6753845739:
5: 64032: loss: 0.6753413975:
5: 67232: loss: 0.6752962842:
5: 70432: loss: 0.6751757520:
5: 73632: loss: 0.6750492138:
5: 76832: loss: 0.6749776859:
5: 80032: loss: 0.6749107112:
5: 83232: loss: 0.6747937543:
5: 86432: loss: 0.6746441742:
5: 89632: loss: 0.6745223636:
5: 92832: loss: 0.6744106470:
5: 96032: loss: 0.6743370747:
5: 99232: loss: 0.6742490480:
5: 102432: loss: 0.6741924060:
5: 105632: loss: 0.6740961965:
5: 108832: loss: 0.6739557930:
5: 112032: loss: 0.6738213981:
5: 115232: loss: 0.6737279802:
5: 118432: loss: 0.6736771116:
5: 121632: loss: 0.6736047285:
5: 124832: loss: 0.6734679532:
5: 128032: loss: 0.6733339069:
5: 131232: loss: 0.6732499169:
5: 134432: loss: 0.6731514460:
5: 137632: loss: 0.6730528955:
5: 140832: loss: 0.6729436475:
5: 144032: loss: 0.6728685654:
5: 147232: loss: 0.6727732851:
5: 150432: loss: 0.6727121190:
Dev-Acc: 5: Accuracy: 0.7998789549: precision: 0.0655558258: recall: 0.1833021595: f1: 0.0965733483
Train-Acc: 5: Accuracy: 0.7864637375: precision: 0.1537968086: recall: 0.2521859181: f1: 0.1910691605
6: 3232: loss: 0.6675594515:
6: 6432: loss: 0.6674409872:
6: 9632: loss: 0.6676411661:
6: 12832: loss: 0.6679799902:
6: 16032: loss: 0.6679404130:
6: 19232: loss: 0.6679588193:
6: 22432: loss: 0.6679286048:
6: 25632: loss: 0.6678593725:
6: 28832: loss: 0.6675652424:
6: 32032: loss: 0.6675173814:
6: 35232: loss: 0.6673759454:
6: 38432: loss: 0.6673218799:
6: 41632: loss: 0.6672401741:
6: 44832: loss: 0.6671483134:
6: 48032: loss: 0.6671177549:
6: 51232: loss: 0.6670535513:
6: 54432: loss: 0.6670556105:
6: 57632: loss: 0.6671306924:
6: 60832: loss: 0.6670300197:
6: 64032: loss: 0.6668635683:
6: 67232: loss: 0.6667001615:
6: 70432: loss: 0.6665378813:
6: 73632: loss: 0.6664291974:
6: 76832: loss: 0.6663364742:
6: 80032: loss: 0.6663332969:
6: 83232: loss: 0.6662333778:
6: 86432: loss: 0.6661659849:
6: 89632: loss: 0.6660812773:
6: 92832: loss: 0.6660113950:
6: 96032: loss: 0.6659094632:
6: 99232: loss: 0.6658545810:
6: 102432: loss: 0.6657267939:
6: 105632: loss: 0.6656448515:
6: 108832: loss: 0.6655378028:
6: 112032: loss: 0.6653957759:
6: 115232: loss: 0.6653384363:
6: 118432: loss: 0.6652806838:
6: 121632: loss: 0.6651873072:
6: 124832: loss: 0.6650866192:
6: 128032: loss: 0.6649632246:
6: 131232: loss: 0.6648748733:
6: 134432: loss: 0.6647670541:
6: 137632: loss: 0.6646839024:
6: 140832: loss: 0.6645951872:
6: 144032: loss: 0.6645013419:
6: 147232: loss: 0.6644178945:
6: 150432: loss: 0.6643029659:
Dev-Acc: 6: Accuracy: 0.8536970019: precision: 0.0678627145: recall: 0.1183472199: f1: 0.0862613869
Train-Acc: 6: Accuracy: 0.8360988498: precision: 0.1748293858: recall: 0.1717835777: f1: 0.1732930994
7: 3232: loss: 0.6586650568:
7: 6432: loss: 0.6597535971:
7: 9632: loss: 0.6598185843:
7: 12832: loss: 0.6598225763:
7: 16032: loss: 0.6596306716:
7: 19232: loss: 0.6593637681:
7: 22432: loss: 0.6592492953:
7: 25632: loss: 0.6592220109:
7: 28832: loss: 0.6593032829:
7: 32032: loss: 0.6594014835:
7: 35232: loss: 0.6593410790:
7: 38432: loss: 0.6592136493:
7: 41632: loss: 0.6592285982:
7: 44832: loss: 0.6590712084:
7: 48032: loss: 0.6589849037:
7: 51232: loss: 0.6589299770:
7: 54432: loss: 0.6588503783:
7: 57632: loss: 0.6587885037:
7: 60832: loss: 0.6587512668:
7: 64032: loss: 0.6586604788:
7: 67232: loss: 0.6585142569:
7: 70432: loss: 0.6584033493:
7: 73632: loss: 0.6584082651:
7: 76832: loss: 0.6583255061:
7: 80032: loss: 0.6582437388:
7: 83232: loss: 0.6582057319:
7: 86432: loss: 0.6581372933:
7: 89632: loss: 0.6580260537:
7: 92832: loss: 0.6579328736:
7: 96032: loss: 0.6578138524:
7: 99232: loss: 0.6576973667:
7: 102432: loss: 0.6576226651:
7: 105632: loss: 0.6575246094:
7: 108832: loss: 0.6574372599:
7: 112032: loss: 0.6573734952:
7: 115232: loss: 0.6572811776:
7: 118432: loss: 0.6572221268:
7: 121632: loss: 0.6571231611:
7: 124832: loss: 0.6570569016:
7: 128032: loss: 0.6569509031:
7: 131232: loss: 0.6568451010:
7: 134432: loss: 0.6567490697:
7: 137632: loss: 0.6566723225:
7: 140832: loss: 0.6565752264:
7: 144032: loss: 0.6564816817:
7: 147232: loss: 0.6563768718:
7: 150432: loss: 0.6562785809:
Dev-Acc: 7: Accuracy: 0.8914510012: precision: 0.0766527197: recall: 0.0778779119: f1: 0.0772604588
Train-Acc: 7: Accuracy: 0.8659062386: precision: 0.1987685874: recall: 0.1124843863: f1: 0.1436668206
8: 3232: loss: 0.6519953090:
8: 6432: loss: 0.6524088097:
8: 9632: loss: 0.6521353215:
8: 12832: loss: 0.6521412720:
8: 16032: loss: 0.6521915457:
8: 19232: loss: 0.6518936520:
8: 22432: loss: 0.6518246738:
8: 25632: loss: 0.6517147616:
8: 28832: loss: 0.6516849112:
8: 32032: loss: 0.6517142903:
8: 35232: loss: 0.6516480239:
8: 38432: loss: 0.6517032386:
8: 41632: loss: 0.6515887068:
8: 44832: loss: 0.6514952886:
8: 48032: loss: 0.6513458660:
8: 51232: loss: 0.6512029774:
8: 54432: loss: 0.6510379379:
8: 57632: loss: 0.6509017620:
8: 60832: loss: 0.6507683755:
8: 64032: loss: 0.6506788576:
8: 67232: loss: 0.6505412053:
8: 70432: loss: 0.6503544558:
8: 73632: loss: 0.6502897805:
8: 76832: loss: 0.6501903921:
8: 80032: loss: 0.6501466387:
8: 83232: loss: 0.6501211069:
8: 86432: loss: 0.6500541129:
8: 89632: loss: 0.6499352156:
8: 92832: loss: 0.6498466836:
8: 96032: loss: 0.6497272120:
8: 99232: loss: 0.6495747089:
8: 102432: loss: 0.6494677155:
8: 105632: loss: 0.6494002509:
8: 108832: loss: 0.6493252765:
8: 112032: loss: 0.6492716569:
8: 115232: loss: 0.6491910355:
8: 118432: loss: 0.6490941413:
8: 121632: loss: 0.6490497938:
8: 124832: loss: 0.6489279133:
8: 128032: loss: 0.6488323194:
8: 131232: loss: 0.6487481953:
8: 134432: loss: 0.6486721683:
8: 137632: loss: 0.6485968784:
8: 140832: loss: 0.6485302180:
8: 144032: loss: 0.6484919636:
8: 147232: loss: 0.6484412761:
8: 150432: loss: 0.6483427696:
Dev-Acc: 8: Accuracy: 0.9166137576: precision: 0.0751094645: recall: 0.0379187213: f1: 0.0503954802
Train-Acc: 8: Accuracy: 0.8827887774: precision: 0.2149390244: recall: 0.0648872526: f1: 0.0996818664
9: 3232: loss: 0.6437860328:
9: 6432: loss: 0.6440333667:
9: 9632: loss: 0.6440664349:
9: 12832: loss: 0.6437214959:
9: 16032: loss: 0.6435842433:
9: 19232: loss: 0.6431050918:
9: 22432: loss: 0.6432870857:
9: 25632: loss: 0.6432456765:
9: 28832: loss: 0.6430638501:
9: 32032: loss: 0.6429459254:
9: 35232: loss: 0.6428019442:
9: 38432: loss: 0.6427424234:
9: 41632: loss: 0.6426886135:
9: 44832: loss: 0.6427264489:
9: 48032: loss: 0.6426791435:
9: 51232: loss: 0.6425412130:
9: 54432: loss: 0.6425331235:
9: 57632: loss: 0.6424616062:
9: 60832: loss: 0.6424158679:
9: 64032: loss: 0.6423550023:
9: 67232: loss: 0.6423350495:
9: 70432: loss: 0.6422336818:
9: 73632: loss: 0.6421968367:
9: 76832: loss: 0.6421595674:
9: 80032: loss: 0.6420424313:
9: 83232: loss: 0.6420064641:
9: 86432: loss: 0.6418767395:
9: 89632: loss: 0.6418144617:
9: 92832: loss: 0.6417225871:
9: 96032: loss: 0.6416839465:
9: 99232: loss: 0.6417183824:
9: 102432: loss: 0.6416829494:
9: 105632: loss: 0.6416051117:
9: 108832: loss: 0.6416007965:
9: 112032: loss: 0.6415043467:
9: 115232: loss: 0.6414462860:
9: 118432: loss: 0.6414123815:
9: 121632: loss: 0.6413177173:
9: 124832: loss: 0.6412636272:
9: 128032: loss: 0.6411719966:
9: 131232: loss: 0.6410463973:
9: 134432: loss: 0.6409857892:
9: 137632: loss: 0.6408333755:
9: 140832: loss: 0.6407437909:
9: 144032: loss: 0.6406399631:
9: 147232: loss: 0.6405563973:
9: 150432: loss: 0.6404571411:
Dev-Acc: 9: Accuracy: 0.9291851521: precision: 0.0558698727: recall: 0.0134330896: f1: 0.0216586703
Train-Acc: 9: Accuracy: 0.8913483620: precision: 0.2338187702: recall: 0.0379988166: f1: 0.0653735226
10: 3232: loss: 0.6356806266:
10: 6432: loss: 0.6367601293:
10: 9632: loss: 0.6363668021:
10: 12832: loss: 0.6358293310:
10: 16032: loss: 0.6355860430:
10: 19232: loss: 0.6358506339:
10: 22432: loss: 0.6358745696:
10: 25632: loss: 0.6358427064:
10: 28832: loss: 0.6357287452:
10: 32032: loss: 0.6357034498:
10: 35232: loss: 0.6355687749:
10: 38432: loss: 0.6353141563:
10: 41632: loss: 0.6351797984:
10: 44832: loss: 0.6352015781:
10: 48032: loss: 0.6350977984:
10: 51232: loss: 0.6350500112:
10: 54432: loss: 0.6350968709:
10: 57632: loss: 0.6350113074:
10: 60832: loss: 0.6349458110:
10: 64032: loss: 0.6348754312:
10: 67232: loss: 0.6348166550:
10: 70432: loss: 0.6347268475:
10: 73632: loss: 0.6345747139:
10: 76832: loss: 0.6344553154:
10: 80032: loss: 0.6342833687:
10: 83232: loss: 0.6343122619:
10: 86432: loss: 0.6342521665:
10: 89632: loss: 0.6341536391:
10: 92832: loss: 0.6341297346:
10: 96032: loss: 0.6340246173:
10: 99232: loss: 0.6339182951:
10: 102432: loss: 0.6337907572:
10: 105632: loss: 0.6337489488:
10: 108832: loss: 0.6336987931:
10: 112032: loss: 0.6336410975:
10: 115232: loss: 0.6336302481:
10: 118432: loss: 0.6335191481:
10: 121632: loss: 0.6334746496:
10: 124832: loss: 0.6333827701:
10: 128032: loss: 0.6333135955:
10: 131232: loss: 0.6332008369:
10: 134432: loss: 0.6331186637:
10: 137632: loss: 0.6329937600:
10: 140832: loss: 0.6328834285:
10: 144032: loss: 0.6328134421:
10: 147232: loss: 0.6327546319:
10: 150432: loss: 0.6326971053:
Dev-Acc: 10: Accuracy: 0.9359918237: precision: 0.0758928571: recall: 0.0086719946: f1: 0.0155653899
Train-Acc: 10: Accuracy: 0.8956413269: precision: 0.2596084119: recall: 0.0235355992: f1: 0.0431585292
11: 3232: loss: 0.6292662460:
11: 6432: loss: 0.6300218719:
11: 9632: loss: 0.6296068348:
11: 12832: loss: 0.6293245520:
11: 16032: loss: 0.6291585104:
11: 19232: loss: 0.6287244238:
11: 22432: loss: 0.6283204504:
11: 25632: loss: 0.6281943897:
11: 28832: loss: 0.6280461675:
11: 32032: loss: 0.6280236636:
11: 35232: loss: 0.6279185419:
11: 38432: loss: 0.6278194309:
11: 41632: loss: 0.6276856127:
11: 44832: loss: 0.6275051880:
11: 48032: loss: 0.6275229160:
11: 51232: loss: 0.6274462525:
11: 54432: loss: 0.6272415970:
11: 57632: loss: 0.6271773086:
11: 60832: loss: 0.6272694174:
11: 64032: loss: 0.6270592057:
11: 67232: loss: 0.6269392753:
11: 70432: loss: 0.6268999752:
11: 73632: loss: 0.6268049865:
11: 76832: loss: 0.6267406645:
11: 80032: loss: 0.6266219326:
11: 83232: loss: 0.6265830683:
11: 86432: loss: 0.6264962948:
11: 89632: loss: 0.6264234981:
11: 92832: loss: 0.6263231629:
11: 96032: loss: 0.6262626738:
11: 99232: loss: 0.6261725485:
11: 102432: loss: 0.6261026213:
11: 105632: loss: 0.6259855210:
11: 108832: loss: 0.6259838091:
11: 112032: loss: 0.6259370551:
11: 115232: loss: 0.6258595127:
11: 118432: loss: 0.6258155154:
11: 121632: loss: 0.6256904794:
11: 124832: loss: 0.6255812281:
11: 128032: loss: 0.6255579634:
11: 131232: loss: 0.6254586262:
11: 134432: loss: 0.6253640693:
11: 137632: loss: 0.6252481100:
11: 140832: loss: 0.6251368776:
11: 144032: loss: 0.6251200396:
11: 147232: loss: 0.6251234895:
11: 150432: loss: 0.6250448714:
Dev-Acc: 11: Accuracy: 0.9392066002: precision: 0.0980392157: recall: 0.0051011733: f1: 0.0096977534
Train-Acc: 11: Accuracy: 0.8979751468: precision: 0.2547770701: recall: 0.0105187036: f1: 0.0202032957
12: 3232: loss: 0.6223434341:
12: 6432: loss: 0.6230957377:
12: 9632: loss: 0.6219704805:
12: 12832: loss: 0.6216269937:
12: 16032: loss: 0.6216541004:
12: 19232: loss: 0.6213681723:
12: 22432: loss: 0.6212650136:
12: 25632: loss: 0.6210398131:
12: 28832: loss: 0.6209782194:
12: 32032: loss: 0.6209378328:
12: 35232: loss: 0.6209456563:
12: 38432: loss: 0.6207917247:
12: 41632: loss: 0.6206814145:
12: 44832: loss: 0.6206204021:
12: 48032: loss: 0.6205430557:
12: 51232: loss: 0.6203574789:
12: 54432: loss: 0.6202031018:
12: 57632: loss: 0.6200542146:
12: 60832: loss: 0.6200899675:
12: 64032: loss: 0.6199903786:
12: 67232: loss: 0.6197750904:
12: 70432: loss: 0.6195240340:
12: 73632: loss: 0.6194718899:
12: 76832: loss: 0.6194923801:
12: 80032: loss: 0.6194512102:
12: 83232: loss: 0.6194055714:
12: 86432: loss: 0.6194016686:
12: 89632: loss: 0.6193070203:
12: 92832: loss: 0.6191008633:
12: 96032: loss: 0.6190748286:
12: 99232: loss: 0.6189592502:
12: 102432: loss: 0.6188846726:
12: 105632: loss: 0.6188232746:
12: 108832: loss: 0.6187696395:
12: 112032: loss: 0.6186044001:
12: 115232: loss: 0.6185435483:
12: 118432: loss: 0.6184979715:
12: 121632: loss: 0.6184034710:
12: 124832: loss: 0.6183355393:
12: 128032: loss: 0.6182494810:
12: 131232: loss: 0.6181342980:
12: 134432: loss: 0.6180603507:
12: 137632: loss: 0.6180050720:
12: 140832: loss: 0.6179404685:
12: 144032: loss: 0.6178081484:
12: 147232: loss: 0.6177554668:
12: 150432: loss: 0.6176360457:
Dev-Acc: 12: Accuracy: 0.9407048821: precision: 0.0940170940: recall: 0.0018704302: f1: 0.0036678893
Train-Acc: 12: Accuracy: 0.8990927339: precision: 0.2570422535: recall: 0.0047991585: f1: 0.0094223943
13: 3232: loss: 0.6163718700:
13: 6432: loss: 0.6152636155:
13: 9632: loss: 0.6132492046:
13: 12832: loss: 0.6137247059:
13: 16032: loss: 0.6142168983:
13: 19232: loss: 0.6140859122:
13: 22432: loss: 0.6141972146:
13: 25632: loss: 0.6138908429:
13: 28832: loss: 0.6133439692:
13: 32032: loss: 0.6132110884:
13: 35232: loss: 0.6130268858:
13: 38432: loss: 0.6129056920:
13: 41632: loss: 0.6129528487:
13: 44832: loss: 0.6128335095:
13: 48032: loss: 0.6128649198:
13: 51232: loss: 0.6126893857:
13: 54432: loss: 0.6126175705:
13: 57632: loss: 0.6123955005:
13: 60832: loss: 0.6122449440:
13: 64032: loss: 0.6121493173:
13: 67232: loss: 0.6120800519:
13: 70432: loss: 0.6120564546:
13: 73632: loss: 0.6121290047:
13: 76832: loss: 0.6119669634:
13: 80032: loss: 0.6119083405:
13: 83232: loss: 0.6118152628:
13: 86432: loss: 0.6116234265:
13: 89632: loss: 0.6115526911:
13: 92832: loss: 0.6115464072:
13: 96032: loss: 0.6114100856:
13: 99232: loss: 0.6113525190:
13: 102432: loss: 0.6112736788:
13: 105632: loss: 0.6111906047:
13: 108832: loss: 0.6111592657:
13: 112032: loss: 0.6110573588:
13: 115232: loss: 0.6110402094:
13: 118432: loss: 0.6108558851:
13: 121632: loss: 0.6107352345:
13: 124832: loss: 0.6106958410:
13: 128032: loss: 0.6106498725:
13: 131232: loss: 0.6105785045:
13: 134432: loss: 0.6105364660:
13: 137632: loss: 0.6104660863:
13: 140832: loss: 0.6104014864:
13: 144032: loss: 0.6102576808:
13: 147232: loss: 0.6101619246:
13: 150432: loss: 0.6100759095:
Dev-Acc: 13: Accuracy: 0.9411414266: precision: 0.0819672131: recall: 0.0008501955: f1: 0.0016829350
Train-Acc: 13: Accuracy: 0.8995792270: precision: 0.2611940299: recall: 0.0023009664: f1: 0.0045617465
14: 3232: loss: 0.6076579106:
14: 6432: loss: 0.6078687793:
14: 9632: loss: 0.6074043840:
14: 12832: loss: 0.6070147581:
14: 16032: loss: 0.6063037814:
14: 19232: loss: 0.6063712537:
14: 22432: loss: 0.6061605248:
14: 25632: loss: 0.6060075287:
14: 28832: loss: 0.6059953338:
14: 32032: loss: 0.6061292397:
14: 35232: loss: 0.6060285218:
14: 38432: loss: 0.6058288263:
14: 41632: loss: 0.6057180196:
14: 44832: loss: 0.6056823581:
14: 48032: loss: 0.6056089255:
14: 51232: loss: 0.6055538028:
14: 54432: loss: 0.6054890438:
14: 57632: loss: 0.6053012171:
14: 60832: loss: 0.6053034854:
14: 64032: loss: 0.6052613536:
14: 67232: loss: 0.6052220548:
14: 70432: loss: 0.6050425105:
14: 73632: loss: 0.6050089933:
14: 76832: loss: 0.6048125700:
14: 80032: loss: 0.6047897663:
14: 83232: loss: 0.6047807873:
14: 86432: loss: 0.6046978154:
14: 89632: loss: 0.6046016741:
14: 92832: loss: 0.6044993640:
14: 96032: loss: 0.6044553780:
14: 99232: loss: 0.6044253119:
14: 102432: loss: 0.6042739247:
14: 105632: loss: 0.6041749886:
14: 108832: loss: 0.6041471851:
14: 112032: loss: 0.6040468699:
14: 115232: loss: 0.6039762121:
14: 118432: loss: 0.6038179801:
14: 121632: loss: 0.6037021991:
14: 124832: loss: 0.6035767271:
14: 128032: loss: 0.6035246396:
14: 131232: loss: 0.6034124867:
14: 134432: loss: 0.6033359678:
14: 137632: loss: 0.6032610542:
14: 140832: loss: 0.6031716536:
14: 144032: loss: 0.6030694147:
14: 147232: loss: 0.6029543644:
14: 150432: loss: 0.6028551212:
Dev-Acc: 14: Accuracy: 0.9414688945: precision: 0.0909090909: recall: 0.0003400782: f1: 0.0006776215
Train-Acc: 14: Accuracy: 0.8998751044: precision: 0.3272727273: recall: 0.0011833542: f1: 0.0023581816
15: 3232: loss: 0.5995272821:
15: 6432: loss: 0.5973695922:
15: 9632: loss: 0.5977815491:
15: 12832: loss: 0.5979846509:
15: 16032: loss: 0.5980904301:
15: 19232: loss: 0.5981679601:
15: 22432: loss: 0.5980979106:
15: 25632: loss: 0.5978801800:
15: 28832: loss: 0.5982221309:
15: 32032: loss: 0.5982380528:
15: 35232: loss: 0.5983955398:
15: 38432: loss: 0.5981790620:
15: 41632: loss: 0.5980342668:
15: 44832: loss: 0.5981003151:
15: 48032: loss: 0.5979597313:
15: 51232: loss: 0.5979549721:
15: 54432: loss: 0.5977313511:
15: 57632: loss: 0.5978060929:
15: 60832: loss: 0.5977123765:
15: 64032: loss: 0.5976644817:
15: 67232: loss: 0.5977060185:
15: 70432: loss: 0.5976325643:
15: 73632: loss: 0.5976617760:
15: 76832: loss: 0.5976334692:
15: 80032: loss: 0.5975603413:
15: 83232: loss: 0.5974992250:
15: 86432: loss: 0.5973383792:
15: 89632: loss: 0.5972845420:
15: 92832: loss: 0.5972439880:
15: 96032: loss: 0.5972030699:
15: 99232: loss: 0.5971750735:
15: 102432: loss: 0.5970365648:
15: 105632: loss: 0.5968960639:
15: 108832: loss: 0.5968452050:
15: 112032: loss: 0.5967608903:
15: 115232: loss: 0.5967183436:
15: 118432: loss: 0.5965987190:
15: 121632: loss: 0.5964547491:
15: 124832: loss: 0.5963158475:
15: 128032: loss: 0.5961851627:
15: 131232: loss: 0.5961170078:
15: 134432: loss: 0.5960677358:
15: 137632: loss: 0.5959419903:
15: 140832: loss: 0.5958529422:
15: 144032: loss: 0.5957955187:
15: 147232: loss: 0.5956591269:
15: 150432: loss: 0.5955802733:
Dev-Acc: 15: Accuracy: 0.9415978789: precision: 0.1428571429: recall: 0.0001700391: f1: 0.0003396739
Train-Acc: 15: Accuracy: 0.8999408484: precision: 0.3548387097: recall: 0.0007231609: f1: 0.0014433801
16: 3232: loss: 0.5922016120:
16: 6432: loss: 0.5928281716:
16: 9632: loss: 0.5914222374:
16: 12832: loss: 0.5910152856:
16: 16032: loss: 0.5908875471:
16: 19232: loss: 0.5907506247:
16: 22432: loss: 0.5909782028:
16: 25632: loss: 0.5904466356:
16: 28832: loss: 0.5906886182:
16: 32032: loss: 0.5905948586:
16: 35232: loss: 0.5903008159:
16: 38432: loss: 0.5903726466:
16: 41632: loss: 0.5905464106:
16: 44832: loss: 0.5906220204:
16: 48032: loss: 0.5903383935:
16: 51232: loss: 0.5902272566:
16: 54432: loss: 0.5901219208:
16: 57632: loss: 0.5899984709:
16: 60832: loss: 0.5898841000:
16: 64032: loss: 0.5900236735:
16: 67232: loss: 0.5899530606:
16: 70432: loss: 0.5899395117:
16: 73632: loss: 0.5900136170:
16: 76832: loss: 0.5899432608:
16: 80032: loss: 0.5898811914:
16: 83232: loss: 0.5897877580:
16: 86432: loss: 0.5898496021:
16: 89632: loss: 0.5899038039:
16: 92832: loss: 0.5897997198:
16: 96032: loss: 0.5896697975:
16: 99232: loss: 0.5896386616:
16: 102432: loss: 0.5895474640:
16: 105632: loss: 0.5894089260:
16: 108832: loss: 0.5893832080:
16: 112032: loss: 0.5892774292:
16: 115232: loss: 0.5892027744:
16: 118432: loss: 0.5891355542:
16: 121632: loss: 0.5890441050:
16: 124832: loss: 0.5890086184:
16: 128032: loss: 0.5889603430:
16: 131232: loss: 0.5888037713:
16: 134432: loss: 0.5887793221:
16: 137632: loss: 0.5886788418:
16: 140832: loss: 0.5884961552:
16: 144032: loss: 0.5884885996:
16: 147232: loss: 0.5884503850:
16: 150432: loss: 0.5883428393:
Dev-Acc: 16: Accuracy: 0.9416177273: precision: 0.2000000000: recall: 0.0001700391: f1: 0.0003397893
Train-Acc: 16: Accuracy: 0.8999474049: precision: 0.3000000000: recall: 0.0003944514: f1: 0.0007878669
17: 3232: loss: 0.5854973036:
17: 6432: loss: 0.5853955242:
17: 9632: loss: 0.5853371189:
17: 12832: loss: 0.5846329884:
17: 16032: loss: 0.5848471990:
17: 19232: loss: 0.5845529625:
17: 22432: loss: 0.5845272530:
17: 25632: loss: 0.5842060983:
17: 28832: loss: 0.5838461331:
17: 32032: loss: 0.5841539726:
17: 35232: loss: 0.5837754993:
17: 38432: loss: 0.5834997175:
17: 41632: loss: 0.5834938824:
17: 44832: loss: 0.5832999279:
17: 48032: loss: 0.5833812251:
17: 51232: loss: 0.5833722415:
17: 54432: loss: 0.5835006634:
17: 57632: loss: 0.5835406913:
17: 60832: loss: 0.5835155124:
17: 64032: loss: 0.5834828795:
17: 67232: loss: 0.5833777255:
17: 70432: loss: 0.5833343404:
17: 73632: loss: 0.5831409467:
17: 76832: loss: 0.5829905719:
17: 80032: loss: 0.5827576083:
17: 83232: loss: 0.5826879616:
17: 86432: loss: 0.5826656102:
17: 89632: loss: 0.5827942100:
17: 92832: loss: 0.5826878283:
17: 96032: loss: 0.5826579919:
17: 99232: loss: 0.5826031980:
17: 102432: loss: 0.5826105256:
17: 105632: loss: 0.5825215573:
17: 108832: loss: 0.5824233899:
17: 112032: loss: 0.5824572555:
17: 115232: loss: 0.5823099915:
17: 118432: loss: 0.5822472685:
17: 121632: loss: 0.5820814388:
17: 124832: loss: 0.5819319319:
17: 128032: loss: 0.5819192569:
17: 131232: loss: 0.5818072942:
17: 134432: loss: 0.5817304384:
17: 137632: loss: 0.5816668132:
17: 140832: loss: 0.5815740319:
17: 144032: loss: 0.5815016051:
17: 147232: loss: 0.5814515623:
17: 150432: loss: 0.5813421110:
Dev-Acc: 17: Accuracy: 0.9416574240: precision: 1.0000000000: recall: 0.0001700391: f1: 0.0003400204
Train-Acc: 17: Accuracy: 0.9000328779: precision: 1.0000000000: recall: 0.0003287095: f1: 0.0006572029
18: 3232: loss: 0.5812989938:
18: 6432: loss: 0.5776537386:
18: 9632: loss: 0.5773379519:
18: 12832: loss: 0.5762256098:
18: 16032: loss: 0.5770515800:
18: 19232: loss: 0.5764472898:
18: 22432: loss: 0.5767079255:
18: 25632: loss: 0.5768114866:
18: 28832: loss: 0.5769184260:
18: 32032: loss: 0.5768376669:
18: 35232: loss: 0.5764819904:
18: 38432: loss: 0.5764102809:
18: 41632: loss: 0.5763697950:
18: 44832: loss: 0.5764065298:
18: 48032: loss: 0.5766077193:
18: 51232: loss: 0.5764698600:
18: 54432: loss: 0.5764608355:
18: 57632: loss: 0.5763464093:
18: 60832: loss: 0.5762993892:
18: 64032: loss: 0.5760749845:
18: 67232: loss: 0.5759647884:
18: 70432: loss: 0.5760047215:
18: 73632: loss: 0.5758865060:
18: 76832: loss: 0.5758545036:
18: 80032: loss: 0.5757000396:
18: 83232: loss: 0.5756780806:
18: 86432: loss: 0.5756365695:
18: 89632: loss: 0.5755939110:
18: 92832: loss: 0.5756453082:
18: 96032: loss: 0.5756336805:
18: 99232: loss: 0.5755358314:
18: 102432: loss: 0.5754752476:
18: 105632: loss: 0.5753264727:
18: 108832: loss: 0.5753260290:
18: 112032: loss: 0.5752593277:
18: 115232: loss: 0.5751355771:
18: 118432: loss: 0.5750304543:
18: 121632: loss: 0.5749714260:
18: 124832: loss: 0.5748930051:
18: 128032: loss: 0.5748035716:
18: 131232: loss: 0.5747524901:
18: 134432: loss: 0.5745905499:
18: 137632: loss: 0.5744547224:
18: 140832: loss: 0.5743288921:
18: 144032: loss: 0.5743234338:
18: 147232: loss: 0.5743069728:
18: 150432: loss: 0.5742778701:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.9000131488: precision: 1.0000000000: recall: 0.0001314838: f1: 0.0002629330
19: 3232: loss: 0.5735039377:
19: 6432: loss: 0.5721717852:
19: 9632: loss: 0.5711781114:
19: 12832: loss: 0.5709741667:
19: 16032: loss: 0.5713717226:
19: 19232: loss: 0.5713713136:
19: 22432: loss: 0.5710124466:
19: 25632: loss: 0.5709065855:
19: 28832: loss: 0.5707940639:
19: 32032: loss: 0.5708877157:
19: 35232: loss: 0.5704779301:
19: 38432: loss: 0.5701881045:
19: 41632: loss: 0.5699230788:
19: 44832: loss: 0.5699552251:
19: 48032: loss: 0.5697905355:
19: 51232: loss: 0.5693809040:
19: 54432: loss: 0.5691764509:
19: 57632: loss: 0.5690074860:
19: 60832: loss: 0.5688977564:
19: 64032: loss: 0.5688642855:
19: 67232: loss: 0.5688450577:
19: 70432: loss: 0.5687906807:
19: 73632: loss: 0.5687578783:
19: 76832: loss: 0.5686083147:
19: 80032: loss: 0.5683989176:
19: 83232: loss: 0.5683730726:
19: 86432: loss: 0.5681669106:
19: 89632: loss: 0.5682694734:
19: 92832: loss: 0.5682635249:
19: 96032: loss: 0.5683294534:
19: 99232: loss: 0.5683242891:
19: 102432: loss: 0.5682580769:
19: 105632: loss: 0.5681986911:
19: 108832: loss: 0.5680799722:
19: 112032: loss: 0.5679787590:
19: 115232: loss: 0.5677975970:
19: 118432: loss: 0.5678013751:
19: 121632: loss: 0.5677741813:
19: 124832: loss: 0.5677360346:
19: 128032: loss: 0.5677314075:
19: 131232: loss: 0.5675722466:
19: 134432: loss: 0.5674675200:
19: 137632: loss: 0.5673803865:
19: 140832: loss: 0.5673134105:
19: 144032: loss: 0.5672661227:
19: 147232: loss: 0.5672921651:
19: 150432: loss: 0.5672540263:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
20: 3232: loss: 0.5621504200:
20: 6432: loss: 0.5637262627:
20: 9632: loss: 0.5636611706:
20: 12832: loss: 0.5637993968:
20: 16032: loss: 0.5636172606:
20: 19232: loss: 0.5634732508:
20: 22432: loss: 0.5632934052:
20: 25632: loss: 0.5627131884:
20: 28832: loss: 0.5627463657:
20: 32032: loss: 0.5627373168:
20: 35232: loss: 0.5626050175:
20: 38432: loss: 0.5624857371:
20: 41632: loss: 0.5627768506:
20: 44832: loss: 0.5625811725:
20: 48032: loss: 0.5626665573:
20: 51232: loss: 0.5626984256:
20: 54432: loss: 0.5628067103:
20: 57632: loss: 0.5627941731:
20: 60832: loss: 0.5626073360:
20: 64032: loss: 0.5625287004:
20: 67232: loss: 0.5624262258:
20: 70432: loss: 0.5626267612:
20: 73632: loss: 0.5627058502:
20: 76832: loss: 0.5625974488:
20: 80032: loss: 0.5624027697:
20: 83232: loss: 0.5622925341:
20: 86432: loss: 0.5621034919:
20: 89632: loss: 0.5620940803:
20: 92832: loss: 0.5619411332:
20: 96032: loss: 0.5618771679:
20: 99232: loss: 0.5616779188:
20: 102432: loss: 0.5615488426:
20: 105632: loss: 0.5616375016:
20: 108832: loss: 0.5616014262:
20: 112032: loss: 0.5614465750:
20: 115232: loss: 0.5614025086:
20: 118432: loss: 0.5612828580:
20: 121632: loss: 0.5611391897:
20: 124832: loss: 0.5609794682:
20: 128032: loss: 0.5608751509:
20: 131232: loss: 0.5606929858:
20: 134432: loss: 0.5606720512:
20: 137632: loss: 0.5606421237:
20: 140832: loss: 0.5604691374:
20: 144032: loss: 0.5603830262:
20: 147232: loss: 0.5603709989:
20: 150432: loss: 0.5602902069:
Dev-Acc: 20: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 20: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
