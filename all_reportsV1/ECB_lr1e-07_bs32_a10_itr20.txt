1: 3232: loss: 0.7112611806:
1: 6432: loss: 0.7097432712:
1: 9632: loss: 0.7084852558:
1: 12832: loss: 0.7071429537:
1: 16032: loss: 0.7061985493:
1: 19232: loss: 0.7053276958:
1: 22432: loss: 0.7043118258:
1: 25632: loss: 0.7035348297:
1: 28832: loss: 0.7026002464:
1: 32032: loss: 0.7015433731:
1: 35232: loss: 0.7005933997:
1: 38432: loss: 0.6997306822:
1: 41632: loss: 0.6988281319:
1: 44832: loss: 0.6978609248:
1: 48032: loss: 0.6969125580:
1: 51232: loss: 0.6959290968:
1: 54432: loss: 0.6949804320:
1: 57632: loss: 0.6940334151:
1: 60832: loss: 0.6931383503:
1: 64032: loss: 0.6921623945:
1: 67232: loss: 0.6911905331:
1: 70432: loss: 0.6902488118:
1: 73632: loss: 0.6893569468:
1: 76832: loss: 0.6884352072:
1: 80032: loss: 0.6875398663:
1: 83232: loss: 0.6866327399:
1: 86432: loss: 0.6857047199:
1: 89632: loss: 0.6848422311:
1: 92832: loss: 0.6838995577:
1: 96032: loss: 0.6830377509:
1: 99232: loss: 0.6821181798:
1: 102432: loss: 0.6812077560:
1: 105632: loss: 0.6802884248:
1: 108832: loss: 0.6793707551:
1: 112032: loss: 0.6784982296:
1: 115232: loss: 0.6775766188:
1: 118432: loss: 0.6767018696:
1: 121632: loss: 0.6758170935:
1: 124832: loss: 0.6749233205:
1: 128032: loss: 0.6740477829:
1: 131232: loss: 0.6731839255:
1: 134432: loss: 0.6722877227:
1: 137632: loss: 0.6714026846:
1: 140832: loss: 0.6704848356:
1: 144032: loss: 0.6695820709:
1: 147232: loss: 0.6686933795:
1: 150432: loss: 0.6678274344:
1: 153632: loss: 0.6669630949:
1: 156832: loss: 0.6661089480:
1: 160032: loss: 0.6652039704:
1: 163232: loss: 0.6643022808:
1: 166432: loss: 0.6634888526:
Dev-Acc: 1: Accuracy: 0.9392264485: precision: 0.0960264901: recall: 0.0049311342: f1: 0.0093805596
Train-Acc: 1: Accuracy: 0.9069274068: precision: 0.2189440994: recall: 0.0092696075: f1: 0.0177861873
2: 3232: loss: 0.6176242077:
2: 6432: loss: 0.6177148038:
2: 9632: loss: 0.6159027708:
2: 12832: loss: 0.6150143029:
2: 16032: loss: 0.6140921875:
2: 19232: loss: 0.6135586536:
2: 22432: loss: 0.6128076198:
2: 25632: loss: 0.6119092467:
2: 28832: loss: 0.6109855837:
2: 32032: loss: 0.6100474994:
2: 35232: loss: 0.6091333382:
2: 38432: loss: 0.6082783175:
2: 41632: loss: 0.6076051418:
2: 44832: loss: 0.6066463295:
2: 48032: loss: 0.6058010809:
2: 51232: loss: 0.6049322071:
2: 54432: loss: 0.6041742979:
2: 57632: loss: 0.6034443866:
2: 60832: loss: 0.6028583143:
2: 64032: loss: 0.6020060028:
2: 67232: loss: 0.6012304881:
2: 70432: loss: 0.6005009481:
2: 73632: loss: 0.5996030311:
2: 76832: loss: 0.5988984134:
2: 80032: loss: 0.5980345382:
2: 83232: loss: 0.5971754073:
2: 86432: loss: 0.5963806740:
2: 89632: loss: 0.5955419912:
2: 92832: loss: 0.5947722282:
2: 96032: loss: 0.5939438224:
2: 99232: loss: 0.5931671056:
2: 102432: loss: 0.5923183392:
2: 105632: loss: 0.5915165666:
2: 108832: loss: 0.5908496621:
2: 112032: loss: 0.5900473552:
2: 115232: loss: 0.5893132832:
2: 118432: loss: 0.5884049184:
2: 121632: loss: 0.5876413628:
2: 124832: loss: 0.5868272346:
2: 128032: loss: 0.5861696402:
2: 131232: loss: 0.5854245764:
2: 134432: loss: 0.5845809420:
2: 137632: loss: 0.5838180621:
2: 140832: loss: 0.5829572519:
2: 144032: loss: 0.5821003547:
2: 147232: loss: 0.5812381383:
2: 150432: loss: 0.5804245246:
2: 153632: loss: 0.5797014088:
2: 156832: loss: 0.5789030584:
2: 160032: loss: 0.5782045595:
2: 163232: loss: 0.5774016899:
2: 166432: loss: 0.5765848769:
Dev-Acc: 2: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 2: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
3: 3232: loss: 0.5341464862:
3: 6432: loss: 0.5343040407:
3: 9632: loss: 0.5343658143:
3: 12832: loss: 0.5337754379:
3: 16032: loss: 0.5323493320:
3: 19232: loss: 0.5313974428:
3: 22432: loss: 0.5304170208:
3: 25632: loss: 0.5303221112:
3: 28832: loss: 0.5291613202:
3: 32032: loss: 0.5287497957:
3: 35232: loss: 0.5280997976:
3: 38432: loss: 0.5269104557:
3: 41632: loss: 0.5261724131:
3: 44832: loss: 0.5256210800:
3: 48032: loss: 0.5249332897:
3: 51232: loss: 0.5239647226:
3: 54432: loss: 0.5231733070:
3: 57632: loss: 0.5224424581:
3: 60832: loss: 0.5217967190:
3: 64032: loss: 0.5208504600:
3: 67232: loss: 0.5204852483:
3: 70432: loss: 0.5198394993:
3: 73632: loss: 0.5190364264:
3: 76832: loss: 0.5185264436:
3: 80032: loss: 0.5175790011:
3: 83232: loss: 0.5169506656:
3: 86432: loss: 0.5163878940:
3: 89632: loss: 0.5155827367:
3: 92832: loss: 0.5149445214:
3: 96032: loss: 0.5142974510:
3: 99232: loss: 0.5134588433:
3: 102432: loss: 0.5127009202:
3: 105632: loss: 0.5121334364:
3: 108832: loss: 0.5116281989:
3: 112032: loss: 0.5107553037:
3: 115232: loss: 0.5101268708:
3: 118432: loss: 0.5095982552:
3: 121632: loss: 0.5089623982:
3: 124832: loss: 0.5082976334:
3: 128032: loss: 0.5075667909:
3: 131232: loss: 0.5066501293:
3: 134432: loss: 0.5061427723:
3: 137632: loss: 0.5054974408:
3: 140832: loss: 0.5049403416:
3: 144032: loss: 0.5042719564:
3: 147232: loss: 0.5037450700:
3: 150432: loss: 0.5031214105:
3: 153632: loss: 0.5024302365:
3: 156832: loss: 0.5015943032:
3: 160032: loss: 0.5009512543:
3: 163232: loss: 0.5003062068:
3: 166432: loss: 0.4994418406:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.4602072278:
4: 6432: loss: 0.4598050711:
4: 9632: loss: 0.4598430326:
4: 12832: loss: 0.4605297347:
4: 16032: loss: 0.4593037827:
4: 19232: loss: 0.4584557288:
4: 22432: loss: 0.4574457143:
4: 25632: loss: 0.4576212151:
4: 28832: loss: 0.4576276073:
4: 32032: loss: 0.4574535792:
4: 35232: loss: 0.4569069166:
4: 38432: loss: 0.4560141691:
4: 41632: loss: 0.4553661878:
4: 44832: loss: 0.4550986308:
4: 48032: loss: 0.4545019643:
4: 51232: loss: 0.4537484303:
4: 54432: loss: 0.4532521909:
4: 57632: loss: 0.4525564096:
4: 60832: loss: 0.4523782824:
4: 64032: loss: 0.4519194218:
4: 67232: loss: 0.4516233653:
4: 70432: loss: 0.4510376940:
4: 73632: loss: 0.4505070895:
4: 76832: loss: 0.4500115030:
4: 80032: loss: 0.4496177770:
4: 83232: loss: 0.4491033691:
4: 86432: loss: 0.4483117442:
4: 89632: loss: 0.4474864620:
4: 92832: loss: 0.4465819996:
4: 96032: loss: 0.4460987840:
4: 99232: loss: 0.4454716956:
4: 102432: loss: 0.4447480482:
4: 105632: loss: 0.4442535437:
4: 108832: loss: 0.4441200189:
4: 112032: loss: 0.4435527752:
4: 115232: loss: 0.4432046033:
4: 118432: loss: 0.4427656743:
4: 121632: loss: 0.4423146684:
4: 124832: loss: 0.4416178055:
4: 128032: loss: 0.4408180117:
4: 131232: loss: 0.4403779233:
4: 134432: loss: 0.4397539541:
4: 137632: loss: 0.4389259609:
4: 140832: loss: 0.4384094129:
4: 144032: loss: 0.4380420940:
4: 147232: loss: 0.4375269289:
4: 150432: loss: 0.4368666131:
4: 153632: loss: 0.4360739636:
4: 156832: loss: 0.4354246147:
4: 160032: loss: 0.4347791512:
4: 163232: loss: 0.4341479198:
4: 166432: loss: 0.4335591441:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4008821326:
5: 6432: loss: 0.4050012395:
5: 9632: loss: 0.4045754439:
5: 12832: loss: 0.4019764914:
5: 16032: loss: 0.4019102100:
5: 19232: loss: 0.4001309077:
5: 22432: loss: 0.4004462555:
5: 25632: loss: 0.3998069543:
5: 28832: loss: 0.3993936720:
5: 32032: loss: 0.3982922009:
5: 35232: loss: 0.3973008976:
5: 38432: loss: 0.3973520795:
5: 41632: loss: 0.3963136141:
5: 44832: loss: 0.3958038843:
5: 48032: loss: 0.3961067354:
5: 51232: loss: 0.3957867972:
5: 54432: loss: 0.3957672196:
5: 57632: loss: 0.3953736485:
5: 60832: loss: 0.3948121879:
5: 64032: loss: 0.3944971442:
5: 67232: loss: 0.3936662471:
5: 70432: loss: 0.3934120740:
5: 73632: loss: 0.3933198259:
5: 76832: loss: 0.3931707590:
5: 80032: loss: 0.3927099455:
5: 83232: loss: 0.3921504242:
5: 86432: loss: 0.3916761024:
5: 89632: loss: 0.3914402977:
5: 92832: loss: 0.3909145886:
5: 96032: loss: 0.3906513781:
5: 99232: loss: 0.3901687390:
5: 102432: loss: 0.3899117822:
5: 105632: loss: 0.3897101518:
5: 108832: loss: 0.3893161281:
5: 112032: loss: 0.3889562348:
5: 115232: loss: 0.3884032610:
5: 118432: loss: 0.3878997896:
5: 121632: loss: 0.3872442271:
5: 124832: loss: 0.3869525471:
5: 128032: loss: 0.3866387366:
5: 131232: loss: 0.3865116455:
5: 134432: loss: 0.3863191281:
5: 137632: loss: 0.3856557172:
5: 140832: loss: 0.3850498585:
5: 144032: loss: 0.3844960495:
5: 147232: loss: 0.3841205153:
5: 150432: loss: 0.3836032050:
5: 153632: loss: 0.3831904639:
5: 156832: loss: 0.3828511916:
5: 160032: loss: 0.3824433122:
5: 163232: loss: 0.3820596414:
5: 166432: loss: 0.3818178630:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.3611842607:
6: 6432: loss: 0.3611181985:
6: 9632: loss: 0.3653124517:
6: 12832: loss: 0.3665210989:
6: 16032: loss: 0.3670206808:
6: 19232: loss: 0.3653385315:
6: 22432: loss: 0.3646759547:
6: 25632: loss: 0.3618423820:
6: 28832: loss: 0.3612937571:
6: 32032: loss: 0.3595066428:
6: 35232: loss: 0.3594196373:
6: 38432: loss: 0.3592826335:
6: 41632: loss: 0.3584990600:
6: 44832: loss: 0.3586640380:
6: 48032: loss: 0.3584985905:
6: 51232: loss: 0.3576664119:
6: 54432: loss: 0.3570222855:
6: 57632: loss: 0.3568387121:
6: 60832: loss: 0.3559785701:
6: 64032: loss: 0.3559642749:
6: 67232: loss: 0.3551299432:
6: 70432: loss: 0.3547059689:
6: 73632: loss: 0.3542875996:
6: 76832: loss: 0.3538148626:
6: 80032: loss: 0.3537074869:
6: 83232: loss: 0.3534686371:
6: 86432: loss: 0.3531846869:
6: 89632: loss: 0.3526395930:
6: 92832: loss: 0.3525749917:
6: 96032: loss: 0.3522818634:
6: 99232: loss: 0.3519189597:
6: 102432: loss: 0.3511604461:
6: 105632: loss: 0.3506025120:
6: 108832: loss: 0.3506198583:
6: 112032: loss: 0.3504603915:
6: 115232: loss: 0.3496106231:
6: 118432: loss: 0.3492901983:
6: 121632: loss: 0.3489797524:
6: 124832: loss: 0.3484878150:
6: 128032: loss: 0.3480126552:
6: 131232: loss: 0.3475433175:
6: 134432: loss: 0.3470349277:
6: 137632: loss: 0.3469572293:
6: 140832: loss: 0.3467092887:
6: 144032: loss: 0.3462120101:
6: 147232: loss: 0.3460500781:
6: 150432: loss: 0.3454784882:
6: 153632: loss: 0.3453652857:
6: 156832: loss: 0.3452000594:
6: 160032: loss: 0.3448262728:
6: 163232: loss: 0.3445821931:
6: 166432: loss: 0.3441817107:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.3427736610:
7: 6432: loss: 0.3379549015:
7: 9632: loss: 0.3338615339:
7: 12832: loss: 0.3296037031:
7: 16032: loss: 0.3298675950:
7: 19232: loss: 0.3343391966:
7: 22432: loss: 0.3329057540:
7: 25632: loss: 0.3324999789:
7: 28832: loss: 0.3309193875:
7: 32032: loss: 0.3285589602:
7: 35232: loss: 0.3283845429:
7: 38432: loss: 0.3277853466:
7: 41632: loss: 0.3267106535:
7: 44832: loss: 0.3253861674:
7: 48032: loss: 0.3257893903:
7: 51232: loss: 0.3263882908:
7: 54432: loss: 0.3267839297:
7: 57632: loss: 0.3269429744:
7: 60832: loss: 0.3263711209:
7: 64032: loss: 0.3262692661:
7: 67232: loss: 0.3257780617:
7: 70432: loss: 0.3253413306:
7: 73632: loss: 0.3248022486:
7: 76832: loss: 0.3243335887:
7: 80032: loss: 0.3241300441:
7: 83232: loss: 0.3241068798:
7: 86432: loss: 0.3236386954:
7: 89632: loss: 0.3236872682:
7: 92832: loss: 0.3238989298:
7: 96032: loss: 0.3238222238:
7: 99232: loss: 0.3235410789:
7: 102432: loss: 0.3233852527:
7: 105632: loss: 0.3224296856:
7: 108832: loss: 0.3222488631:
7: 112032: loss: 0.3218185801:
7: 115232: loss: 0.3219857781:
7: 118432: loss: 0.3215474099:
7: 121632: loss: 0.3209669996:
7: 124832: loss: 0.3207115288:
7: 128032: loss: 0.3204382777:
7: 131232: loss: 0.3202426589:
7: 134432: loss: 0.3200875488:
7: 137632: loss: 0.3197892522:
7: 140832: loss: 0.3194806225:
7: 144032: loss: 0.3191946712:
7: 147232: loss: 0.3191504110:
7: 150432: loss: 0.3187149853:
7: 153632: loss: 0.3184008068:
7: 156832: loss: 0.3182885645:
7: 160032: loss: 0.3185399470:
7: 163232: loss: 0.3181838765:
7: 166432: loss: 0.3181672922:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3064984752:
8: 6432: loss: 0.3086067788:
8: 9632: loss: 0.3094005804:
8: 12832: loss: 0.3063627038:
8: 16032: loss: 0.3069139097:
8: 19232: loss: 0.3066068557:
8: 22432: loss: 0.3036129140:
8: 25632: loss: 0.3036659537:
8: 28832: loss: 0.3049741214:
8: 32032: loss: 0.3048726288:
8: 35232: loss: 0.3038217919:
8: 38432: loss: 0.3043855033:
8: 41632: loss: 0.3045312056:
8: 44832: loss: 0.3041729428:
8: 48032: loss: 0.3040294000:
8: 51232: loss: 0.3048506869:
8: 54432: loss: 0.3044015731:
8: 57632: loss: 0.3044724572:
8: 60832: loss: 0.3045399051:
8: 64032: loss: 0.3043267774:
8: 67232: loss: 0.3040202749:
8: 70432: loss: 0.3037416903:
8: 73632: loss: 0.3033734244:
8: 76832: loss: 0.3036951746:
8: 80032: loss: 0.3044613354:
8: 83232: loss: 0.3042138742:
8: 86432: loss: 0.3044754513:
8: 89632: loss: 0.3040157878:
8: 92832: loss: 0.3032949097:
8: 96032: loss: 0.3028294327:
8: 99232: loss: 0.3026062908:
8: 102432: loss: 0.3020812273:
8: 105632: loss: 0.3019609552:
8: 108832: loss: 0.3016820252:
8: 112032: loss: 0.3012537943:
8: 115232: loss: 0.3010593095:
8: 118432: loss: 0.3012725667:
8: 121632: loss: 0.3008020449:
8: 124832: loss: 0.3006218304:
8: 128032: loss: 0.3005071429:
8: 131232: loss: 0.3004138810:
8: 134432: loss: 0.3002621255:
8: 137632: loss: 0.3000257389:
8: 140832: loss: 0.2998404352:
8: 144032: loss: 0.2997944262:
8: 147232: loss: 0.2996335560:
8: 150432: loss: 0.2996147188:
8: 153632: loss: 0.2996317860:
8: 156832: loss: 0.2995521791:
8: 160032: loss: 0.2998669722:
8: 163232: loss: 0.2999573001:
8: 166432: loss: 0.3000412808:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.2801035039:
9: 6432: loss: 0.2847698764:
9: 9632: loss: 0.2872940315:
9: 12832: loss: 0.2930272645:
9: 16032: loss: 0.2899470701:
9: 19232: loss: 0.2883596615:
9: 22432: loss: 0.2885406491:
9: 25632: loss: 0.2876373960:
9: 28832: loss: 0.2872359295:
9: 32032: loss: 0.2875808939:
9: 35232: loss: 0.2878397866:
9: 38432: loss: 0.2883891152:
9: 41632: loss: 0.2893360952:
9: 44832: loss: 0.2899014127:
9: 48032: loss: 0.2897637125:
9: 51232: loss: 0.2901352512:
9: 54432: loss: 0.2901742313:
9: 57632: loss: 0.2913326762:
9: 60832: loss: 0.2911581017:
9: 64032: loss: 0.2912457778:
9: 67232: loss: 0.2911130709:
9: 70432: loss: 0.2906763243:
9: 73632: loss: 0.2907733988:
9: 76832: loss: 0.2896054214:
9: 80032: loss: 0.2894745195:
9: 83232: loss: 0.2888938469:
9: 86432: loss: 0.2888875036:
9: 89632: loss: 0.2889040595:
9: 92832: loss: 0.2889207834:
9: 96032: loss: 0.2886452503:
9: 99232: loss: 0.2882045909:
9: 102432: loss: 0.2880463164:
9: 105632: loss: 0.2880593490:
9: 108832: loss: 0.2879056048:
9: 112032: loss: 0.2877641826:
9: 115232: loss: 0.2876859567:
9: 118432: loss: 0.2875002768:
9: 121632: loss: 0.2880730919:
9: 124832: loss: 0.2878989399:
9: 128032: loss: 0.2881107594:
9: 131232: loss: 0.2880528152:
9: 134432: loss: 0.2875903034:
9: 137632: loss: 0.2875429536:
9: 140832: loss: 0.2875530061:
9: 144032: loss: 0.2873137888:
9: 147232: loss: 0.2872176480:
9: 150432: loss: 0.2870657708:
9: 153632: loss: 0.2866720721:
9: 156832: loss: 0.2866068649:
9: 160032: loss: 0.2865981842:
9: 163232: loss: 0.2865917956:
9: 166432: loss: 0.2863811982:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.2617894167:
10: 6432: loss: 0.2709736247:
10: 9632: loss: 0.2753865610:
10: 12832: loss: 0.2762602125:
10: 16032: loss: 0.2778608782:
10: 19232: loss: 0.2774624698:
10: 22432: loss: 0.2758517750:
10: 25632: loss: 0.2745855886:
10: 28832: loss: 0.2775462128:
10: 32032: loss: 0.2779138966:
10: 35232: loss: 0.2783498760:
10: 38432: loss: 0.2774378879:
10: 41632: loss: 0.2780741422:
10: 44832: loss: 0.2778610584:
10: 48032: loss: 0.2777368878:
10: 51232: loss: 0.2780521162:
10: 54432: loss: 0.2774082286:
10: 57632: loss: 0.2767480382:
10: 60832: loss: 0.2775374834:
10: 64032: loss: 0.2778246050:
10: 67232: loss: 0.2774657900:
10: 70432: loss: 0.2765726796:
10: 73632: loss: 0.2764474905:
10: 76832: loss: 0.2759957783:
10: 80032: loss: 0.2762409405:
10: 83232: loss: 0.2763327671:
10: 86432: loss: 0.2760327327:
10: 89632: loss: 0.2756450263:
10: 92832: loss: 0.2758334784:
10: 96032: loss: 0.2761876235:
10: 99232: loss: 0.2763618067:
10: 102432: loss: 0.2764356824:
10: 105632: loss: 0.2771495596:
10: 108832: loss: 0.2770671148:
10: 112032: loss: 0.2773853806:
10: 115232: loss: 0.2772470056:
10: 118432: loss: 0.2769397546:
10: 121632: loss: 0.2767230697:
10: 124832: loss: 0.2767532377:
10: 128032: loss: 0.2763922406:
10: 131232: loss: 0.2762094501:
10: 134432: loss: 0.2759531054:
10: 137632: loss: 0.2761234320:
10: 140832: loss: 0.2759228382:
10: 144032: loss: 0.2758903380:
10: 147232: loss: 0.2755038699:
10: 150432: loss: 0.2754388743:
10: 153632: loss: 0.2752002214:
10: 156832: loss: 0.2751808809:
10: 160032: loss: 0.2752914447:
10: 163232: loss: 0.2754221918:
10: 166432: loss: 0.2754970449:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.2678352639:
11: 6432: loss: 0.2676925294:
11: 9632: loss: 0.2732976530:
11: 12832: loss: 0.2727909932:
11: 16032: loss: 0.2718745456:
11: 19232: loss: 0.2681961085:
11: 22432: loss: 0.2692999696:
11: 25632: loss: 0.2692136611:
11: 28832: loss: 0.2674535606:
11: 32032: loss: 0.2667483249:
11: 35232: loss: 0.2682421889:
11: 38432: loss: 0.2675239536:
11: 41632: loss: 0.2677828825:
11: 44832: loss: 0.2685134806:
11: 48032: loss: 0.2676360884:
11: 51232: loss: 0.2677720256:
11: 54432: loss: 0.2668070341:
11: 57632: loss: 0.2669502513:
11: 60832: loss: 0.2675421318:
11: 64032: loss: 0.2680689205:
11: 67232: loss: 0.2681694272:
11: 70432: loss: 0.2684972559:
11: 73632: loss: 0.2680505124:
11: 76832: loss: 0.2685529988:
11: 80032: loss: 0.2682622458:
11: 83232: loss: 0.2684991922:
11: 86432: loss: 0.2688333328:
11: 89632: loss: 0.2690013785:
11: 92832: loss: 0.2687493327:
11: 96032: loss: 0.2683582405:
11: 99232: loss: 0.2689739201:
11: 102432: loss: 0.2691084582:
11: 105632: loss: 0.2692058469:
11: 108832: loss: 0.2693028164:
11: 112032: loss: 0.2690510586:
11: 115232: loss: 0.2685703152:
11: 118432: loss: 0.2686406381:
11: 121632: loss: 0.2679634431:
11: 124832: loss: 0.2683350150:
11: 128032: loss: 0.2681491899:
11: 131232: loss: 0.2680494650:
11: 134432: loss: 0.2682468168:
11: 137632: loss: 0.2678843844:
11: 140832: loss: 0.2676996100:
11: 144032: loss: 0.2674621907:
11: 147232: loss: 0.2672881880:
11: 150432: loss: 0.2672317660:
11: 153632: loss: 0.2667867930:
11: 156832: loss: 0.2665576608:
11: 160032: loss: 0.2667426408:
11: 163232: loss: 0.2664821131:
11: 166432: loss: 0.2662222322:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.2645723457:
12: 6432: loss: 0.2569098076:
12: 9632: loss: 0.2585812088:
12: 12832: loss: 0.2585330380:
12: 16032: loss: 0.2604210625:
12: 19232: loss: 0.2606862654:
12: 22432: loss: 0.2617301480:
12: 25632: loss: 0.2592273378:
12: 28832: loss: 0.2574909506:
12: 32032: loss: 0.2576706591:
12: 35232: loss: 0.2584704391:
12: 38432: loss: 0.2580774276:
12: 41632: loss: 0.2597137654:
12: 44832: loss: 0.2581015335:
12: 48032: loss: 0.2585371958:
12: 51232: loss: 0.2582564472:
12: 54432: loss: 0.2578584371:
12: 57632: loss: 0.2579508026:
12: 60832: loss: 0.2578888088:
12: 64032: loss: 0.2574303043:
12: 67232: loss: 0.2571517474:
12: 70432: loss: 0.2575113643:
12: 73632: loss: 0.2569860378:
12: 76832: loss: 0.2575616023:
12: 80032: loss: 0.2574854981:
12: 83232: loss: 0.2579137849:
12: 86432: loss: 0.2578542133:
12: 89632: loss: 0.2574040256:
12: 92832: loss: 0.2572490955:
12: 96032: loss: 0.2571231094:
12: 99232: loss: 0.2574926790:
12: 102432: loss: 0.2575260979:
12: 105632: loss: 0.2574936693:
12: 108832: loss: 0.2578224550:
12: 112032: loss: 0.2584763817:
12: 115232: loss: 0.2592226321:
12: 118432: loss: 0.2589859549:
12: 121632: loss: 0.2586374292:
12: 124832: loss: 0.2585961583:
12: 128032: loss: 0.2584128999:
12: 131232: loss: 0.2586697418:
12: 134432: loss: 0.2584335589:
12: 137632: loss: 0.2584030721:
12: 140832: loss: 0.2587882820:
12: 144032: loss: 0.2584262783:
12: 147232: loss: 0.2581437856:
12: 150432: loss: 0.2581514375:
12: 153632: loss: 0.2580744872:
12: 156832: loss: 0.2579459740:
12: 160032: loss: 0.2579365383:
12: 163232: loss: 0.2577569632:
12: 166432: loss: 0.2578239990:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2596737207:
13: 6432: loss: 0.2619295051:
13: 9632: loss: 0.2629307402:
13: 12832: loss: 0.2587271609:
13: 16032: loss: 0.2537972225:
13: 19232: loss: 0.2516951214:
13: 22432: loss: 0.2526081262:
13: 25632: loss: 0.2516906071:
13: 28832: loss: 0.2518854222:
13: 32032: loss: 0.2524103229:
13: 35232: loss: 0.2533922183:
13: 38432: loss: 0.2542645968:
13: 41632: loss: 0.2542179005:
13: 44832: loss: 0.2534350587:
13: 48032: loss: 0.2526780865:
13: 51232: loss: 0.2526212651:
13: 54432: loss: 0.2523969403:
13: 57632: loss: 0.2517835926:
13: 60832: loss: 0.2521980379:
13: 64032: loss: 0.2513124521:
13: 67232: loss: 0.2510945500:
13: 70432: loss: 0.2511106781:
13: 73632: loss: 0.2509132222:
13: 76832: loss: 0.2505726918:
13: 80032: loss: 0.2503950505:
13: 83232: loss: 0.2499446071:
13: 86432: loss: 0.2502481657:
13: 89632: loss: 0.2503083811:
13: 92832: loss: 0.2505105970:
13: 96032: loss: 0.2512776539:
13: 99232: loss: 0.2512986110:
13: 102432: loss: 0.2516514534:
13: 105632: loss: 0.2513192086:
13: 108832: loss: 0.2508692537:
13: 112032: loss: 0.2505118401:
13: 115232: loss: 0.2508628039:
13: 118432: loss: 0.2508329039:
13: 121632: loss: 0.2507351120:
13: 124832: loss: 0.2508261572:
13: 128032: loss: 0.2507722738:
13: 131232: loss: 0.2511598996:
13: 134432: loss: 0.2514111367:
13: 137632: loss: 0.2516454977:
13: 140832: loss: 0.2517055670:
13: 144032: loss: 0.2511798207:
13: 147232: loss: 0.2508983871:
13: 150432: loss: 0.2509048589:
13: 153632: loss: 0.2509847213:
13: 156832: loss: 0.2508465549:
13: 160032: loss: 0.2510277124:
13: 163232: loss: 0.2509851062:
13: 166432: loss: 0.2509699670:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.9094973207: precision: 1.0000000000: recall: 0.0044704490: f1: 0.0089011061
14: 3232: loss: 0.2401652895:
14: 6432: loss: 0.2447462367:
14: 9632: loss: 0.2507624433:
14: 12832: loss: 0.2456395140:
14: 16032: loss: 0.2450021133:
14: 19232: loss: 0.2453274437:
14: 22432: loss: 0.2454028114:
14: 25632: loss: 0.2471409292:
14: 28832: loss: 0.2473919894:
14: 32032: loss: 0.2473140381:
14: 35232: loss: 0.2477065229:
14: 38432: loss: 0.2473612639:
14: 41632: loss: 0.2471121511:
14: 44832: loss: 0.2476015612:
14: 48032: loss: 0.2481163732:
14: 51232: loss: 0.2474176078:
14: 54432: loss: 0.2473906468:
14: 57632: loss: 0.2469295489:
14: 60832: loss: 0.2464598042:
14: 64032: loss: 0.2455362754:
14: 67232: loss: 0.2448825974:
14: 70432: loss: 0.2451180982:
14: 73632: loss: 0.2456415298:
14: 76832: loss: 0.2457689317:
14: 80032: loss: 0.2451238040:
14: 83232: loss: 0.2455410656:
14: 86432: loss: 0.2457468488:
14: 89632: loss: 0.2461707327:
14: 92832: loss: 0.2467855547:
14: 96032: loss: 0.2462494793:
14: 99232: loss: 0.2459431547:
14: 102432: loss: 0.2460804892:
14: 105632: loss: 0.2457374847:
14: 108832: loss: 0.2453396956:
14: 112032: loss: 0.2453484716:
14: 115232: loss: 0.2456287160:
14: 118432: loss: 0.2457143029:
14: 121632: loss: 0.2457434427:
14: 124832: loss: 0.2460314523:
14: 128032: loss: 0.2459775283:
14: 131232: loss: 0.2457937023:
14: 134432: loss: 0.2457811694:
14: 137632: loss: 0.2458460913:
14: 140832: loss: 0.2455891847:
14: 144032: loss: 0.2453934351:
14: 147232: loss: 0.2456444467:
14: 150432: loss: 0.2456105352:
14: 153632: loss: 0.2456632127:
14: 156832: loss: 0.2455126838:
14: 160032: loss: 0.2452235235:
14: 163232: loss: 0.2449988119:
14: 166432: loss: 0.2450181867:
Dev-Acc: 14: Accuracy: 0.9418558478: precision: 0.6567164179: recall: 0.0074817208: f1: 0.0147948890
Train-Acc: 14: Accuracy: 0.9109556079: precision: 0.8095238095: recall: 0.0268226941: f1: 0.0519249125
15: 3232: loss: 0.2405529891:
15: 6432: loss: 0.2420241298:
15: 9632: loss: 0.2387944730:
15: 12832: loss: 0.2395692375:
15: 16032: loss: 0.2386925288:
15: 19232: loss: 0.2396493886:
15: 22432: loss: 0.2419920023:
15: 25632: loss: 0.2423551980:
15: 28832: loss: 0.2413228600:
15: 32032: loss: 0.2404620721:
15: 35232: loss: 0.2401527384:
15: 38432: loss: 0.2404237185:
15: 41632: loss: 0.2398480388:
15: 44832: loss: 0.2397302793:
15: 48032: loss: 0.2393677184:
15: 51232: loss: 0.2388014026:
15: 54432: loss: 0.2395463017:
15: 57632: loss: 0.2394428401:
15: 60832: loss: 0.2393484378:
15: 64032: loss: 0.2398970482:
15: 67232: loss: 0.2394397993:
15: 70432: loss: 0.2387312761:
15: 73632: loss: 0.2390691935:
15: 76832: loss: 0.2388379225:
15: 80032: loss: 0.2392955091:
15: 83232: loss: 0.2397835491:
15: 86432: loss: 0.2395958864:
15: 89632: loss: 0.2401144842:
15: 92832: loss: 0.2404913265:
15: 96032: loss: 0.2405193238:
15: 99232: loss: 0.2404132058:
15: 102432: loss: 0.2403276513:
15: 105632: loss: 0.2400051514:
15: 108832: loss: 0.2397700630:
15: 112032: loss: 0.2396475950:
15: 115232: loss: 0.2393164538:
15: 118432: loss: 0.2396311207:
15: 121632: loss: 0.2398176908:
15: 124832: loss: 0.2396592622:
15: 128032: loss: 0.2395275848:
15: 131232: loss: 0.2395131582:
15: 134432: loss: 0.2394838309:
15: 137632: loss: 0.2394786984:
15: 140832: loss: 0.2397596691:
15: 144032: loss: 0.2398077671:
15: 147232: loss: 0.2400289141:
15: 150432: loss: 0.2398707718:
15: 153632: loss: 0.2396275821:
15: 156832: loss: 0.2395184203:
15: 160032: loss: 0.2395489752:
15: 163232: loss: 0.2393002226:
15: 166432: loss: 0.2394128242:
Dev-Acc: 15: Accuracy: 0.9420840740: precision: 0.7244897959: recall: 0.0120727767: f1: 0.0237497909
Train-Acc: 15: Accuracy: 0.9114695787: precision: 0.8119122257: recall: 0.0340543028: f1: 0.0653669001
16: 3232: loss: 0.2439832942:
16: 6432: loss: 0.2380255203:
16: 9632: loss: 0.2399846190:
16: 12832: loss: 0.2407783911:
16: 16032: loss: 0.2396249901:
16: 19232: loss: 0.2403509659:
16: 22432: loss: 0.2391644002:
16: 25632: loss: 0.2386044606:
16: 28832: loss: 0.2381735441:
16: 32032: loss: 0.2397643483:
16: 35232: loss: 0.2397146917:
16: 38432: loss: 0.2383613102:
16: 41632: loss: 0.2393765446:
16: 44832: loss: 0.2398012840:
16: 48032: loss: 0.2378117776:
16: 51232: loss: 0.2372424568:
16: 54432: loss: 0.2367368089:
16: 57632: loss: 0.2360088493:
16: 60832: loss: 0.2354713436:
16: 64032: loss: 0.2361771140:
16: 67232: loss: 0.2358092482:
16: 70432: loss: 0.2355649612:
16: 73632: loss: 0.2351086042:
16: 76832: loss: 0.2347340670:
16: 80032: loss: 0.2344384299:
16: 83232: loss: 0.2346930943:
16: 86432: loss: 0.2349274567:
16: 89632: loss: 0.2349330485:
16: 92832: loss: 0.2349256479:
16: 96032: loss: 0.2347272226:
16: 99232: loss: 0.2349809170:
16: 102432: loss: 0.2348549171:
16: 105632: loss: 0.2342328523:
16: 108832: loss: 0.2340696529:
16: 112032: loss: 0.2338212397:
16: 115232: loss: 0.2338124014:
16: 118432: loss: 0.2340890427:
16: 121632: loss: 0.2339330563:
16: 124832: loss: 0.2339868171:
16: 128032: loss: 0.2340902044:
16: 131232: loss: 0.2338203990:
16: 134432: loss: 0.2337525639:
16: 137632: loss: 0.2338243414:
16: 140832: loss: 0.2339231344:
16: 144032: loss: 0.2341351411:
16: 147232: loss: 0.2341277038:
16: 150432: loss: 0.2340723245:
16: 153632: loss: 0.2340480309:
16: 156832: loss: 0.2340782188:
16: 160032: loss: 0.2345349392:
16: 163232: loss: 0.2347478699:
16: 166432: loss: 0.2347106238:
Dev-Acc: 16: Accuracy: 0.9424114823: precision: 0.6370106762: recall: 0.0304370005: f1: 0.0580980201
Train-Acc: 16: Accuracy: 0.9130653143: precision: 0.8474399164: recall: 0.0533166787: f1: 0.1003216230
17: 3232: loss: 0.2191870544:
17: 6432: loss: 0.2376218126:
17: 9632: loss: 0.2387371298:
17: 12832: loss: 0.2339951050:
17: 16032: loss: 0.2365784193:
17: 19232: loss: 0.2358725771:
17: 22432: loss: 0.2355050646:
17: 25632: loss: 0.2351662847:
17: 28832: loss: 0.2361340353:
17: 32032: loss: 0.2361682996:
17: 35232: loss: 0.2364448918:
17: 38432: loss: 0.2351892929:
17: 41632: loss: 0.2356938114:
17: 44832: loss: 0.2359654730:
17: 48032: loss: 0.2356467005:
17: 51232: loss: 0.2354437037:
17: 54432: loss: 0.2343779726:
17: 57632: loss: 0.2335314169:
17: 60832: loss: 0.2335675290:
17: 64032: loss: 0.2329628373:
17: 67232: loss: 0.2327338368:
17: 70432: loss: 0.2325779918:
17: 73632: loss: 0.2326781193:
17: 76832: loss: 0.2325234606:
17: 80032: loss: 0.2323669648:
17: 83232: loss: 0.2321992247:
17: 86432: loss: 0.2319286863:
17: 89632: loss: 0.2314389963:
17: 92832: loss: 0.2315489698:
17: 96032: loss: 0.2310140672:
17: 99232: loss: 0.2315254242:
17: 102432: loss: 0.2311775618:
17: 105632: loss: 0.2313196312:
17: 108832: loss: 0.2317486164:
17: 112032: loss: 0.2314702679:
17: 115232: loss: 0.2316773252:
17: 118432: loss: 0.2317437045:
17: 121632: loss: 0.2321175509:
17: 124832: loss: 0.2324369265:
17: 128032: loss: 0.2322898413:
17: 131232: loss: 0.2320970423:
17: 134432: loss: 0.2316399504:
17: 137632: loss: 0.2316539036:
17: 140832: loss: 0.2312789250:
17: 144032: loss: 0.2310060183:
17: 147232: loss: 0.2309584772:
17: 150432: loss: 0.2310108338:
17: 153632: loss: 0.2305982509:
17: 156832: loss: 0.2301741534:
17: 160032: loss: 0.2303659643:
17: 163232: loss: 0.2305308578:
17: 166432: loss: 0.2303457541:
Dev-Acc: 17: Accuracy: 0.9420642257: precision: 0.5483870968: recall: 0.0404693079: f1: 0.0753760887
Train-Acc: 17: Accuracy: 0.9140813351: precision: 0.8430566968: recall: 0.0674511866: f1: 0.1249086925
18: 3232: loss: 0.2258099861:
18: 6432: loss: 0.2306106512:
18: 9632: loss: 0.2281182733:
18: 12832: loss: 0.2282480170:
18: 16032: loss: 0.2276208784:
18: 19232: loss: 0.2286417049:
18: 22432: loss: 0.2285649451:
18: 25632: loss: 0.2296080617:
18: 28832: loss: 0.2296048142:
18: 32032: loss: 0.2286727941:
18: 35232: loss: 0.2285802223:
18: 38432: loss: 0.2288191209:
18: 41632: loss: 0.2281315272:
18: 44832: loss: 0.2287198056:
18: 48032: loss: 0.2278162319:
18: 51232: loss: 0.2270401977:
18: 54432: loss: 0.2273349840:
18: 57632: loss: 0.2282514289:
18: 60832: loss: 0.2272322306:
18: 64032: loss: 0.2266876218:
18: 67232: loss: 0.2267113471:
18: 70432: loss: 0.2267594176:
18: 73632: loss: 0.2261081148:
18: 76832: loss: 0.2259301474:
18: 80032: loss: 0.2256583835:
18: 83232: loss: 0.2258624599:
18: 86432: loss: 0.2253141860:
18: 89632: loss: 0.2257959667:
18: 92832: loss: 0.2265967727:
18: 96032: loss: 0.2266267577:
18: 99232: loss: 0.2269245316:
18: 102432: loss: 0.2270810654:
18: 105632: loss: 0.2270015640:
18: 108832: loss: 0.2266216349:
18: 112032: loss: 0.2267265803:
18: 115232: loss: 0.2264996351:
18: 118432: loss: 0.2264201716:
18: 121632: loss: 0.2261216084:
18: 124832: loss: 0.2261318430:
18: 128032: loss: 0.2258370553:
18: 131232: loss: 0.2259416109:
18: 134432: loss: 0.2261330378:
18: 137632: loss: 0.2257085836:
18: 140832: loss: 0.2258786693:
18: 144032: loss: 0.2257213794:
18: 147232: loss: 0.2255057702:
18: 150432: loss: 0.2255950608:
18: 153632: loss: 0.2259034080:
18: 156832: loss: 0.2261281274:
18: 160032: loss: 0.2261178533:
18: 163232: loss: 0.2262355205:
18: 166432: loss: 0.2263574427:
Dev-Acc: 18: Accuracy: 0.9430365562: precision: 0.5940860215: recall: 0.0751572862: f1: 0.1334339623
Train-Acc: 18: Accuracy: 0.9159938097: precision: 0.8419182948: recall: 0.0934849780: f1: 0.1682840237
19: 3232: loss: 0.2359708828:
19: 6432: loss: 0.2272717504:
19: 9632: loss: 0.2252728639:
19: 12832: loss: 0.2267961530:
19: 16032: loss: 0.2265969281:
19: 19232: loss: 0.2271206375:
19: 22432: loss: 0.2270360634:
19: 25632: loss: 0.2264066234:
19: 28832: loss: 0.2259351167:
19: 32032: loss: 0.2249052984:
19: 35232: loss: 0.2242357512:
19: 38432: loss: 0.2242570504:
19: 41632: loss: 0.2234991673:
19: 44832: loss: 0.2243124122:
19: 48032: loss: 0.2239515888:
19: 51232: loss: 0.2234029831:
19: 54432: loss: 0.2239269006:
19: 57632: loss: 0.2235130134:
19: 60832: loss: 0.2230250412:
19: 64032: loss: 0.2224247093:
19: 67232: loss: 0.2227020887:
19: 70432: loss: 0.2226830764:
19: 73632: loss: 0.2223436412:
19: 76832: loss: 0.2228622404:
19: 80032: loss: 0.2233733918:
19: 83232: loss: 0.2234792446:
19: 86432: loss: 0.2239748837:
19: 89632: loss: 0.2237028652:
19: 92832: loss: 0.2231415802:
19: 96032: loss: 0.2233116687:
19: 99232: loss: 0.2229106116:
19: 102432: loss: 0.2230213567:
19: 105632: loss: 0.2228178250:
19: 108832: loss: 0.2232028127:
19: 112032: loss: 0.2230438022:
19: 115232: loss: 0.2230983421:
19: 118432: loss: 0.2229379311:
19: 121632: loss: 0.2231567018:
19: 124832: loss: 0.2232471066:
19: 128032: loss: 0.2235390797:
19: 131232: loss: 0.2235751059:
19: 134432: loss: 0.2234001775:
19: 137632: loss: 0.2237484055:
19: 140832: loss: 0.2235588138:
19: 144032: loss: 0.2235525623:
19: 147232: loss: 0.2232708574:
19: 150432: loss: 0.2231619257:
19: 153632: loss: 0.2232259453:
19: 156832: loss: 0.2232671320:
19: 160032: loss: 0.2230071651:
19: 163232: loss: 0.2229916772:
19: 166432: loss: 0.2228496757:
Dev-Acc: 19: Accuracy: 0.9433243275: precision: 0.5780240074: recall: 0.1064444822: f1: 0.1797817346
Train-Acc: 19: Accuracy: 0.9180377722: precision: 0.8354997759: recall: 0.1225428966: f1: 0.2137369568
20: 3232: loss: 0.2275374868:
20: 6432: loss: 0.2294340112:
20: 9632: loss: 0.2267297416:
20: 12832: loss: 0.2260001178:
20: 16032: loss: 0.2230294378:
20: 19232: loss: 0.2194040106:
20: 22432: loss: 0.2171917862:
20: 25632: loss: 0.2187312088:
20: 28832: loss: 0.2201694972:
20: 32032: loss: 0.2197916856:
20: 35232: loss: 0.2199367821:
20: 38432: loss: 0.2196607141:
20: 41632: loss: 0.2195713293:
20: 44832: loss: 0.2199134015:
20: 48032: loss: 0.2202058289:
20: 51232: loss: 0.2197279308:
20: 54432: loss: 0.2199380207:
20: 57632: loss: 0.2202703481:
20: 60832: loss: 0.2206214844:
20: 64032: loss: 0.2200395735:
20: 67232: loss: 0.2194654944:
20: 70432: loss: 0.2195779532:
20: 73632: loss: 0.2194722871:
20: 76832: loss: 0.2198148207:
20: 80032: loss: 0.2196341636:
20: 83232: loss: 0.2194322577:
20: 86432: loss: 0.2189354634:
20: 89632: loss: 0.2188087385:
20: 92832: loss: 0.2187726338:
20: 96032: loss: 0.2188036366:
20: 99232: loss: 0.2189957719:
20: 102432: loss: 0.2190143459:
20: 105632: loss: 0.2189548026:
20: 108832: loss: 0.2192633989:
20: 112032: loss: 0.2194550916:
20: 115232: loss: 0.2193906112:
20: 118432: loss: 0.2195881616:
20: 121632: loss: 0.2196091309:
20: 124832: loss: 0.2196460001:
20: 128032: loss: 0.2195015018:
20: 131232: loss: 0.2193059820:
20: 134432: loss: 0.2191862478:
20: 137632: loss: 0.2190677654:
20: 140832: loss: 0.2191984392:
20: 144032: loss: 0.2194483926:
20: 147232: loss: 0.2194915103:
20: 150432: loss: 0.2199071323:
20: 153632: loss: 0.2195499076:
20: 156832: loss: 0.2198533285:
20: 160032: loss: 0.2194473807:
20: 163232: loss: 0.2193975151:
20: 166432: loss: 0.2197331893:
Dev-Acc: 20: Accuracy: 0.9434037209: precision: 0.5666917860: recall: 0.1278694100: f1: 0.2086570477
Train-Acc: 20: Accuracy: 0.9204881787: precision: 0.8192166053: recall: 0.1608704227: f1: 0.2689306517
