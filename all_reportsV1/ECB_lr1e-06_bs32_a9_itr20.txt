1: 3232: loss: 0.7026987231:
1: 6432: loss: 0.6935261956:
1: 9632: loss: 0.6846001079:
1: 12832: loss: 0.6755978528:
1: 16032: loss: 0.6675465329:
1: 19232: loss: 0.6595849530:
1: 22432: loss: 0.6512692812:
1: 25632: loss: 0.6431162832:
1: 28832: loss: 0.6355345607:
1: 32032: loss: 0.6278062194:
1: 35232: loss: 0.6199751998:
1: 38432: loss: 0.6122839498:
1: 41632: loss: 0.6047787972:
1: 44832: loss: 0.5975931887:
1: 48032: loss: 0.5901457568:
1: 51232: loss: 0.5828796433:
1: 54432: loss: 0.5754340971:
1: 57632: loss: 0.5684377816:
1: 60832: loss: 0.5613009478:
1: 64032: loss: 0.5541520987:
1: 67232: loss: 0.5477258897:
1: 70432: loss: 0.5412988318:
1: 73632: loss: 0.5353812424:
1: 76832: loss: 0.5289908084:
1: 80032: loss: 0.5236915012:
1: 83232: loss: 0.5183397282:
1: 86432: loss: 0.5126908966:
1: 89632: loss: 0.5073225532:
1: 92832: loss: 0.5025477925:
1: 96032: loss: 0.4971918555:
1: 99232: loss: 0.4922396768:
1: 102432: loss: 0.4878034593:
1: 105632: loss: 0.4828151686:
1: 108832: loss: 0.4786425460:
1: 112032: loss: 0.4744788063:
1: 115232: loss: 0.4700865412:
1: 118432: loss: 0.4660368533:
1: 121632: loss: 0.4622521010:
1: 124832: loss: 0.4584548495:
1: 128032: loss: 0.4547037775:
1: 131232: loss: 0.4511491087:
1: 134432: loss: 0.4481438743:
1: 137632: loss: 0.4447461647:
1: 140832: loss: 0.4411708479:
1: 144032: loss: 0.4379432475:
1: 147232: loss: 0.4346600823:
1: 150432: loss: 0.4316024751:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 3232: loss: 0.2935267926:
2: 6432: loss: 0.2913772039:
2: 9632: loss: 0.2893907453:
2: 12832: loss: 0.2866156455:
2: 16032: loss: 0.2860950642:
2: 19232: loss: 0.2844119084:
2: 22432: loss: 0.2815898949:
2: 25632: loss: 0.2790262262:
2: 28832: loss: 0.2806147984:
2: 32032: loss: 0.2793000674:
2: 35232: loss: 0.2798182323:
2: 38432: loss: 0.2801174259:
2: 41632: loss: 0.2799345498:
2: 44832: loss: 0.2790590226:
2: 48032: loss: 0.2787629057:
2: 51232: loss: 0.2785037949:
2: 54432: loss: 0.2774005436:
2: 57632: loss: 0.2756379977:
2: 60832: loss: 0.2744957358:
2: 64032: loss: 0.2735845484:
2: 67232: loss: 0.2727274754:
2: 70432: loss: 0.2725354170:
2: 73632: loss: 0.2723669355:
2: 76832: loss: 0.2716805850:
2: 80032: loss: 0.2703385722:
2: 83232: loss: 0.2696553494:
2: 86432: loss: 0.2689151157:
2: 89632: loss: 0.2681719206:
2: 92832: loss: 0.2675657143:
2: 96032: loss: 0.2670958661:
2: 99232: loss: 0.2661692362:
2: 102432: loss: 0.2658025202:
2: 105632: loss: 0.2648938432:
2: 108832: loss: 0.2637858565:
2: 112032: loss: 0.2629150427:
2: 115232: loss: 0.2626040261:
2: 118432: loss: 0.2620050077:
2: 121632: loss: 0.2613567174:
2: 124832: loss: 0.2601346420:
2: 128032: loss: 0.2596221580:
2: 131232: loss: 0.2588356950:
2: 134432: loss: 0.2580184086:
2: 137632: loss: 0.2573827836:
2: 140832: loss: 0.2566313005:
2: 144032: loss: 0.2560939141:
2: 147232: loss: 0.2555820919:
2: 150432: loss: 0.2551876381:
Dev-Acc: 2: Accuracy: 0.9435227513: precision: 0.5554903112: recall: 0.1608569971: f1: 0.2494725738
Train-Acc: 2: Accuracy: 0.9159687161: precision: 0.8161936996: recall: 0.2061008481: f1: 0.3290993072
3: 3232: loss: 0.2327719428:
3: 6432: loss: 0.2349595064:
3: 9632: loss: 0.2321896719:
3: 12832: loss: 0.2276161734:
3: 16032: loss: 0.2280718718:
3: 19232: loss: 0.2290073475:
3: 22432: loss: 0.2281025683:
3: 25632: loss: 0.2273138800:
3: 28832: loss: 0.2270084693:
3: 32032: loss: 0.2275952313:
3: 35232: loss: 0.2277560640:
3: 38432: loss: 0.2258615897:
3: 41632: loss: 0.2250202114:
3: 44832: loss: 0.2251651078:
3: 48032: loss: 0.2244229229:
3: 51232: loss: 0.2247733580:
3: 54432: loss: 0.2244335850:
3: 57632: loss: 0.2242246337:
3: 60832: loss: 0.2229013951:
3: 64032: loss: 0.2228571163:
3: 67232: loss: 0.2226020256:
3: 70432: loss: 0.2215255909:
3: 73632: loss: 0.2217575534:
3: 76832: loss: 0.2215502811:
3: 80032: loss: 0.2216354844:
3: 83232: loss: 0.2213863929:
3: 86432: loss: 0.2209901378:
3: 89632: loss: 0.2207630600:
3: 92832: loss: 0.2202115046:
3: 96032: loss: 0.2196879184:
3: 99232: loss: 0.2191119872:
3: 102432: loss: 0.2190847635:
3: 105632: loss: 0.2188840182:
3: 108832: loss: 0.2184931680:
3: 112032: loss: 0.2185208664:
3: 115232: loss: 0.2181860593:
3: 118432: loss: 0.2182318699:
3: 121632: loss: 0.2182968359:
3: 124832: loss: 0.2179555943:
3: 128032: loss: 0.2170662278:
3: 131232: loss: 0.2166392666:
3: 134432: loss: 0.2166718939:
3: 137632: loss: 0.2165590682:
3: 140832: loss: 0.2163721141:
3: 144032: loss: 0.2163334430:
3: 147232: loss: 0.2159354839:
3: 150432: loss: 0.2156223766:
Dev-Acc: 3: Accuracy: 0.9373908639: precision: 0.4517435321: recall: 0.3414385309: f1: 0.3889211699
Train-Acc: 3: Accuracy: 0.9260995388: precision: 0.7844654629: recall: 0.3598711459: f1: 0.4933976295
4: 3232: loss: 0.2017948691:
4: 6432: loss: 0.2021738600:
4: 9632: loss: 0.2020450440:
4: 12832: loss: 0.2044142826:
4: 16032: loss: 0.2049211346:
4: 19232: loss: 0.2053232208:
4: 22432: loss: 0.2054055292:
4: 25632: loss: 0.2052903664:
4: 28832: loss: 0.2069038774:
4: 32032: loss: 0.2041519075:
4: 35232: loss: 0.2041439099:
4: 38432: loss: 0.2026806480:
4: 41632: loss: 0.2024281155:
4: 44832: loss: 0.2016883897:
4: 48032: loss: 0.2012636463:
4: 51232: loss: 0.2006225805:
4: 54432: loss: 0.2007584204:
4: 57632: loss: 0.2005068985:
4: 60832: loss: 0.2002666777:
4: 64032: loss: 0.2006256359:
4: 67232: loss: 0.2013054224:
4: 70432: loss: 0.2013736617:
4: 73632: loss: 0.2011269184:
4: 76832: loss: 0.2002324407:
4: 80032: loss: 0.1997566647:
4: 83232: loss: 0.1992355317:
4: 86432: loss: 0.1987325218:
4: 89632: loss: 0.1987323675:
4: 92832: loss: 0.1984336086:
4: 96032: loss: 0.1987563379:
4: 99232: loss: 0.1984499220:
4: 102432: loss: 0.1984708230:
4: 105632: loss: 0.1983326990:
4: 108832: loss: 0.1982509687:
4: 112032: loss: 0.1980335055:
4: 115232: loss: 0.1981261575:
4: 118432: loss: 0.1976219644:
4: 121632: loss: 0.1975235928:
4: 124832: loss: 0.1971527471:
4: 128032: loss: 0.1970724583:
4: 131232: loss: 0.1974522143:
4: 134432: loss: 0.1972602264:
4: 137632: loss: 0.1969628121:
4: 140832: loss: 0.1969925940:
4: 144032: loss: 0.1970307859:
4: 147232: loss: 0.1966003787:
4: 150432: loss: 0.1967953704:
Dev-Acc: 4: Accuracy: 0.9342455268: precision: 0.4276290260: recall: 0.3747661962: f1: 0.3994562755
Train-Acc: 4: Accuracy: 0.9297679067: precision: 0.7815220095: recall: 0.4131878246: f1: 0.5405754096
5: 3232: loss: 0.1930601639:
5: 6432: loss: 0.1950612682:
5: 9632: loss: 0.1903118365:
5: 12832: loss: 0.1906106722:
5: 16032: loss: 0.1894935658:
5: 19232: loss: 0.1875754017:
5: 22432: loss: 0.1873028790:
5: 25632: loss: 0.1866692683:
5: 28832: loss: 0.1879381942:
5: 32032: loss: 0.1874911256:
5: 35232: loss: 0.1900303637:
5: 38432: loss: 0.1891455509:
5: 41632: loss: 0.1892209679:
5: 44832: loss: 0.1886811709:
5: 48032: loss: 0.1897745204:
5: 51232: loss: 0.1895887263:
5: 54432: loss: 0.1901073462:
5: 57632: loss: 0.1896376871:
5: 60832: loss: 0.1900340888:
5: 64032: loss: 0.1898206174:
5: 67232: loss: 0.1898279960:
5: 70432: loss: 0.1890225338:
5: 73632: loss: 0.1883488946:
5: 76832: loss: 0.1880802201:
5: 80032: loss: 0.1882566469:
5: 83232: loss: 0.1882137676:
5: 86432: loss: 0.1880756048:
5: 89632: loss: 0.1883020727:
5: 92832: loss: 0.1882677493:
5: 96032: loss: 0.1885085748:
5: 99232: loss: 0.1884464293:
5: 102432: loss: 0.1889090084:
5: 105632: loss: 0.1888680655:
5: 108832: loss: 0.1885491044:
5: 112032: loss: 0.1880214342:
5: 115232: loss: 0.1879126541:
5: 118432: loss: 0.1878022910:
5: 121632: loss: 0.1874424934:
5: 124832: loss: 0.1871000897:
5: 128032: loss: 0.1870060188:
5: 131232: loss: 0.1870602201:
5: 134432: loss: 0.1869251854:
5: 137632: loss: 0.1867750767:
5: 140832: loss: 0.1866722508:
5: 144032: loss: 0.1866607985:
5: 147232: loss: 0.1865132016:
5: 150432: loss: 0.1868201551:
Dev-Acc: 5: Accuracy: 0.9335708022: precision: 0.4247411243: recall: 0.3905798334: f1: 0.4069448135
Train-Acc: 5: Accuracy: 0.9320360422: precision: 0.7829520381: recall: 0.4432318717: f1: 0.5660313995
6: 3232: loss: 0.1878471692:
6: 6432: loss: 0.1860470872:
6: 9632: loss: 0.1855826089:
6: 12832: loss: 0.1846180969:
6: 16032: loss: 0.1830954320:
6: 19232: loss: 0.1840056022:
6: 22432: loss: 0.1856526031:
6: 25632: loss: 0.1846445566:
6: 28832: loss: 0.1832582034:
6: 32032: loss: 0.1831125735:
6: 35232: loss: 0.1832240958:
6: 38432: loss: 0.1835126319:
6: 41632: loss: 0.1817786571:
6: 44832: loss: 0.1808071146:
6: 48032: loss: 0.1816239622:
6: 51232: loss: 0.1814569606:
6: 54432: loss: 0.1821083138:
6: 57632: loss: 0.1823164960:
6: 60832: loss: 0.1813923214:
6: 64032: loss: 0.1811364830:
6: 67232: loss: 0.1804302501:
6: 70432: loss: 0.1799059834:
6: 73632: loss: 0.1798734164:
6: 76832: loss: 0.1800325325:
6: 80032: loss: 0.1797748140:
6: 83232: loss: 0.1796976326:
6: 86432: loss: 0.1798852413:
6: 89632: loss: 0.1797435475:
6: 92832: loss: 0.1796626780:
6: 96032: loss: 0.1799501391:
6: 99232: loss: 0.1798070224:
6: 102432: loss: 0.1793702793:
6: 105632: loss: 0.1793482789:
6: 108832: loss: 0.1793659554:
6: 112032: loss: 0.1792069881:
6: 115232: loss: 0.1792655808:
6: 118432: loss: 0.1793710728:
6: 121632: loss: 0.1793953402:
6: 124832: loss: 0.1792965661:
6: 128032: loss: 0.1790660017:
6: 131232: loss: 0.1792025387:
6: 134432: loss: 0.1788540595:
6: 137632: loss: 0.1785640755:
6: 140832: loss: 0.1786489807:
6: 144032: loss: 0.1788947040:
6: 147232: loss: 0.1790309083:
6: 150432: loss: 0.1788740294:
Dev-Acc: 6: Accuracy: 0.9325587153: precision: 0.4190526688: recall: 0.4031627274: f1: 0.4109541555
Train-Acc: 6: Accuracy: 0.9340148568: precision: 0.7841608084: recall: 0.4693314049: f1: 0.5872095414
7: 3232: loss: 0.1593329846:
7: 6432: loss: 0.1757063727:
7: 9632: loss: 0.1761004147:
7: 12832: loss: 0.1744077971:
7: 16032: loss: 0.1750691457:
7: 19232: loss: 0.1740045558:
7: 22432: loss: 0.1733789956:
7: 25632: loss: 0.1727933478:
7: 28832: loss: 0.1728389839:
7: 32032: loss: 0.1746274017:
7: 35232: loss: 0.1748868104:
7: 38432: loss: 0.1743094330:
7: 41632: loss: 0.1745975931:
7: 44832: loss: 0.1737447109:
7: 48032: loss: 0.1742831689:
7: 51232: loss: 0.1747543893:
7: 54432: loss: 0.1745284943:
7: 57632: loss: 0.1741306696:
7: 60832: loss: 0.1740893445:
7: 64032: loss: 0.1738239568:
7: 67232: loss: 0.1733923349:
7: 70432: loss: 0.1737578719:
7: 73632: loss: 0.1741541889:
7: 76832: loss: 0.1737605863:
7: 80032: loss: 0.1742894467:
7: 83232: loss: 0.1742589984:
7: 86432: loss: 0.1739854425:
7: 89632: loss: 0.1736213597:
7: 92832: loss: 0.1739583227:
7: 96032: loss: 0.1738308941:
7: 99232: loss: 0.1735475052:
7: 102432: loss: 0.1733356366:
7: 105632: loss: 0.1732601627:
7: 108832: loss: 0.1732738942:
7: 112032: loss: 0.1734799065:
7: 115232: loss: 0.1733199515:
7: 118432: loss: 0.1730162411:
7: 121632: loss: 0.1728942839:
7: 124832: loss: 0.1730960970:
7: 128032: loss: 0.1729121797:
7: 131232: loss: 0.1727839637:
7: 134432: loss: 0.1726491984:
7: 137632: loss: 0.1725548440:
7: 140832: loss: 0.1725135739:
7: 144032: loss: 0.1723280520:
7: 147232: loss: 0.1726791929:
7: 150432: loss: 0.1727253251:
Dev-Acc: 7: Accuracy: 0.9316161275: precision: 0.4133972931: recall: 0.4103043700: f1: 0.4118450247
Train-Acc: 7: Accuracy: 0.9357701540: precision: 0.7889537971: recall: 0.4883308132: f1: 0.6032648420
8: 3232: loss: 0.1708847837:
8: 6432: loss: 0.1739362615:
8: 9632: loss: 0.1703583645:
8: 12832: loss: 0.1695066046:
8: 16032: loss: 0.1722037307:
8: 19232: loss: 0.1707686657:
8: 22432: loss: 0.1713855955:
8: 25632: loss: 0.1715852807:
8: 28832: loss: 0.1713634522:
8: 32032: loss: 0.1713772564:
8: 35232: loss: 0.1717166170:
8: 38432: loss: 0.1715415475:
8: 41632: loss: 0.1713449779:
8: 44832: loss: 0.1714580903:
8: 48032: loss: 0.1709626737:
8: 51232: loss: 0.1702070199:
8: 54432: loss: 0.1701031028:
8: 57632: loss: 0.1697629880:
8: 60832: loss: 0.1700889699:
8: 64032: loss: 0.1697394601:
8: 67232: loss: 0.1695429637:
8: 70432: loss: 0.1696929217:
8: 73632: loss: 0.1694445162:
8: 76832: loss: 0.1696571886:
8: 80032: loss: 0.1693146445:
8: 83232: loss: 0.1690834747:
8: 86432: loss: 0.1688063038:
8: 89632: loss: 0.1684124832:
8: 92832: loss: 0.1686379121:
8: 96032: loss: 0.1684221944:
8: 99232: loss: 0.1678654522:
8: 102432: loss: 0.1674379812:
8: 105632: loss: 0.1679315357:
8: 108832: loss: 0.1682700918:
8: 112032: loss: 0.1681871406:
8: 115232: loss: 0.1683015489:
8: 118432: loss: 0.1680938234:
8: 121632: loss: 0.1685365079:
8: 124832: loss: 0.1682227263:
8: 128032: loss: 0.1678969665:
8: 131232: loss: 0.1678128499:
8: 134432: loss: 0.1676459307:
8: 137632: loss: 0.1673438060:
8: 140832: loss: 0.1675121642:
8: 144032: loss: 0.1676558646:
8: 147232: loss: 0.1678534403:
8: 150432: loss: 0.1679161341:
Dev-Acc: 8: Accuracy: 0.9301575422: precision: 0.4058230319: recall: 0.4242475769: f1: 0.4148308255
Train-Acc: 8: Accuracy: 0.9373151064: precision: 0.7910172272: recall: 0.5071329959: f1: 0.6180346913
9: 3232: loss: 0.1525712200:
9: 6432: loss: 0.1591386684:
9: 9632: loss: 0.1630608140:
9: 12832: loss: 0.1644668930:
9: 16032: loss: 0.1646912140:
9: 19232: loss: 0.1632198699:
9: 22432: loss: 0.1644361344:
9: 25632: loss: 0.1639129332:
9: 28832: loss: 0.1633571572:
9: 32032: loss: 0.1640724188:
9: 35232: loss: 0.1623635572:
9: 38432: loss: 0.1626887126:
9: 41632: loss: 0.1621960513:
9: 44832: loss: 0.1624569463:
9: 48032: loss: 0.1628809554:
9: 51232: loss: 0.1632935780:
9: 54432: loss: 0.1635183534:
9: 57632: loss: 0.1633105423:
9: 60832: loss: 0.1631526304:
9: 64032: loss: 0.1636733352:
9: 67232: loss: 0.1643497210:
9: 70432: loss: 0.1643825004:
9: 73632: loss: 0.1642837709:
9: 76832: loss: 0.1645737446:
9: 80032: loss: 0.1643537891:
9: 83232: loss: 0.1642405070:
9: 86432: loss: 0.1641490130:
9: 89632: loss: 0.1637967433:
9: 92832: loss: 0.1635747582:
9: 96032: loss: 0.1636668902:
9: 99232: loss: 0.1642290634:
9: 102432: loss: 0.1641737208:
9: 105632: loss: 0.1642966855:
9: 108832: loss: 0.1643955315:
9: 112032: loss: 0.1641963127:
9: 115232: loss: 0.1644085241:
9: 118432: loss: 0.1644249387:
9: 121632: loss: 0.1640634742:
9: 124832: loss: 0.1641714881:
9: 128032: loss: 0.1639177730:
9: 131232: loss: 0.1638259392:
9: 134432: loss: 0.1638897230:
9: 137632: loss: 0.1637891031:
9: 140832: loss: 0.1637624223:
9: 144032: loss: 0.1635476853:
9: 147232: loss: 0.1637918522:
9: 150432: loss: 0.1634910220:
Dev-Acc: 9: Accuracy: 0.9288874865: precision: 0.3979365079: recall: 0.4262880463: f1: 0.4116246614
Train-Acc: 9: Accuracy: 0.9393728375: precision: 0.8034863687: recall: 0.5212017619: f1: 0.6322673259
10: 3232: loss: 0.1591104187:
10: 6432: loss: 0.1628466403:
10: 9632: loss: 0.1580936386:
10: 12832: loss: 0.1579832557:
10: 16032: loss: 0.1561590022:
10: 19232: loss: 0.1597798736:
10: 22432: loss: 0.1615844938:
10: 25632: loss: 0.1630468105:
10: 28832: loss: 0.1627624076:
10: 32032: loss: 0.1636522589:
10: 35232: loss: 0.1626002817:
10: 38432: loss: 0.1614615056:
10: 41632: loss: 0.1604139192:
10: 44832: loss: 0.1607567019:
10: 48032: loss: 0.1599081708:
10: 51232: loss: 0.1605014053:
10: 54432: loss: 0.1612222418:
10: 57632: loss: 0.1608095089:
10: 60832: loss: 0.1606786875:
10: 64032: loss: 0.1607305619:
10: 67232: loss: 0.1613311981:
10: 70432: loss: 0.1609730014:
10: 73632: loss: 0.1605576198:
10: 76832: loss: 0.1602428468:
10: 80032: loss: 0.1599052758:
10: 83232: loss: 0.1604002087:
10: 86432: loss: 0.1607883657:
10: 89632: loss: 0.1602849235:
10: 92832: loss: 0.1608406156:
10: 96032: loss: 0.1608587702:
10: 99232: loss: 0.1603878564:
10: 102432: loss: 0.1602526398:
10: 105632: loss: 0.1604854166:
10: 108832: loss: 0.1604217556:
10: 112032: loss: 0.1603174440:
10: 115232: loss: 0.1603605544:
10: 118432: loss: 0.1603207621:
10: 121632: loss: 0.1600783886:
10: 124832: loss: 0.1599920980:
10: 128032: loss: 0.1600254026:
10: 131232: loss: 0.1597732492:
10: 134432: loss: 0.1597681288:
10: 137632: loss: 0.1596972928:
10: 140832: loss: 0.1593604503:
10: 144032: loss: 0.1597001237:
10: 147232: loss: 0.1596284357:
10: 150432: loss: 0.1596610446:
Dev-Acc: 10: Accuracy: 0.9269030690: precision: 0.3866341166: recall: 0.4308791022: f1: 0.4075593084
Train-Acc: 10: Accuracy: 0.9412793517: precision: 0.8099516240: recall: 0.5393465255: f1: 0.6475138122
11: 3232: loss: 0.1547348724:
11: 6432: loss: 0.1577810731:
11: 9632: loss: 0.1568926978:
11: 12832: loss: 0.1555245855:
11: 16032: loss: 0.1590204887:
11: 19232: loss: 0.1578541027:
11: 22432: loss: 0.1559149117:
11: 25632: loss: 0.1547317695:
11: 28832: loss: 0.1548269561:
11: 32032: loss: 0.1547889495:
11: 35232: loss: 0.1549969240:
11: 38432: loss: 0.1546647695:
11: 41632: loss: 0.1542406381:
11: 44832: loss: 0.1534095373:
11: 48032: loss: 0.1541917945:
11: 51232: loss: 0.1547515829:
11: 54432: loss: 0.1544466192:
11: 57632: loss: 0.1545110799:
11: 60832: loss: 0.1552489696:
11: 64032: loss: 0.1546217025:
11: 67232: loss: 0.1544633023:
11: 70432: loss: 0.1549668357:
11: 73632: loss: 0.1558053688:
11: 76832: loss: 0.1558529276:
11: 80032: loss: 0.1557351900:
11: 83232: loss: 0.1559651292:
11: 86432: loss: 0.1563356942:
11: 89632: loss: 0.1557058215:
11: 92832: loss: 0.1552933121:
11: 96032: loss: 0.1552377076:
11: 99232: loss: 0.1552503532:
11: 102432: loss: 0.1553747541:
11: 105632: loss: 0.1553266182:
11: 108832: loss: 0.1555710055:
11: 112032: loss: 0.1556770671:
11: 115232: loss: 0.1556680037:
11: 118432: loss: 0.1558291991:
11: 121632: loss: 0.1555730533:
11: 124832: loss: 0.1555553413:
11: 128032: loss: 0.1558882621:
11: 131232: loss: 0.1558649712:
11: 134432: loss: 0.1560225202:
11: 137632: loss: 0.1558809048:
11: 140832: loss: 0.1557736971:
11: 144032: loss: 0.1557776058:
11: 147232: loss: 0.1560556259:
11: 150432: loss: 0.1559538834:
Dev-Acc: 11: Accuracy: 0.9252559543: precision: 0.3780991736: recall: 0.4356401972: f1: 0.4048352690
Train-Acc: 11: Accuracy: 0.9427914023: precision: 0.8121822542: recall: 0.5566366445: f1: 0.6605554689
12: 3232: loss: 0.1439886091:
12: 6432: loss: 0.1561549038:
12: 9632: loss: 0.1525909710:
12: 12832: loss: 0.1526383681:
12: 16032: loss: 0.1512506792:
12: 19232: loss: 0.1511513186:
12: 22432: loss: 0.1518399927:
12: 25632: loss: 0.1511021820:
12: 28832: loss: 0.1517478758:
12: 32032: loss: 0.1525121483:
12: 35232: loss: 0.1534755093:
12: 38432: loss: 0.1522601054:
12: 41632: loss: 0.1523262340:
12: 44832: loss: 0.1523740299:
12: 48032: loss: 0.1517803515:
12: 51232: loss: 0.1515551339:
12: 54432: loss: 0.1513919159:
12: 57632: loss: 0.1517545269:
12: 60832: loss: 0.1518758156:
12: 64032: loss: 0.1517566599:
12: 67232: loss: 0.1515231370:
12: 70432: loss: 0.1506821358:
12: 73632: loss: 0.1511380726:
12: 76832: loss: 0.1519336301:
12: 80032: loss: 0.1523657909:
12: 83232: loss: 0.1526110810:
12: 86432: loss: 0.1522621998:
12: 89632: loss: 0.1522428600:
12: 92832: loss: 0.1520271805:
12: 96032: loss: 0.1524263900:
12: 99232: loss: 0.1529252295:
12: 102432: loss: 0.1528060856:
12: 105632: loss: 0.1526182121:
12: 108832: loss: 0.1526393043:
12: 112032: loss: 0.1522120744:
12: 115232: loss: 0.1524262660:
12: 118432: loss: 0.1527532159:
12: 121632: loss: 0.1526512304:
12: 124832: loss: 0.1526208987:
12: 128032: loss: 0.1525081881:
12: 131232: loss: 0.1524653385:
12: 134432: loss: 0.1525344888:
12: 137632: loss: 0.1525344692:
12: 140832: loss: 0.1529401628:
12: 144032: loss: 0.1529646782:
12: 147232: loss: 0.1528241913:
12: 150432: loss: 0.1527820890:
Dev-Acc: 12: Accuracy: 0.9241645336: precision: 0.3719476744: recall: 0.4351300799: f1: 0.4010657472
Train-Acc: 12: Accuracy: 0.9437709451: precision: 0.8171684451: recall: 0.5638682532: f1: 0.6672890652
13: 3232: loss: 0.1622325101:
13: 6432: loss: 0.1639021785:
13: 9632: loss: 0.1545324777:
13: 12832: loss: 0.1560066553:
13: 16032: loss: 0.1548316674:
13: 19232: loss: 0.1551255035:
13: 22432: loss: 0.1564863144:
13: 25632: loss: 0.1561627785:
13: 28832: loss: 0.1549000521:
13: 32032: loss: 0.1541451009:
13: 35232: loss: 0.1531566453:
13: 38432: loss: 0.1536227425:
13: 41632: loss: 0.1547460163:
13: 44832: loss: 0.1541063011:
13: 48032: loss: 0.1537699813:
13: 51232: loss: 0.1534405367:
13: 54432: loss: 0.1533370051:
13: 57632: loss: 0.1524482846:
13: 60832: loss: 0.1522983636:
13: 64032: loss: 0.1521187944:
13: 67232: loss: 0.1522030294:
13: 70432: loss: 0.1519161961:
13: 73632: loss: 0.1518734879:
13: 76832: loss: 0.1518327591:
13: 80032: loss: 0.1515798414:
13: 83232: loss: 0.1514311418:
13: 86432: loss: 0.1509978127:
13: 89632: loss: 0.1510588428:
13: 92832: loss: 0.1510656835:
13: 96032: loss: 0.1512014601:
13: 99232: loss: 0.1510498009:
13: 102432: loss: 0.1508912075:
13: 105632: loss: 0.1508407737:
13: 108832: loss: 0.1506806399:
13: 112032: loss: 0.1503824555:
13: 115232: loss: 0.1504774482:
13: 118432: loss: 0.1504401078:
13: 121632: loss: 0.1501217143:
13: 124832: loss: 0.1500406348:
13: 128032: loss: 0.1500674282:
13: 131232: loss: 0.1504436360:
13: 134432: loss: 0.1505363738:
13: 137632: loss: 0.1505267365:
13: 140832: loss: 0.1504387148:
13: 144032: loss: 0.1503024085:
13: 147232: loss: 0.1501609353:
13: 150432: loss: 0.1498975902:
Dev-Acc: 13: Accuracy: 0.9242538214: precision: 0.3710460497: recall: 0.4288386329: f1: 0.3978545512
Train-Acc: 13: Accuracy: 0.9447504878: precision: 0.8297645577: recall: 0.5630136086: f1: 0.6708444305
14: 3232: loss: 0.1489795628:
14: 6432: loss: 0.1539856758:
14: 9632: loss: 0.1518459850:
14: 12832: loss: 0.1528360805:
14: 16032: loss: 0.1493846216:
14: 19232: loss: 0.1502129311:
14: 22432: loss: 0.1496971553:
14: 25632: loss: 0.1497622238:
14: 28832: loss: 0.1494176664:
14: 32032: loss: 0.1504724959:
14: 35232: loss: 0.1495876106:
14: 38432: loss: 0.1493400028:
14: 41632: loss: 0.1488436155:
14: 44832: loss: 0.1487482844:
14: 48032: loss: 0.1489800590:
14: 51232: loss: 0.1499880203:
14: 54432: loss: 0.1503629510:
14: 57632: loss: 0.1497484466:
14: 60832: loss: 0.1504781432:
14: 64032: loss: 0.1504216619:
14: 67232: loss: 0.1498415931:
14: 70432: loss: 0.1497716752:
14: 73632: loss: 0.1496385399:
14: 76832: loss: 0.1488309096:
14: 80032: loss: 0.1487189066:
14: 83232: loss: 0.1489264117:
14: 86432: loss: 0.1490086099:
14: 89632: loss: 0.1490593383:
14: 92832: loss: 0.1490631545:
14: 96032: loss: 0.1487267909:
14: 99232: loss: 0.1487043903:
14: 102432: loss: 0.1483511912:
14: 105632: loss: 0.1482702271:
14: 108832: loss: 0.1485459312:
14: 112032: loss: 0.1486329527:
14: 115232: loss: 0.1487145230:
14: 118432: loss: 0.1485751546:
14: 121632: loss: 0.1483143915:
14: 124832: loss: 0.1481383475:
14: 128032: loss: 0.1480114589:
14: 131232: loss: 0.1478842894:
14: 134432: loss: 0.1481035310:
14: 137632: loss: 0.1478787763:
14: 140832: loss: 0.1475695751:
14: 144032: loss: 0.1474109080:
14: 147232: loss: 0.1470160498:
14: 150432: loss: 0.1467803901:
Dev-Acc: 14: Accuracy: 0.9231128097: precision: 0.3653402537: recall: 0.4308791022: f1: 0.3954123430
Train-Acc: 14: Accuracy: 0.9459338784: precision: 0.8328093741: recall: 0.5747156663: f1: 0.6800995799
15: 3232: loss: 0.1406899581:
15: 6432: loss: 0.1350071642:
15: 9632: loss: 0.1365167762:
15: 12832: loss: 0.1358804372:
15: 16032: loss: 0.1356351848:
15: 19232: loss: 0.1359645359:
15: 22432: loss: 0.1399674693:
15: 25632: loss: 0.1394621037:
15: 28832: loss: 0.1408324572:
15: 32032: loss: 0.1422676421:
15: 35232: loss: 0.1424769431:
15: 38432: loss: 0.1422806873:
15: 41632: loss: 0.1424137415:
15: 44832: loss: 0.1429149361:
15: 48032: loss: 0.1437523210:
15: 51232: loss: 0.1435313640:
15: 54432: loss: 0.1433368701:
15: 57632: loss: 0.1432424611:
15: 60832: loss: 0.1430606489:
15: 64032: loss: 0.1430974108:
15: 67232: loss: 0.1432951309:
15: 70432: loss: 0.1433368655:
15: 73632: loss: 0.1442812159:
15: 76832: loss: 0.1442016341:
15: 80032: loss: 0.1441028964:
15: 83232: loss: 0.1446953614:
15: 86432: loss: 0.1444615541:
15: 89632: loss: 0.1446056429:
15: 92832: loss: 0.1449798620:
15: 96032: loss: 0.1452592871:
15: 99232: loss: 0.1451714766:
15: 102432: loss: 0.1450265861:
15: 105632: loss: 0.1447949918:
15: 108832: loss: 0.1448732754:
15: 112032: loss: 0.1452927676:
15: 115232: loss: 0.1453348502:
15: 118432: loss: 0.1450884075:
15: 121632: loss: 0.1449555892:
15: 124832: loss: 0.1445949143:
15: 128032: loss: 0.1443238616:
15: 131232: loss: 0.1445331182:
15: 134432: loss: 0.1445892278:
15: 137632: loss: 0.1446151068:
15: 140832: loss: 0.1444319183:
15: 144032: loss: 0.1448094678:
15: 147232: loss: 0.1447344984:
15: 150432: loss: 0.1447135658:
Dev-Acc: 15: Accuracy: 0.9220511317: precision: 0.3593104431: recall: 0.4288386329: f1: 0.3910077519
Train-Acc: 15: Accuracy: 0.9468476772: precision: 0.8365130336: recall: 0.5822759845: f1: 0.6866157603
16: 3232: loss: 0.1419868989:
16: 6432: loss: 0.1368626748:
16: 9632: loss: 0.1385632814:
16: 12832: loss: 0.1396923256:
16: 16032: loss: 0.1389164597:
16: 19232: loss: 0.1363674630:
16: 22432: loss: 0.1378201371:
16: 25632: loss: 0.1379498320:
16: 28832: loss: 0.1401238855:
16: 32032: loss: 0.1405633831:
16: 35232: loss: 0.1401728788:
16: 38432: loss: 0.1412978134:
16: 41632: loss: 0.1416347145:
16: 44832: loss: 0.1423600240:
16: 48032: loss: 0.1421339664:
16: 51232: loss: 0.1417103711:
16: 54432: loss: 0.1412656592:
16: 57632: loss: 0.1406898677:
16: 60832: loss: 0.1405063623:
16: 64032: loss: 0.1408741356:
16: 67232: loss: 0.1407103083:
16: 70432: loss: 0.1408592833:
16: 73632: loss: 0.1413714805:
16: 76832: loss: 0.1418337008:
16: 80032: loss: 0.1419998940:
16: 83232: loss: 0.1424352073:
16: 86432: loss: 0.1425919308:
16: 89632: loss: 0.1428851575:
16: 92832: loss: 0.1429324570:
16: 96032: loss: 0.1431503784:
16: 99232: loss: 0.1432496269:
16: 102432: loss: 0.1432081667:
16: 105632: loss: 0.1429551778:
16: 108832: loss: 0.1430232001:
16: 112032: loss: 0.1427157525:
16: 115232: loss: 0.1426440005:
16: 118432: loss: 0.1426650236:
16: 121632: loss: 0.1427606683:
16: 124832: loss: 0.1427056007:
16: 128032: loss: 0.1427534413:
16: 131232: loss: 0.1424595634:
16: 134432: loss: 0.1424731046:
16: 137632: loss: 0.1424579836:
16: 140832: loss: 0.1422815016:
16: 144032: loss: 0.1423467437:
16: 147232: loss: 0.1424454233:
16: 150432: loss: 0.1423602899:
Dev-Acc: 16: Accuracy: 0.9207016826: precision: 0.3524395359: recall: 0.4286685938: f1: 0.3868344330
Train-Acc: 16: Accuracy: 0.9478995204: precision: 0.8388206845: recall: 0.5929261718: f1: 0.6947579247
17: 3232: loss: 0.1403771259:
17: 6432: loss: 0.1417051702:
17: 9632: loss: 0.1429768649:
17: 12832: loss: 0.1423875648:
17: 16032: loss: 0.1429569057:
17: 19232: loss: 0.1435930649:
17: 22432: loss: 0.1439943049:
17: 25632: loss: 0.1430949611:
17: 28832: loss: 0.1416632019:
17: 32032: loss: 0.1417847761:
17: 35232: loss: 0.1409120688:
17: 38432: loss: 0.1393513971:
17: 41632: loss: 0.1396024807:
17: 44832: loss: 0.1393169053:
17: 48032: loss: 0.1390464726:
17: 51232: loss: 0.1395186428:
17: 54432: loss: 0.1397456096:
17: 57632: loss: 0.1401794244:
17: 60832: loss: 0.1409831454:
17: 64032: loss: 0.1411295522:
17: 67232: loss: 0.1413464222:
17: 70432: loss: 0.1412609096:
17: 73632: loss: 0.1410320978:
17: 76832: loss: 0.1412567024:
17: 80032: loss: 0.1401140719:
17: 83232: loss: 0.1400520961:
17: 86432: loss: 0.1404824992:
17: 89632: loss: 0.1411413915:
17: 92832: loss: 0.1411000061:
17: 96032: loss: 0.1415964940:
17: 99232: loss: 0.1421145261:
17: 102432: loss: 0.1422019018:
17: 105632: loss: 0.1417587629:
17: 108832: loss: 0.1412416720:
17: 112032: loss: 0.1412291137:
17: 115232: loss: 0.1411952353:
17: 118432: loss: 0.1410405281:
17: 121632: loss: 0.1406951963:
17: 124832: loss: 0.1406675520:
17: 128032: loss: 0.1403987248:
17: 131232: loss: 0.1406304868:
17: 134432: loss: 0.1407448526:
17: 137632: loss: 0.1404944415:
17: 140832: loss: 0.1404252103:
17: 144032: loss: 0.1406530010:
17: 147232: loss: 0.1404669475:
17: 150432: loss: 0.1400931589:
Dev-Acc: 17: Accuracy: 0.9204933047: precision: 0.3505747126: recall: 0.4252678116: f1: 0.3843257779
Train-Acc: 17: Accuracy: 0.9485109448: precision: 0.8426037701: recall: 0.5965419762: f1: 0.6985373364
18: 3232: loss: 0.1375283782:
18: 6432: loss: 0.1404760667:
18: 9632: loss: 0.1375567971:
18: 12832: loss: 0.1341480259:
18: 16032: loss: 0.1374189858:
18: 19232: loss: 0.1373988831:
18: 22432: loss: 0.1385327290:
18: 25632: loss: 0.1384205418:
18: 28832: loss: 0.1380436207:
18: 32032: loss: 0.1384281356:
18: 35232: loss: 0.1388964822:
18: 38432: loss: 0.1384001794:
18: 41632: loss: 0.1387809978:
18: 44832: loss: 0.1386087681:
18: 48032: loss: 0.1387466958:
18: 51232: loss: 0.1380855681:
18: 54432: loss: 0.1383687082:
18: 57632: loss: 0.1378400060:
18: 60832: loss: 0.1384606991:
18: 64032: loss: 0.1385583727:
18: 67232: loss: 0.1379178704:
18: 70432: loss: 0.1382761039:
18: 73632: loss: 0.1383073875:
18: 76832: loss: 0.1384026319:
18: 80032: loss: 0.1381329476:
18: 83232: loss: 0.1380394517:
18: 86432: loss: 0.1384566671:
18: 89632: loss: 0.1383333608:
18: 92832: loss: 0.1386551388:
18: 96032: loss: 0.1390203202:
18: 99232: loss: 0.1388564589:
18: 102432: loss: 0.1387671522:
18: 105632: loss: 0.1386176200:
18: 108832: loss: 0.1388092032:
18: 112032: loss: 0.1386683409:
18: 115232: loss: 0.1380936814:
18: 118432: loss: 0.1380584071:
18: 121632: loss: 0.1383742410:
18: 124832: loss: 0.1383949236:
18: 128032: loss: 0.1383498357:
18: 131232: loss: 0.1385443463:
18: 134432: loss: 0.1384099822:
18: 137632: loss: 0.1382393592:
18: 140832: loss: 0.1380667385:
18: 144032: loss: 0.1380535998:
18: 147232: loss: 0.1383170533:
18: 150432: loss: 0.1381935476:
Dev-Acc: 18: Accuracy: 0.9184294939: precision: 0.3422330097: recall: 0.4315592586: f1: 0.3817402422
Train-Acc: 18: Accuracy: 0.9491420388: precision: 0.8393716517: recall: 0.6077180987: f1: 0.7050030506
19: 3232: loss: 0.1356394524:
19: 6432: loss: 0.1327525020:
19: 9632: loss: 0.1290994233:
19: 12832: loss: 0.1302169544:
19: 16032: loss: 0.1341430361:
19: 19232: loss: 0.1343480719:
19: 22432: loss: 0.1351999483:
19: 25632: loss: 0.1366414744:
19: 28832: loss: 0.1362870517:
19: 32032: loss: 0.1365607013:
19: 35232: loss: 0.1359238971:
19: 38432: loss: 0.1352359717:
19: 41632: loss: 0.1355128833:
19: 44832: loss: 0.1359098345:
19: 48032: loss: 0.1355796885:
19: 51232: loss: 0.1354034185:
19: 54432: loss: 0.1353662942:
19: 57632: loss: 0.1355231026:
19: 60832: loss: 0.1349102981:
19: 64032: loss: 0.1350490858:
19: 67232: loss: 0.1353030573:
19: 70432: loss: 0.1351999555:
19: 73632: loss: 0.1353435441:
19: 76832: loss: 0.1351794701:
19: 80032: loss: 0.1347435205:
19: 83232: loss: 0.1344174652:
19: 86432: loss: 0.1340916920:
19: 89632: loss: 0.1346926967:
19: 92832: loss: 0.1348082113:
19: 96032: loss: 0.1350615795:
19: 99232: loss: 0.1354290787:
19: 102432: loss: 0.1353733785:
19: 105632: loss: 0.1354805220:
19: 108832: loss: 0.1355378716:
19: 112032: loss: 0.1354224995:
19: 115232: loss: 0.1354415064:
19: 118432: loss: 0.1355503226:
19: 121632: loss: 0.1356718409:
19: 124832: loss: 0.1356602148:
19: 128032: loss: 0.1356112824:
19: 131232: loss: 0.1355483836:
19: 134432: loss: 0.1358463424:
19: 137632: loss: 0.1357212115:
19: 140832: loss: 0.1357658575:
19: 144032: loss: 0.1359076999:
19: 147232: loss: 0.1361711393:
19: 150432: loss: 0.1361072045:
Dev-Acc: 19: Accuracy: 0.9168816209: precision: 0.3338215712: recall: 0.4262880463: f1: 0.3744305877
Train-Acc: 19: Accuracy: 0.9496679902: precision: 0.8396726913: recall: 0.6138978371: f1: 0.7092511013
20: 3232: loss: 0.1314870892:
20: 6432: loss: 0.1357545089:
20: 9632: loss: 0.1300758051:
20: 12832: loss: 0.1315388371:
20: 16032: loss: 0.1320494124:
20: 19232: loss: 0.1333952828:
20: 22432: loss: 0.1331212833:
20: 25632: loss: 0.1334227004:
20: 28832: loss: 0.1339637373:
20: 32032: loss: 0.1345070681:
20: 35232: loss: 0.1348083441:
20: 38432: loss: 0.1343308727:
20: 41632: loss: 0.1350495149:
20: 44832: loss: 0.1335567529:
20: 48032: loss: 0.1340270734:
20: 51232: loss: 0.1339720241:
20: 54432: loss: 0.1341696951:
20: 57632: loss: 0.1348091429:
20: 60832: loss: 0.1348546275:
20: 64032: loss: 0.1348322783:
20: 67232: loss: 0.1343139921:
20: 70432: loss: 0.1351856882:
20: 73632: loss: 0.1354816676:
20: 76832: loss: 0.1350516444:
20: 80032: loss: 0.1349477104:
20: 83232: loss: 0.1346873911:
20: 86432: loss: 0.1344406924:
20: 89632: loss: 0.1347924706:
20: 92832: loss: 0.1346748644:
20: 96032: loss: 0.1342758518:
20: 99232: loss: 0.1341129666:
20: 102432: loss: 0.1340368607:
20: 105632: loss: 0.1344911248:
20: 108832: loss: 0.1346660910:
20: 112032: loss: 0.1343226474:
20: 115232: loss: 0.1344214173:
20: 118432: loss: 0.1340883163:
20: 121632: loss: 0.1339555886:
20: 124832: loss: 0.1339819207:
20: 128032: loss: 0.1338990786:
20: 131232: loss: 0.1339894729:
20: 134432: loss: 0.1339229423:
20: 137632: loss: 0.1339154683:
20: 140832: loss: 0.1336289919:
20: 144032: loss: 0.1335955416:
20: 147232: loss: 0.1336966823:
20: 150432: loss: 0.1336390467:
Dev-Acc: 20: Accuracy: 0.9168915749: precision: 0.3306177868: recall: 0.4140452304: f1: 0.3676581610
Train-Acc: 20: Accuracy: 0.9503319860: precision: 0.8498446354: recall: 0.6113339031: f1: 0.7111230069
