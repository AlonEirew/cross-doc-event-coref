1: 6464: loss: 0.7089721620:
1: 12864: loss: 0.7075022840:
1: 19264: loss: 0.7063602706:
1: 25664: loss: 0.7052174830:
1: 32064: loss: 0.7041636893:
1: 38464: loss: 0.7032791486:
1: 44864: loss: 0.7023751908:
1: 51264: loss: 0.7014656557:
1: 57664: loss: 0.7004883348:
1: 64064: loss: 0.6995545398:
1: 70464: loss: 0.6986641612:
1: 76864: loss: 0.6977021382:
1: 83264: loss: 0.6967659691:
1: 89664: loss: 0.6959219919:
1: 96064: loss: 0.6950046935:
1: 102464: loss: 0.6940528903:
1: 108864: loss: 0.6932380469:
1: 115264: loss: 0.6923198513:
1: 121664: loss: 0.6914316732:
Dev-Acc: 1: Accuracy: 0.7450388670: precision: 0.0677544610: recall: 0.2640707363: f1: 0.1078397334
Train-Acc: 1: Accuracy: 0.7355039716: precision: 0.1869351924: recall: 0.3331799356: f1: 0.2394971882
2: 6464: loss: 0.6725194293:
2: 12864: loss: 0.6718821812:
2: 19264: loss: 0.6712509435:
2: 25664: loss: 0.6705159158:
2: 32064: loss: 0.6695394546:
2: 38464: loss: 0.6685159678:
2: 44864: loss: 0.6676060465:
2: 51264: loss: 0.6666918422:
2: 57664: loss: 0.6657693417:
2: 64064: loss: 0.6649029729:
2: 70464: loss: 0.6640474283:
2: 76864: loss: 0.6632945689:
2: 83264: loss: 0.6625655249:
2: 89664: loss: 0.6618092771:
2: 96064: loss: 0.6609321917:
2: 102464: loss: 0.6600905303:
2: 108864: loss: 0.6592955603:
2: 115264: loss: 0.6584754853:
2: 121664: loss: 0.6576895103:
Dev-Acc: 2: Accuracy: 0.9235096574: precision: 0.0970017637: recall: 0.0374086040: f1: 0.0539943551
Train-Acc: 2: Accuracy: 0.8682861328: precision: 0.3504210912: recall: 0.0629149957: f1: 0.1066770706
3: 6464: loss: 0.6416756201:
3: 12864: loss: 0.6404601505:
3: 19264: loss: 0.6401863458:
3: 25664: loss: 0.6391023615:
3: 32064: loss: 0.6383494205:
3: 38464: loss: 0.6372082038:
3: 44864: loss: 0.6363730214:
3: 51264: loss: 0.6357509632:
3: 57664: loss: 0.6350230267:
3: 64064: loss: 0.6342604892:
3: 70464: loss: 0.6332772520:
3: 76864: loss: 0.6323903318:
3: 83264: loss: 0.6316736926:
3: 89664: loss: 0.6308714802:
3: 96064: loss: 0.6300297300:
3: 102464: loss: 0.6291996657:
3: 108864: loss: 0.6283396073:
3: 115264: loss: 0.6274223158:
3: 121664: loss: 0.6265898882:
Dev-Acc: 3: Accuracy: 0.9411712289: precision: 0.1129032258: recall: 0.0011902738: f1: 0.0023557126
Train-Acc: 3: Accuracy: 0.8749096394: precision: 0.4625850340: recall: 0.0044704490: f1: 0.0088553197
4: 6464: loss: 0.6105504042:
4: 12864: loss: 0.6098951316:
4: 19264: loss: 0.6089070684:
4: 25664: loss: 0.6080640417:
4: 32064: loss: 0.6074945600:
4: 38464: loss: 0.6070281929:
4: 44864: loss: 0.6059856283:
4: 51264: loss: 0.6052967155:
4: 57664: loss: 0.6044502568:
4: 64064: loss: 0.6036089489:
4: 70464: loss: 0.6029249991:
4: 76864: loss: 0.6021469958:
4: 83264: loss: 0.6014039117:
4: 89664: loss: 0.6007543875:
4: 96064: loss: 0.5999998917:
4: 102464: loss: 0.5993341857:
4: 108864: loss: 0.5985077464:
4: 115264: loss: 0.5976215055:
4: 121664: loss: 0.5969484863:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.5000000000: recall: 0.0001700391: f1: 0.0003399626
Train-Acc: 4: Accuracy: 0.8750821948: precision: 1.0000000000: recall: 0.0006574190: f1: 0.0013139741
5: 6464: loss: 0.5802602708:
5: 12864: loss: 0.5794244641:
5: 19264: loss: 0.5795955328:
5: 25664: loss: 0.5788286445:
5: 32064: loss: 0.5779753721:
5: 38464: loss: 0.5776387437:
5: 44864: loss: 0.5766138577:
5: 51264: loss: 0.5758639880:
5: 57664: loss: 0.5751391794:
5: 64064: loss: 0.5748700009:
5: 70464: loss: 0.5741956489:
5: 76864: loss: 0.5736184999:
5: 83264: loss: 0.5727310546:
5: 89664: loss: 0.5719684052:
5: 96064: loss: 0.5712283927:
5: 102464: loss: 0.5705814256:
5: 108864: loss: 0.5699001674:
5: 115264: loss: 0.5692125757:
5: 121664: loss: 0.5684426247:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 6464: loss: 0.5540183872:
6: 12864: loss: 0.5526979125:
6: 19264: loss: 0.5515378910:
6: 25664: loss: 0.5515314402:
6: 32064: loss: 0.5512260392:
6: 38464: loss: 0.5506161426:
6: 44864: loss: 0.5497152658:
6: 51264: loss: 0.5494374566:
6: 57664: loss: 0.5482013319:
6: 64064: loss: 0.5476276822:
6: 70464: loss: 0.5466989732:
6: 76864: loss: 0.5458149881:
6: 83264: loss: 0.5453233712:
6: 89664: loss: 0.5446842014:
6: 96064: loss: 0.5440412645:
6: 102464: loss: 0.5434277003:
6: 108864: loss: 0.5424988409:
6: 115264: loss: 0.5418887377:
6: 121664: loss: 0.5410493510:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 6464: loss: 0.5269321105:
7: 12864: loss: 0.5272850348:
7: 19264: loss: 0.5274151445:
7: 25664: loss: 0.5258506954:
7: 32064: loss: 0.5255002491:
7: 38464: loss: 0.5243408419:
7: 44864: loss: 0.5239298914:
7: 51264: loss: 0.5235736052:
7: 57664: loss: 0.5227781613:
7: 64064: loss: 0.5219479087:
7: 70464: loss: 0.5213624222:
7: 76864: loss: 0.5205997266:
7: 83264: loss: 0.5201986051:
7: 89664: loss: 0.5193583558:
7: 96064: loss: 0.5188299964:
7: 102464: loss: 0.5180368479:
7: 108864: loss: 0.5172175707:
7: 115264: loss: 0.5162790140:
7: 121664: loss: 0.5155101924:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.5024618238:
8: 12864: loss: 0.5028174302:
8: 19264: loss: 0.5033761101:
8: 25664: loss: 0.5020029769:
8: 32064: loss: 0.5013556429:
8: 38464: loss: 0.5006587943:
8: 44864: loss: 0.4996382293:
8: 51264: loss: 0.4990607645:
8: 57664: loss: 0.4979350602:
8: 64064: loss: 0.4973553619:
8: 70464: loss: 0.4964103444:
8: 76864: loss: 0.4955067424:
8: 83264: loss: 0.4951015486:
8: 89664: loss: 0.4940076605:
8: 96064: loss: 0.4935761620:
8: 102464: loss: 0.4931371884:
8: 108864: loss: 0.4922662456:
8: 115264: loss: 0.4917701901:
8: 121664: loss: 0.4912276799:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.4777568600:
9: 12864: loss: 0.4783946745:
9: 19264: loss: 0.4766527609:
9: 25664: loss: 0.4767610834:
9: 32064: loss: 0.4766189363:
9: 38464: loss: 0.4760295646:
9: 44864: loss: 0.4759407320:
9: 51264: loss: 0.4750787381:
9: 57664: loss: 0.4746209719:
9: 64064: loss: 0.4745687459:
9: 70464: loss: 0.4739789746:
9: 76864: loss: 0.4735663568:
9: 83264: loss: 0.4726971947:
9: 89664: loss: 0.4720493818:
9: 96064: loss: 0.4713571413:
9: 102464: loss: 0.4704494787:
9: 108864: loss: 0.4699393767:
9: 115264: loss: 0.4696058607:
9: 121664: loss: 0.4688743296:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.4576910642:
10: 12864: loss: 0.4600294673:
10: 19264: loss: 0.4572521428:
10: 25664: loss: 0.4567687844:
10: 32064: loss: 0.4549055597:
10: 38464: loss: 0.4535928001:
10: 44864: loss: 0.4531129152:
10: 51264: loss: 0.4529571824:
10: 57664: loss: 0.4528592940:
10: 64064: loss: 0.4529294159:
10: 70464: loss: 0.4523581076:
10: 76864: loss: 0.4515826861:
10: 83264: loss: 0.4512899932:
10: 89664: loss: 0.4509019951:
10: 96064: loss: 0.4500803117:
10: 102464: loss: 0.4495975680:
10: 108864: loss: 0.4491251325:
10: 115264: loss: 0.4481670526:
10: 121664: loss: 0.4478013596:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.4369909313:
11: 12864: loss: 0.4366180272:
11: 19264: loss: 0.4370956387:
11: 25664: loss: 0.4359362874:
11: 32064: loss: 0.4346881359:
11: 38464: loss: 0.4348283759:
11: 44864: loss: 0.4342118247:
11: 51264: loss: 0.4344831919:
11: 57664: loss: 0.4342306722:
11: 64064: loss: 0.4333913100:
11: 70464: loss: 0.4331990336:
11: 76864: loss: 0.4323465176:
11: 83264: loss: 0.4315731690:
11: 89664: loss: 0.4308729605:
11: 96064: loss: 0.4306884004:
11: 102464: loss: 0.4300550875:
11: 108864: loss: 0.4296833309:
11: 115264: loss: 0.4290316915:
11: 121664: loss: 0.4288493218:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.4191834018:
12: 12864: loss: 0.4165947156:
12: 19264: loss: 0.4183221518:
12: 25664: loss: 0.4188801969:
12: 32064: loss: 0.4170152277:
12: 38464: loss: 0.4167029325:
12: 44864: loss: 0.4165133235:
12: 51264: loss: 0.4161044999:
12: 57664: loss: 0.4152448928:
12: 64064: loss: 0.4145378128:
12: 70464: loss: 0.4146041109:
12: 76864: loss: 0.4140016953:
12: 83264: loss: 0.4134332031:
12: 89664: loss: 0.4133524898:
12: 96064: loss: 0.4134700176:
12: 102464: loss: 0.4127056742:
12: 108864: loss: 0.4121197555:
12: 115264: loss: 0.4120682580:
12: 121664: loss: 0.4113841285:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.4055268744:
13: 12864: loss: 0.4018644090:
13: 19264: loss: 0.4014202139:
13: 25664: loss: 0.4025208277:
13: 32064: loss: 0.4028821255:
13: 38464: loss: 0.4013334524:
13: 44864: loss: 0.4009586368:
13: 51264: loss: 0.3998535482:
13: 57664: loss: 0.3994407728:
13: 64064: loss: 0.3988087874:
13: 70464: loss: 0.3979063046:
13: 76864: loss: 0.3975304136:
13: 83264: loss: 0.3977083778:
13: 89664: loss: 0.3978378043:
13: 96064: loss: 0.3972360413:
13: 102464: loss: 0.3968136889:
13: 108864: loss: 0.3966548227:
13: 115264: loss: 0.3961957848:
13: 121664: loss: 0.3958543199:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.3839787796:
14: 12864: loss: 0.3851401798:
14: 19264: loss: 0.3867144509:
14: 25664: loss: 0.3880709451:
14: 32064: loss: 0.3874125710:
14: 38464: loss: 0.3877651718:
14: 44864: loss: 0.3873016426:
14: 51264: loss: 0.3867516363:
14: 57664: loss: 0.3861418492:
14: 64064: loss: 0.3858486136:
14: 70464: loss: 0.3855256715:
14: 76864: loss: 0.3857313336:
14: 83264: loss: 0.3850447061:
14: 89664: loss: 0.3841900629:
14: 96064: loss: 0.3838785526:
14: 102464: loss: 0.3835320922:
14: 108864: loss: 0.3830197956:
14: 115264: loss: 0.3828446315:
14: 121664: loss: 0.3821641833:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.3804638681:
15: 12864: loss: 0.3777370466:
15: 19264: loss: 0.3753689833:
15: 25664: loss: 0.3773051967:
15: 32064: loss: 0.3760612654:
15: 38464: loss: 0.3760582687:
15: 44864: loss: 0.3752957488:
15: 51264: loss: 0.3743370203:
15: 57664: loss: 0.3737091766:
15: 64064: loss: 0.3727915916:
15: 70464: loss: 0.3731188791:
15: 76864: loss: 0.3724071535:
15: 83264: loss: 0.3714351363:
15: 89664: loss: 0.3711034844:
15: 96064: loss: 0.3709539774:
15: 102464: loss: 0.3705156939:
15: 108864: loss: 0.3702715586:
15: 115264: loss: 0.3697253341:
15: 121664: loss: 0.3696989129:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.3712764075:
16: 12864: loss: 0.3694932978:
16: 19264: loss: 0.3687828384:
16: 25664: loss: 0.3638161200:
16: 32064: loss: 0.3632916561:
16: 38464: loss: 0.3627208741:
16: 44864: loss: 0.3636282662:
16: 51264: loss: 0.3629390487:
16: 57664: loss: 0.3626701662:
16: 64064: loss: 0.3629639678:
16: 70464: loss: 0.3620381302:
16: 76864: loss: 0.3623340252:
16: 83264: loss: 0.3614948300:
16: 89664: loss: 0.3607328634:
16: 96064: loss: 0.3604267423:
16: 102464: loss: 0.3601941760:
16: 108864: loss: 0.3595836806:
16: 115264: loss: 0.3590869646:
16: 121664: loss: 0.3586675042:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.3525718975:
17: 12864: loss: 0.3553610992:
17: 19264: loss: 0.3551351500:
17: 25664: loss: 0.3548817066:
17: 32064: loss: 0.3533129778:
17: 38464: loss: 0.3531918780:
17: 44864: loss: 0.3525588264:
17: 51264: loss: 0.3519444568:
17: 57664: loss: 0.3530186939:
17: 64064: loss: 0.3522380230:
17: 70464: loss: 0.3514918266:
17: 76864: loss: 0.3506561860:
17: 83264: loss: 0.3514240168:
17: 89664: loss: 0.3506995346:
17: 96064: loss: 0.3506587213:
17: 102464: loss: 0.3498459908:
17: 108864: loss: 0.3491608614:
17: 115264: loss: 0.3492929685:
17: 121664: loss: 0.3486410287:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.3461883925:
18: 12864: loss: 0.3452409421:
18: 19264: loss: 0.3436619157:
18: 25664: loss: 0.3443031266:
18: 32064: loss: 0.3450022058:
18: 38464: loss: 0.3452738466:
18: 44864: loss: 0.3447732660:
18: 51264: loss: 0.3437543426:
18: 57664: loss: 0.3432078657:
18: 64064: loss: 0.3418639974:
18: 70464: loss: 0.3419923692:
18: 76864: loss: 0.3420767057:
18: 83264: loss: 0.3412512938:
18: 89664: loss: 0.3407732778:
18: 96064: loss: 0.3396423237:
18: 102464: loss: 0.3392892991:
18: 108864: loss: 0.3391420348:
18: 115264: loss: 0.3396629645:
18: 121664: loss: 0.3394351315:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 6464: loss: 0.3239979513:
19: 12864: loss: 0.3308819460:
19: 19264: loss: 0.3317765028:
19: 25664: loss: 0.3322299626:
19: 32064: loss: 0.3340186193:
19: 38464: loss: 0.3324316776:
19: 44864: loss: 0.3314681317:
19: 51264: loss: 0.3312139288:
19: 57664: loss: 0.3314993323:
19: 64064: loss: 0.3317006066:
19: 70464: loss: 0.3318838541:
19: 76864: loss: 0.3321144474:
19: 83264: loss: 0.3317730835:
19: 89664: loss: 0.3319914770:
19: 96064: loss: 0.3317734156:
19: 102464: loss: 0.3323198755:
19: 108864: loss: 0.3315364143:
19: 115264: loss: 0.3318497474:
19: 121664: loss: 0.3313214259:
Dev-Acc: 19: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.8750329018: precision: 1.0000000000: recall: 0.0002629676: f1: 0.0005257969
20: 6464: loss: 0.3334398806:
20: 12864: loss: 0.3328785923:
20: 19264: loss: 0.3305406731:
20: 25664: loss: 0.3295194501:
20: 32064: loss: 0.3300162573:
20: 38464: loss: 0.3291183635:
20: 44864: loss: 0.3272380540:
20: 51264: loss: 0.3269265446:
20: 57664: loss: 0.3282359924:
20: 64064: loss: 0.3280431356:
20: 70464: loss: 0.3282596804:
20: 76864: loss: 0.3283907769:
20: 83264: loss: 0.3272053585:
20: 89664: loss: 0.3271954904:
20: 96064: loss: 0.3265921368:
20: 102464: loss: 0.3262002797:
20: 108864: loss: 0.3250948429:
20: 115264: loss: 0.3244557343:
20: 121664: loss: 0.3236812070:
Dev-Acc: 20: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 20: Accuracy: 0.8753533959: precision: 1.0000000000: recall: 0.0028269016: f1: 0.0056378655
