1: 6464: loss: 0.7004354429:
1: 12864: loss: 0.6908712867:
1: 19264: loss: 0.6820006537:
1: 25664: loss: 0.6733438900:
1: 32064: loss: 0.6647042774:
1: 38464: loss: 0.6565190132:
1: 44864: loss: 0.6481315072:
1: 51264: loss: 0.6405291387:
1: 57664: loss: 0.6326451916:
1: 64064: loss: 0.6247651739:
1: 70464: loss: 0.6172472589:
1: 76864: loss: 0.6096094053:
1: 83264: loss: 0.6020893299:
1: 89664: loss: 0.5948996095:
1: 96064: loss: 0.5874179289:
1: 102464: loss: 0.5801159043:
1: 108864: loss: 0.5731015381:
1: 115264: loss: 0.5658726774:
1: 121664: loss: 0.5593825183:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 6464: loss: 0.4285789365:
2: 12864: loss: 0.4251108421:
2: 19264: loss: 0.4187538330:
2: 25664: loss: 0.4154599735:
2: 32064: loss: 0.4109045661:
2: 38464: loss: 0.4056770158:
2: 44864: loss: 0.4003288335:
2: 51264: loss: 0.3968212976:
2: 57664: loss: 0.3928002372:
2: 64064: loss: 0.3894726938:
2: 70464: loss: 0.3863594066:
2: 76864: loss: 0.3839204707:
2: 83264: loss: 0.3811585965:
2: 89664: loss: 0.3784898324:
2: 96064: loss: 0.3750438551:
2: 102464: loss: 0.3722833313:
2: 108864: loss: 0.3699599670:
2: 115264: loss: 0.3673211758:
2: 121664: loss: 0.3652301735:
Dev-Acc: 2: Accuracy: 0.9416970611: precision: 0.8571428571: recall: 0.0010202347: f1: 0.0020380435
Train-Acc: 2: Accuracy: 0.8759697080: precision: 0.9469696970: recall: 0.0082177372: f1: 0.0162940755
3: 6464: loss: 0.3206507966:
3: 12864: loss: 0.3163708155:
3: 19264: loss: 0.3173269261:
3: 25664: loss: 0.3146114481:
3: 32064: loss: 0.3139164327:
3: 38464: loss: 0.3099931745:
3: 44864: loss: 0.3083798420:
3: 51264: loss: 0.3070617549:
3: 57664: loss: 0.3063192463:
3: 64064: loss: 0.3043785110:
3: 70464: loss: 0.3024524996:
3: 76864: loss: 0.3004111242:
3: 83264: loss: 0.2996759041:
3: 89664: loss: 0.2985531515:
3: 96064: loss: 0.2966309633:
3: 102464: loss: 0.2956200200:
3: 108864: loss: 0.2942037670:
3: 115264: loss: 0.2925039331:
3: 121664: loss: 0.2914313252:
Dev-Acc: 3: Accuracy: 0.9430762529: precision: 0.5365853659: recall: 0.1795612991: f1: 0.2690788635
Train-Acc: 3: Accuracy: 0.8961524963: precision: 0.8832638475: recall: 0.1949904674: f1: 0.3194571598
4: 6464: loss: 0.2651566258:
4: 12864: loss: 0.2681538330:
4: 19264: loss: 0.2665928073:
4: 25664: loss: 0.2663908431:
4: 32064: loss: 0.2669760062:
4: 38464: loss: 0.2675530179:
4: 44864: loss: 0.2652796119:
4: 51264: loss: 0.2639171657:
4: 57664: loss: 0.2626871277:
4: 64064: loss: 0.2618974027:
4: 70464: loss: 0.2613167368:
4: 76864: loss: 0.2603242726:
4: 83264: loss: 0.2593911233:
4: 89664: loss: 0.2584802615:
4: 96064: loss: 0.2576520510:
4: 102464: loss: 0.2570369389:
4: 108864: loss: 0.2557112121:
4: 115264: loss: 0.2545755021:
4: 121664: loss: 0.2542517380:
Dev-Acc: 4: Accuracy: 0.9214161038: precision: 0.3602084190: recall: 0.4466927393: f1: 0.3988158494
Train-Acc: 4: Accuracy: 0.9069341421: precision: 0.7257727167: recall: 0.4106238906: f1: 0.5244993072
5: 6464: loss: 0.2331629361:
5: 12864: loss: 0.2348880165:
5: 19264: loss: 0.2365275004:
5: 25664: loss: 0.2370564915:
5: 32064: loss: 0.2355546844:
5: 38464: loss: 0.2373329723:
5: 44864: loss: 0.2350148651:
5: 51264: loss: 0.2352869216:
5: 57664: loss: 0.2346762029:
5: 64064: loss: 0.2357081042:
5: 70464: loss: 0.2355600866:
5: 76864: loss: 0.2351595767:
5: 83264: loss: 0.2342296782:
5: 89664: loss: 0.2336881061:
5: 96064: loss: 0.2328872888:
5: 102464: loss: 0.2326926809:
5: 108864: loss: 0.2321611161:
5: 115264: loss: 0.2324321248:
5: 121664: loss: 0.2316364438:
Dev-Acc: 5: Accuracy: 0.9184096456: precision: 0.3576118677: recall: 0.5000850196: f1: 0.4170152428
Train-Acc: 5: Accuracy: 0.9135083556: precision: 0.7459068010: recall: 0.4672276642: f1: 0.5745583896
6: 6464: loss: 0.2354512665:
6: 12864: loss: 0.2271101126:
6: 19264: loss: 0.2243112286:
6: 25664: loss: 0.2241117929:
6: 32064: loss: 0.2238303078:
6: 38464: loss: 0.2231936777:
6: 44864: loss: 0.2214557183:
6: 51264: loss: 0.2223363688:
6: 57664: loss: 0.2218324671:
6: 64064: loss: 0.2211185317:
6: 70464: loss: 0.2203220513:
6: 76864: loss: 0.2194012647:
6: 83264: loss: 0.2194892040:
6: 89664: loss: 0.2189391151:
6: 96064: loss: 0.2182898346:
6: 102464: loss: 0.2187313181:
6: 108864: loss: 0.2181620301:
6: 115264: loss: 0.2180781849:
6: 121664: loss: 0.2174088780:
Dev-Acc: 6: Accuracy: 0.9198285341: precision: 0.3666787923: recall: 0.5141982656: f1: 0.4280860702
Train-Acc: 6: Accuracy: 0.9180856347: precision: 0.7732721776: recall: 0.4876733943: f1: 0.5981293340
7: 6464: loss: 0.2157787722:
7: 12864: loss: 0.2151687876:
7: 19264: loss: 0.2128928365:
7: 25664: loss: 0.2130572407:
7: 32064: loss: 0.2142818712:
7: 38464: loss: 0.2120619261:
7: 44864: loss: 0.2120907796:
7: 51264: loss: 0.2121444699:
7: 57664: loss: 0.2118790391:
7: 64064: loss: 0.2108902168:
7: 70464: loss: 0.2104015172:
7: 76864: loss: 0.2102117617:
7: 83264: loss: 0.2103077330:
7: 89664: loss: 0.2099087301:
7: 96064: loss: 0.2093677492:
7: 102464: loss: 0.2091722093:
7: 108864: loss: 0.2082448982:
7: 115264: loss: 0.2076867605:
7: 121664: loss: 0.2073745006:
Dev-Acc: 7: Accuracy: 0.9246804714: precision: 0.3837051143: recall: 0.4796803265: f1: 0.4263583466
Train-Acc: 7: Accuracy: 0.9206578135: precision: 0.8083240844: recall: 0.4787982381: f1: 0.6013789687
8: 6464: loss: 0.1966977178:
8: 12864: loss: 0.2011353486:
8: 19264: loss: 0.2051941198:
8: 25664: loss: 0.2023274285:
8: 32064: loss: 0.2017006270:
8: 38464: loss: 0.2008730570:
8: 44864: loss: 0.2004168470:
8: 51264: loss: 0.2005953154:
8: 57664: loss: 0.2005588131:
8: 64064: loss: 0.2007898015:
8: 70464: loss: 0.2000645688:
8: 76864: loss: 0.1994980906:
8: 83264: loss: 0.1989993931:
8: 89664: loss: 0.1983260623:
8: 96064: loss: 0.1986631584:
8: 102464: loss: 0.1988611127:
8: 108864: loss: 0.1988720702:
8: 115264: loss: 0.1989703986:
8: 121664: loss: 0.1990572588:
Dev-Acc: 8: Accuracy: 0.9216542244: precision: 0.3697142118: recall: 0.4861418126: f1: 0.4200088145
Train-Acc: 8: Accuracy: 0.9229012132: precision: 0.8098883573: recall: 0.5007560318: f1: 0.6188657784
9: 6464: loss: 0.1871651620:
9: 12864: loss: 0.1938152099:
9: 19264: loss: 0.1937266548:
9: 25664: loss: 0.1949241547:
9: 32064: loss: 0.1951024262:
9: 38464: loss: 0.1953442450:
9: 44864: loss: 0.1954103422:
9: 51264: loss: 0.1940038007:
9: 57664: loss: 0.1952854231:
9: 64064: loss: 0.1965044199:
9: 70464: loss: 0.1957084311:
9: 76864: loss: 0.1955570052:
9: 83264: loss: 0.1950639819:
9: 89664: loss: 0.1947188191:
9: 96064: loss: 0.1949493265:
9: 102464: loss: 0.1940301758:
9: 108864: loss: 0.1933862141:
9: 115264: loss: 0.1932962291:
9: 121664: loss: 0.1930479096:
Dev-Acc: 9: Accuracy: 0.9198682308: precision: 0.3586970516: recall: 0.4737289577: f1: 0.4082649472
Train-Acc: 9: Accuracy: 0.9250871539: precision: 0.8169526781: recall: 0.5164026034: f1: 0.6328043181
10: 6464: loss: 0.1860640220:
10: 12864: loss: 0.1891929469:
10: 19264: loss: 0.1890903089:
10: 25664: loss: 0.1896246097:
10: 32064: loss: 0.1873987576:
10: 38464: loss: 0.1876247849:
10: 44864: loss: 0.1887097195:
10: 51264: loss: 0.1881196459:
10: 57664: loss: 0.1885608511:
10: 64064: loss: 0.1894367885:
10: 70464: loss: 0.1898100346:
10: 76864: loss: 0.1896641322:
10: 83264: loss: 0.1890942567:
10: 89664: loss: 0.1890588766:
10: 96064: loss: 0.1884456657:
10: 102464: loss: 0.1881251298:
10: 108864: loss: 0.1879915396:
10: 115264: loss: 0.1875508482:
10: 121664: loss: 0.1877279457:
Dev-Acc: 10: Accuracy: 0.9183005095: precision: 0.3504512521: recall: 0.4687978235: f1: 0.4010765202
Train-Acc: 10: Accuracy: 0.9274456501: precision: 0.8223883613: recall: 0.5351390441: f1: 0.6483730933
11: 6464: loss: 0.1892275552:
11: 12864: loss: 0.1895916430:
11: 19264: loss: 0.1882039165:
11: 25664: loss: 0.1847530380:
11: 32064: loss: 0.1838637284:
11: 38464: loss: 0.1841237522:
11: 44864: loss: 0.1830643917:
11: 51264: loss: 0.1846608655:
11: 57664: loss: 0.1845563578:
11: 64064: loss: 0.1838518834:
11: 70464: loss: 0.1848441497:
11: 76864: loss: 0.1839344007:
11: 83264: loss: 0.1840836963:
11: 89664: loss: 0.1834961302:
11: 96064: loss: 0.1833332771:
11: 102464: loss: 0.1829065858:
11: 108864: loss: 0.1826321840:
11: 115264: loss: 0.1826173244:
11: 121664: loss: 0.1829788564:
Dev-Acc: 11: Accuracy: 0.9145499468: precision: 0.3347452499: recall: 0.4703281755: f1: 0.3911199095
Train-Acc: 11: Accuracy: 0.9299356341: precision: 0.8243571082: recall: 0.5584774177: f1: 0.6658567174
12: 6464: loss: 0.1815398151:
12: 12864: loss: 0.1772363091:
12: 19264: loss: 0.1789061139:
12: 25664: loss: 0.1806396467:
12: 32064: loss: 0.1802997951:
12: 38464: loss: 0.1798040547:
12: 44864: loss: 0.1801042465:
12: 51264: loss: 0.1793205547:
12: 57664: loss: 0.1791401845:
12: 64064: loss: 0.1780800327:
12: 70464: loss: 0.1796169616:
12: 76864: loss: 0.1793070249:
12: 83264: loss: 0.1796218999:
12: 89664: loss: 0.1795927334:
12: 96064: loss: 0.1798327437:
12: 102464: loss: 0.1790584240:
12: 108864: loss: 0.1789296217:
12: 115264: loss: 0.1792967099:
12: 121664: loss: 0.1787225670:
Dev-Acc: 12: Accuracy: 0.9121090770: precision: 0.3250675755: recall: 0.4703281755: f1: 0.3844336345
Train-Acc: 12: Accuracy: 0.9317271113: precision: 0.8288082309: recall: 0.5719545066: f1: 0.6768321145
13: 6464: loss: 0.1764581924:
13: 12864: loss: 0.1748232229:
13: 19264: loss: 0.1751329456:
13: 25664: loss: 0.1781287828:
13: 32064: loss: 0.1773848921:
13: 38464: loss: 0.1751129003:
13: 44864: loss: 0.1748017712:
13: 51264: loss: 0.1735761070:
13: 57664: loss: 0.1740501119:
13: 64064: loss: 0.1744496750:
13: 70464: loss: 0.1736173221:
13: 76864: loss: 0.1735630365:
13: 83264: loss: 0.1743698942:
13: 89664: loss: 0.1746954137:
13: 96064: loss: 0.1747189228:
13: 102464: loss: 0.1748570046:
13: 108864: loss: 0.1749918716:
13: 115264: loss: 0.1748702812:
13: 121664: loss: 0.1749447223:
Dev-Acc: 13: Accuracy: 0.9092712998: precision: 0.3143279845: recall: 0.4696480190: f1: 0.3766021271
Train-Acc: 13: Accuracy: 0.9336500168: precision: 0.8344737089: recall: 0.5853001118: f1: 0.6880216383
14: 6464: loss: 0.1634583846:
14: 12864: loss: 0.1706570627:
14: 19264: loss: 0.1726559238:
14: 25664: loss: 0.1742797770:
14: 32064: loss: 0.1723955489:
14: 38464: loss: 0.1721866158:
14: 44864: loss: 0.1722350198:
14: 51264: loss: 0.1725744764:
14: 57664: loss: 0.1721095096:
14: 64064: loss: 0.1720443197:
14: 70464: loss: 0.1722239016:
14: 76864: loss: 0.1729107639:
14: 83264: loss: 0.1724080166:
14: 89664: loss: 0.1726232343:
14: 96064: loss: 0.1722343494:
14: 102464: loss: 0.1720781883:
14: 108864: loss: 0.1721811582:
14: 115264: loss: 0.1721543493:
14: 121664: loss: 0.1718697455:
Dev-Acc: 14: Accuracy: 0.9075051546: precision: 0.3067505335: recall: 0.4643768067: f1: 0.3694534632
Train-Acc: 14: Accuracy: 0.9351539016: precision: 0.8438556933: recall: 0.5904937216: f1: 0.6947979114
15: 6464: loss: 0.1780683196:
15: 12864: loss: 0.1750320360:
15: 19264: loss: 0.1725177920:
15: 25664: loss: 0.1742451543:
15: 32064: loss: 0.1728687154:
15: 38464: loss: 0.1725375897:
15: 44864: loss: 0.1722806154:
15: 51264: loss: 0.1713285322:
15: 57664: loss: 0.1716260641:
15: 64064: loss: 0.1706088453:
15: 70464: loss: 0.1700188144:
15: 76864: loss: 0.1700779507:
15: 83264: loss: 0.1693219895:
15: 89664: loss: 0.1695197467:
15: 96064: loss: 0.1696326163:
15: 102464: loss: 0.1692635713:
15: 108864: loss: 0.1693074805:
15: 115264: loss: 0.1692599049:
15: 121664: loss: 0.1689126491:
Dev-Acc: 15: Accuracy: 0.9026333690: precision: 0.2940930038: recall: 0.4774698181: f1: 0.3639898892
Train-Acc: 15: Accuracy: 0.9365509152: precision: 0.8394054740: recall: 0.6089014529: f1: 0.7058106306
16: 6464: loss: 0.1831982823:
16: 12864: loss: 0.1733983467:
16: 19264: loss: 0.1720720158:
16: 25664: loss: 0.1680672493:
16: 32064: loss: 0.1673209778:
16: 38464: loss: 0.1666555803:
16: 44864: loss: 0.1664780859:
16: 51264: loss: 0.1669390323:
16: 57664: loss: 0.1657875294:
16: 64064: loss: 0.1663384400:
16: 70464: loss: 0.1659076846:
16: 76864: loss: 0.1666124872:
16: 83264: loss: 0.1665037988:
16: 89664: loss: 0.1661541434:
16: 96064: loss: 0.1662381888:
16: 102464: loss: 0.1665755394:
16: 108864: loss: 0.1664684964:
16: 115264: loss: 0.1662010060:
16: 121664: loss: 0.1657649576:
Dev-Acc: 16: Accuracy: 0.9011152387: precision: 0.2879684418: recall: 0.4716884884: f1: 0.3576124791
Train-Acc: 16: Accuracy: 0.9374712706: precision: 0.8455454545: recall: 0.6114653869: f1: 0.7097020335
17: 6464: loss: 0.1643447163:
17: 12864: loss: 0.1668203901:
17: 19264: loss: 0.1682409260:
17: 25664: loss: 0.1694050890:
17: 32064: loss: 0.1672200609:
17: 38464: loss: 0.1680092802:
17: 44864: loss: 0.1662068531:
17: 51264: loss: 0.1656783266:
17: 57664: loss: 0.1669338055:
17: 64064: loss: 0.1664089367:
17: 70464: loss: 0.1663953211:
17: 76864: loss: 0.1656297515:
17: 83264: loss: 0.1664838325:
17: 89664: loss: 0.1656524363:
17: 96064: loss: 0.1654496451:
17: 102464: loss: 0.1647297799:
17: 108864: loss: 0.1641479088:
17: 115264: loss: 0.1642214528:
17: 121664: loss: 0.1634805387:
Dev-Acc: 17: Accuracy: 0.8991010189: precision: 0.2822908205: recall: 0.4727087230: f1: 0.3534871893
Train-Acc: 17: Accuracy: 0.9386382103: precision: 0.8472022956: recall: 0.6211294458: f1: 0.7167621287
18: 6464: loss: 0.1675872921:
18: 12864: loss: 0.1634929362:
18: 19264: loss: 0.1625946878:
18: 25664: loss: 0.1623298350:
18: 32064: loss: 0.1627727650:
18: 38464: loss: 0.1640051057:
18: 44864: loss: 0.1636230146:
18: 51264: loss: 0.1630726609:
18: 57664: loss: 0.1632962567:
18: 64064: loss: 0.1627000955:
18: 70464: loss: 0.1625984158:
18: 76864: loss: 0.1626528723:
18: 83264: loss: 0.1617833456:
18: 89664: loss: 0.1616696476:
18: 96064: loss: 0.1606431405:
18: 102464: loss: 0.1607013204:
18: 108864: loss: 0.1607452695:
18: 115264: loss: 0.1612946553:
18: 121664: loss: 0.1610213758:
Dev-Acc: 18: Accuracy: 0.8953703046: precision: 0.2740310078: recall: 0.4808706002: f1: 0.3491142522
Train-Acc: 18: Accuracy: 0.9397969246: precision: 0.8473874350: recall: 0.6322398264: f1: 0.7241716867
19: 6464: loss: 0.1544791107:
19: 12864: loss: 0.1581521261:
19: 19264: loss: 0.1582302147:
19: 25664: loss: 0.1581851547:
19: 32064: loss: 0.1603776754:
19: 38464: loss: 0.1589535138:
19: 44864: loss: 0.1582077153:
19: 51264: loss: 0.1581566423:
19: 57664: loss: 0.1587002745:
19: 64064: loss: 0.1590693168:
19: 70464: loss: 0.1590098905:
19: 76864: loss: 0.1592023916:
19: 83264: loss: 0.1593729921:
19: 89664: loss: 0.1593913285:
19: 96064: loss: 0.1591855422:
19: 102464: loss: 0.1594519620:
19: 108864: loss: 0.1588789376:
19: 115264: loss: 0.1591237354:
19: 121664: loss: 0.1584546955:
Dev-Acc: 19: Accuracy: 0.8932766914: precision: 0.2689354441: recall: 0.4824009522: f1: 0.3453438831
Train-Acc: 19: Accuracy: 0.9403474927: precision: 0.8457992694: recall: 0.6393399514: f1: 0.7282189524
20: 6464: loss: 0.1640192138:
20: 12864: loss: 0.1600443592:
20: 19264: loss: 0.1586465770:
20: 25664: loss: 0.1587284808:
20: 32064: loss: 0.1591896950:
20: 38464: loss: 0.1588008019:
20: 44864: loss: 0.1570429310:
20: 51264: loss: 0.1573292502:
20: 57664: loss: 0.1583687721:
20: 64064: loss: 0.1581126513:
20: 70464: loss: 0.1583643555:
20: 76864: loss: 0.1589483731:
20: 83264: loss: 0.1579003524:
20: 89664: loss: 0.1585665334:
20: 96064: loss: 0.1584027486:
20: 102464: loss: 0.1579913913:
20: 108864: loss: 0.1579098438:
20: 115264: loss: 0.1572788334:
20: 121664: loss: 0.1566997140:
Dev-Acc: 20: Accuracy: 0.8956878185: precision: 0.2726737338: recall: 0.4723686448: f1: 0.3457589147
Train-Acc: 20: Accuracy: 0.9406186938: precision: 0.8590054851: recall: 0.6280323450: f1: 0.7255810421
