1: 3232: loss: 0.7068718487:
1: 6432: loss: 0.7063348055:
1: 9632: loss: 0.7051870642:
1: 12832: loss: 0.7043769459:
1: 16032: loss: 0.7034020436:
1: 19232: loss: 0.7026152936:
1: 22432: loss: 0.7020176870:
1: 25632: loss: 0.7013365881:
1: 28832: loss: 0.7005833348:
1: 32032: loss: 0.6996372836:
1: 35232: loss: 0.6989054259:
1: 38432: loss: 0.6982270889:
1: 41632: loss: 0.6974227142:
1: 44832: loss: 0.6967066399:
1: 48032: loss: 0.6958791595:
1: 51232: loss: 0.6951980587:
1: 54432: loss: 0.6944344595:
1: 57632: loss: 0.6938123621:
1: 60832: loss: 0.6930908548:
1: 64032: loss: 0.6923712539:
1: 67232: loss: 0.6916680370:
1: 70432: loss: 0.6909044956:
1: 73632: loss: 0.6902155048:
1: 76832: loss: 0.6894534286:
1: 80032: loss: 0.6887057102:
1: 83232: loss: 0.6879535543:
1: 86432: loss: 0.6872804705:
1: 89632: loss: 0.6865139532:
1: 92832: loss: 0.6857803238:
1: 96032: loss: 0.6850452602:
1: 99232: loss: 0.6843352371:
1: 102432: loss: 0.6836367743:
1: 105632: loss: 0.6829412204:
Dev-Acc: 1: Accuracy: 0.8600472212: precision: 0.0878933654: recall: 0.1491242986: f1: 0.1105996595
Train-Acc: 1: Accuracy: 0.8272960186: precision: 0.3247684164: recall: 0.1936098876: f1: 0.2425964826
2: 3232: loss: 0.6587429065:
2: 6432: loss: 0.6576669326:
2: 9632: loss: 0.6572669635:
2: 12832: loss: 0.6563652489:
2: 16032: loss: 0.6555088596:
2: 19232: loss: 0.6549285064:
2: 22432: loss: 0.6541985283:
2: 25632: loss: 0.6536529309:
2: 28832: loss: 0.6527620879:
2: 32032: loss: 0.6521026273:
2: 35232: loss: 0.6515169558:
2: 38432: loss: 0.6508782626:
2: 41632: loss: 0.6502962634:
2: 44832: loss: 0.6496062143:
2: 48032: loss: 0.6489623110:
2: 51232: loss: 0.6484179424:
2: 54432: loss: 0.6477227955:
2: 57632: loss: 0.6470837800:
2: 60832: loss: 0.6465121919:
2: 64032: loss: 0.6458098465:
2: 67232: loss: 0.6451189385:
2: 70432: loss: 0.6445135020:
2: 73632: loss: 0.6438366368:
2: 76832: loss: 0.6432224937:
2: 80032: loss: 0.6425408731:
2: 83232: loss: 0.6418656338:
2: 86432: loss: 0.6412339460:
2: 89632: loss: 0.6406013743:
2: 92832: loss: 0.6399182392:
2: 96032: loss: 0.6393259588:
2: 99232: loss: 0.6385582751:
2: 102432: loss: 0.6379182788:
2: 105632: loss: 0.6372641832:
Dev-Acc: 2: Accuracy: 0.9410322905: precision: 0.1900000000: recall: 0.0032307431: f1: 0.0063534526
Train-Acc: 2: Accuracy: 0.8579787016: precision: 0.6630036630: recall: 0.0118992834: f1: 0.0233789718
3: 3232: loss: 0.6129398179:
3: 6432: loss: 0.6137961781:
3: 9632: loss: 0.6134730367:
3: 12832: loss: 0.6118154620:
3: 16032: loss: 0.6114015363:
3: 19232: loss: 0.6110175385:
3: 22432: loss: 0.6103998379:
3: 25632: loss: 0.6099538325:
3: 28832: loss: 0.6092370583:
3: 32032: loss: 0.6088357086:
3: 35232: loss: 0.6081095109:
3: 38432: loss: 0.6075546627:
3: 41632: loss: 0.6066881796:
3: 44832: loss: 0.6061751306:
3: 48032: loss: 0.6055881344:
3: 51232: loss: 0.6051155221:
3: 54432: loss: 0.6046955782:
3: 57632: loss: 0.6039704447:
3: 60832: loss: 0.6035856763:
3: 64032: loss: 0.6029493630:
3: 67232: loss: 0.6023550700:
3: 70432: loss: 0.6017623415:
3: 73632: loss: 0.6011719044:
3: 76832: loss: 0.6005154114:
3: 80032: loss: 0.5998519908:
3: 83232: loss: 0.5993177587:
3: 86432: loss: 0.5988095785:
3: 89632: loss: 0.5983046898:
3: 92832: loss: 0.5976726090:
3: 96032: loss: 0.5970312935:
3: 99232: loss: 0.5964674143:
3: 102432: loss: 0.5956604075:
3: 105632: loss: 0.5950957160:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8572273850: precision: 1.0000000000: recall: 0.0005916771: f1: 0.0011826544
4: 3232: loss: 0.5762108192:
4: 6432: loss: 0.5716735421:
4: 9632: loss: 0.5724200297:
4: 12832: loss: 0.5726130948:
4: 16032: loss: 0.5713787253:
4: 19232: loss: 0.5703684408:
4: 22432: loss: 0.5697026218:
4: 25632: loss: 0.5694229767:
4: 28832: loss: 0.5695345053:
4: 32032: loss: 0.5687375199:
4: 35232: loss: 0.5684481990:
4: 38432: loss: 0.5678690212:
4: 41632: loss: 0.5670865710:
4: 44832: loss: 0.5666102160:
4: 48032: loss: 0.5658600673:
4: 51232: loss: 0.5652914915:
4: 54432: loss: 0.5648695663:
4: 57632: loss: 0.5640836221:
4: 60832: loss: 0.5633107776:
4: 64032: loss: 0.5630218850:
4: 67232: loss: 0.5624409177:
4: 70432: loss: 0.5617714113:
4: 73632: loss: 0.5609945224:
4: 76832: loss: 0.5605263906:
4: 80032: loss: 0.5601058247:
4: 83232: loss: 0.5596435941:
4: 86432: loss: 0.5590088209:
4: 89632: loss: 0.5583871027:
4: 92832: loss: 0.5578169986:
4: 96032: loss: 0.5571121256:
4: 99232: loss: 0.5567250018:
4: 102432: loss: 0.5561283642:
4: 105632: loss: 0.5556418917:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.5370616111:
5: 6432: loss: 0.5362548937:
5: 9632: loss: 0.5346345790:
5: 12832: loss: 0.5345730250:
5: 16032: loss: 0.5345986465:
5: 19232: loss: 0.5342916504:
5: 22432: loss: 0.5333658345:
5: 25632: loss: 0.5327492965:
5: 28832: loss: 0.5324200148:
5: 32032: loss: 0.5315462230:
5: 35232: loss: 0.5313097426:
5: 38432: loss: 0.5308395737:
5: 41632: loss: 0.5300542141:
5: 44832: loss: 0.5295001494:
5: 48032: loss: 0.5287713779:
5: 51232: loss: 0.5283403089:
5: 54432: loss: 0.5278696858:
5: 57632: loss: 0.5276512081:
5: 60832: loss: 0.5271672494:
5: 64032: loss: 0.5263853446:
5: 67232: loss: 0.5260962964:
5: 70432: loss: 0.5256759098:
5: 73632: loss: 0.5252537145:
5: 76832: loss: 0.5245260332:
5: 80032: loss: 0.5238775268:
5: 83232: loss: 0.5234151664:
5: 86432: loss: 0.5230241466:
5: 89632: loss: 0.5224885605:
5: 92832: loss: 0.5220758053:
5: 96032: loss: 0.5212234771:
5: 99232: loss: 0.5205895146:
5: 102432: loss: 0.5200446737:
5: 105632: loss: 0.5195625265:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.5048070127:
6: 6432: loss: 0.5033156431:
6: 9632: loss: 0.5025130797:
6: 12832: loss: 0.5006808048:
6: 16032: loss: 0.5007488001:
6: 19232: loss: 0.4992941767:
6: 22432: loss: 0.4989577052:
6: 25632: loss: 0.4983061352:
6: 28832: loss: 0.4980483121:
6: 32032: loss: 0.4977427047:
6: 35232: loss: 0.4970505875:
6: 38432: loss: 0.4963972665:
6: 41632: loss: 0.4963018680:
6: 44832: loss: 0.4951643721:
6: 48032: loss: 0.4945136373:
6: 51232: loss: 0.4941141030:
6: 54432: loss: 0.4932189271:
6: 57632: loss: 0.4924624805:
6: 60832: loss: 0.4920531602:
6: 64032: loss: 0.4915924693:
6: 67232: loss: 0.4913432750:
6: 70432: loss: 0.4911633508:
6: 73632: loss: 0.4908202838:
6: 76832: loss: 0.4903897140:
6: 80032: loss: 0.4899198917:
6: 83232: loss: 0.4890731734:
6: 86432: loss: 0.4888189655:
6: 89632: loss: 0.4883733052:
6: 92832: loss: 0.4881476301:
6: 96032: loss: 0.4875461150:
6: 99232: loss: 0.4868289667:
6: 102432: loss: 0.4866341870:
6: 105632: loss: 0.4862828884:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.4720939782:
7: 6432: loss: 0.4686055495:
7: 9632: loss: 0.4721983637:
7: 12832: loss: 0.4704418100:
7: 16032: loss: 0.4687574446:
7: 19232: loss: 0.4692451298:
7: 22432: loss: 0.4695983091:
7: 25632: loss: 0.4685908314:
7: 28832: loss: 0.4678133228:
7: 32032: loss: 0.4676779708:
7: 35232: loss: 0.4680698134:
7: 38432: loss: 0.4669119481:
7: 41632: loss: 0.4660446716:
7: 44832: loss: 0.4658525017:
7: 48032: loss: 0.4651171208:
7: 51232: loss: 0.4646339244:
7: 54432: loss: 0.4643358862:
7: 57632: loss: 0.4639269865:
7: 60832: loss: 0.4632137231:
7: 64032: loss: 0.4631759924:
7: 67232: loss: 0.4628625713:
7: 70432: loss: 0.4623471721:
7: 73632: loss: 0.4615282708:
7: 76832: loss: 0.4612109643:
7: 80032: loss: 0.4604957135:
7: 83232: loss: 0.4602811543:
7: 86432: loss: 0.4596224919:
7: 89632: loss: 0.4590183559:
7: 92832: loss: 0.4584503940:
7: 96032: loss: 0.4579554511:
7: 99232: loss: 0.4577660041:
7: 102432: loss: 0.4574266123:
7: 105632: loss: 0.4570418854:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.4458869594:
8: 6432: loss: 0.4438831809:
8: 9632: loss: 0.4430794855:
8: 12832: loss: 0.4426062220:
8: 16032: loss: 0.4411635557:
8: 19232: loss: 0.4385517200:
8: 22432: loss: 0.4383957025:
8: 25632: loss: 0.4391334440:
8: 28832: loss: 0.4386181861:
8: 32032: loss: 0.4380065192:
8: 35232: loss: 0.4362627312:
8: 38432: loss: 0.4359399695:
8: 41632: loss: 0.4366537909:
8: 44832: loss: 0.4367519407:
8: 48032: loss: 0.4363661947:
8: 51232: loss: 0.4359027353:
8: 54432: loss: 0.4353007161:
8: 57632: loss: 0.4353725866:
8: 60832: loss: 0.4351777747:
8: 64032: loss: 0.4348722023:
8: 67232: loss: 0.4347781401:
8: 70432: loss: 0.4345148066:
8: 73632: loss: 0.4343228598:
8: 76832: loss: 0.4338135642:
8: 80032: loss: 0.4339900314:
8: 83232: loss: 0.4341797362:
8: 86432: loss: 0.4336397067:
8: 89632: loss: 0.4335788366:
8: 92832: loss: 0.4329023525:
8: 96032: loss: 0.4322171025:
8: 99232: loss: 0.4317986481:
8: 102432: loss: 0.4314457287:
8: 105632: loss: 0.4310399482:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.4125023463:
9: 6432: loss: 0.4134137392:
9: 9632: loss: 0.4130690074:
9: 12832: loss: 0.4131271937:
9: 16032: loss: 0.4142498986:
9: 19232: loss: 0.4139981640:
9: 22432: loss: 0.4141577742:
9: 25632: loss: 0.4147545270:
9: 28832: loss: 0.4145029554:
9: 32032: loss: 0.4146071126:
9: 35232: loss: 0.4139382806:
9: 38432: loss: 0.4145528535:
9: 41632: loss: 0.4138892632:
9: 44832: loss: 0.4142745963:
9: 48032: loss: 0.4139026278:
9: 51232: loss: 0.4140584313:
9: 54432: loss: 0.4139765518:
9: 57632: loss: 0.4133523202:
9: 60832: loss: 0.4135300099:
9: 64032: loss: 0.4126261076:
9: 67232: loss: 0.4125859968:
9: 70432: loss: 0.4119562164:
9: 73632: loss: 0.4107854915:
9: 76832: loss: 0.4108528607:
9: 80032: loss: 0.4103966280:
9: 83232: loss: 0.4103184692:
9: 86432: loss: 0.4105266112:
9: 89632: loss: 0.4102779482:
9: 92832: loss: 0.4100846276:
9: 96032: loss: 0.4096590402:
9: 99232: loss: 0.4093497475:
9: 102432: loss: 0.4092433450:
9: 105632: loss: 0.4090730626:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.3954370221:
10: 6432: loss: 0.3962918787:
10: 9632: loss: 0.3993481532:
10: 12832: loss: 0.3960104250:
10: 16032: loss: 0.3965488466:
10: 19232: loss: 0.3968276992:
10: 22432: loss: 0.3971728802:
10: 25632: loss: 0.3957767515:
10: 28832: loss: 0.3964426686:
10: 32032: loss: 0.3962003875:
10: 35232: loss: 0.3951460872:
10: 38432: loss: 0.3944982744:
10: 41632: loss: 0.3948833831:
10: 44832: loss: 0.3942188212:
10: 48032: loss: 0.3931482792:
10: 51232: loss: 0.3933742220:
10: 54432: loss: 0.3923484337:
10: 57632: loss: 0.3920259683:
10: 60832: loss: 0.3928849171:
10: 64032: loss: 0.3927434755:
10: 67232: loss: 0.3924847126:
10: 70432: loss: 0.3920842287:
10: 73632: loss: 0.3922508735:
10: 76832: loss: 0.3922432759:
10: 80032: loss: 0.3928352956:
10: 83232: loss: 0.3927235470:
10: 86432: loss: 0.3927157641:
10: 89632: loss: 0.3925734112:
10: 92832: loss: 0.3918367662:
10: 96032: loss: 0.3911300346:
10: 99232: loss: 0.3908810273:
10: 102432: loss: 0.3905175814:
10: 105632: loss: 0.3901350527:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.3791150874:
11: 6432: loss: 0.3726468806:
11: 9632: loss: 0.3793514772:
11: 12832: loss: 0.3791362965:
11: 16032: loss: 0.3783494027:
11: 19232: loss: 0.3772268900:
11: 22432: loss: 0.3785392705:
11: 25632: loss: 0.3804913141:
11: 28832: loss: 0.3789051932:
11: 32032: loss: 0.3789430653:
11: 35232: loss: 0.3793312378:
11: 38432: loss: 0.3792866423:
11: 41632: loss: 0.3791527481:
11: 44832: loss: 0.3785942515:
11: 48032: loss: 0.3782545004:
11: 51232: loss: 0.3777292441:
11: 54432: loss: 0.3779184096:
11: 57632: loss: 0.3776318126:
11: 60832: loss: 0.3778644403:
11: 64032: loss: 0.3774693794:
11: 67232: loss: 0.3775077777:
11: 70432: loss: 0.3772418466:
11: 73632: loss: 0.3764885063:
11: 76832: loss: 0.3763075636:
11: 80032: loss: 0.3760124375:
11: 83232: loss: 0.3760380511:
11: 86432: loss: 0.3758521493:
11: 89632: loss: 0.3755726853:
11: 92832: loss: 0.3755956789:
11: 96032: loss: 0.3749323709:
11: 99232: loss: 0.3745551779:
11: 102432: loss: 0.3742210398:
11: 105632: loss: 0.3738181892:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.3616600214:
12: 6432: loss: 0.3683223713:
12: 9632: loss: 0.3702149453:
12: 12832: loss: 0.3701344951:
12: 16032: loss: 0.3698357615:
12: 19232: loss: 0.3680825886:
12: 22432: loss: 0.3675816956:
12: 25632: loss: 0.3687078835:
12: 28832: loss: 0.3683222587:
12: 32032: loss: 0.3683137206:
12: 35232: loss: 0.3673785307:
12: 38432: loss: 0.3659015367:
12: 41632: loss: 0.3647887751:
12: 44832: loss: 0.3644245624:
12: 48032: loss: 0.3637810328:
12: 51232: loss: 0.3634509792:
12: 54432: loss: 0.3636243976:
12: 57632: loss: 0.3630087940:
12: 60832: loss: 0.3625466498:
12: 64032: loss: 0.3626116904:
12: 67232: loss: 0.3618977370:
12: 70432: loss: 0.3616300056:
12: 73632: loss: 0.3612098813:
12: 76832: loss: 0.3612888209:
12: 80032: loss: 0.3613874407:
12: 83232: loss: 0.3606104144:
12: 86432: loss: 0.3607275265:
12: 89632: loss: 0.3602755010:
12: 92832: loss: 0.3600719580:
12: 96032: loss: 0.3601284230:
12: 99232: loss: 0.3599874179:
12: 102432: loss: 0.3599352616:
12: 105632: loss: 0.3596666212:
Dev-Acc: 12: Accuracy: 0.9416673183: precision: 0.7500000000: recall: 0.0005101173: f1: 0.0010195412
Train-Acc: 12: Accuracy: 0.8575279117: precision: 1.0000000000: recall: 0.0026954178: f1: 0.0053763441
13: 3232: loss: 0.3529801969:
13: 6432: loss: 0.3581276298:
13: 9632: loss: 0.3596283262:
13: 12832: loss: 0.3596470957:
13: 16032: loss: 0.3563666192:
13: 19232: loss: 0.3576945848:
13: 22432: loss: 0.3546784393:
13: 25632: loss: 0.3544488467:
13: 28832: loss: 0.3552559782:
13: 32032: loss: 0.3547563821:
13: 35232: loss: 0.3542568535:
13: 38432: loss: 0.3525904048:
13: 41632: loss: 0.3517367848:
13: 44832: loss: 0.3516608336:
13: 48032: loss: 0.3514862722:
13: 51232: loss: 0.3510133918:
13: 54432: loss: 0.3494444365:
13: 57632: loss: 0.3490570237:
13: 60832: loss: 0.3484514053:
13: 64032: loss: 0.3482675679:
13: 67232: loss: 0.3475976124:
13: 70432: loss: 0.3481252004:
13: 73632: loss: 0.3478405242:
13: 76832: loss: 0.3481542699:
13: 80032: loss: 0.3478257153:
13: 83232: loss: 0.3481848650:
13: 86432: loss: 0.3480711346:
13: 89632: loss: 0.3481204114:
13: 92832: loss: 0.3479356896:
13: 96032: loss: 0.3478015720:
13: 99232: loss: 0.3477149648:
13: 102432: loss: 0.3475554775:
13: 105632: loss: 0.3472045203:
Dev-Acc: 13: Accuracy: 0.9417467117: precision: 0.8571428571: recall: 0.0020404693: f1: 0.0040712468
Train-Acc: 13: Accuracy: 0.8587676287: precision: 0.9576719577: recall: 0.0118992834: f1: 0.0235064935
14: 3232: loss: 0.3416733831:
14: 6432: loss: 0.3355677386:
14: 9632: loss: 0.3387625180:
14: 12832: loss: 0.3389138978:
14: 16032: loss: 0.3389830133:
14: 19232: loss: 0.3378914269:
14: 22432: loss: 0.3406840601:
14: 25632: loss: 0.3410488172:
14: 28832: loss: 0.3424471455:
14: 32032: loss: 0.3412608263:
14: 35232: loss: 0.3393933469:
14: 38432: loss: 0.3382811473:
14: 41632: loss: 0.3380043673:
14: 44832: loss: 0.3379901250:
14: 48032: loss: 0.3378327500:
14: 51232: loss: 0.3376004601:
14: 54432: loss: 0.3381384998:
14: 57632: loss: 0.3371679291:
14: 60832: loss: 0.3372471677:
14: 64032: loss: 0.3379790836:
14: 67232: loss: 0.3381687777:
14: 70432: loss: 0.3378212795:
14: 73632: loss: 0.3373627131:
14: 76832: loss: 0.3374716472:
14: 80032: loss: 0.3372581552:
14: 83232: loss: 0.3372409647:
14: 86432: loss: 0.3371862395:
14: 89632: loss: 0.3371054943:
14: 92832: loss: 0.3369852239:
14: 96032: loss: 0.3364091219:
14: 99232: loss: 0.3361712591:
14: 102432: loss: 0.3361553427:
14: 105632: loss: 0.3358900671:
Dev-Acc: 14: Accuracy: 0.9421932101: precision: 0.6410256410: recall: 0.0212548886: f1: 0.0411454905
Train-Acc: 14: Accuracy: 0.8638203740: precision: 0.8572864322: recall: 0.0560778384: f1: 0.1052696532
15: 3232: loss: 0.3366952530:
15: 6432: loss: 0.3354621612:
15: 9632: loss: 0.3374299560:
15: 12832: loss: 0.3341612522:
15: 16032: loss: 0.3293822232:
15: 19232: loss: 0.3295005216:
15: 22432: loss: 0.3288515174:
15: 25632: loss: 0.3284720914:
15: 28832: loss: 0.3275800668:
15: 32032: loss: 0.3302158930:
15: 35232: loss: 0.3313404738:
15: 38432: loss: 0.3314624571:
15: 41632: loss: 0.3308553534:
15: 44832: loss: 0.3305351469:
15: 48032: loss: 0.3301443696:
15: 51232: loss: 0.3294748332:
15: 54432: loss: 0.3291355537:
15: 57632: loss: 0.3289154886:
15: 60832: loss: 0.3286258862:
15: 64032: loss: 0.3284552591:
15: 67232: loss: 0.3277065894:
15: 70432: loss: 0.3273555649:
15: 73632: loss: 0.3275293509:
15: 76832: loss: 0.3272186284:
15: 80032: loss: 0.3273032151:
15: 83232: loss: 0.3275052329:
15: 86432: loss: 0.3273993161:
15: 89632: loss: 0.3273764794:
15: 92832: loss: 0.3271032599:
15: 96032: loss: 0.3267773893:
15: 99232: loss: 0.3265367542:
15: 102432: loss: 0.3266300528:
15: 105632: loss: 0.3264755801:
Dev-Acc: 15: Accuracy: 0.9423717856: precision: 0.5758835759: recall: 0.0471008332: f1: 0.0870795347
Train-Acc: 15: Accuracy: 0.8675112724: precision: 0.8694779116: recall: 0.0853987246: f1: 0.1555222987
16: 3232: loss: 0.3174660562:
16: 6432: loss: 0.3185499393:
16: 9632: loss: 0.3214318161:
16: 12832: loss: 0.3226884225:
16: 16032: loss: 0.3199383901:
16: 19232: loss: 0.3199236230:
16: 22432: loss: 0.3185417386:
16: 25632: loss: 0.3193551930:
16: 28832: loss: 0.3199515679:
16: 32032: loss: 0.3199565987:
16: 35232: loss: 0.3197965131:
16: 38432: loss: 0.3197773004:
16: 41632: loss: 0.3192436141:
16: 44832: loss: 0.3188764534:
16: 48032: loss: 0.3195810974:
16: 51232: loss: 0.3191868489:
16: 54432: loss: 0.3192571443:
16: 57632: loss: 0.3189495300:
16: 60832: loss: 0.3187450202:
16: 64032: loss: 0.3192458300:
16: 67232: loss: 0.3195079817:
16: 70432: loss: 0.3193474636:
16: 73632: loss: 0.3186255138:
16: 76832: loss: 0.3181113174:
16: 80032: loss: 0.3178804921:
16: 83232: loss: 0.3177902251:
16: 86432: loss: 0.3175887349:
16: 89632: loss: 0.3175807975:
16: 92832: loss: 0.3173319366:
16: 96032: loss: 0.3169525919:
16: 99232: loss: 0.3175507365:
16: 102432: loss: 0.3174248433:
16: 105632: loss: 0.3175698574:
Dev-Acc: 16: Accuracy: 0.9423122406: precision: 0.5435630689: recall: 0.0710763476: f1: 0.1257142857
Train-Acc: 16: Accuracy: 0.8721695542: precision: 0.8913894325: recall: 0.1197817369: f1: 0.2111851637
17: 3232: loss: 0.3136497158:
17: 6432: loss: 0.3129095512:
17: 9632: loss: 0.3119881186:
17: 12832: loss: 0.3133015312:
17: 16032: loss: 0.3129607967:
17: 19232: loss: 0.3129491006:
17: 22432: loss: 0.3125649874:
17: 25632: loss: 0.3141996825:
17: 28832: loss: 0.3128107193:
17: 32032: loss: 0.3130252381:
17: 35232: loss: 0.3113732232:
17: 38432: loss: 0.3108980211:
17: 41632: loss: 0.3112652310:
17: 44832: loss: 0.3105668728:
17: 48032: loss: 0.3103926098:
17: 51232: loss: 0.3107155835:
17: 54432: loss: 0.3110922729:
17: 57632: loss: 0.3115237642:
17: 60832: loss: 0.3114250486:
17: 64032: loss: 0.3108648527:
17: 67232: loss: 0.3104362210:
17: 70432: loss: 0.3100187675:
17: 73632: loss: 0.3096519987:
17: 76832: loss: 0.3102731273:
17: 80032: loss: 0.3100903445:
17: 83232: loss: 0.3097856921:
17: 86432: loss: 0.3096777478:
17: 89632: loss: 0.3096023606:
17: 92832: loss: 0.3095796553:
17: 96032: loss: 0.3093917478:
17: 99232: loss: 0.3094342483:
17: 102432: loss: 0.3091020417:
17: 105632: loss: 0.3091797997:
Dev-Acc: 17: Accuracy: 0.9424908757: precision: 0.5405921681: recall: 0.0962421357: f1: 0.1633949192
Train-Acc: 17: Accuracy: 0.8750716448: precision: 0.8916700862: recall: 0.1428571429: f1: 0.2462601995
18: 3232: loss: 0.3035776798:
18: 6432: loss: 0.3105614568:
18: 9632: loss: 0.3035307641:
18: 12832: loss: 0.3057743973:
18: 16032: loss: 0.3055526541:
18: 19232: loss: 0.3059320479:
18: 22432: loss: 0.3020726115:
18: 25632: loss: 0.3013589147:
18: 28832: loss: 0.3011574519:
18: 32032: loss: 0.3020250935:
18: 35232: loss: 0.3020256244:
18: 38432: loss: 0.3016613900:
18: 41632: loss: 0.3013731159:
18: 44832: loss: 0.3015466581:
18: 48032: loss: 0.3011211261:
18: 51232: loss: 0.3016388765:
18: 54432: loss: 0.3009410693:
18: 57632: loss: 0.3012763609:
18: 60832: loss: 0.3012074801:
18: 64032: loss: 0.3016104277:
18: 67232: loss: 0.3008831367:
18: 70432: loss: 0.3002530069:
18: 73632: loss: 0.3008151506:
18: 76832: loss: 0.3012967744:
18: 80032: loss: 0.3012996430:
18: 83232: loss: 0.3011485640:
18: 86432: loss: 0.3013513967:
18: 89632: loss: 0.3014902445:
18: 92832: loss: 0.3017075958:
18: 96032: loss: 0.3019911420:
18: 99232: loss: 0.3021948957:
18: 102432: loss: 0.3019133497:
18: 105632: loss: 0.3016085641:
Dev-Acc: 18: Accuracy: 0.9429075718: precision: 0.5415304120: recall: 0.1407923822: f1: 0.2234817814
Train-Acc: 18: Accuracy: 0.8800867796: precision: 0.9059488202: recall: 0.1792124121: f1: 0.2992316136
19: 3232: loss: 0.3117444220:
19: 6432: loss: 0.3031695249:
19: 9632: loss: 0.2998023659:
19: 12832: loss: 0.2999867692:
19: 16032: loss: 0.2968184960:
19: 19232: loss: 0.2953096919:
19: 22432: loss: 0.2963042997:
19: 25632: loss: 0.2966829587:
19: 28832: loss: 0.2969983296:
19: 32032: loss: 0.2976032434:
19: 35232: loss: 0.2961486796:
19: 38432: loss: 0.2962984459:
19: 41632: loss: 0.2965572184:
19: 44832: loss: 0.2970439855:
19: 48032: loss: 0.2965211212:
19: 51232: loss: 0.2963226392:
19: 54432: loss: 0.2953038514:
19: 57632: loss: 0.2949086158:
19: 60832: loss: 0.2950150820:
19: 64032: loss: 0.2944339136:
19: 67232: loss: 0.2940291003:
19: 70432: loss: 0.2944263883:
19: 73632: loss: 0.2948934961:
19: 76832: loss: 0.2950550079:
19: 80032: loss: 0.2954153599:
19: 83232: loss: 0.2954679269:
19: 86432: loss: 0.2959503291:
19: 89632: loss: 0.2954516687:
19: 92832: loss: 0.2949817692:
19: 96032: loss: 0.2952583587:
19: 99232: loss: 0.2951651265:
19: 102432: loss: 0.2945399753:
19: 105632: loss: 0.2949827243:
Dev-Acc: 19: Accuracy: 0.9413399100: precision: 0.4935064935: recall: 0.2003060704: f1: 0.2849540397
Train-Acc: 19: Accuracy: 0.8862665296: precision: 0.9118193891: recall: 0.2256919335: f1: 0.3618254637
20: 3232: loss: 0.2877588826:
20: 6432: loss: 0.2907059794:
20: 9632: loss: 0.2925425748:
20: 12832: loss: 0.2904188075:
20: 16032: loss: 0.2897097543:
20: 19232: loss: 0.2915883809:
20: 22432: loss: 0.2912698137:
20: 25632: loss: 0.2922395242:
20: 28832: loss: 0.2918346384:
20: 32032: loss: 0.2921363388:
20: 35232: loss: 0.2906789526:
20: 38432: loss: 0.2914284016:
20: 41632: loss: 0.2912421707:
20: 44832: loss: 0.2906289599:
20: 48032: loss: 0.2901042378:
20: 51232: loss: 0.2900031965:
20: 54432: loss: 0.2900859015:
20: 57632: loss: 0.2893951414:
20: 60832: loss: 0.2899105233:
20: 64032: loss: 0.2893081639:
20: 67232: loss: 0.2898777325:
20: 70432: loss: 0.2900762891:
20: 73632: loss: 0.2901205287:
20: 76832: loss: 0.2903768600:
20: 80032: loss: 0.2898350340:
20: 83232: loss: 0.2902129628:
20: 86432: loss: 0.2896352418:
20: 89632: loss: 0.2896164607:
20: 92832: loss: 0.2898064739:
20: 96032: loss: 0.2893135849:
20: 99232: loss: 0.2888837507:
20: 102432: loss: 0.2885663070:
20: 105632: loss: 0.2881655274:
Dev-Acc: 20: Accuracy: 0.9358925819: precision: 0.4312144213: recall: 0.3091311002: f1: 0.3601069625
Train-Acc: 20: Accuracy: 0.8908496499: precision: 0.8705347925: recall: 0.2771678391: f1: 0.4204647452
21: 3232: loss: 0.2875170974:
21: 6432: loss: 0.2817232657:
21: 9632: loss: 0.2842517439:
21: 12832: loss: 0.2833947477:
21: 16032: loss: 0.2821540693:
21: 19232: loss: 0.2832484893:
21: 22432: loss: 0.2823440693:
21: 25632: loss: 0.2853462084:
21: 28832: loss: 0.2849447050:
21: 32032: loss: 0.2847270694:
21: 35232: loss: 0.2854810174:
21: 38432: loss: 0.2864620885:
21: 41632: loss: 0.2852558086:
21: 44832: loss: 0.2857807455:
21: 48032: loss: 0.2851241937:
21: 51232: loss: 0.2853369032:
21: 54432: loss: 0.2854020905:
21: 57632: loss: 0.2851177121:
21: 60832: loss: 0.2857334746:
21: 64032: loss: 0.2852609920:
21: 67232: loss: 0.2845750342:
21: 70432: loss: 0.2837711892:
21: 73632: loss: 0.2836565552:
21: 76832: loss: 0.2833952626:
21: 80032: loss: 0.2836654027:
21: 83232: loss: 0.2837379251:
21: 86432: loss: 0.2833248238:
21: 89632: loss: 0.2830492405:
21: 92832: loss: 0.2833187421:
21: 96032: loss: 0.2832298152:
21: 99232: loss: 0.2828280559:
21: 102432: loss: 0.2826914361:
21: 105632: loss: 0.2821632111:
Dev-Acc: 21: Accuracy: 0.9270915985: precision: 0.3795764242: recall: 0.3931304200: f1: 0.3862345473
Train-Acc: 21: Accuracy: 0.8929252625: precision: 0.8064671815: recall: 0.3295641312: f1: 0.4679143137
22: 3232: loss: 0.2852167105:
22: 6432: loss: 0.2795662027:
22: 9632: loss: 0.2814277779:
22: 12832: loss: 0.2824590859:
22: 16032: loss: 0.2815964096:
22: 19232: loss: 0.2839564300:
22: 22432: loss: 0.2839759027:
22: 25632: loss: 0.2832254000:
22: 28832: loss: 0.2811478699:
22: 32032: loss: 0.2813311189:
22: 35232: loss: 0.2805280558:
22: 38432: loss: 0.2808411914:
22: 41632: loss: 0.2813604984:
22: 44832: loss: 0.2813203372:
22: 48032: loss: 0.2805014050:
22: 51232: loss: 0.2807458843:
22: 54432: loss: 0.2794123746:
22: 57632: loss: 0.2787621009:
22: 60832: loss: 0.2788080484:
22: 64032: loss: 0.2785512849:
22: 67232: loss: 0.2784091640:
22: 70432: loss: 0.2784554544:
22: 73632: loss: 0.2782069826:
22: 76832: loss: 0.2785288738:
22: 80032: loss: 0.2782524706:
22: 83232: loss: 0.2782003728:
22: 86432: loss: 0.2781255313:
22: 89632: loss: 0.2781693842:
22: 92832: loss: 0.2772200883:
22: 96032: loss: 0.2767768400:
22: 99232: loss: 0.2768772830:
22: 102432: loss: 0.2768543887:
22: 105632: loss: 0.2765506023:
Dev-Acc: 22: Accuracy: 0.9221106172: precision: 0.3577517700: recall: 0.4210168339: f1: 0.3868145602
Train-Acc: 22: Accuracy: 0.8955361247: precision: 0.7896825397: recall: 0.3663138518: f1: 0.5004715498
23: 3232: loss: 0.2779522486:
23: 6432: loss: 0.2794704982:
23: 9632: loss: 0.2747440438:
23: 12832: loss: 0.2751937586:
23: 16032: loss: 0.2763307775:
23: 19232: loss: 0.2734711453:
23: 22432: loss: 0.2718271401:
23: 25632: loss: 0.2714740340:
23: 28832: loss: 0.2713078069:
23: 32032: loss: 0.2705727203:
23: 35232: loss: 0.2704341481:
23: 38432: loss: 0.2690806008:
23: 41632: loss: 0.2685645771:
23: 44832: loss: 0.2687674150:
23: 48032: loss: 0.2696096531:
23: 51232: loss: 0.2700378781:
23: 54432: loss: 0.2698533948:
23: 57632: loss: 0.2702574807:
23: 60832: loss: 0.2702409717:
23: 64032: loss: 0.2709190407:
23: 67232: loss: 0.2707356075:
23: 70432: loss: 0.2708476270:
23: 73632: loss: 0.2710580317:
23: 76832: loss: 0.2708466004:
23: 80032: loss: 0.2705756669:
23: 83232: loss: 0.2705101284:
23: 86432: loss: 0.2704700805:
23: 89632: loss: 0.2709051699:
23: 92832: loss: 0.2709717359:
23: 96032: loss: 0.2712802112:
23: 99232: loss: 0.2712643532:
23: 102432: loss: 0.2715209637:
23: 105632: loss: 0.2718362917:
Dev-Acc: 23: Accuracy: 0.9200964570: precision: 0.3517608518: recall: 0.4381907839: f1: 0.3902475960
Train-Acc: 23: Accuracy: 0.8983818293: precision: 0.7895292101: recall: 0.3935967392: f1: 0.5253136790
24: 3232: loss: 0.2712633257:
24: 6432: loss: 0.2723008839:
24: 9632: loss: 0.2655150959:
24: 12832: loss: 0.2665029032:
24: 16032: loss: 0.2678095152:
24: 19232: loss: 0.2679658034:
24: 22432: loss: 0.2698888534:
24: 25632: loss: 0.2698549280:
24: 28832: loss: 0.2695095369:
24: 32032: loss: 0.2693818522:
24: 35232: loss: 0.2702221171:
24: 38432: loss: 0.2700102453:
24: 41632: loss: 0.2700959011:
24: 44832: loss: 0.2696340405:
24: 48032: loss: 0.2698391587:
24: 51232: loss: 0.2710577094:
24: 54432: loss: 0.2698775105:
24: 57632: loss: 0.2685606816:
24: 60832: loss: 0.2682494862:
24: 64032: loss: 0.2678717661:
24: 67232: loss: 0.2685836981:
24: 70432: loss: 0.2680769738:
24: 73632: loss: 0.2681429473:
24: 76832: loss: 0.2681107671:
24: 80032: loss: 0.2682574345:
24: 83232: loss: 0.2679176657:
24: 86432: loss: 0.2680353785:
24: 89632: loss: 0.2676616123:
24: 92832: loss: 0.2680375741:
24: 96032: loss: 0.2677657978:
24: 99232: loss: 0.2679416959:
24: 102432: loss: 0.2675981503:
24: 105632: loss: 0.2671441734:
Dev-Acc: 24: Accuracy: 0.9186477661: precision: 0.3477404099: recall: 0.4500935215: f1: 0.3923515897
Train-Acc: 24: Accuracy: 0.8997623920: precision: 0.7846211741: recall: 0.4112155677: f1: 0.5396195488
25: 3232: loss: 0.2588243376:
25: 6432: loss: 0.2611383878:
25: 9632: loss: 0.2588806673:
25: 12832: loss: 0.2628047422:
25: 16032: loss: 0.2650395472:
25: 19232: loss: 0.2652864051:
25: 22432: loss: 0.2624693031:
25: 25632: loss: 0.2629593583:
25: 28832: loss: 0.2624272310:
25: 32032: loss: 0.2615027041:
25: 35232: loss: 0.2611880539:
25: 38432: loss: 0.2608353936:
25: 41632: loss: 0.2608850147:
25: 44832: loss: 0.2614513050:
25: 48032: loss: 0.2611536943:
25: 51232: loss: 0.2612630320:
25: 54432: loss: 0.2624827101:
25: 57632: loss: 0.2628284372:
25: 60832: loss: 0.2632679514:
25: 64032: loss: 0.2628007838:
25: 67232: loss: 0.2631392168:
25: 70432: loss: 0.2630874758:
25: 73632: loss: 0.2633509598:
25: 76832: loss: 0.2629044137:
25: 80032: loss: 0.2625918196:
25: 83232: loss: 0.2624163886:
25: 86432: loss: 0.2629236677:
25: 89632: loss: 0.2631944969:
25: 92832: loss: 0.2628036892:
25: 96032: loss: 0.2628839690:
25: 99232: loss: 0.2630350752:
25: 102432: loss: 0.2631740173:
25: 105632: loss: 0.2629994865:
Dev-Acc: 25: Accuracy: 0.9175364971: precision: 0.3447482750: recall: 0.4587655161: f1: 0.3936674692
Train-Acc: 25: Accuracy: 0.9012086987: precision: 0.7838818974: recall: 0.4258760108: f1: 0.5519062833
26: 3232: loss: 0.2648475400:
26: 6432: loss: 0.2610718432:
26: 9632: loss: 0.2643355122:
26: 12832: loss: 0.2619528783:
26: 16032: loss: 0.2590630600:
26: 19232: loss: 0.2586052195:
26: 22432: loss: 0.2588757936:
26: 25632: loss: 0.2596925266:
26: 28832: loss: 0.2615902561:
26: 32032: loss: 0.2630433657:
26: 35232: loss: 0.2632089960:
26: 38432: loss: 0.2633565663:
26: 41632: loss: 0.2639441123:
26: 44832: loss: 0.2624387272:
26: 48032: loss: 0.2621169893:
26: 51232: loss: 0.2628833522:
26: 54432: loss: 0.2624325246:
26: 57632: loss: 0.2625531316:
26: 60832: loss: 0.2624613763:
26: 64032: loss: 0.2620653397:
26: 67232: loss: 0.2617036017:
26: 70432: loss: 0.2612573179:
26: 73632: loss: 0.2603833306:
26: 76832: loss: 0.2602984499:
26: 80032: loss: 0.2598295656:
26: 83232: loss: 0.2596201807:
26: 86432: loss: 0.2589153246:
26: 89632: loss: 0.2590727684:
26: 92832: loss: 0.2589632501:
26: 96032: loss: 0.2585666843:
26: 99232: loss: 0.2588873870:
26: 102432: loss: 0.2585258302:
26: 105632: loss: 0.2589214549:
Dev-Acc: 26: Accuracy: 0.9161077142: precision: 0.3411111111: recall: 0.4698180582: f1: 0.3952506974
Train-Acc: 26: Accuracy: 0.9020915627: precision: 0.7811325188: recall: 0.4371178752: f1: 0.5605530498
27: 3232: loss: 0.2482681084:
27: 6432: loss: 0.2600684958:
27: 9632: loss: 0.2553170905:
27: 12832: loss: 0.2595996086:
27: 16032: loss: 0.2562103128:
27: 19232: loss: 0.2573986772:
27: 22432: loss: 0.2554951005:
27: 25632: loss: 0.2565144272:
27: 28832: loss: 0.2554951325:
27: 32032: loss: 0.2567094707:
27: 35232: loss: 0.2565392278:
27: 38432: loss: 0.2564511577:
27: 41632: loss: 0.2569698679:
27: 44832: loss: 0.2571194913:
27: 48032: loss: 0.2569563038:
27: 51232: loss: 0.2576441660:
27: 54432: loss: 0.2577670142:
27: 57632: loss: 0.2578868888:
27: 60832: loss: 0.2572998850:
27: 64032: loss: 0.2565205079:
27: 67232: loss: 0.2565293323:
27: 70432: loss: 0.2556724614:
27: 73632: loss: 0.2558635196:
27: 76832: loss: 0.2556327037:
27: 80032: loss: 0.2553012392:
27: 83232: loss: 0.2549055647:
27: 86432: loss: 0.2552332594:
27: 89632: loss: 0.2550099962:
27: 92832: loss: 0.2551427427:
27: 96032: loss: 0.2552304345:
27: 99232: loss: 0.2547160005:
27: 102432: loss: 0.2551795729:
27: 105632: loss: 0.2553346375:
Dev-Acc: 27: Accuracy: 0.9147185683: precision: 0.3378346080: recall: 0.4807005611: f1: 0.3967997754
Train-Acc: 27: Accuracy: 0.9037538767: precision: 0.7838920032: recall: 0.4504634804: f1: 0.5721442886
28: 3232: loss: 0.2472071455:
28: 6432: loss: 0.2527399510:
28: 9632: loss: 0.2529438089:
28: 12832: loss: 0.2514824358:
28: 16032: loss: 0.2517185081:
28: 19232: loss: 0.2495057865:
28: 22432: loss: 0.2500779114:
28: 25632: loss: 0.2509121006:
28: 28832: loss: 0.2516353837:
28: 32032: loss: 0.2517634436:
28: 35232: loss: 0.2522640756:
28: 38432: loss: 0.2524015363:
28: 41632: loss: 0.2530897041:
28: 44832: loss: 0.2530275533:
28: 48032: loss: 0.2535072569:
28: 51232: loss: 0.2531407853:
28: 54432: loss: 0.2525227146:
28: 57632: loss: 0.2528495576:
28: 60832: loss: 0.2529893650:
28: 64032: loss: 0.2531970795:
28: 67232: loss: 0.2527179238:
28: 70432: loss: 0.2524108848:
28: 73632: loss: 0.2519400749:
28: 76832: loss: 0.2518159361:
28: 80032: loss: 0.2514493730:
28: 83232: loss: 0.2514616560:
28: 86432: loss: 0.2514112389:
28: 89632: loss: 0.2510312285:
28: 92832: loss: 0.2507705220:
28: 96032: loss: 0.2515857249:
28: 99232: loss: 0.2515557468:
28: 102432: loss: 0.2514724899:
28: 105632: loss: 0.2517233640:
Dev-Acc: 28: Accuracy: 0.9135080576: precision: 0.3356894554: recall: 0.4926032988: f1: 0.3992833023
Train-Acc: 28: Accuracy: 0.9051626325: precision: 0.7855467441: recall: 0.4623627638: f1: 0.5821056117
29: 3232: loss: 0.2492442759:
29: 6432: loss: 0.2468463751:
29: 9632: loss: 0.2395714369:
29: 12832: loss: 0.2440060372:
29: 16032: loss: 0.2430218562:
29: 19232: loss: 0.2464127435:
29: 22432: loss: 0.2481280184:
29: 25632: loss: 0.2482113720:
29: 28832: loss: 0.2490209017:
29: 32032: loss: 0.2488235814:
29: 35232: loss: 0.2489032421:
29: 38432: loss: 0.2496492394:
29: 41632: loss: 0.2495973223:
29: 44832: loss: 0.2488042701:
29: 48032: loss: 0.2480250798:
29: 51232: loss: 0.2480337727:
29: 54432: loss: 0.2483021572:
29: 57632: loss: 0.2481240587:
29: 60832: loss: 0.2484379404:
29: 64032: loss: 0.2482283498:
29: 67232: loss: 0.2482903276:
29: 70432: loss: 0.2478667292:
29: 73632: loss: 0.2474207814:
29: 76832: loss: 0.2472318810:
29: 80032: loss: 0.2476324187:
29: 83232: loss: 0.2473821845:
29: 86432: loss: 0.2476872349:
29: 89632: loss: 0.2478780332:
29: 92832: loss: 0.2481297070:
29: 96032: loss: 0.2481790223:
29: 99232: loss: 0.2482402396:
29: 102432: loss: 0.2483133999:
29: 105632: loss: 0.2484055937:
Dev-Acc: 29: Accuracy: 0.9126845598: precision: 0.3342795504: recall: 0.5005951369: f1: 0.4008714597
Train-Acc: 29: Accuracy: 0.9067122340: precision: 0.7875354108: recall: 0.4751824338: f1: 0.5927262290
30: 3232: loss: 0.2330031898:
30: 6432: loss: 0.2311199521:
30: 9632: loss: 0.2278742299:
30: 12832: loss: 0.2305891794:
30: 16032: loss: 0.2350913105:
30: 19232: loss: 0.2345083190:
30: 22432: loss: 0.2381035890:
30: 25632: loss: 0.2379437479:
30: 28832: loss: 0.2391897987:
30: 32032: loss: 0.2404790852:
30: 35232: loss: 0.2412624502:
30: 38432: loss: 0.2407634797:
30: 41632: loss: 0.2415645498:
30: 44832: loss: 0.2426986100:
30: 48032: loss: 0.2433273088:
30: 51232: loss: 0.2446916603:
30: 54432: loss: 0.2452625817:
30: 57632: loss: 0.2457359225:
30: 60832: loss: 0.2459360371:
30: 64032: loss: 0.2463573844:
30: 67232: loss: 0.2460944602:
30: 70432: loss: 0.2461861400:
30: 73632: loss: 0.2454466332:
30: 76832: loss: 0.2453800053:
30: 80032: loss: 0.2450564488:
30: 83232: loss: 0.2446274141:
30: 86432: loss: 0.2447451676:
30: 89632: loss: 0.2448826841:
30: 92832: loss: 0.2450719434:
30: 96032: loss: 0.2450932757:
30: 99232: loss: 0.2449024666:
30: 102432: loss: 0.2449789639:
30: 105632: loss: 0.2450116500:
Dev-Acc: 30: Accuracy: 0.9116625786: precision: 0.3319617438: recall: 0.5075667404: f1: 0.4013985074
Train-Acc: 30: Accuracy: 0.9079049826: precision: 0.7887594829: recall: 0.4853066860: f1: 0.6008954009
31: 3232: loss: 0.2397735676:
31: 6432: loss: 0.2353877081:
31: 9632: loss: 0.2368394549:
31: 12832: loss: 0.2407487964:
31: 16032: loss: 0.2394176650:
31: 19232: loss: 0.2408967955:
31: 22432: loss: 0.2419454507:
31: 25632: loss: 0.2421542914:
31: 28832: loss: 0.2416336225:
31: 32032: loss: 0.2406799618:
31: 35232: loss: 0.2406685540:
31: 38432: loss: 0.2411317107:
31: 41632: loss: 0.2404585407:
31: 44832: loss: 0.2403854297:
31: 48032: loss: 0.2400641825:
31: 51232: loss: 0.2408867829:
31: 54432: loss: 0.2401944255:
31: 57632: loss: 0.2403568054:
31: 60832: loss: 0.2403495625:
31: 64032: loss: 0.2405634728:
31: 67232: loss: 0.2407316627:
31: 70432: loss: 0.2402546288:
31: 73632: loss: 0.2409620952:
31: 76832: loss: 0.2411948569:
31: 80032: loss: 0.2411328468:
31: 83232: loss: 0.2415950745:
31: 86432: loss: 0.2417760681:
31: 89632: loss: 0.2418949871:
31: 92832: loss: 0.2420216992:
31: 96032: loss: 0.2417309955:
31: 99232: loss: 0.2421508211:
31: 102432: loss: 0.2423154959:
31: 105632: loss: 0.2422715408:
Dev-Acc: 31: Accuracy: 0.9105115533: precision: 0.3288239145: recall: 0.5124978745: f1: 0.4006114176
Train-Acc: 31: Accuracy: 0.9087126851: precision: 0.7887264697: recall: 0.4930642298: f1: 0.6067961165
32: 3232: loss: 0.2333231124:
32: 6432: loss: 0.2450803823:
32: 9632: loss: 0.2461830781:
32: 12832: loss: 0.2456579921:
32: 16032: loss: 0.2414656194:
32: 19232: loss: 0.2416175657:
32: 22432: loss: 0.2440384943:
32: 25632: loss: 0.2427082077:
32: 28832: loss: 0.2416268845:
32: 32032: loss: 0.2402666552:
32: 35232: loss: 0.2402616377:
32: 38432: loss: 0.2387011288:
32: 41632: loss: 0.2388138971:
32: 44832: loss: 0.2399792995:
32: 48032: loss: 0.2409585718:
32: 51232: loss: 0.2415142935:
32: 54432: loss: 0.2417077874:
32: 57632: loss: 0.2411041264:
32: 60832: loss: 0.2405589074:
32: 64032: loss: 0.2400078013:
32: 67232: loss: 0.2395711106:
32: 70432: loss: 0.2393083191:
32: 73632: loss: 0.2392982965:
32: 76832: loss: 0.2394359225:
32: 80032: loss: 0.2396426302:
32: 83232: loss: 0.2403242898:
32: 86432: loss: 0.2402951224:
32: 89632: loss: 0.2398798221:
32: 92832: loss: 0.2402096596:
32: 96032: loss: 0.2403683531:
32: 99232: loss: 0.2398993190:
32: 102432: loss: 0.2394210171:
32: 105632: loss: 0.2395415812:
Dev-Acc: 32: Accuracy: 0.9093109965: precision: 0.3258523031: recall: 0.5184492433: f1: 0.4001837511
Train-Acc: 32: Accuracy: 0.9094170928: precision: 0.7887528533: recall: 0.4997699034: f1: 0.6118556079
33: 3232: loss: 0.2352461195:
33: 6432: loss: 0.2362171707:
33: 9632: loss: 0.2383656022:
33: 12832: loss: 0.2423505659:
33: 16032: loss: 0.2425665153:
33: 19232: loss: 0.2401126055:
33: 22432: loss: 0.2397920431:
33: 25632: loss: 0.2386364912:
33: 28832: loss: 0.2368046365:
33: 32032: loss: 0.2372756633:
33: 35232: loss: 0.2367689489:
33: 38432: loss: 0.2360202042:
33: 41632: loss: 0.2351180462:
33: 44832: loss: 0.2346048574:
33: 48032: loss: 0.2345184533:
33: 51232: loss: 0.2348291205:
33: 54432: loss: 0.2359003652:
33: 57632: loss: 0.2362703154:
33: 60832: loss: 0.2366849390:
33: 64032: loss: 0.2361850289:
33: 67232: loss: 0.2356611041:
33: 70432: loss: 0.2357550894:
33: 73632: loss: 0.2368471332:
33: 76832: loss: 0.2368913933:
33: 80032: loss: 0.2373761560:
33: 83232: loss: 0.2369346909:
33: 86432: loss: 0.2369952957:
33: 89632: loss: 0.2375054777:
33: 92832: loss: 0.2379513317:
33: 96032: loss: 0.2380777619:
33: 99232: loss: 0.2376937754:
33: 102432: loss: 0.2374687491:
33: 105632: loss: 0.2371270881:
Dev-Acc: 33: Accuracy: 0.9084973931: precision: 0.3246194226: recall: 0.5257609250: f1: 0.4014020511
Train-Acc: 33: Accuracy: 0.9103562236: precision: 0.7904449457: recall: 0.5068700283: f1: 0.6176647306
34: 3232: loss: 0.2479113801:
34: 6432: loss: 0.2400301439:
34: 9632: loss: 0.2415905978:
34: 12832: loss: 0.2393553194:
34: 16032: loss: 0.2381009623:
34: 19232: loss: 0.2373187419:
34: 22432: loss: 0.2372522955:
34: 25632: loss: 0.2378774689:
34: 28832: loss: 0.2386367356:
34: 32032: loss: 0.2384004167:
34: 35232: loss: 0.2392154386:
34: 38432: loss: 0.2386904610:
34: 41632: loss: 0.2382068379:
34: 44832: loss: 0.2385489615:
34: 48032: loss: 0.2383204483:
34: 51232: loss: 0.2384545350:
34: 54432: loss: 0.2375543510:
34: 57632: loss: 0.2376433831:
34: 60832: loss: 0.2378130211:
34: 64032: loss: 0.2374069381:
34: 67232: loss: 0.2366473925:
34: 70432: loss: 0.2356283032:
34: 73632: loss: 0.2357083351:
34: 76832: loss: 0.2359045704:
34: 80032: loss: 0.2358531046:
34: 83232: loss: 0.2355892600:
34: 86432: loss: 0.2358651462:
34: 89632: loss: 0.2356504263:
34: 92832: loss: 0.2352688073:
34: 96032: loss: 0.2350334732:
34: 99232: loss: 0.2352106454:
34: 102432: loss: 0.2353036997:
34: 105632: loss: 0.2352600267:
Dev-Acc: 34: Accuracy: 0.9078425169: precision: 0.3239640384: recall: 0.5330726067: f1: 0.4030080987
Train-Acc: 34: Accuracy: 0.9107224941: precision: 0.7901535958: recall: 0.5106830583: f1: 0.6203977318
35: 3232: loss: 0.2166214717:
35: 6432: loss: 0.2240797751:
35: 9632: loss: 0.2233101034:
35: 12832: loss: 0.2242129632:
35: 16032: loss: 0.2260189111:
35: 19232: loss: 0.2287318144:
35: 22432: loss: 0.2306168548:
35: 25632: loss: 0.2312194151:
35: 28832: loss: 0.2329630438:
35: 32032: loss: 0.2346471061:
35: 35232: loss: 0.2341418770:
35: 38432: loss: 0.2332277334:
35: 41632: loss: 0.2328390358:
35: 44832: loss: 0.2326235483:
35: 48032: loss: 0.2315566744:
35: 51232: loss: 0.2324972062:
35: 54432: loss: 0.2321279389:
35: 57632: loss: 0.2326605673:
35: 60832: loss: 0.2328907817:
35: 64032: loss: 0.2327174377:
35: 67232: loss: 0.2332252998:
35: 70432: loss: 0.2327002293:
35: 73632: loss: 0.2326006363:
35: 76832: loss: 0.2330893471:
35: 80032: loss: 0.2326437193:
35: 83232: loss: 0.2324944804:
35: 86432: loss: 0.2326269074:
35: 89632: loss: 0.2332342429:
35: 92832: loss: 0.2335233367:
35: 96032: loss: 0.2335653227:
35: 99232: loss: 0.2330686001:
35: 102432: loss: 0.2329494760:
35: 105632: loss: 0.2328466809:
Dev-Acc: 35: Accuracy: 0.9066915512: precision: 0.3217285700: recall: 0.5405543275: f1: 0.4033752062
Train-Acc: 35: Accuracy: 0.9112108946: precision: 0.7896749522: recall: 0.5158766682: f1: 0.6240655321
36: 3232: loss: 0.2392917624:
36: 6432: loss: 0.2308616400:
36: 9632: loss: 0.2250883485:
36: 12832: loss: 0.2256571541:
36: 16032: loss: 0.2259877533:
36: 19232: loss: 0.2274313411:
36: 22432: loss: 0.2277484265:
36: 25632: loss: 0.2286118658:
36: 28832: loss: 0.2295799062:
36: 32032: loss: 0.2289463439:
36: 35232: loss: 0.2295538508:
36: 38432: loss: 0.2298712004:
36: 41632: loss: 0.2296003928:
36: 44832: loss: 0.2299424790:
36: 48032: loss: 0.2306574827:
36: 51232: loss: 0.2316055993:
36: 54432: loss: 0.2319279997:
36: 57632: loss: 0.2314337572:
36: 60832: loss: 0.2313739910:
36: 64032: loss: 0.2318915180:
36: 67232: loss: 0.2312983701:
36: 70432: loss: 0.2310383555:
36: 73632: loss: 0.2303905846:
36: 76832: loss: 0.2312538217:
36: 80032: loss: 0.2306007097:
36: 83232: loss: 0.2307834678:
36: 86432: loss: 0.2304345940:
36: 89632: loss: 0.2308121441:
36: 92832: loss: 0.2308594494:
36: 96032: loss: 0.2309860939:
36: 99232: loss: 0.2307174783:
36: 102432: loss: 0.2307885832:
36: 105632: loss: 0.2305602343:
Dev-Acc: 36: Accuracy: 0.9059076905: precision: 0.3199000000: recall: 0.5439551097: f1: 0.4028713557
Train-Acc: 36: Accuracy: 0.9116241336: precision: 0.7895577518: recall: 0.5199526658: f1: 0.6270017441
37: 3232: loss: 0.2401237390:
37: 6432: loss: 0.2372060208:
37: 9632: loss: 0.2343147277:
37: 12832: loss: 0.2336784702:
37: 16032: loss: 0.2347339726:
37: 19232: loss: 0.2333022991:
37: 22432: loss: 0.2318575806:
37: 25632: loss: 0.2318595803:
37: 28832: loss: 0.2323270619:
37: 32032: loss: 0.2323844664:
37: 35232: loss: 0.2329050690:
37: 38432: loss: 0.2327452191:
37: 41632: loss: 0.2317012573:
37: 44832: loss: 0.2321337907:
37: 48032: loss: 0.2308430665:
37: 51232: loss: 0.2307040053:
37: 54432: loss: 0.2303309005:
37: 57632: loss: 0.2307347142:
37: 60832: loss: 0.2301042470:
37: 64032: loss: 0.2298560136:
37: 67232: loss: 0.2296556273:
37: 70432: loss: 0.2300503157:
37: 73632: loss: 0.2298647199:
37: 76832: loss: 0.2295595936:
37: 80032: loss: 0.2298595603:
37: 83232: loss: 0.2299492492:
37: 86432: loss: 0.2298073037:
37: 89632: loss: 0.2297561769:
37: 92832: loss: 0.2291663479:
37: 96032: loss: 0.2292720088:
37: 99232: loss: 0.2292306638:
37: 102432: loss: 0.2288038130:
37: 105632: loss: 0.2283146265:
Dev-Acc: 37: Accuracy: 0.9051039815: precision: 0.3176913177: recall: 0.5456555008: f1: 0.4015767739
Train-Acc: 37: Accuracy: 0.9122721553: precision: 0.7905365274: recall: 0.5250147919: f1: 0.6309801288
38: 3232: loss: 0.2246401779:
38: 6432: loss: 0.2231371608:
38: 9632: loss: 0.2218926203:
38: 12832: loss: 0.2250100443:
38: 16032: loss: 0.2264784732:
38: 19232: loss: 0.2261809793:
38: 22432: loss: 0.2269653133:
38: 25632: loss: 0.2278715971:
38: 28832: loss: 0.2283568858:
38: 32032: loss: 0.2274891242:
38: 35232: loss: 0.2272177584:
38: 38432: loss: 0.2269660947:
38: 41632: loss: 0.2282640558:
38: 44832: loss: 0.2280765377:
38: 48032: loss: 0.2290558165:
38: 51232: loss: 0.2286389496:
38: 54432: loss: 0.2277778610:
38: 57632: loss: 0.2279083319:
38: 60832: loss: 0.2279889088:
38: 64032: loss: 0.2282114285:
38: 67232: loss: 0.2280829459:
38: 70432: loss: 0.2280320512:
38: 73632: loss: 0.2279305364:
38: 76832: loss: 0.2280806538:
38: 80032: loss: 0.2278180226:
38: 83232: loss: 0.2269145146:
38: 86432: loss: 0.2265391062:
38: 89632: loss: 0.2268113233:
38: 92832: loss: 0.2262022175:
38: 96032: loss: 0.2263420484:
38: 99232: loss: 0.2260470294:
38: 102432: loss: 0.2262883727:
38: 105632: loss: 0.2262507374:
Dev-Acc: 38: Accuracy: 0.9045681953: precision: 0.3163275337: recall: 0.5471858527: f1: 0.4008969727
Train-Acc: 38: Accuracy: 0.9129389524: precision: 0.7921707485: recall: 0.5294852409: f1: 0.6347229884
39: 3232: loss: 0.2156490556:
39: 6432: loss: 0.2160250464:
39: 9632: loss: 0.2161934889:
39: 12832: loss: 0.2170239939:
39: 16032: loss: 0.2180087983:
39: 19232: loss: 0.2193943291:
39: 22432: loss: 0.2193407759:
39: 25632: loss: 0.2196298122:
39: 28832: loss: 0.2190740854:
39: 32032: loss: 0.2208828782:
39: 35232: loss: 0.2198303945:
39: 38432: loss: 0.2205057053:
39: 41632: loss: 0.2202083146:
39: 44832: loss: 0.2204403321:
39: 48032: loss: 0.2204695927:
39: 51232: loss: 0.2212940012:
39: 54432: loss: 0.2216977876:
39: 57632: loss: 0.2226024856:
39: 60832: loss: 0.2233700352:
39: 64032: loss: 0.2234135893:
39: 67232: loss: 0.2234887887:
39: 70432: loss: 0.2241338819:
39: 73632: loss: 0.2244470096:
39: 76832: loss: 0.2239736310:
39: 80032: loss: 0.2239030537:
39: 83232: loss: 0.2238984728:
39: 86432: loss: 0.2241711702:
39: 89632: loss: 0.2240471634:
39: 92832: loss: 0.2245419440:
39: 96032: loss: 0.2246839894:
39: 99232: loss: 0.2249955132:
39: 102432: loss: 0.2248384096:
39: 105632: loss: 0.2245284112:
Dev-Acc: 39: Accuracy: 0.9038736224: precision: 0.3146001753: recall: 0.5492263221: f1: 0.4000495417
Train-Acc: 39: Accuracy: 0.9135869741: precision: 0.7937438905: recall: 0.5338242062: f1: 0.6383396879
40: 3232: loss: 0.2217791902:
40: 6432: loss: 0.2196524626:
40: 9632: loss: 0.2177310119:
40: 12832: loss: 0.2204759073:
40: 16032: loss: 0.2226870169:
40: 19232: loss: 0.2237601863:
40: 22432: loss: 0.2225888534:
40: 25632: loss: 0.2217088857:
40: 28832: loss: 0.2201605769:
40: 32032: loss: 0.2204994893:
40: 35232: loss: 0.2196866385:
40: 38432: loss: 0.2206473283:
40: 41632: loss: 0.2207923521:
40: 44832: loss: 0.2210723041:
40: 48032: loss: 0.2204745015:
40: 51232: loss: 0.2203374377:
40: 54432: loss: 0.2208726035:
40: 57632: loss: 0.2218516702:
40: 60832: loss: 0.2211905053:
40: 64032: loss: 0.2215744138:
40: 67232: loss: 0.2218134492:
40: 70432: loss: 0.2218535103:
40: 73632: loss: 0.2216219097:
40: 76832: loss: 0.2220357378:
40: 80032: loss: 0.2222317073:
40: 83232: loss: 0.2227636623:
40: 86432: loss: 0.2225487300:
40: 89632: loss: 0.2220767360:
40: 92832: loss: 0.2221323411:
40: 96032: loss: 0.2219907780:
40: 99232: loss: 0.2223043495:
40: 102432: loss: 0.2224377459:
40: 105632: loss: 0.2228060221:
Dev-Acc: 40: Accuracy: 0.9030798674: precision: 0.3125663034: recall: 0.5510967523: f1: 0.3988923077
Train-Acc: 40: Accuracy: 0.9140471816: precision: 0.7947844702: recall: 0.5369798172: f1: 0.6409290647
41: 3232: loss: 0.2341716488:
41: 6432: loss: 0.2351256680:
41: 9632: loss: 0.2325413963:
41: 12832: loss: 0.2315589303:
41: 16032: loss: 0.2336851602:
41: 19232: loss: 0.2308763511:
41: 22432: loss: 0.2292414321:
41: 25632: loss: 0.2281441875:
41: 28832: loss: 0.2265984663:
41: 32032: loss: 0.2266873198:
41: 35232: loss: 0.2264699136:
41: 38432: loss: 0.2265432651:
41: 41632: loss: 0.2252777834:
41: 44832: loss: 0.2247066760:
41: 48032: loss: 0.2241251477:
41: 51232: loss: 0.2232402556:
41: 54432: loss: 0.2227935511:
41: 57632: loss: 0.2227501133:
41: 60832: loss: 0.2219834092:
41: 64032: loss: 0.2223441054:
41: 67232: loss: 0.2218617459:
41: 70432: loss: 0.2228432246:
41: 73632: loss: 0.2223097770:
41: 76832: loss: 0.2225543677:
41: 80032: loss: 0.2221168520:
41: 83232: loss: 0.2216129568:
41: 86432: loss: 0.2212504423:
41: 89632: loss: 0.2206965968:
41: 92832: loss: 0.2207663983:
41: 96032: loss: 0.2208943256:
41: 99232: loss: 0.2207457399:
41: 102432: loss: 0.2204731754:
41: 105632: loss: 0.2208838605:
Dev-Acc: 41: Accuracy: 0.9022166133: precision: 0.3101108563: recall: 0.5517769087: f1: 0.3970633221
Train-Acc: 41: Accuracy: 0.9144134521: precision: 0.7953884906: recall: 0.5397409769: f1: 0.6430893354
42: 3232: loss: 0.2162403286:
42: 6432: loss: 0.2157243490:
42: 9632: loss: 0.2196560853:
42: 12832: loss: 0.2160880170:
42: 16032: loss: 0.2175635829:
42: 19232: loss: 0.2172013415:
42: 22432: loss: 0.2172316643:
42: 25632: loss: 0.2186510880:
42: 28832: loss: 0.2202278417:
42: 32032: loss: 0.2195631009:
42: 35232: loss: 0.2197374059:
42: 38432: loss: 0.2200385965:
42: 41632: loss: 0.2196789368:
42: 44832: loss: 0.2191314952:
42: 48032: loss: 0.2181796829:
42: 51232: loss: 0.2184284712:
42: 54432: loss: 0.2183794844:
42: 57632: loss: 0.2189304555:
42: 60832: loss: 0.2193242658:
42: 64032: loss: 0.2184291315:
42: 67232: loss: 0.2192299422:
42: 70432: loss: 0.2190110059:
42: 73632: loss: 0.2195898494:
42: 76832: loss: 0.2198745913:
42: 80032: loss: 0.2200127242:
42: 83232: loss: 0.2202487347:
42: 86432: loss: 0.2203636310:
42: 89632: loss: 0.2199281698:
42: 92832: loss: 0.2203592453:
42: 96032: loss: 0.2198994330:
42: 99232: loss: 0.2194694650:
42: 102432: loss: 0.2195979717:
42: 105632: loss: 0.2193138960:
Dev-Acc: 42: Accuracy: 0.9016609788: precision: 0.3087146383: recall: 0.5529671825: f1: 0.3962229668
Train-Acc: 42: Accuracy: 0.9148266912: precision: 0.7958574181: recall: 0.5430938137: f1: 0.6456175999
43: 3232: loss: 0.2144732049:
43: 6432: loss: 0.2114345801:
43: 9632: loss: 0.2154706239:
43: 12832: loss: 0.2124031037:
43: 16032: loss: 0.2132598186:
43: 19232: loss: 0.2157733714:
43: 22432: loss: 0.2172720150:
43: 25632: loss: 0.2158188773:
43: 28832: loss: 0.2165395697:
43: 32032: loss: 0.2169765952:
43: 35232: loss: 0.2168417073:
43: 38432: loss: 0.2171926946:
43: 41632: loss: 0.2173221878:
43: 44832: loss: 0.2171305139:
43: 48032: loss: 0.2175890568:
43: 51232: loss: 0.2169743731:
43: 54432: loss: 0.2167393176:
43: 57632: loss: 0.2175213417:
43: 60832: loss: 0.2176278883:
43: 64032: loss: 0.2171790668:
43: 67232: loss: 0.2172118255:
43: 70432: loss: 0.2174268448:
43: 73632: loss: 0.2175607216:
43: 76832: loss: 0.2181611740:
43: 80032: loss: 0.2179293663:
43: 83232: loss: 0.2171746180:
43: 86432: loss: 0.2177217834:
43: 89632: loss: 0.2179882062:
43: 92832: loss: 0.2176404338:
43: 96032: loss: 0.2179165277:
43: 99232: loss: 0.2177221639:
43: 102432: loss: 0.2176763720:
43: 105632: loss: 0.2174949674:
Dev-Acc: 43: Accuracy: 0.9007778764: precision: 0.3064561601: recall: 0.5544975344: f1: 0.3947463987
Train-Acc: 43: Accuracy: 0.9151835442: precision: 0.7962039877: recall: 0.5460521991: f1: 0.6478181180
44: 3232: loss: 0.2241516422:
44: 6432: loss: 0.2352543022:
44: 9632: loss: 0.2281008485:
44: 12832: loss: 0.2254610797:
44: 16032: loss: 0.2239522836:
44: 19232: loss: 0.2230324072:
44: 22432: loss: 0.2229096277:
44: 25632: loss: 0.2225250471:
44: 28832: loss: 0.2212825147:
44: 32032: loss: 0.2204149177:
44: 35232: loss: 0.2192995578:
44: 38432: loss: 0.2194258344:
44: 41632: loss: 0.2187282138:
44: 44832: loss: 0.2182295052:
44: 48032: loss: 0.2187595382:
44: 51232: loss: 0.2182413741:
44: 54432: loss: 0.2181500417:
44: 57632: loss: 0.2188338835:
44: 60832: loss: 0.2185626444:
44: 64032: loss: 0.2188758368:
44: 67232: loss: 0.2186041556:
44: 70432: loss: 0.2180330310:
44: 73632: loss: 0.2175546727:
44: 76832: loss: 0.2173567914:
44: 80032: loss: 0.2171856846:
44: 83232: loss: 0.2176200635:
44: 86432: loss: 0.2174495899:
44: 89632: loss: 0.2173869774:
44: 92832: loss: 0.2168317235:
44: 96032: loss: 0.2166289363:
44: 99232: loss: 0.2165496613:
44: 102432: loss: 0.2162750701:
44: 105632: loss: 0.2160398738:
Dev-Acc: 44: Accuracy: 0.9002321959: precision: 0.3050625817: recall: 0.5553477300: f1: 0.3938023754
Train-Acc: 44: Accuracy: 0.9154934883: precision: 0.7969033738: recall: 0.5481559398: f1: 0.6495287061
45: 3232: loss: 0.2230890881:
45: 6432: loss: 0.2108795545:
45: 9632: loss: 0.2101788952:
45: 12832: loss: 0.2101536604:
45: 16032: loss: 0.2123326290:
45: 19232: loss: 0.2139539724:
45: 22432: loss: 0.2132812977:
45: 25632: loss: 0.2118177567:
45: 28832: loss: 0.2110214059:
45: 32032: loss: 0.2111843160:
45: 35232: loss: 0.2114010710:
45: 38432: loss: 0.2114165016:
45: 41632: loss: 0.2114460901:
45: 44832: loss: 0.2124690027:
45: 48032: loss: 0.2126158754:
45: 51232: loss: 0.2123371954:
45: 54432: loss: 0.2126152643:
45: 57632: loss: 0.2131376658:
45: 60832: loss: 0.2132115890:
45: 64032: loss: 0.2133433012:
45: 67232: loss: 0.2134484626:
45: 70432: loss: 0.2130602593:
45: 73632: loss: 0.2132271941:
45: 76832: loss: 0.2136535835:
45: 80032: loss: 0.2133143786:
45: 83232: loss: 0.2139209627:
45: 86432: loss: 0.2140926983:
45: 89632: loss: 0.2136265724:
45: 92832: loss: 0.2134261993:
45: 96032: loss: 0.2139331835:
45: 99232: loss: 0.2143506362:
45: 102432: loss: 0.2145857464:
45: 105632: loss: 0.2145671900:
Dev-Acc: 45: Accuracy: 0.8996765018: precision: 0.3036576309: recall: 0.5561979255: f1: 0.3928421305
Train-Acc: 45: Accuracy: 0.9160006642: precision: 0.7979461824: recall: 0.5517060022: f1: 0.6523631841
46: 3232: loss: 0.2193611556:
46: 6432: loss: 0.2130571767:
46: 9632: loss: 0.2154770699:
46: 12832: loss: 0.2144217776:
46: 16032: loss: 0.2120384724:
46: 19232: loss: 0.2135135419:
46: 22432: loss: 0.2144485460:
46: 25632: loss: 0.2142574152:
46: 28832: loss: 0.2141388821:
46: 32032: loss: 0.2128865419:
46: 35232: loss: 0.2117641808:
46: 38432: loss: 0.2116735045:
46: 41632: loss: 0.2119058795:
46: 44832: loss: 0.2120954213:
46: 48032: loss: 0.2128772834:
46: 51232: loss: 0.2135977213:
46: 54432: loss: 0.2137128323:
46: 57632: loss: 0.2141070332:
46: 60832: loss: 0.2135827318:
46: 64032: loss: 0.2132202513:
46: 67232: loss: 0.2133636195:
46: 70432: loss: 0.2130677361:
46: 73632: loss: 0.2124624883:
46: 76832: loss: 0.2120559999:
46: 80032: loss: 0.2120851713:
46: 83232: loss: 0.2121307176:
46: 86432: loss: 0.2121886652:
46: 89632: loss: 0.2124094046:
46: 92832: loss: 0.2124789286:
46: 96032: loss: 0.2129348836:
46: 99232: loss: 0.2130925440:
46: 102432: loss: 0.2134079525:
46: 105632: loss: 0.2135612499:
Dev-Acc: 46: Accuracy: 0.8991010189: precision: 0.3021410114: recall: 0.5567080428: f1: 0.3916970748
Train-Acc: 46: Accuracy: 0.9163387418: precision: 0.7982962612: recall: 0.5544671619: f1: 0.6544072005
47: 3232: loss: 0.2198650818:
47: 6432: loss: 0.2104896414:
47: 9632: loss: 0.2133923535:
47: 12832: loss: 0.2132118506:
47: 16032: loss: 0.2146333370:
47: 19232: loss: 0.2137296473:
47: 22432: loss: 0.2133135535:
47: 25632: loss: 0.2125822095:
47: 28832: loss: 0.2118388065:
47: 32032: loss: 0.2126193864:
47: 35232: loss: 0.2111521245:
47: 38432: loss: 0.2096280071:
47: 41632: loss: 0.2094509227:
47: 44832: loss: 0.2099981623:
47: 48032: loss: 0.2103624674:
47: 51232: loss: 0.2099126886:
47: 54432: loss: 0.2105848547:
47: 57632: loss: 0.2107141272:
47: 60832: loss: 0.2110738798:
47: 64032: loss: 0.2108973939:
47: 67232: loss: 0.2119777742:
47: 70432: loss: 0.2120313648:
47: 73632: loss: 0.2120039706:
47: 76832: loss: 0.2118902565:
47: 80032: loss: 0.2117032186:
47: 83232: loss: 0.2117868794:
47: 86432: loss: 0.2118232942:
47: 89632: loss: 0.2116969956:
47: 92832: loss: 0.2119371888:
47: 96032: loss: 0.2118015107:
47: 99232: loss: 0.2119153518:
47: 102432: loss: 0.2115060059:
47: 105632: loss: 0.2117817931:
Dev-Acc: 47: Accuracy: 0.8992002606: precision: 0.3028207965: recall: 0.5585784730: f1: 0.3927311854
Train-Acc: 47: Accuracy: 0.9166110754: precision: 0.7989048338: recall: 0.5563079350: f1: 0.6558927257
48: 3232: loss: 0.2128810673:
48: 6432: loss: 0.2081938719:
48: 9632: loss: 0.2085635899:
48: 12832: loss: 0.2095872200:
48: 16032: loss: 0.2096320584:
48: 19232: loss: 0.2091522851:
48: 22432: loss: 0.2100632729:
48: 25632: loss: 0.2120832355:
48: 28832: loss: 0.2121649684:
48: 32032: loss: 0.2121050718:
48: 35232: loss: 0.2109884916:
48: 38432: loss: 0.2114995900:
48: 41632: loss: 0.2118247933:
48: 44832: loss: 0.2116944568:
48: 48032: loss: 0.2105884195:
48: 51232: loss: 0.2100986275:
48: 54432: loss: 0.2095136397:
48: 57632: loss: 0.2102576184:
48: 60832: loss: 0.2114342645:
48: 64032: loss: 0.2113469577:
48: 67232: loss: 0.2113956234:
48: 70432: loss: 0.2118169941:
48: 73632: loss: 0.2110745223:
48: 76832: loss: 0.2108822053:
48: 80032: loss: 0.2106641486:
48: 83232: loss: 0.2105552714:
48: 86432: loss: 0.2098591143:
48: 89632: loss: 0.2106585797:
48: 92832: loss: 0.2106359216:
48: 96032: loss: 0.2106807327:
48: 99232: loss: 0.2103577693:
48: 102432: loss: 0.2102704089:
48: 105632: loss: 0.2103947853:
Dev-Acc: 48: Accuracy: 0.8988926411: precision: 0.3021397741: recall: 0.5594286686: f1: 0.3923673226
Train-Acc: 48: Accuracy: 0.9174187779: precision: 0.8027929798: recall: 0.5593320623: f1: 0.6593048936
49: 3232: loss: 0.2218385942:
49: 6432: loss: 0.2116723561:
49: 9632: loss: 0.2118302141:
49: 12832: loss: 0.2077719130:
49: 16032: loss: 0.2091586923:
49: 19232: loss: 0.2112295087:
49: 22432: loss: 0.2115481901:
49: 25632: loss: 0.2105165011:
49: 28832: loss: 0.2099630448:
49: 32032: loss: 0.2102501979:
49: 35232: loss: 0.2118003637:
49: 38432: loss: 0.2115471693:
49: 41632: loss: 0.2105431188:
49: 44832: loss: 0.2122061859:
49: 48032: loss: 0.2122311050:
49: 51232: loss: 0.2119470483:
49: 54432: loss: 0.2119582241:
49: 57632: loss: 0.2106887636:
49: 60832: loss: 0.2101753043:
49: 64032: loss: 0.2100183056:
49: 67232: loss: 0.2099696810:
49: 70432: loss: 0.2104144816:
49: 73632: loss: 0.2106128496:
49: 76832: loss: 0.2105102610:
49: 80032: loss: 0.2100287121:
49: 83232: loss: 0.2098640934:
49: 86432: loss: 0.2094333011:
49: 89632: loss: 0.2095152316:
49: 92832: loss: 0.2090240477:
49: 96032: loss: 0.2091699751:
49: 99232: loss: 0.2094312875:
49: 102432: loss: 0.2089941119:
49: 105632: loss: 0.2090558331:
Dev-Acc: 49: Accuracy: 0.8992002606: precision: 0.3030023945: recall: 0.5594286686: f1: 0.3930939722
Train-Acc: 49: Accuracy: 0.9180010557: precision: 0.8054298643: recall: 0.5616987706: f1: 0.6618381812
50: 3232: loss: 0.2022193496:
50: 6432: loss: 0.2046932698:
50: 9632: loss: 0.2010794520:
50: 12832: loss: 0.2007341022:
50: 16032: loss: 0.2025867644:
50: 19232: loss: 0.2036146705:
50: 22432: loss: 0.2030982496:
50: 25632: loss: 0.2037034122:
50: 28832: loss: 0.2036196479:
50: 32032: loss: 0.2034143009:
50: 35232: loss: 0.2031829305:
50: 38432: loss: 0.2037696950:
50: 41632: loss: 0.2050338001:
50: 44832: loss: 0.2054470676:
50: 48032: loss: 0.2051804051:
50: 51232: loss: 0.2042877647:
50: 54432: loss: 0.2047203844:
50: 57632: loss: 0.2052120539:
50: 60832: loss: 0.2060229424:
50: 64032: loss: 0.2073639060:
50: 67232: loss: 0.2068649108:
50: 70432: loss: 0.2066851802:
50: 73632: loss: 0.2060879075:
50: 76832: loss: 0.2063515405:
50: 80032: loss: 0.2066654508:
50: 83232: loss: 0.2069519351:
50: 86432: loss: 0.2070674021:
50: 89632: loss: 0.2070136924:
50: 92832: loss: 0.2077132339:
50: 96032: loss: 0.2078469666:
50: 99232: loss: 0.2080992401:
50: 102432: loss: 0.2080048833:
50: 105632: loss: 0.2080651907:
Dev-Acc: 50: Accuracy: 0.8991506696: precision: 0.3030442380: recall: 0.5602788641: f1: 0.3933389041
Train-Acc: 50: Accuracy: 0.9184894562: precision: 0.8071858540: recall: 0.5641969627: f1: 0.6641643772
