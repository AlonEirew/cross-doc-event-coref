1: 6464: loss: 0.7056183130:
1: 12864: loss: 0.7049724665:
1: 19264: loss: 0.7038608984:
1: 25664: loss: 0.7028562401:
1: 32064: loss: 0.7017640493:
1: 38464: loss: 0.7010371466:
1: 44864: loss: 0.7002557760:
1: 51264: loss: 0.6995038862:
1: 57664: loss: 0.6986960534:
1: 64064: loss: 0.6979622448:
1: 70464: loss: 0.6972028954:
1: 76864: loss: 0.6962952303:
1: 83264: loss: 0.6955215263:
1: 89664: loss: 0.6946647029:
Dev-Acc: 1: Accuracy: 0.5964438915: precision: 0.0671792192: recall: 0.4591055943: f1: 0.1172078486
Train-Acc: 1: Accuracy: 0.6364253759: precision: 0.2320720398: recall: 0.5116691868: f1: 0.3193156642
2: 6464: loss: 0.6829457700:
2: 12864: loss: 0.6818002099:
2: 19264: loss: 0.6807123156:
2: 25664: loss: 0.6798244724:
2: 32064: loss: 0.6791695578:
2: 38464: loss: 0.6782851582:
2: 44864: loss: 0.6775528664:
2: 51264: loss: 0.6768925223:
2: 57664: loss: 0.6760435239:
2: 64064: loss: 0.6752607513:
2: 70464: loss: 0.6744327219:
2: 76864: loss: 0.6735898064:
2: 83264: loss: 0.6728914425:
2: 89664: loss: 0.6722406872:
Dev-Acc: 2: Accuracy: 0.8367895484: precision: 0.0887937743: recall: 0.1940146234: f1: 0.1218301212
Train-Acc: 2: Accuracy: 0.8127013445: precision: 0.4007380074: recall: 0.2498849517: f1: 0.3078231293
3: 6464: loss: 0.6598911446:
3: 12864: loss: 0.6595269033:
3: 19264: loss: 0.6590351478:
3: 25664: loss: 0.6583277915:
3: 32064: loss: 0.6577664504:
3: 38464: loss: 0.6570001716:
3: 44864: loss: 0.6562814642:
3: 51264: loss: 0.6555832870:
3: 57664: loss: 0.6548863159:
3: 64064: loss: 0.6541678624:
3: 70464: loss: 0.6533138246:
3: 76864: loss: 0.6525793777:
3: 83264: loss: 0.6517366768:
3: 89664: loss: 0.6509627426:
Dev-Acc: 3: Accuracy: 0.9215450883: precision: 0.1667763158: recall: 0.0862098283: f1: 0.1136643874
Train-Acc: 3: Accuracy: 0.8412990570: precision: 0.6302400573: recall: 0.1156399974: f1: 0.1954227308
4: 6464: loss: 0.6403502649:
4: 12864: loss: 0.6398286045:
4: 19264: loss: 0.6392159873:
4: 25664: loss: 0.6386020505:
4: 32064: loss: 0.6373947885:
4: 38464: loss: 0.6367270495:
4: 44864: loss: 0.6359900128:
4: 51264: loss: 0.6352860083:
4: 57664: loss: 0.6345908722:
4: 64064: loss: 0.6338231147:
4: 70464: loss: 0.6331335091:
4: 76864: loss: 0.6323440407:
4: 83264: loss: 0.6315976378:
4: 89664: loss: 0.6308477770:
Dev-Acc: 4: Accuracy: 0.9387700558: precision: 0.1914893617: recall: 0.0153035198: f1: 0.0283419934
Train-Acc: 4: Accuracy: 0.8385488391: precision: 0.7500000000: recall: 0.0469397147: f1: 0.0883499350
5: 6464: loss: 0.6197063899:
5: 12864: loss: 0.6191877455:
5: 19264: loss: 0.6188783624:
5: 25664: loss: 0.6182302444:
5: 32064: loss: 0.6177410362:
5: 38464: loss: 0.6168272147:
5: 44864: loss: 0.6159892154:
5: 51264: loss: 0.6152724479:
5: 57664: loss: 0.6145674370:
5: 64064: loss: 0.6138520072:
5: 70464: loss: 0.6130546501:
5: 76864: loss: 0.6124050785:
5: 83264: loss: 0.6118400035:
5: 89664: loss: 0.6112545397:
Dev-Acc: 5: Accuracy: 0.9413895011: precision: 0.2592592593: recall: 0.0023805475: f1: 0.0047177759
Train-Acc: 5: Accuracy: 0.8353275061: precision: 0.8640000000: recall: 0.0142002498: f1: 0.0279412716
6: 6464: loss: 0.6020046443:
6: 12864: loss: 0.5999456275:
6: 19264: loss: 0.5985549098:
6: 25664: loss: 0.5982244933:
6: 32064: loss: 0.5972834684:
6: 38464: loss: 0.5963482043:
6: 44864: loss: 0.5960451839:
6: 51264: loss: 0.5955937520:
6: 57664: loss: 0.5948548095:
6: 64064: loss: 0.5941429794:
6: 70464: loss: 0.5935852049:
6: 76864: loss: 0.5930866329:
6: 83264: loss: 0.5925665118:
6: 89664: loss: 0.5916712353:
Dev-Acc: 6: Accuracy: 0.9416276217: precision: 0.3333333333: recall: 0.0003400782: f1: 0.0006794632
Train-Acc: 6: Accuracy: 0.8342537284: precision: 1.0000000000: recall: 0.0055223194: f1: 0.0109839817
7: 6464: loss: 0.5840873033:
7: 12864: loss: 0.5835743397:
7: 19264: loss: 0.5824378602:
7: 25664: loss: 0.5808646530:
7: 32064: loss: 0.5800826309:
7: 38464: loss: 0.5791672697:
7: 44864: loss: 0.5783108475:
7: 51264: loss: 0.5776015786:
7: 57664: loss: 0.5767861605:
7: 64064: loss: 0.5759984306:
7: 70464: loss: 0.5754810199:
7: 76864: loss: 0.5748872989:
7: 83264: loss: 0.5742505704:
7: 89664: loss: 0.5735793101:
Dev-Acc: 7: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8336072564: precision: 1.0000000000: recall: 0.0016435474: f1: 0.0032817012
8: 6464: loss: 0.5618839598:
8: 12864: loss: 0.5621407703:
8: 19264: loss: 0.5615554885:
8: 25664: loss: 0.5615960844:
8: 32064: loss: 0.5611868379:
8: 38464: loss: 0.5605016065:
8: 44864: loss: 0.5598686076:
8: 51264: loss: 0.5595124939:
8: 57664: loss: 0.5590965513:
8: 64064: loss: 0.5584149596:
8: 70464: loss: 0.5577547207:
8: 76864: loss: 0.5574421001:
8: 83264: loss: 0.5565590187:
8: 89664: loss: 0.5559571297:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8334319592: precision: 1.0000000000: recall: 0.0005916771: f1: 0.0011826544
9: 6464: loss: 0.5490128887:
9: 12864: loss: 0.5461885221:
9: 19264: loss: 0.5456560015:
9: 25664: loss: 0.5457163230:
9: 32064: loss: 0.5449224947:
9: 38464: loss: 0.5440462151:
9: 44864: loss: 0.5439461199:
9: 51264: loss: 0.5428851285:
9: 57664: loss: 0.5417521226:
9: 64064: loss: 0.5411040635:
9: 70464: loss: 0.5403492949:
9: 76864: loss: 0.5398110133:
9: 83264: loss: 0.5392938995:
9: 89664: loss: 0.5385928942:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8333662152: precision: 1.0000000000: recall: 0.0001972257: f1: 0.0003943736
10: 6464: loss: 0.5249373761:
10: 12864: loss: 0.5276908171:
10: 19264: loss: 0.5270028849:
10: 25664: loss: 0.5261259101:
10: 32064: loss: 0.5255999698:
10: 38464: loss: 0.5255795158:
10: 44864: loss: 0.5250974836:
10: 51264: loss: 0.5246719987:
10: 57664: loss: 0.5242236445:
10: 64064: loss: 0.5240056039:
10: 70464: loss: 0.5240593244:
10: 76864: loss: 0.5235088144:
10: 83264: loss: 0.5227056336:
10: 89664: loss: 0.5217644911:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.5079465398:
11: 12864: loss: 0.5093240204:
11: 19264: loss: 0.5103314919:
11: 25664: loss: 0.5117778175:
11: 32064: loss: 0.5113551205:
11: 38464: loss: 0.5110934735:
11: 44864: loss: 0.5101580042:
11: 51264: loss: 0.5094513621:
11: 57664: loss: 0.5086761681:
11: 64064: loss: 0.5077597990:
11: 70464: loss: 0.5075097388:
11: 76864: loss: 0.5070287870:
11: 83264: loss: 0.5067155441:
11: 89664: loss: 0.5056578727:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.5002078503:
12: 12864: loss: 0.4987060350:
12: 19264: loss: 0.4952704947:
12: 25664: loss: 0.4954094934:
12: 32064: loss: 0.4957344630:
12: 38464: loss: 0.4950588725:
12: 44864: loss: 0.4943423320:
12: 51264: loss: 0.4938081675:
12: 57664: loss: 0.4931569744:
12: 64064: loss: 0.4924072364:
12: 70464: loss: 0.4919418802:
12: 76864: loss: 0.4916584645:
12: 83264: loss: 0.4910740510:
12: 89664: loss: 0.4905670161:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.4771482050:
13: 12864: loss: 0.4813731733:
13: 19264: loss: 0.4801384448:
13: 25664: loss: 0.4799688227:
13: 32064: loss: 0.4805119133:
13: 38464: loss: 0.4806756099:
13: 44864: loss: 0.4794960159:
13: 51264: loss: 0.4785557708:
13: 57664: loss: 0.4781322538:
13: 64064: loss: 0.4779630150:
13: 70464: loss: 0.4776780466:
13: 76864: loss: 0.4770356947:
13: 83264: loss: 0.4765265410:
13: 89664: loss: 0.4761365804:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8333552480: precision: 1.0000000000: recall: 0.0001314838: f1: 0.0002629330
14: 6464: loss: 0.4605964056:
14: 12864: loss: 0.4628424978:
14: 19264: loss: 0.4633181627:
14: 25664: loss: 0.4647846808:
14: 32064: loss: 0.4654032139:
14: 38464: loss: 0.4656009387:
14: 44864: loss: 0.4653669844:
14: 51264: loss: 0.4645836959:
14: 57664: loss: 0.4641752070:
14: 64064: loss: 0.4648154091:
14: 70464: loss: 0.4635581763:
14: 76864: loss: 0.4637119965:
14: 83264: loss: 0.4630985047:
14: 89664: loss: 0.4623894154:
Dev-Acc: 14: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8335853219: precision: 1.0000000000: recall: 0.0015120636: f1: 0.0030195615
15: 6464: loss: 0.4551898077:
15: 12864: loss: 0.4520259105:
15: 19264: loss: 0.4520000755:
15: 25664: loss: 0.4521607144:
15: 32064: loss: 0.4523566954:
15: 38464: loss: 0.4519262215:
15: 44864: loss: 0.4508614223:
15: 51264: loss: 0.4500933015:
15: 57664: loss: 0.4493485104:
15: 64064: loss: 0.4490248662:
15: 70464: loss: 0.4490652189:
15: 76864: loss: 0.4486713330:
15: 83264: loss: 0.4487775279:
15: 89664: loss: 0.4487798326:
Dev-Acc: 15: Accuracy: 0.9416673183: precision: 0.7500000000: recall: 0.0005101173: f1: 0.0010195412
Train-Acc: 15: Accuracy: 0.8353494406: precision: 1.0000000000: recall: 0.0120965091: f1: 0.0239038649
16: 6464: loss: 0.4450261050:
16: 12864: loss: 0.4441880651:
16: 19264: loss: 0.4434776571:
16: 25664: loss: 0.4428521997:
16: 32064: loss: 0.4416047288:
16: 38464: loss: 0.4399279632:
16: 44864: loss: 0.4395723957:
16: 51264: loss: 0.4391093976:
16: 57664: loss: 0.4385618404:
16: 64064: loss: 0.4376758325:
16: 70464: loss: 0.4377050909:
16: 76864: loss: 0.4376234929:
16: 83264: loss: 0.4369953556:
16: 89664: loss: 0.4368511844:
Dev-Acc: 16: Accuracy: 0.9416871667: precision: 0.8333333333: recall: 0.0008501955: f1: 0.0016986581
Train-Acc: 16: Accuracy: 0.8373873830: precision: 1.0000000000: recall: 0.0243245020: f1: 0.0474937424
17: 6464: loss: 0.4329012063:
17: 12864: loss: 0.4296091567:
17: 19264: loss: 0.4312702859:
17: 25664: loss: 0.4301918157:
17: 32064: loss: 0.4306048730:
17: 38464: loss: 0.4304330618:
17: 44864: loss: 0.4300859611:
17: 51264: loss: 0.4303899112:
17: 57664: loss: 0.4294320946:
17: 64064: loss: 0.4281663330:
17: 70464: loss: 0.4271539121:
17: 76864: loss: 0.4264225030:
17: 83264: loss: 0.4257681105:
17: 89664: loss: 0.4251383407:
Dev-Acc: 17: Accuracy: 0.9417268634: precision: 0.8333333333: recall: 0.0017003911: f1: 0.0033938571
Train-Acc: 17: Accuracy: 0.8383625746: precision: 1.0000000000: recall: 0.0301755309: f1: 0.0585832802
18: 6464: loss: 0.4199251345:
18: 12864: loss: 0.4165805894:
18: 19264: loss: 0.4172744438:
18: 25664: loss: 0.4168833540:
18: 32064: loss: 0.4157434225:
18: 38464: loss: 0.4156487350:
18: 44864: loss: 0.4147049930:
18: 51264: loss: 0.4145978574:
18: 57664: loss: 0.4147760573:
18: 64064: loss: 0.4148106551:
18: 70464: loss: 0.4146758039:
18: 76864: loss: 0.4140754629:
18: 83264: loss: 0.4137460162:
18: 89664: loss: 0.4135630938:
Dev-Acc: 18: Accuracy: 0.9418062568: precision: 0.6333333333: recall: 0.0064614861: f1: 0.0127924592
Train-Acc: 18: Accuracy: 0.8395130634: precision: 0.9878892734: recall: 0.0375386234: f1: 0.0723288365
19: 6464: loss: 0.4117354521:
19: 12864: loss: 0.4090546562:
19: 19264: loss: 0.4067678900:
19: 25664: loss: 0.4084833556:
19: 32064: loss: 0.4073019493:
19: 38464: loss: 0.4062784847:
19: 44864: loss: 0.4067191076:
19: 51264: loss: 0.4063468990:
19: 57664: loss: 0.4052737710:
19: 64064: loss: 0.4046312958:
19: 70464: loss: 0.4039952276:
19: 76864: loss: 0.4041749539:
19: 83264: loss: 0.4037272403:
19: 89664: loss: 0.4029147833:
Dev-Acc: 19: Accuracy: 0.9419947267: precision: 0.6206896552: recall: 0.0153035198: f1: 0.0298705609
Train-Acc: 19: Accuracy: 0.8420003057: precision: 0.9509692132: recall: 0.0548287424: f1: 0.1036797613
20: 6464: loss: 0.4023242015:
20: 12864: loss: 0.3993062167:
20: 19264: loss: 0.3986854148:
20: 25664: loss: 0.3996854816:
20: 32064: loss: 0.3993641251:
20: 38464: loss: 0.3959685783:
20: 44864: loss: 0.3957262503:
20: 51264: loss: 0.3949328482:
20: 57664: loss: 0.3944890733:
20: 64064: loss: 0.3945536905:
20: 70464: loss: 0.3943104288:
20: 76864: loss: 0.3943524896:
20: 83264: loss: 0.3937076780:
20: 89664: loss: 0.3933513427:
Dev-Acc: 20: Accuracy: 0.9424313307: precision: 0.6525096525: recall: 0.0287366094: f1: 0.0550488599
Train-Acc: 20: Accuracy: 0.8445861340: precision: 0.9117882919: recall: 0.0747485372: f1: 0.1381698870
