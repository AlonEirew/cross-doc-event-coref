1: 3232: loss: 0.6949639827:
1: 6432: loss: 0.6944759008:
1: 9632: loss: 0.6944804664:
1: 12832: loss: 0.6939803915:
1: 16032: loss: 0.6932487283:
1: 19232: loss: 0.6928575428:
1: 22432: loss: 0.6925964670:
1: 25632: loss: 0.6921842877:
1: 28832: loss: 0.6918339347:
1: 32032: loss: 0.6913246658:
1: 35232: loss: 0.6908156798:
1: 38432: loss: 0.6905497409:
1: 41632: loss: 0.6900999504:
1: 44832: loss: 0.6896716172:
Dev-Acc: 1: Accuracy: 0.3895062804: precision: 0.0702337004: recall: 0.7731678286: f1: 0.1287700716
Train-Acc: 1: Accuracy: 0.6006618142: precision: 0.4445916115: recall: 0.7944250871: f1: 0.5701210163
2: 3232: loss: 0.6830385500:
2: 6432: loss: 0.6830329409:
2: 9632: loss: 0.6826375687:
2: 12832: loss: 0.6823686545:
2: 16032: loss: 0.6815975764:
2: 19232: loss: 0.6809837464:
2: 22432: loss: 0.6805041816:
2: 25632: loss: 0.6800916306:
2: 28832: loss: 0.6796494652:
2: 32032: loss: 0.6791848547:
2: 35232: loss: 0.6788812989:
2: 38432: loss: 0.6784427741:
2: 41632: loss: 0.6780038696:
2: 44832: loss: 0.6775184132:
Dev-Acc: 2: Accuracy: 0.5277623534: precision: 0.0813208873: recall: 0.6888284305: f1: 0.1454682562
Train-Acc: 2: Accuracy: 0.7572590113: precision: 0.6191491815: recall: 0.7061337190: f1: 0.6597868485
3: 3232: loss: 0.6702766758:
3: 6432: loss: 0.6701339141:
3: 9632: loss: 0.6698522286:
3: 12832: loss: 0.6697022653:
3: 16032: loss: 0.6694982107:
3: 19232: loss: 0.6691373465:
3: 22432: loss: 0.6686952800:
3: 25632: loss: 0.6682847960:
3: 28832: loss: 0.6678720374:
3: 32032: loss: 0.6674141783:
3: 35232: loss: 0.6669574202:
3: 38432: loss: 0.6664391479:
3: 41632: loss: 0.6660089512:
3: 44832: loss: 0.6654617407:
Dev-Acc: 3: Accuracy: 0.6437430382: precision: 0.0983518836: recall: 0.6250637647: f1: 0.1699609312
Train-Acc: 3: Accuracy: 0.8243376613: precision: 0.8001167932: recall: 0.6305305371: f1: 0.7052724465
4: 3232: loss: 0.6597798783:
4: 6432: loss: 0.6592403442:
4: 9632: loss: 0.6585968024:
4: 12832: loss: 0.6581688105:
4: 16032: loss: 0.6577346973:
4: 19232: loss: 0.6572149222:
4: 22432: loss: 0.6568245772:
4: 25632: loss: 0.6562707991:
4: 28832: loss: 0.6558238139:
4: 32032: loss: 0.6555643205:
4: 35232: loss: 0.6550496505:
4: 38432: loss: 0.6544349959:
4: 41632: loss: 0.6541407097:
4: 44832: loss: 0.6536772160:
Dev-Acc: 4: Accuracy: 0.7251448631: precision: 0.1195159378: recall: 0.5827240265: f1: 0.1983504558
Train-Acc: 4: Accuracy: 0.8399403691: precision: 0.9014928405: recall: 0.5835908224: f1: 0.7085162423
5: 3232: loss: 0.6465050513:
5: 6432: loss: 0.6465164256:
5: 9632: loss: 0.6462164183:
5: 12832: loss: 0.6455862427:
5: 16032: loss: 0.6454649847:
5: 19232: loss: 0.6451671829:
5: 22432: loss: 0.6446939208:
5: 25632: loss: 0.6443010249:
5: 28832: loss: 0.6438466760:
5: 32032: loss: 0.6434345447:
5: 35232: loss: 0.6430305752:
5: 38432: loss: 0.6427871278:
5: 41632: loss: 0.6422573731:
5: 44832: loss: 0.6418028528:
Dev-Acc: 5: Accuracy: 0.7753413320: precision: 0.1408921455: recall: 0.5590885904: f1: 0.2250667397
Train-Acc: 5: Accuracy: 0.8418250084: precision: 0.9446978970: recall: 0.5581487082: f1: 0.7017108852
6: 3232: loss: 0.6347543705:
6: 6432: loss: 0.6352699614:
6: 9632: loss: 0.6358717062:
6: 12832: loss: 0.6350142705:
6: 16032: loss: 0.6342515545:
6: 19232: loss: 0.6337247294:
6: 22432: loss: 0.6333420717:
6: 25632: loss: 0.6329334990:
6: 28832: loss: 0.6322826955:
6: 32032: loss: 0.6315508979:
6: 35232: loss: 0.6311495839:
6: 38432: loss: 0.6309016072:
6: 41632: loss: 0.6304722932:
6: 44832: loss: 0.6299138898:
Dev-Acc: 6: Accuracy: 0.8046316504: precision: 0.1586493301: recall: 0.5456555008: f1: 0.2458250345
Train-Acc: 6: Accuracy: 0.8412113786: precision: 0.9604578564: recall: 0.5461179410: f1: 0.6963118189
7: 3232: loss: 0.6232529294:
7: 6432: loss: 0.6227626166:
7: 9632: loss: 0.6220464706:
7: 12832: loss: 0.6222197081:
7: 16032: loss: 0.6221779916:
7: 19232: loss: 0.6216839705:
7: 22432: loss: 0.6208671841:
7: 25632: loss: 0.6206999514:
7: 28832: loss: 0.6202389356:
7: 32032: loss: 0.6198259077:
7: 35232: loss: 0.6193796029:
7: 38432: loss: 0.6188613578:
7: 41632: loss: 0.6183766644:
7: 44832: loss: 0.6179007156:
Dev-Acc: 7: Accuracy: 0.8213406801: precision: 0.1727395412: recall: 0.5441251488: f1: 0.2622305990
Train-Acc: 7: Accuracy: 0.8397212625: precision: 0.9661197025: recall: 0.5380316876: f1: 0.6911578414
8: 3232: loss: 0.6118434000:
8: 6432: loss: 0.6093469062:
8: 9632: loss: 0.6087385120:
8: 12832: loss: 0.6080141048:
8: 16032: loss: 0.6080604285:
8: 19232: loss: 0.6078264603:
8: 22432: loss: 0.6081106126:
8: 25632: loss: 0.6077274000:
8: 28832: loss: 0.6072894351:
8: 32032: loss: 0.6067411440:
8: 35232: loss: 0.6063853813:
8: 38432: loss: 0.6059417995:
8: 41632: loss: 0.6055118004:
8: 44832: loss: 0.6051454822:
Dev-Acc: 8: Accuracy: 0.8305286169: precision: 0.1818646668: recall: 0.5442951879: f1: 0.2726343582
Train-Acc: 8: Accuracy: 0.8407950401: precision: 0.9685141509: recall: 0.5399382026: f1: 0.6933434638
9: 3232: loss: 0.6023898661:
9: 6432: loss: 0.6013242754:
9: 9632: loss: 0.5997467405:
9: 12832: loss: 0.5990197773:
9: 16032: loss: 0.5981600218:
9: 19232: loss: 0.5970411640:
9: 22432: loss: 0.5964696590:
9: 25632: loss: 0.5960770922:
9: 28832: loss: 0.5956144940:
9: 32032: loss: 0.5949063987:
9: 35232: loss: 0.5943769408:
9: 38432: loss: 0.5935692048:
9: 41632: loss: 0.5930886071:
9: 44832: loss: 0.5926363919:
Dev-Acc: 9: Accuracy: 0.8359858394: precision: 0.1879871081: recall: 0.5454854617: f1: 0.2796130044
Train-Acc: 9: Accuracy: 0.8414962888: precision: 0.9692941176: recall: 0.5416474919: f1: 0.6949517102
10: 3232: loss: 0.5893896025:
10: 6432: loss: 0.5868610522:
10: 9632: loss: 0.5851575792:
10: 12832: loss: 0.5840827905:
10: 16032: loss: 0.5834099631:
10: 19232: loss: 0.5829206052:
10: 22432: loss: 0.5822367252:
10: 25632: loss: 0.5816119751:
10: 28832: loss: 0.5807756505:
10: 32032: loss: 0.5809253029:
10: 35232: loss: 0.5803859151:
10: 38432: loss: 0.5801227079:
10: 41632: loss: 0.5798734343:
10: 44832: loss: 0.5793101827:
Dev-Acc: 10: Accuracy: 0.8367399573: precision: 0.1898503960: recall: 0.5502465567: f1: 0.2822995725
Train-Acc: 10: Accuracy: 0.8432275057: precision: 0.9696863705: recall: 0.5467753599: f1: 0.6992601312
11: 3232: loss: 0.5739207536:
11: 6432: loss: 0.5728345948:
11: 9632: loss: 0.5713086655:
11: 12832: loss: 0.5710608821:
11: 16032: loss: 0.5707690203:
11: 19232: loss: 0.5704694034:
11: 22432: loss: 0.5696996079:
11: 25632: loss: 0.5688886926:
11: 28832: loss: 0.5684133979:
11: 32032: loss: 0.5676627117:
11: 35232: loss: 0.5674181562:
11: 38432: loss: 0.5668992804:
11: 41632: loss: 0.5664726927:
11: 44832: loss: 0.5660234466:
Dev-Acc: 11: Accuracy: 0.8352416754: precision: 0.1894834376: recall: 0.5563679646: f1: 0.2826903970
Train-Acc: 11: Accuracy: 0.8452216387: precision: 0.9700046147: recall: 0.5527578726: f1: 0.7042170945
12: 3232: loss: 0.5604381669:
12: 6432: loss: 0.5570959675:
12: 9632: loss: 0.5570540325:
12: 12832: loss: 0.5568749633:
12: 16032: loss: 0.5564293817:
12: 19232: loss: 0.5553936921:
12: 22432: loss: 0.5548182756:
12: 25632: loss: 0.5549980649:
12: 28832: loss: 0.5544952432:
12: 32032: loss: 0.5541260373:
12: 35232: loss: 0.5537668347:
12: 38432: loss: 0.5534274581:
12: 41632: loss: 0.5528090574:
12: 44832: loss: 0.5524345465:
Dev-Acc: 12: Accuracy: 0.8321062922: precision: 0.1869328494: recall: 0.5604489032: f1: 0.2803555480
Train-Acc: 12: Accuracy: 0.8479828238: precision: 0.9702205047: recall: 0.5611728354: f1: 0.7110666833
13: 3232: loss: 0.5462316868:
13: 6432: loss: 0.5460395889:
13: 9632: loss: 0.5438371636:
13: 12832: loss: 0.5430303819:
13: 16032: loss: 0.5425928849:
13: 19232: loss: 0.5413268677:
13: 22432: loss: 0.5408999974:
13: 25632: loss: 0.5406329602:
13: 28832: loss: 0.5401095772:
13: 32032: loss: 0.5398063307:
13: 35232: loss: 0.5396398030:
13: 38432: loss: 0.5394595076:
13: 41632: loss: 0.5389750881:
13: 44832: loss: 0.5387962097:
Dev-Acc: 13: Accuracy: 0.8278297782: precision: 0.1844985973: recall: 0.5703111716: f1: 0.2788029925
Train-Acc: 13: Accuracy: 0.8515328765: precision: 0.9701292911: recall: 0.5722174742: f1: 0.7198445189
14: 3232: loss: 0.5302605325:
14: 6432: loss: 0.5327216570:
14: 9632: loss: 0.5329296284:
14: 12832: loss: 0.5321004824:
14: 16032: loss: 0.5312755438:
14: 19232: loss: 0.5299481130:
14: 22432: loss: 0.5289816269:
14: 25632: loss: 0.5280456960:
14: 28832: loss: 0.5275411629:
14: 32032: loss: 0.5271997041:
14: 35232: loss: 0.5269035940:
14: 38432: loss: 0.5263125358:
14: 41632: loss: 0.5256748030:
14: 44832: loss: 0.5249943095:
Dev-Acc: 14: Accuracy: 0.8235831261: precision: 0.1817598288: recall: 0.5777928924: f1: 0.2765299479
Train-Acc: 14: Accuracy: 0.8538776636: precision: 0.9700671289: recall: 0.5795148248: f1: 0.7255741213
15: 3232: loss: 0.5213742235:
15: 6432: loss: 0.5184312132:
15: 9632: loss: 0.5159148457:
15: 12832: loss: 0.5157180756:
15: 16032: loss: 0.5148728392:
15: 19232: loss: 0.5150735073:
15: 22432: loss: 0.5138606488:
15: 25632: loss: 0.5140690362:
15: 28832: loss: 0.5133167764:
15: 32032: loss: 0.5127949442:
15: 35232: loss: 0.5123333871:
15: 38432: loss: 0.5120411738:
15: 41632: loss: 0.5116285405:
15: 44832: loss: 0.5111039524:
Dev-Acc: 15: Accuracy: 0.8177587390: precision: 0.1776974703: recall: 0.5852746132: f1: 0.2726228664
Train-Acc: 15: Accuracy: 0.8568360806: precision: 0.9703013223: recall: 0.5885214647: f1: 0.7326594918
16: 3232: loss: 0.4977170235:
16: 6432: loss: 0.5033005108:
16: 9632: loss: 0.5018742034:
16: 12832: loss: 0.5020696737:
16: 16032: loss: 0.5005669478:
16: 19232: loss: 0.5005027713:
16: 22432: loss: 0.5003583612:
16: 25632: loss: 0.5002248714:
16: 28832: loss: 0.4998512371:
16: 32032: loss: 0.4989751828:
16: 35232: loss: 0.4982868405:
16: 38432: loss: 0.4981398225:
16: 41632: loss: 0.4978540899:
16: 44832: loss: 0.4973605801:
Dev-Acc: 16: Accuracy: 0.8110414147: precision: 0.1728063634: recall: 0.5910559429: f1: 0.2674257578
Train-Acc: 16: Accuracy: 0.8593999743: precision: 0.9706732313: recall: 0.5962132667: f1: 0.7386983791
17: 3232: loss: 0.4870173797:
17: 6432: loss: 0.4884503405:
17: 9632: loss: 0.4880949515:
17: 12832: loss: 0.4865783138:
17: 16032: loss: 0.4864826998:
17: 19232: loss: 0.4862956650:
17: 22432: loss: 0.4876684803:
17: 25632: loss: 0.4868068773:
17: 28832: loss: 0.4861188863:
17: 32032: loss: 0.4853373701:
17: 35232: loss: 0.4846627698:
17: 38432: loss: 0.4843561752:
17: 41632: loss: 0.4834653062:
17: 44832: loss: 0.4827285718:
Dev-Acc: 17: Accuracy: 0.8038577437: precision: 0.1680768679: recall: 0.5978575072: f1: 0.2623880597
Train-Acc: 17: Accuracy: 0.8624460101: precision: 0.9709044908: recall: 0.6054828742: f1: 0.7458395757
18: 3232: loss: 0.4750932711:
18: 6432: loss: 0.4739274946:
18: 9632: loss: 0.4723811015:
18: 12832: loss: 0.4719121583:
18: 16032: loss: 0.4719734784:
18: 19232: loss: 0.4724693189:
18: 22432: loss: 0.4716809647:
18: 25632: loss: 0.4709002588:
18: 28832: loss: 0.4703012853:
18: 32032: loss: 0.4696602991:
18: 35232: loss: 0.4698501190:
18: 38432: loss: 0.4695558667:
18: 41632: loss: 0.4696620022:
18: 44832: loss: 0.4692028571:
Dev-Acc: 18: Accuracy: 0.7966939211: precision: 0.1639909839: recall: 0.6061894236: f1: 0.2581462708
Train-Acc: 18: Accuracy: 0.8654701710: precision: 0.9712237690: recall: 0.6146209980: f1: 0.7528284414
19: 3232: loss: 0.4607688484:
19: 6432: loss: 0.4604643945:
19: 9632: loss: 0.4614156695:
19: 12832: loss: 0.4594751400:
19: 16032: loss: 0.4585470789:
19: 19232: loss: 0.4591267572:
19: 22432: loss: 0.4587070001:
19: 25632: loss: 0.4582584929:
19: 28832: loss: 0.4589763538:
19: 32032: loss: 0.4583826554:
19: 35232: loss: 0.4573005558:
19: 38432: loss: 0.4563928716:
19: 41632: loss: 0.4557237423:
19: 44832: loss: 0.4556796465:
Dev-Acc: 19: Accuracy: 0.7882500887: precision: 0.1590207322: recall: 0.6129909879: f1: 0.2525305593
Train-Acc: 19: Accuracy: 0.8688010573: precision: 0.9707083078: recall: 0.6252711853: f1: 0.7606061818
20: 3232: loss: 0.4579420236:
20: 6432: loss: 0.4564706677:
20: 9632: loss: 0.4533039458:
20: 12832: loss: 0.4509887587:
20: 16032: loss: 0.4499828801:
20: 19232: loss: 0.4488825217:
20: 22432: loss: 0.4483343353:
20: 25632: loss: 0.4474163064:
20: 28832: loss: 0.4474838053:
20: 32032: loss: 0.4463929323:
20: 35232: loss: 0.4460158141:
20: 38432: loss: 0.4444890409:
20: 41632: loss: 0.4432722531:
20: 44832: loss: 0.4424592090:
Dev-Acc: 20: Accuracy: 0.7788537741: precision: 0.1541088671: recall: 0.6214929434: f1: 0.2469761470
Train-Acc: 20: Accuracy: 0.8714526892: precision: 0.9680456777: recall: 0.6353296956: f1: 0.7671667857
