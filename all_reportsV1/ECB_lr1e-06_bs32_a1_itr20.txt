1: 3232: loss: 0.6824802572:
1: 6432: loss: 0.6800210869:
1: 9632: loss: 0.6759896537:
1: 12832: loss: 0.6726527488:
1: 16032: loss: 0.6692183443:
1: 19232: loss: 0.6655555837:
1: 22432: loss: 0.6618330154:
1: 25632: loss: 0.6578602353:
1: 28832: loss: 0.6541675358:
Dev-Acc: 1: Accuracy: 0.2346106470: precision: 0.0677141471: recall: 0.9489882673: f1: 0.1264085344
Train-Acc: 1: Accuracy: 0.8424824476: precision: 0.7896258409: recall: 0.9337321675: f1: 0.8556539551
2: 3232: loss: 0.6106066954:
2: 6432: loss: 0.6062625229:
2: 9632: loss: 0.6010920370:
2: 12832: loss: 0.5964829576:
2: 16032: loss: 0.5924147884:
2: 19232: loss: 0.5871057542:
2: 22432: loss: 0.5818561677:
2: 25632: loss: 0.5774547882:
2: 28832: loss: 0.5725869281:
Dev-Acc: 2: Accuracy: 0.3227595687: precision: 0.0739713677: recall: 0.9207617752: f1: 0.1369412657
Train-Acc: 2: Accuracy: 0.9140425324: precision: 0.9374218641: recall: 0.8873183880: f1: 0.9116822588
3: 3232: loss: 0.5171326354:
3: 6432: loss: 0.5155109137:
3: 9632: loss: 0.5093199165:
3: 12832: loss: 0.5041209058:
3: 16032: loss: 0.4985665913:
3: 19232: loss: 0.4935616947:
3: 22432: loss: 0.4894816514:
3: 25632: loss: 0.4849001237:
3: 28832: loss: 0.4805852339:
Dev-Acc: 3: Accuracy: 0.3991704881: precision: 0.0792895948: recall: 0.8760414895: f1: 0.1454175958
Train-Acc: 3: Accuracy: 0.9167708158: precision: 0.9646024185: recall: 0.8652948524: f1: 0.9122539507
4: 3232: loss: 0.4288259307:
4: 6432: loss: 0.4244346158:
4: 9632: loss: 0.4198487606:
4: 12832: loss: 0.4160005408:
4: 16032: loss: 0.4125710983:
4: 19232: loss: 0.4090182376:
4: 22432: loss: 0.4042455391:
4: 25632: loss: 0.4013421505:
4: 28832: loss: 0.3972620419:
Dev-Acc: 4: Accuracy: 0.4305048287: precision: 0.0818384012: recall: 0.8571671484: f1: 0.1494116601
Train-Acc: 4: Accuracy: 0.9158175588: precision: 0.9706801607: recall: 0.8575373085: f1: 0.9106077001
5: 3232: loss: 0.3475487860:
5: 6432: loss: 0.3498460077:
5: 9632: loss: 0.3463228266:
5: 12832: loss: 0.3450497476:
5: 16032: loss: 0.3424367464:
5: 19232: loss: 0.3394428713:
5: 22432: loss: 0.3372116678:
5: 25632: loss: 0.3361226112:
5: 28832: loss: 0.3328544158:
Dev-Acc: 5: Accuracy: 0.4308918118: precision: 0.0819581601: recall: 0.8580173440: f1: 0.1496241605
Train-Acc: 5: Accuracy: 0.9186115861: precision: 0.9724716183: recall: 0.8616133062: f1: 0.9136921361
6: 3232: loss: 0.3008547555:
6: 6432: loss: 0.3000553229:
6: 9632: loss: 0.2975502239:
6: 12832: loss: 0.2986446560:
6: 16032: loss: 0.2966300022:
6: 19232: loss: 0.2953293397:
6: 22432: loss: 0.2928190268:
6: 25632: loss: 0.2912327365:
6: 28832: loss: 0.2890736458:
Dev-Acc: 6: Accuracy: 0.4151650965: precision: 0.0804724932: recall: 0.8653290257: f1: 0.1472511574
Train-Acc: 6: Accuracy: 0.9240024090: precision: 0.9719721917: recall: 0.8731838801: f1: 0.9199335088
7: 3232: loss: 0.2710732086:
7: 6432: loss: 0.2686254380:
7: 9632: loss: 0.2673931438:
7: 12832: loss: 0.2643307678:
7: 16032: loss: 0.2626419137:
7: 19232: loss: 0.2590847574:
7: 22432: loss: 0.2569161970:
7: 25632: loss: 0.2551059410:
7: 28832: loss: 0.2548800029:
Dev-Acc: 7: Accuracy: 0.4024944305: precision: 0.0790623451: recall: 0.8677095732: f1: 0.1449201278
Train-Acc: 7: Accuracy: 0.9292946458: precision: 0.9720254446: recall: 0.8840312931: f1: 0.9259425030
8: 3232: loss: 0.2385329676:
8: 6432: loss: 0.2378197999:
8: 9632: loss: 0.2352285821:
8: 12832: loss: 0.2339854489:
8: 16032: loss: 0.2322958622:
8: 19232: loss: 0.2315226011:
8: 22432: loss: 0.2305961436:
8: 25632: loss: 0.2305399523:
8: 28832: loss: 0.2288408139:
Dev-Acc: 8: Accuracy: 0.3845154047: precision: 0.0777052435: recall: 0.8784220371: f1: 0.1427801501
Train-Acc: 8: Accuracy: 0.9356058836: precision: 0.9711319681: recall: 0.8979028335: f1: 0.9330828352
9: 3232: loss: 0.2180040628:
9: 6432: loss: 0.2182525311:
9: 9632: loss: 0.2174501010:
9: 12832: loss: 0.2156423945:
9: 16032: loss: 0.2119417395:
9: 19232: loss: 0.2102323372:
9: 22432: loss: 0.2097028814:
9: 25632: loss: 0.2090869875:
9: 28832: loss: 0.2089094975:
Dev-Acc: 9: Accuracy: 0.3707235157: precision: 0.0766627428: recall: 0.8859037579: f1: 0.1411140152
Train-Acc: 9: Accuracy: 0.9397804737: precision: 0.9709257304: recall: 0.9067122477: f1: 0.9377209682
10: 3232: loss: 0.2116005104:
10: 6432: loss: 0.2059979213:
10: 9632: loss: 0.2019671694:
10: 12832: loss: 0.1993342628:
10: 16032: loss: 0.1977591577:
10: 19232: loss: 0.1965135656:
10: 22432: loss: 0.1956624877:
10: 25632: loss: 0.1947025585:
10: 28832: loss: 0.1941275417:
Dev-Acc: 10: Accuracy: 0.3597297072: precision: 0.0757523148: recall: 0.8903247747: f1: 0.1396248050
Train-Acc: 10: Accuracy: 0.9416212440: precision: 0.9699853075: recall: 0.9114456643: f1: 0.9398047722
11: 3232: loss: 0.1797441319:
11: 6432: loss: 0.1833574884:
11: 9632: loss: 0.1819273105:
11: 12832: loss: 0.1826138779:
11: 16032: loss: 0.1828337659:
11: 19232: loss: 0.1819069505:
11: 22432: loss: 0.1817000238:
11: 25632: loss: 0.1807806582:
11: 28832: loss: 0.1810367979:
Dev-Acc: 11: Accuracy: 0.3458485305: precision: 0.0747560302: recall: 0.8974664173: f1: 0.1380157942
Train-Acc: 11: Accuracy: 0.9443824291: precision: 0.9699645415: recall: 0.9171652094: f1: 0.9428262486
12: 3232: loss: 0.1794878665:
12: 6432: loss: 0.1774220398:
12: 9632: loss: 0.1745377199:
12: 12832: loss: 0.1759455353:
12: 16032: loss: 0.1756138616:
12: 19232: loss: 0.1764149460:
12: 22432: loss: 0.1769194388:
12: 25632: loss: 0.1738394954:
12: 28832: loss: 0.1723107432:
Dev-Acc: 12: Accuracy: 0.3361446261: precision: 0.0741748657: recall: 0.9037578643: f1: 0.1370976063
Train-Acc: 12: Accuracy: 0.9460259676: precision: 0.9699383528: recall: 0.9205837880: f1: 0.9446168376
13: 3232: loss: 0.1659480117:
13: 6432: loss: 0.1704040120:
13: 9632: loss: 0.1692692825:
13: 12832: loss: 0.1666431314:
13: 16032: loss: 0.1658926047:
13: 19232: loss: 0.1658140744:
13: 22432: loss: 0.1654459143:
13: 25632: loss: 0.1643324430:
13: 28832: loss: 0.1647248633:
Dev-Acc: 13: Accuracy: 0.3263017833: precision: 0.0734438407: recall: 0.9078388029: f1: 0.1358939116
Train-Acc: 13: Accuracy: 0.9484583735: precision: 0.9705456301: recall: 0.9249884952: f1: 0.9472196041
14: 3232: loss: 0.1524852447:
14: 6432: loss: 0.1619982271:
14: 9632: loss: 0.1626550665:
14: 12832: loss: 0.1623049727:
14: 16032: loss: 0.1602600332:
14: 19232: loss: 0.1602871545:
14: 22432: loss: 0.1584136640:
14: 25632: loss: 0.1570958240:
14: 28832: loss: 0.1566881467:
Dev-Acc: 14: Accuracy: 0.3178877532: precision: 0.0730498092: recall: 0.9144703282: f1: 0.1352921939
Train-Acc: 14: Accuracy: 0.9503977895: precision: 0.9708591065: recall: 0.9286700414: f1: 0.9492960586
15: 3232: loss: 0.1596754309:
15: 6432: loss: 0.1567139333:
15: 9632: loss: 0.1568530224:
15: 12832: loss: 0.1527657328:
15: 16032: loss: 0.1538382222:
15: 19232: loss: 0.1516909852:
15: 22432: loss: 0.1511307471:
15: 25632: loss: 0.1502959369:
15: 28832: loss: 0.1497543344:
Dev-Acc: 15: Accuracy: 0.3102178872: precision: 0.0724631839: recall: 0.9170209148: f1: 0.1343129319
Train-Acc: 15: Accuracy: 0.9516140223: precision: 0.9708684625: recall: 0.9311682335: f1: 0.9506040268
16: 3232: loss: 0.1388920080:
16: 6432: loss: 0.1479991050:
16: 9632: loss: 0.1501777347:
16: 12832: loss: 0.1524691036:
16: 16032: loss: 0.1494451441:
16: 19232: loss: 0.1479221010:
16: 22432: loss: 0.1467691122:
16: 25632: loss: 0.1458663806:
16: 28832: loss: 0.1446530724:
Dev-Acc: 16: Accuracy: 0.3021412194: precision: 0.0720659708: recall: 0.9228022445: f1: 0.1336913545
Train-Acc: 16: Accuracy: 0.9532904029: precision: 0.9711630450: recall: 0.9343238446: f1: 0.9523873346
17: 3232: loss: 0.1452614980:
17: 6432: loss: 0.1446704231:
17: 9632: loss: 0.1464163157:
17: 12832: loss: 0.1417125042:
17: 16032: loss: 0.1408988432:
17: 19232: loss: 0.1413951495:
17: 22432: loss: 0.1409888197:
17: 25632: loss: 0.1413239810:
17: 28832: loss: 0.1398562127:
Dev-Acc: 17: Accuracy: 0.2941836119: precision: 0.0714979906: recall: 0.9256929094: f1: 0.1327432549
Train-Acc: 17: Accuracy: 0.9548682570: precision: 0.9711289664: recall: 0.9376109395: f1: 0.9540756598
18: 3232: loss: 0.1518144941:
18: 6432: loss: 0.1458659949:
18: 9632: loss: 0.1404245959:
18: 12832: loss: 0.1392591568:
18: 16032: loss: 0.1374923285:
18: 19232: loss: 0.1363051734:
18: 22432: loss: 0.1365150269:
18: 25632: loss: 0.1350862350:
18: 28832: loss: 0.1348764455:
Dev-Acc: 18: Accuracy: 0.2892621756: precision: 0.0712590313: recall: 0.9290936915: f1: 0.1323659444
Train-Acc: 18: Accuracy: 0.9562488198: precision: 0.9713393100: recall: 0.9402406153: f1: 0.9555369968
19: 3232: loss: 0.1290436975:
19: 6432: loss: 0.1288035738:
19: 9632: loss: 0.1307598493:
19: 12832: loss: 0.1325471425:
19: 16032: loss: 0.1327641010:
19: 19232: loss: 0.1325710010:
19: 22432: loss: 0.1314607240:
19: 25632: loss: 0.1319243717:
19: 28832: loss: 0.1300716557:
Dev-Acc: 19: Accuracy: 0.2858191729: precision: 0.0709380071: recall: 0.9290936915: f1: 0.1318119316
Train-Acc: 19: Accuracy: 0.9573664069: precision: 0.9724938875: recall: 0.9413582276: f1: 0.9566727910
20: 3232: loss: 0.1364805535:
20: 6432: loss: 0.1361212815:
20: 9632: loss: 0.1337086250:
20: 12832: loss: 0.1329654077:
20: 16032: loss: 0.1311666262:
20: 19232: loss: 0.1305231752:
20: 22432: loss: 0.1296468496:
20: 25632: loss: 0.1294135655:
20: 28832: loss: 0.1293551295:
Dev-Acc: 20: Accuracy: 0.2792308331: precision: 0.0704865087: recall: 0.9314742391: f1: 0.1310557669
Train-Acc: 20: Accuracy: 0.9583854079: precision: 0.9727439148: recall: 0.9431990007: f1: 0.9577436582
