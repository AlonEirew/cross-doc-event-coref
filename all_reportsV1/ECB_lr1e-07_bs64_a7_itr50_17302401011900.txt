1: 6464: loss: 0.7089721620:
1: 12864: loss: 0.7075022840:
1: 19264: loss: 0.7063602706:
1: 25664: loss: 0.7052174830:
1: 32064: loss: 0.7041636893:
1: 38464: loss: 0.7032791486:
1: 44864: loss: 0.7023751908:
1: 51264: loss: 0.7014656557:
1: 57664: loss: 0.7004883348:
1: 64064: loss: 0.6995545398:
1: 70464: loss: 0.6986641612:
1: 76864: loss: 0.6977021382:
1: 83264: loss: 0.6967659691:
1: 89664: loss: 0.6959219919:
1: 96064: loss: 0.6950046935:
1: 102464: loss: 0.6940528903:
1: 108864: loss: 0.6932380469:
1: 115264: loss: 0.6923198513:
1: 121664: loss: 0.6914316732:
Dev-Acc: 1: Accuracy: 0.7450388670: precision: 0.0677544610: recall: 0.2640707363: f1: 0.1078397334
Train-Acc: 1: Accuracy: 0.7355039716: precision: 0.1869351924: recall: 0.3331799356: f1: 0.2394971882
2: 6464: loss: 0.6725194293:
2: 12864: loss: 0.6718821812:
2: 19264: loss: 0.6712509435:
2: 25664: loss: 0.6705159158:
2: 32064: loss: 0.6695394546:
2: 38464: loss: 0.6685159678:
2: 44864: loss: 0.6676060465:
2: 51264: loss: 0.6666918422:
2: 57664: loss: 0.6657693417:
2: 64064: loss: 0.6649029729:
2: 70464: loss: 0.6640474283:
2: 76864: loss: 0.6632945689:
2: 83264: loss: 0.6625655249:
2: 89664: loss: 0.6618092771:
2: 96064: loss: 0.6609321917:
2: 102464: loss: 0.6600905303:
2: 108864: loss: 0.6592955603:
2: 115264: loss: 0.6584754853:
2: 121664: loss: 0.6576895103:
Dev-Acc: 2: Accuracy: 0.9235096574: precision: 0.0970017637: recall: 0.0374086040: f1: 0.0539943551
Train-Acc: 2: Accuracy: 0.8682861328: precision: 0.3504210912: recall: 0.0629149957: f1: 0.1066770706
3: 6464: loss: 0.6416756201:
3: 12864: loss: 0.6404601505:
3: 19264: loss: 0.6401863458:
3: 25664: loss: 0.6391023615:
3: 32064: loss: 0.6383494205:
3: 38464: loss: 0.6372082038:
3: 44864: loss: 0.6363730214:
3: 51264: loss: 0.6357509632:
3: 57664: loss: 0.6350230267:
3: 64064: loss: 0.6342604892:
3: 70464: loss: 0.6332772520:
3: 76864: loss: 0.6323903318:
3: 83264: loss: 0.6316736926:
3: 89664: loss: 0.6308714802:
3: 96064: loss: 0.6300297300:
3: 102464: loss: 0.6291996657:
3: 108864: loss: 0.6283396073:
3: 115264: loss: 0.6274223158:
3: 121664: loss: 0.6265898882:
Dev-Acc: 3: Accuracy: 0.9411712289: precision: 0.1129032258: recall: 0.0011902738: f1: 0.0023557126
Train-Acc: 3: Accuracy: 0.8749096394: precision: 0.4625850340: recall: 0.0044704490: f1: 0.0088553197
4: 6464: loss: 0.6105504042:
4: 12864: loss: 0.6098951316:
4: 19264: loss: 0.6089070684:
4: 25664: loss: 0.6080640417:
4: 32064: loss: 0.6074945600:
4: 38464: loss: 0.6070281929:
4: 44864: loss: 0.6059856283:
4: 51264: loss: 0.6052967155:
4: 57664: loss: 0.6044502568:
4: 64064: loss: 0.6036089489:
4: 70464: loss: 0.6029249991:
4: 76864: loss: 0.6021469958:
4: 83264: loss: 0.6014039117:
4: 89664: loss: 0.6007543875:
4: 96064: loss: 0.5999998917:
4: 102464: loss: 0.5993341857:
4: 108864: loss: 0.5985077464:
4: 115264: loss: 0.5976215055:
4: 121664: loss: 0.5969484863:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.5000000000: recall: 0.0001700391: f1: 0.0003399626
Train-Acc: 4: Accuracy: 0.8750821948: precision: 1.0000000000: recall: 0.0006574190: f1: 0.0013139741
5: 6464: loss: 0.5802602708:
5: 12864: loss: 0.5794244641:
5: 19264: loss: 0.5795955328:
5: 25664: loss: 0.5788286445:
5: 32064: loss: 0.5779753721:
5: 38464: loss: 0.5776387437:
5: 44864: loss: 0.5766138577:
5: 51264: loss: 0.5758639880:
5: 57664: loss: 0.5751391794:
5: 64064: loss: 0.5748700009:
5: 70464: loss: 0.5741956489:
5: 76864: loss: 0.5736184999:
5: 83264: loss: 0.5727310546:
5: 89664: loss: 0.5719684052:
5: 96064: loss: 0.5712283927:
5: 102464: loss: 0.5705814256:
5: 108864: loss: 0.5699001674:
5: 115264: loss: 0.5692125757:
5: 121664: loss: 0.5684426247:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 6464: loss: 0.5540183872:
6: 12864: loss: 0.5526979125:
6: 19264: loss: 0.5515378910:
6: 25664: loss: 0.5515314402:
6: 32064: loss: 0.5512260392:
6: 38464: loss: 0.5506161426:
6: 44864: loss: 0.5497152658:
6: 51264: loss: 0.5494374566:
6: 57664: loss: 0.5482013319:
6: 64064: loss: 0.5476276822:
6: 70464: loss: 0.5466989732:
6: 76864: loss: 0.5458149881:
6: 83264: loss: 0.5453233712:
6: 89664: loss: 0.5446842014:
6: 96064: loss: 0.5440412645:
6: 102464: loss: 0.5434277003:
6: 108864: loss: 0.5424988409:
6: 115264: loss: 0.5418887377:
6: 121664: loss: 0.5410493510:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 6464: loss: 0.5269321105:
7: 12864: loss: 0.5272850348:
7: 19264: loss: 0.5274151445:
7: 25664: loss: 0.5258506954:
7: 32064: loss: 0.5255002491:
7: 38464: loss: 0.5243408419:
7: 44864: loss: 0.5239298914:
7: 51264: loss: 0.5235736052:
7: 57664: loss: 0.5227781613:
7: 64064: loss: 0.5219479087:
7: 70464: loss: 0.5213624222:
7: 76864: loss: 0.5205997266:
7: 83264: loss: 0.5201986051:
7: 89664: loss: 0.5193583558:
7: 96064: loss: 0.5188299964:
7: 102464: loss: 0.5180368479:
7: 108864: loss: 0.5172175707:
7: 115264: loss: 0.5162790140:
7: 121664: loss: 0.5155101924:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.5024618238:
8: 12864: loss: 0.5028174302:
8: 19264: loss: 0.5033761101:
8: 25664: loss: 0.5020029769:
8: 32064: loss: 0.5013556429:
8: 38464: loss: 0.5006587943:
8: 44864: loss: 0.4996382293:
8: 51264: loss: 0.4990607645:
8: 57664: loss: 0.4979350602:
8: 64064: loss: 0.4973553619:
8: 70464: loss: 0.4964103444:
8: 76864: loss: 0.4955067424:
8: 83264: loss: 0.4951015486:
8: 89664: loss: 0.4940076605:
8: 96064: loss: 0.4935761620:
8: 102464: loss: 0.4931371884:
8: 108864: loss: 0.4922662456:
8: 115264: loss: 0.4917701901:
8: 121664: loss: 0.4912276799:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.4777568600:
9: 12864: loss: 0.4783946745:
9: 19264: loss: 0.4766527609:
9: 25664: loss: 0.4767610834:
9: 32064: loss: 0.4766189363:
9: 38464: loss: 0.4760295646:
9: 44864: loss: 0.4759407320:
9: 51264: loss: 0.4750787381:
9: 57664: loss: 0.4746209719:
9: 64064: loss: 0.4745687459:
9: 70464: loss: 0.4739789746:
9: 76864: loss: 0.4735663568:
9: 83264: loss: 0.4726971947:
9: 89664: loss: 0.4720493818:
9: 96064: loss: 0.4713571413:
9: 102464: loss: 0.4704494787:
9: 108864: loss: 0.4699393767:
9: 115264: loss: 0.4696058607:
9: 121664: loss: 0.4688743296:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.4576910642:
10: 12864: loss: 0.4600294673:
10: 19264: loss: 0.4572521428:
10: 25664: loss: 0.4567687844:
10: 32064: loss: 0.4549055597:
10: 38464: loss: 0.4535928001:
10: 44864: loss: 0.4531129152:
10: 51264: loss: 0.4529571824:
10: 57664: loss: 0.4528592940:
10: 64064: loss: 0.4529294159:
10: 70464: loss: 0.4523581076:
10: 76864: loss: 0.4515826861:
10: 83264: loss: 0.4512899932:
10: 89664: loss: 0.4509019951:
10: 96064: loss: 0.4500803117:
10: 102464: loss: 0.4495975680:
10: 108864: loss: 0.4491251325:
10: 115264: loss: 0.4481670526:
10: 121664: loss: 0.4478013596:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.4369909313:
11: 12864: loss: 0.4366180272:
11: 19264: loss: 0.4370956387:
11: 25664: loss: 0.4359362874:
11: 32064: loss: 0.4346881359:
11: 38464: loss: 0.4348283759:
11: 44864: loss: 0.4342118247:
11: 51264: loss: 0.4344831919:
11: 57664: loss: 0.4342306722:
11: 64064: loss: 0.4333913100:
11: 70464: loss: 0.4331990336:
11: 76864: loss: 0.4323465176:
11: 83264: loss: 0.4315731690:
11: 89664: loss: 0.4308729605:
11: 96064: loss: 0.4306884004:
11: 102464: loss: 0.4300550875:
11: 108864: loss: 0.4296833309:
11: 115264: loss: 0.4290316915:
11: 121664: loss: 0.4288493218:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.4191834018:
12: 12864: loss: 0.4165947156:
12: 19264: loss: 0.4183221518:
12: 25664: loss: 0.4188801969:
12: 32064: loss: 0.4170152277:
12: 38464: loss: 0.4167029325:
12: 44864: loss: 0.4165133235:
12: 51264: loss: 0.4161044999:
12: 57664: loss: 0.4152448928:
12: 64064: loss: 0.4145378128:
12: 70464: loss: 0.4146041109:
12: 76864: loss: 0.4140016953:
12: 83264: loss: 0.4134332031:
12: 89664: loss: 0.4133524898:
12: 96064: loss: 0.4134700176:
12: 102464: loss: 0.4127056742:
12: 108864: loss: 0.4121197555:
12: 115264: loss: 0.4120682580:
12: 121664: loss: 0.4113841285:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.4055268744:
13: 12864: loss: 0.4018644090:
13: 19264: loss: 0.4014202139:
13: 25664: loss: 0.4025208277:
13: 32064: loss: 0.4028821255:
13: 38464: loss: 0.4013334524:
13: 44864: loss: 0.4009586368:
13: 51264: loss: 0.3998535482:
13: 57664: loss: 0.3994407728:
13: 64064: loss: 0.3988087874:
13: 70464: loss: 0.3979063046:
13: 76864: loss: 0.3975304136:
13: 83264: loss: 0.3977083778:
13: 89664: loss: 0.3978378043:
13: 96064: loss: 0.3972360413:
13: 102464: loss: 0.3968136889:
13: 108864: loss: 0.3966548227:
13: 115264: loss: 0.3961957848:
13: 121664: loss: 0.3958543199:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.3839787796:
14: 12864: loss: 0.3851401798:
14: 19264: loss: 0.3867144509:
14: 25664: loss: 0.3880709451:
14: 32064: loss: 0.3874125710:
14: 38464: loss: 0.3877651718:
14: 44864: loss: 0.3873016426:
14: 51264: loss: 0.3867516363:
14: 57664: loss: 0.3861418492:
14: 64064: loss: 0.3858486136:
14: 70464: loss: 0.3855256715:
14: 76864: loss: 0.3857313336:
14: 83264: loss: 0.3850447061:
14: 89664: loss: 0.3841900629:
14: 96064: loss: 0.3838785526:
14: 102464: loss: 0.3835320922:
14: 108864: loss: 0.3830197956:
14: 115264: loss: 0.3828446315:
14: 121664: loss: 0.3821641833:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.3804638681:
15: 12864: loss: 0.3777370466:
15: 19264: loss: 0.3753689833:
15: 25664: loss: 0.3773051967:
15: 32064: loss: 0.3760612654:
15: 38464: loss: 0.3760582687:
15: 44864: loss: 0.3752957488:
15: 51264: loss: 0.3743370203:
15: 57664: loss: 0.3737091766:
15: 64064: loss: 0.3727915916:
15: 70464: loss: 0.3731188791:
15: 76864: loss: 0.3724071535:
15: 83264: loss: 0.3714351363:
15: 89664: loss: 0.3711034844:
15: 96064: loss: 0.3709539774:
15: 102464: loss: 0.3705156939:
15: 108864: loss: 0.3702715586:
15: 115264: loss: 0.3697253341:
15: 121664: loss: 0.3696989129:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.3712764075:
16: 12864: loss: 0.3694932978:
16: 19264: loss: 0.3687828384:
16: 25664: loss: 0.3638161200:
16: 32064: loss: 0.3632916561:
16: 38464: loss: 0.3627208741:
16: 44864: loss: 0.3636282662:
16: 51264: loss: 0.3629390487:
16: 57664: loss: 0.3626701662:
16: 64064: loss: 0.3629639678:
16: 70464: loss: 0.3620381302:
16: 76864: loss: 0.3623340252:
16: 83264: loss: 0.3614948300:
16: 89664: loss: 0.3607328634:
16: 96064: loss: 0.3604267423:
16: 102464: loss: 0.3601941760:
16: 108864: loss: 0.3595836806:
16: 115264: loss: 0.3590869646:
16: 121664: loss: 0.3586675042:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.3525718975:
17: 12864: loss: 0.3553610992:
17: 19264: loss: 0.3551351500:
17: 25664: loss: 0.3548817066:
17: 32064: loss: 0.3533129778:
17: 38464: loss: 0.3531918780:
17: 44864: loss: 0.3525588264:
17: 51264: loss: 0.3519444568:
17: 57664: loss: 0.3530186939:
17: 64064: loss: 0.3522380230:
17: 70464: loss: 0.3514918266:
17: 76864: loss: 0.3506561860:
17: 83264: loss: 0.3514240168:
17: 89664: loss: 0.3506995346:
17: 96064: loss: 0.3506587213:
17: 102464: loss: 0.3498459908:
17: 108864: loss: 0.3491608614:
17: 115264: loss: 0.3492929685:
17: 121664: loss: 0.3486410287:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.3461883925:
18: 12864: loss: 0.3452409421:
18: 19264: loss: 0.3436619157:
18: 25664: loss: 0.3443031266:
18: 32064: loss: 0.3450022058:
18: 38464: loss: 0.3452738466:
18: 44864: loss: 0.3447732660:
18: 51264: loss: 0.3437543426:
18: 57664: loss: 0.3432078657:
18: 64064: loss: 0.3418639974:
18: 70464: loss: 0.3419923692:
18: 76864: loss: 0.3420767057:
18: 83264: loss: 0.3412512938:
18: 89664: loss: 0.3407732778:
18: 96064: loss: 0.3396423237:
18: 102464: loss: 0.3392892991:
18: 108864: loss: 0.3391420348:
18: 115264: loss: 0.3396629645:
18: 121664: loss: 0.3394351315:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.8750000596: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 6464: loss: 0.3239979513:
19: 12864: loss: 0.3308819460:
19: 19264: loss: 0.3317765028:
19: 25664: loss: 0.3322299626:
19: 32064: loss: 0.3340186193:
19: 38464: loss: 0.3324316776:
19: 44864: loss: 0.3314681317:
19: 51264: loss: 0.3312139288:
19: 57664: loss: 0.3314993323:
19: 64064: loss: 0.3317006066:
19: 70464: loss: 0.3318838541:
19: 76864: loss: 0.3321144474:
19: 83264: loss: 0.3317730835:
19: 89664: loss: 0.3319914770:
19: 96064: loss: 0.3317734156:
19: 102464: loss: 0.3323198755:
19: 108864: loss: 0.3315364143:
19: 115264: loss: 0.3318497474:
19: 121664: loss: 0.3313214259:
Dev-Acc: 19: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.8750329018: precision: 1.0000000000: recall: 0.0002629676: f1: 0.0005257969
20: 6464: loss: 0.3334398806:
20: 12864: loss: 0.3328785923:
20: 19264: loss: 0.3305406731:
20: 25664: loss: 0.3295194501:
20: 32064: loss: 0.3300162573:
20: 38464: loss: 0.3291183635:
20: 44864: loss: 0.3272380540:
20: 51264: loss: 0.3269265446:
20: 57664: loss: 0.3282359924:
20: 64064: loss: 0.3280431356:
20: 70464: loss: 0.3282596804:
20: 76864: loss: 0.3283907769:
20: 83264: loss: 0.3272053585:
20: 89664: loss: 0.3271954904:
20: 96064: loss: 0.3265921368:
20: 102464: loss: 0.3262002797:
20: 108864: loss: 0.3250948429:
20: 115264: loss: 0.3244557343:
20: 121664: loss: 0.3236812070:
Dev-Acc: 20: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 20: Accuracy: 0.8753533959: precision: 1.0000000000: recall: 0.0028269016: f1: 0.0056378655
21: 6464: loss: 0.3225869021:
21: 12864: loss: 0.3180382373:
21: 19264: loss: 0.3177471977:
21: 25664: loss: 0.3180351686:
21: 32064: loss: 0.3164047535:
21: 38464: loss: 0.3162564506:
21: 44864: loss: 0.3161809578:
21: 51264: loss: 0.3172372668:
21: 57664: loss: 0.3182720783:
21: 64064: loss: 0.3191120605:
21: 70464: loss: 0.3181031692:
21: 76864: loss: 0.3176278871:
21: 83264: loss: 0.3176880794:
21: 89664: loss: 0.3175352178:
21: 96064: loss: 0.3170622034:
21: 102464: loss: 0.3170593441:
21: 108864: loss: 0.3169251760:
21: 115264: loss: 0.3171151532:
21: 121664: loss: 0.3168688149:
Dev-Acc: 21: Accuracy: 0.9416970611: precision: 0.8571428571: recall: 0.0010202347: f1: 0.0020380435
Train-Acc: 21: Accuracy: 0.8758711219: precision: 0.9732142857: recall: 0.0071658668: f1: 0.0142269791
22: 6464: loss: 0.3047555295:
22: 12864: loss: 0.3085649505:
22: 19264: loss: 0.3076452699:
22: 25664: loss: 0.3089199502:
22: 32064: loss: 0.3085834214:
22: 38464: loss: 0.3097831441:
22: 44864: loss: 0.3102093718:
22: 51264: loss: 0.3109769873:
22: 57664: loss: 0.3112553041:
22: 64064: loss: 0.3112339922:
22: 70464: loss: 0.3113639050:
22: 76864: loss: 0.3106233965:
22: 83264: loss: 0.3096381352:
22: 89664: loss: 0.3094331146:
22: 96064: loss: 0.3093892418:
22: 102464: loss: 0.3088305093:
22: 108864: loss: 0.3095556367:
22: 115264: loss: 0.3098581347:
22: 121664: loss: 0.3101612217:
Dev-Acc: 22: Accuracy: 0.9419748783: precision: 0.7462686567: recall: 0.0085019554: f1: 0.0168123739
Train-Acc: 22: Accuracy: 0.8775557876: precision: 0.8494382022: recall: 0.0248504372: f1: 0.0482881962
23: 6464: loss: 0.3109397894:
23: 12864: loss: 0.3087690871:
23: 19264: loss: 0.3066447472:
23: 25664: loss: 0.3078294918:
23: 32064: loss: 0.3092474521:
23: 38464: loss: 0.3087703282:
23: 44864: loss: 0.3086219034:
23: 51264: loss: 0.3081870750:
23: 57664: loss: 0.3085991027:
23: 64064: loss: 0.3078196100:
23: 70464: loss: 0.3074137952:
23: 76864: loss: 0.3069598124:
23: 83264: loss: 0.3057958110:
23: 89664: loss: 0.3055706046:
23: 96064: loss: 0.3048774928:
23: 102464: loss: 0.3041539349:
23: 108864: loss: 0.3045134293:
23: 115264: loss: 0.3045080510:
23: 121664: loss: 0.3041704293:
Dev-Acc: 23: Accuracy: 0.9424710274: precision: 0.7515151515: recall: 0.0210848495: f1: 0.0410188554
Train-Acc: 23: Accuracy: 0.8783857226: precision: 0.8159509202: recall: 0.0349746894: f1: 0.0670743239
24: 6464: loss: 0.2959999777:
24: 12864: loss: 0.2970539900:
24: 19264: loss: 0.2981292172:
24: 25664: loss: 0.2983145000:
24: 32064: loss: 0.2995963797:
24: 38464: loss: 0.2999301568:
24: 44864: loss: 0.3006705674:
24: 51264: loss: 0.3001722656:
24: 57664: loss: 0.3012672373:
24: 64064: loss: 0.3013827358:
24: 70464: loss: 0.3019451804:
24: 76864: loss: 0.3007758443:
24: 83264: loss: 0.3001995815:
24: 89664: loss: 0.2999154932:
24: 96064: loss: 0.3003594668:
24: 102464: loss: 0.3000219379:
24: 108864: loss: 0.2995924589:
24: 115264: loss: 0.2995831263:
24: 121664: loss: 0.2988752051:
Dev-Acc: 24: Accuracy: 0.9422725439: precision: 0.6340425532: recall: 0.0253358272: f1: 0.0487246566
Train-Acc: 24: Accuracy: 0.8796348572: precision: 0.8472906404: recall: 0.0452304254: f1: 0.0858765525
25: 6464: loss: 0.2902185510:
25: 12864: loss: 0.2928658210:
25: 19264: loss: 0.2940115451:
25: 25664: loss: 0.2958106821:
25: 32064: loss: 0.2943179971:
25: 38464: loss: 0.2935982240:
25: 44864: loss: 0.2945715684:
25: 51264: loss: 0.2945736962:
25: 57664: loss: 0.2942060855:
25: 64064: loss: 0.2945141300:
25: 70464: loss: 0.2940395644:
25: 76864: loss: 0.2949792228:
25: 83264: loss: 0.2950915005:
25: 89664: loss: 0.2945208115:
25: 96064: loss: 0.2944725203:
25: 102464: loss: 0.2938873207:
25: 108864: loss: 0.2942747626:
25: 115264: loss: 0.2940293742:
25: 121664: loss: 0.2937114551:
Dev-Acc: 25: Accuracy: 0.9422923923: precision: 0.5802469136: recall: 0.0399591906: f1: 0.0747693287
Train-Acc: 25: Accuracy: 0.8815084696: precision: 0.8666666667: recall: 0.0615344159: f1: 0.1149100730
26: 6464: loss: 0.2818827042:
26: 12864: loss: 0.2824929766:
26: 19264: loss: 0.2863293512:
26: 25664: loss: 0.2866184389:
26: 32064: loss: 0.2869179162:
26: 38464: loss: 0.2894775534:
26: 44864: loss: 0.2897374400:
26: 51264: loss: 0.2886817101:
26: 57664: loss: 0.2898109837:
26: 64064: loss: 0.2900611700:
26: 70464: loss: 0.2898817593:
26: 76864: loss: 0.2891321940:
26: 83264: loss: 0.2882839924:
26: 89664: loss: 0.2885077320:
26: 96064: loss: 0.2880492598:
26: 102464: loss: 0.2878306905:
26: 108864: loss: 0.2883385252:
26: 115264: loss: 0.2885007830:
26: 121664: loss: 0.2887048401:
Dev-Acc: 26: Accuracy: 0.9423717856: precision: 0.5548872180: recall: 0.0627444312: f1: 0.1127406049
Train-Acc: 26: Accuracy: 0.8840724230: precision: 0.8704697987: recall: 0.0852672408: f1: 0.1553200407
27: 6464: loss: 0.2878965007:
27: 12864: loss: 0.2892865165:
27: 19264: loss: 0.2871219761:
27: 25664: loss: 0.2898213402:
27: 32064: loss: 0.2897139112:
27: 38464: loss: 0.2883751313:
27: 44864: loss: 0.2875641257:
27: 51264: loss: 0.2889097299:
27: 57664: loss: 0.2878023624:
27: 64064: loss: 0.2874616664:
27: 70464: loss: 0.2869486221:
27: 76864: loss: 0.2867441753:
27: 83264: loss: 0.2866064491:
27: 89664: loss: 0.2863330799:
27: 96064: loss: 0.2857608311:
27: 102464: loss: 0.2848200233:
27: 108864: loss: 0.2848292925:
27: 115264: loss: 0.2846184556:
27: 121664: loss: 0.2841368914:
Dev-Acc: 27: Accuracy: 0.9423519373: precision: 0.5412311266: recall: 0.0792382248: f1: 0.1382379116
Train-Acc: 27: Accuracy: 0.8859789371: precision: 0.8711111111: recall: 0.1030832950: f1: 0.1843513021
28: 6464: loss: 0.2772815849:
28: 12864: loss: 0.2821612388:
28: 19264: loss: 0.2854036648:
28: 25664: loss: 0.2820124257:
28: 32064: loss: 0.2830008075:
28: 38464: loss: 0.2842436615:
28: 44864: loss: 0.2848038565:
28: 51264: loss: 0.2835386615:
28: 57664: loss: 0.2831729225:
28: 64064: loss: 0.2832627152:
28: 70464: loss: 0.2831010544:
28: 76864: loss: 0.2829755036:
28: 83264: loss: 0.2822249653:
28: 89664: loss: 0.2816959564:
28: 96064: loss: 0.2807100965:
28: 102464: loss: 0.2806287858:
28: 108864: loss: 0.2806967833:
28: 115264: loss: 0.2797851173:
28: 121664: loss: 0.2799074627:
Dev-Acc: 28: Accuracy: 0.9425702095: precision: 0.5411868911: recall: 0.1038938956: f1: 0.1743223966
Train-Acc: 28: Accuracy: 0.8880909085: precision: 0.8831168831: recall: 0.1207021235: f1: 0.2123770966
29: 6464: loss: 0.2821702088:
29: 12864: loss: 0.2782134967:
29: 19264: loss: 0.2781393821:
29: 25664: loss: 0.2787273706:
29: 32064: loss: 0.2780791013:
29: 38464: loss: 0.2776439318:
29: 44864: loss: 0.2775720632:
29: 51264: loss: 0.2777517374:
29: 57664: loss: 0.2769762783:
29: 64064: loss: 0.2777432065:
29: 70464: loss: 0.2780959101:
29: 76864: loss: 0.2776537488:
29: 83264: loss: 0.2770097748:
29: 89664: loss: 0.2761558686:
29: 96064: loss: 0.2761854877:
29: 102464: loss: 0.2759444457:
29: 108864: loss: 0.2759446003:
29: 115264: loss: 0.2763753422:
29: 121664: loss: 0.2761001734:
Dev-Acc: 29: Accuracy: 0.9433342218: precision: 0.5635276532: recall: 0.1282094882: f1: 0.2088931985
Train-Acc: 29: Accuracy: 0.8910739422: precision: 0.8956310680: recall: 0.1455525606: f1: 0.2504099983
30: 6464: loss: 0.2758490342:
30: 12864: loss: 0.2729425829:
30: 19264: loss: 0.2736128426:
30: 25664: loss: 0.2722145701:
30: 32064: loss: 0.2747843720:
30: 38464: loss: 0.2731266932:
30: 44864: loss: 0.2736278344:
30: 51264: loss: 0.2730619481:
30: 57664: loss: 0.2727203084:
30: 64064: loss: 0.2720539317:
30: 70464: loss: 0.2717839146:
30: 76864: loss: 0.2707133862:
30: 83264: loss: 0.2716608964:
30: 89664: loss: 0.2721445222:
30: 96064: loss: 0.2720123329:
30: 102464: loss: 0.2721587228:
30: 108864: loss: 0.2720682660:
30: 115264: loss: 0.2720674790:
30: 121664: loss: 0.2719207861:
Dev-Acc: 30: Accuracy: 0.9434632659: precision: 0.5589941973: recall: 0.1474239075: f1: 0.2333153929
Train-Acc: 30: Accuracy: 0.8940487504: precision: 0.8991046832: recall: 0.1716520939: f1: 0.2882693900
31: 6464: loss: 0.2683353674:
31: 12864: loss: 0.2706338379:
31: 19264: loss: 0.2738886497:
31: 25664: loss: 0.2750171852:
31: 32064: loss: 0.2725131770:
31: 38464: loss: 0.2719166596:
31: 44864: loss: 0.2715049068:
31: 51264: loss: 0.2716888990:
31: 57664: loss: 0.2718100241:
31: 64064: loss: 0.2720233633:
31: 70464: loss: 0.2702386342:
31: 76864: loss: 0.2703385172:
31: 83264: loss: 0.2697546055:
31: 89664: loss: 0.2699225666:
31: 96064: loss: 0.2698888951:
31: 102464: loss: 0.2692345045:
31: 108864: loss: 0.2691448743:
31: 115264: loss: 0.2691459906:
31: 121664: loss: 0.2687285445:
Dev-Acc: 31: Accuracy: 0.9429075718: precision: 0.5311732941: recall: 0.1839823159: f1: 0.2733013387
Train-Acc: 31: Accuracy: 0.8962510824: precision: 0.8834519573: recall: 0.1958451121: f1: 0.3206156164
32: 6464: loss: 0.2802122180:
32: 12864: loss: 0.2728644250:
32: 19264: loss: 0.2693811347:
32: 25664: loss: 0.2709632387:
32: 32064: loss: 0.2693229347:
32: 38464: loss: 0.2703297759:
32: 44864: loss: 0.2686948113:
32: 51264: loss: 0.2683959663:
32: 57664: loss: 0.2667041711:
32: 64064: loss: 0.2674666933:
32: 70464: loss: 0.2666601948:
32: 76864: loss: 0.2673719750:
32: 83264: loss: 0.2669295276:
32: 89664: loss: 0.2663724015:
32: 96064: loss: 0.2656870315:
32: 102464: loss: 0.2658818864:
32: 108864: loss: 0.2654737447:
32: 115264: loss: 0.2654229136:
32: 121664: loss: 0.2654339619:
Dev-Acc: 32: Accuracy: 0.9407445788: precision: 0.4845814978: recall: 0.2431559259: f1: 0.3238224638
Train-Acc: 32: Accuracy: 0.8990451694: precision: 0.8615916955: recall: 0.2291762540: f1: 0.3620501636
33: 6464: loss: 0.2647427720:
33: 12864: loss: 0.2627509195:
33: 19264: loss: 0.2614647249:
33: 25664: loss: 0.2576434996:
33: 32064: loss: 0.2602790633:
33: 38464: loss: 0.2597226716:
33: 44864: loss: 0.2602466686:
33: 51264: loss: 0.2602095904:
33: 57664: loss: 0.2608176291:
33: 64064: loss: 0.2614603448:
33: 70464: loss: 0.2613682498:
33: 76864: loss: 0.2610559129:
33: 83264: loss: 0.2611691938:
33: 89664: loss: 0.2613575759:
33: 96064: loss: 0.2617281353:
33: 102464: loss: 0.2619499497:
33: 108864: loss: 0.2623236497:
33: 115264: loss: 0.2625560413:
33: 121664: loss: 0.2620107990:
Dev-Acc: 33: Accuracy: 0.9353964925: precision: 0.4251425856: recall: 0.3041999660: f1: 0.3546436713
Train-Acc: 33: Accuracy: 0.9007708430: precision: 0.8153660499: recall: 0.2665176517: f1: 0.4017242234
34: 6464: loss: 0.2635930502:
34: 12864: loss: 0.2597791459:
34: 19264: loss: 0.2576380941:
34: 25664: loss: 0.2590780806:
34: 32064: loss: 0.2596828335:
34: 38464: loss: 0.2604072684:
34: 44864: loss: 0.2597152557:
34: 51264: loss: 0.2603653602:
34: 57664: loss: 0.2605482344:
34: 64064: loss: 0.2599463140:
34: 70464: loss: 0.2600963126:
34: 76864: loss: 0.2602864714:
34: 83264: loss: 0.2609860969:
34: 89664: loss: 0.2605713067:
34: 96064: loss: 0.2601620758:
34: 102464: loss: 0.2606138353:
34: 108864: loss: 0.2601407990:
34: 115264: loss: 0.2596025670:
34: 121664: loss: 0.2592909768:
Dev-Acc: 34: Accuracy: 0.9306536913: precision: 0.3941536110: recall: 0.3507906819: f1: 0.3712100765
Train-Acc: 34: Accuracy: 0.9020117521: precision: 0.7785121166: recall: 0.3020182762: f1: 0.4352027283
35: 6464: loss: 0.2549055995:
35: 12864: loss: 0.2631510383:
35: 19264: loss: 0.2572027325:
35: 25664: loss: 0.2583629037:
35: 32064: loss: 0.2562539883:
35: 38464: loss: 0.2562311021:
35: 44864: loss: 0.2569639350:
35: 51264: loss: 0.2570401138:
35: 57664: loss: 0.2562173818:
35: 64064: loss: 0.2560131771:
35: 70464: loss: 0.2566978275:
35: 76864: loss: 0.2576081134:
35: 83264: loss: 0.2573346158:
35: 89664: loss: 0.2567407418:
35: 96064: loss: 0.2564354681:
35: 102464: loss: 0.2556786736:
35: 108864: loss: 0.2560256342:
35: 115264: loss: 0.2559927497:
35: 121664: loss: 0.2559556542:
Dev-Acc: 35: Accuracy: 0.9271312952: precision: 0.3781442612: recall: 0.3859887774: f1: 0.3820262538
Train-Acc: 35: Accuracy: 0.9027513266: precision: 0.7512274959: recall: 0.3319308395: f1: 0.4604231260
36: 6464: loss: 0.2635664241:
36: 12864: loss: 0.2569272919:
36: 19264: loss: 0.2572342643:
36: 25664: loss: 0.2592840007:
36: 32064: loss: 0.2589301351:
36: 38464: loss: 0.2608551977:
36: 44864: loss: 0.2593287662:
36: 51264: loss: 0.2585246328:
36: 57664: loss: 0.2565922238:
36: 64064: loss: 0.2560436307:
36: 70464: loss: 0.2554284535:
36: 76864: loss: 0.2549689041:
36: 83264: loss: 0.2548478894:
36: 89664: loss: 0.2542484749:
36: 96064: loss: 0.2538013102:
36: 102464: loss: 0.2537844343:
36: 108864: loss: 0.2545379081:
36: 115264: loss: 0.2542023376:
36: 121664: loss: 0.2535012981:
Dev-Acc: 36: Accuracy: 0.9250476360: precision: 0.3695211355: recall: 0.4028226492: f1: 0.3854539538
Train-Acc: 36: Accuracy: 0.9035484791: precision: 0.7421243379: recall: 0.3500098613: f1: 0.4756756757
37: 6464: loss: 0.2541795874:
37: 12864: loss: 0.2461331172:
37: 19264: loss: 0.2481283960:
37: 25664: loss: 0.2515257991:
37: 32064: loss: 0.2509036621:
37: 38464: loss: 0.2510369984:
37: 44864: loss: 0.2511646286:
37: 51264: loss: 0.2508377338:
37: 57664: loss: 0.2503477338:
37: 64064: loss: 0.2503097065:
37: 70464: loss: 0.2510981278:
37: 76864: loss: 0.2519139071:
37: 83264: loss: 0.2518833245:
37: 89664: loss: 0.2518168119:
37: 96064: loss: 0.2514793811:
37: 102464: loss: 0.2513115196:
37: 108864: loss: 0.2513307281:
37: 115264: loss: 0.2509007823:
37: 121664: loss: 0.2507998448:
Dev-Acc: 37: Accuracy: 0.9234104753: precision: 0.3625897129: recall: 0.4123448393: f1: 0.3858699976
Train-Acc: 37: Accuracy: 0.9038360715: precision: 0.7307641720: recall: 0.3652619815: f1: 0.4870693434
38: 6464: loss: 0.2437706244:
38: 12864: loss: 0.2443859621:
38: 19264: loss: 0.2493300784:
38: 25664: loss: 0.2473287006:
38: 32064: loss: 0.2470439007:
38: 38464: loss: 0.2464280120:
38: 44864: loss: 0.2467470888:
38: 51264: loss: 0.2454374221:
38: 57664: loss: 0.2454449083:
38: 64064: loss: 0.2456647312:
38: 70464: loss: 0.2452944435:
38: 76864: loss: 0.2455037825:
38: 83264: loss: 0.2462717286:
38: 89664: loss: 0.2468843474:
38: 96064: loss: 0.2472872543:
38: 102464: loss: 0.2475487029:
38: 108864: loss: 0.2471338172:
38: 115264: loss: 0.2475434125:
38: 121664: loss: 0.2482790761:
Dev-Acc: 38: Accuracy: 0.9225174785: precision: 0.3604516503: recall: 0.4233973814: f1: 0.3893971382
Train-Acc: 38: Accuracy: 0.9043948650: precision: 0.7253938248: recall: 0.3784103609: f1: 0.4973645554
39: 6464: loss: 0.2530546625:
39: 12864: loss: 0.2499754094:
39: 19264: loss: 0.2499129710:
39: 25664: loss: 0.2486329409:
39: 32064: loss: 0.2495835448:
39: 38464: loss: 0.2508926823:
39: 44864: loss: 0.2497684722:
39: 51264: loss: 0.2505217608:
39: 57664: loss: 0.2504232141:
39: 64064: loss: 0.2491178753:
39: 70464: loss: 0.2484501424:
39: 76864: loss: 0.2478862897:
39: 83264: loss: 0.2470058050:
39: 89664: loss: 0.2464641669:
39: 96064: loss: 0.2463519199:
39: 102464: loss: 0.2469080537:
39: 108864: loss: 0.2461112646:
39: 115264: loss: 0.2460220695:
39: 121664: loss: 0.2461583427:
Dev-Acc: 39: Accuracy: 0.9222694039: precision: 0.3611150619: recall: 0.4317292977: f1: 0.3932775713
Train-Acc: 39: Accuracy: 0.9051673412: precision: 0.7235415905: recall: 0.3905726119: f1: 0.5073008283
40: 6464: loss: 0.2495106326:
40: 12864: loss: 0.2417886567:
40: 19264: loss: 0.2448567269:
40: 25664: loss: 0.2461045984:
40: 32064: loss: 0.2450821948:
40: 38464: loss: 0.2444312391:
40: 44864: loss: 0.2453666984:
40: 51264: loss: 0.2455820942:
40: 57664: loss: 0.2448621024:
40: 64064: loss: 0.2453745374:
40: 70464: loss: 0.2447627102:
40: 76864: loss: 0.2443894403:
40: 83264: loss: 0.2436929414:
40: 89664: loss: 0.2443476900:
40: 96064: loss: 0.2442181305:
40: 102464: loss: 0.2436099964:
40: 108864: loss: 0.2435491688:
40: 115264: loss: 0.2432780274:
40: 121664: loss: 0.2437451822:
Dev-Acc: 40: Accuracy: 0.9217038155: precision: 0.3596760681: recall: 0.4380207448: f1: 0.3950011500
Train-Acc: 40: Accuracy: 0.9056850672: precision: 0.7215234931: recall: 0.3997764775: f1: 0.5144887686
41: 6464: loss: 0.2347636716:
41: 12864: loss: 0.2355792835:
41: 19264: loss: 0.2369185857:
41: 25664: loss: 0.2383594370:
41: 32064: loss: 0.2411672450:
41: 38464: loss: 0.2420367275:
41: 44864: loss: 0.2427716673:
41: 51264: loss: 0.2425670228:
41: 57664: loss: 0.2437742905:
41: 64064: loss: 0.2438192797:
41: 70464: loss: 0.2443775498:
41: 76864: loss: 0.2439935968:
41: 83264: loss: 0.2441388470:
41: 89664: loss: 0.2433595968:
41: 96064: loss: 0.2435803526:
41: 102464: loss: 0.2431064834:
41: 108864: loss: 0.2427235161:
41: 115264: loss: 0.2423226608:
41: 121664: loss: 0.2415079509:
Dev-Acc: 41: Accuracy: 0.9209398031: precision: 0.3573089020: recall: 0.4443121918: f1: 0.3960891314
Train-Acc: 41: Accuracy: 0.9063507318: precision: 0.7217249797: recall: 0.4081914404: f1: 0.5214579659
42: 6464: loss: 0.2522879156:
42: 12864: loss: 0.2455778643:
42: 19264: loss: 0.2406871354:
42: 25664: loss: 0.2394530628:
42: 32064: loss: 0.2389969569:
42: 38464: loss: 0.2408236733:
42: 44864: loss: 0.2405968397:
42: 51264: loss: 0.2413858558:
42: 57664: loss: 0.2412680709:
42: 64064: loss: 0.2406354339:
42: 70464: loss: 0.2404129686:
42: 76864: loss: 0.2411177893:
42: 83264: loss: 0.2404580816:
42: 89664: loss: 0.2401481425:
42: 96064: loss: 0.2398449043:
42: 102464: loss: 0.2394053049:
42: 108864: loss: 0.2389703905:
42: 115264: loss: 0.2394694417:
42: 121664: loss: 0.2392619059:
Dev-Acc: 42: Accuracy: 0.9206619859: precision: 0.3574605742: recall: 0.4509437171: f1: 0.3987969925
Train-Acc: 42: Accuracy: 0.9070738554: precision: 0.7231050646: recall: 0.4158175005: f1: 0.5280073462
43: 6464: loss: 0.2385019384:
43: 12864: loss: 0.2381565620:
43: 19264: loss: 0.2370065682:
43: 25664: loss: 0.2374384004:
43: 32064: loss: 0.2377315886:
43: 38464: loss: 0.2375810262:
43: 44864: loss: 0.2390301855:
43: 51264: loss: 0.2378361174:
43: 57664: loss: 0.2369185977:
43: 64064: loss: 0.2365469284:
43: 70464: loss: 0.2368062133:
43: 76864: loss: 0.2369920041:
43: 83264: loss: 0.2371030433:
43: 89664: loss: 0.2370850463:
43: 96064: loss: 0.2370699544:
43: 102464: loss: 0.2370079801:
43: 108864: loss: 0.2373606970:
43: 115264: loss: 0.2379608201:
43: 121664: loss: 0.2376684293:
Dev-Acc: 43: Accuracy: 0.9201460481: precision: 0.3568503105: recall: 0.4592756334: f1: 0.4016356877
Train-Acc: 43: Accuracy: 0.9081010818: precision: 0.7261904762: recall: 0.4250871080: f1: 0.5362637363
44: 6464: loss: 0.2437986300:
44: 12864: loss: 0.2388465656:
44: 19264: loss: 0.2387292423:
44: 25664: loss: 0.2388367730:
44: 32064: loss: 0.2378331277:
44: 38464: loss: 0.2366349890:
44: 44864: loss: 0.2368096966:
44: 51264: loss: 0.2361498238:
44: 57664: loss: 0.2357853453:
44: 64064: loss: 0.2354630199:
44: 70464: loss: 0.2350265597:
44: 76864: loss: 0.2350415789:
44: 83264: loss: 0.2355339418:
44: 89664: loss: 0.2349713182:
44: 96064: loss: 0.2352442819:
44: 102464: loss: 0.2351493898:
44: 108864: loss: 0.2350104829:
44: 115264: loss: 0.2352255072:
44: 121664: loss: 0.2354469004:
Dev-Acc: 44: Accuracy: 0.9197292924: precision: 0.3568002074: recall: 0.4679476280: f1: 0.4048845079
Train-Acc: 44: Accuracy: 0.9087667465: precision: 0.7274438171: recall: 0.4319900072: f1: 0.5420722653
45: 6464: loss: 0.2339496380:
45: 12864: loss: 0.2355056174:
45: 19264: loss: 0.2326682175:
45: 25664: loss: 0.2321562138:
45: 32064: loss: 0.2354815031:
45: 38464: loss: 0.2349258265:
45: 44864: loss: 0.2340554607:
45: 51264: loss: 0.2335978701:
45: 57664: loss: 0.2334207220:
45: 64064: loss: 0.2332545546:
45: 70464: loss: 0.2335524278:
45: 76864: loss: 0.2332947851:
45: 83264: loss: 0.2336463067:
45: 89664: loss: 0.2335499057:
45: 96064: loss: 0.2332731496:
45: 102464: loss: 0.2338797708:
45: 108864: loss: 0.2336109966:
45: 115264: loss: 0.2339188282:
45: 121664: loss: 0.2337111864:
Dev-Acc: 45: Accuracy: 0.9190446734: precision: 0.3548674822: recall: 0.4735589186: f1: 0.4057105397
Train-Acc: 45: Accuracy: 0.9094570279: precision: 0.7288006112: recall: 0.4390243902: f1: 0.5479609420
46: 6464: loss: 0.2349123210:
46: 12864: loss: 0.2337588259:
46: 19264: loss: 0.2350334713:
46: 25664: loss: 0.2335612160:
46: 32064: loss: 0.2327755621:
46: 38464: loss: 0.2318895673:
46: 44864: loss: 0.2324144954:
46: 51264: loss: 0.2323859233:
46: 57664: loss: 0.2324653878:
46: 64064: loss: 0.2322347790:
46: 70464: loss: 0.2310455797:
46: 76864: loss: 0.2315386494:
46: 83264: loss: 0.2315064121:
46: 89664: loss: 0.2313151835:
46: 96064: loss: 0.2313667375:
46: 102464: loss: 0.2313202798:
46: 108864: loss: 0.2319251535:
46: 115264: loss: 0.2317490789:
46: 121664: loss: 0.2320362536:
Dev-Acc: 46: Accuracy: 0.9184691906: precision: 0.3536707592: recall: 0.4800204047: f1: 0.4072711534
Train-Acc: 46: Accuracy: 0.9101637602: precision: 0.7307236062: recall: 0.4454670962: f1: 0.5535043294
47: 6464: loss: 0.2323858003:
47: 12864: loss: 0.2298503358:
47: 19264: loss: 0.2324658138:
47: 25664: loss: 0.2319156148:
47: 32064: loss: 0.2316593115:
47: 38464: loss: 0.2330531141:
47: 44864: loss: 0.2324772543:
47: 51264: loss: 0.2323364838:
47: 57664: loss: 0.2331717117:
47: 64064: loss: 0.2322780377:
47: 70464: loss: 0.2318893784:
47: 76864: loss: 0.2312284365:
47: 83264: loss: 0.2313528638:
47: 89664: loss: 0.2319318500:
47: 96064: loss: 0.2316046754:
47: 102464: loss: 0.2312274900:
47: 108864: loss: 0.2313735899:
47: 115264: loss: 0.2307291104:
47: 121664: loss: 0.2302123989:
Dev-Acc: 47: Accuracy: 0.9181814194: precision: 0.3533787973: recall: 0.4846114606: f1: 0.4087193460
Train-Acc: 47: Accuracy: 0.9106075168: precision: 0.7311919752: recall: 0.4504634804: f1: 0.5574810837
48: 6464: loss: 0.2198060938:
48: 12864: loss: 0.2261486109:
48: 19264: loss: 0.2266002672:
48: 25664: loss: 0.2272430711:
48: 32064: loss: 0.2278231479:
48: 38464: loss: 0.2284476800:
48: 44864: loss: 0.2273661645:
48: 51264: loss: 0.2259523081:
48: 57664: loss: 0.2268021634:
48: 64064: loss: 0.2273050676:
48: 70464: loss: 0.2282166163:
48: 76864: loss: 0.2292201743:
48: 83264: loss: 0.2291223300:
48: 89664: loss: 0.2289906087:
48: 96064: loss: 0.2294603883:
48: 102464: loss: 0.2287857757:
48: 108864: loss: 0.2289956039:
48: 115264: loss: 0.2287231425:
48: 121664: loss: 0.2287658063:
Dev-Acc: 48: Accuracy: 0.9180623889: precision: 0.3536510282: recall: 0.4883523210: f1: 0.4102271104
Train-Acc: 48: Accuracy: 0.9113224149: precision: 0.7337140440: recall: 0.4561172835: f1: 0.5625329387
49: 6464: loss: 0.2215003879:
49: 12864: loss: 0.2254675959:
49: 19264: loss: 0.2274079095:
49: 25664: loss: 0.2271693233:
49: 32064: loss: 0.2269598415:
49: 38464: loss: 0.2256434994:
49: 44864: loss: 0.2257600399:
49: 51264: loss: 0.2248824275:
49: 57664: loss: 0.2250882375:
49: 64064: loss: 0.2251339660:
49: 70464: loss: 0.2254523641:
49: 76864: loss: 0.2261306016:
49: 83264: loss: 0.2262953862:
49: 89664: loss: 0.2265452246:
49: 96064: loss: 0.2266820680:
49: 102464: loss: 0.2269442771:
49: 108864: loss: 0.2276515587:
49: 115264: loss: 0.2277338877:
49: 121664: loss: 0.2274417572:
Dev-Acc: 49: Accuracy: 0.9179135561: precision: 0.3535749265: recall: 0.4910729468: f1: 0.4111324649
Train-Acc: 49: Accuracy: 0.9118648171: precision: 0.7355597564: recall: 0.4604562488: f1: 0.5663688190
50: 6464: loss: 0.2355573908:
50: 12864: loss: 0.2315418757:
50: 19264: loss: 0.2282748468:
50: 25664: loss: 0.2289396155:
50: 32064: loss: 0.2283036529:
50: 38464: loss: 0.2289782156:
50: 44864: loss: 0.2292638552:
50: 51264: loss: 0.2295631139:
50: 57664: loss: 0.2290356739:
50: 64064: loss: 0.2292728003:
50: 70464: loss: 0.2293670057:
50: 76864: loss: 0.2287982496:
50: 83264: loss: 0.2280148677:
50: 89664: loss: 0.2279598274:
50: 96064: loss: 0.2269527204:
50: 102464: loss: 0.2266031897:
50: 108864: loss: 0.2266623450:
50: 115264: loss: 0.2264013741:
50: 121664: loss: 0.2258890669:
Dev-Acc: 50: Accuracy: 0.9179631472: precision: 0.3546107930: recall: 0.4949838463: f1: 0.4132008517
Train-Acc: 50: Accuracy: 0.9124811292: precision: 0.7388708495: recall: 0.4637433436: f1: 0.5698360126
