1: 6464: loss: 0.7101154518:
1: 12864: loss: 0.7090834016:
1: 19264: loss: 0.7080456412:
1: 25664: loss: 0.7069274053:
1: 32064: loss: 0.7058862802:
1: 38464: loss: 0.7049527779:
1: 44864: loss: 0.7039571129:
1: 51264: loss: 0.7029434896:
1: 57664: loss: 0.7020210694:
1: 64064: loss: 0.7008899724:
1: 70464: loss: 0.6997987125:
1: 76864: loss: 0.6987765153:
1: 83264: loss: 0.6977061049:
1: 89664: loss: 0.6966994394:
1: 96064: loss: 0.6956880055:
1: 102464: loss: 0.6946710028:
1: 108864: loss: 0.6937234224:
1: 115264: loss: 0.6926533040:
1: 121664: loss: 0.6916799066:
1: 128064: loss: 0.6906944517:
1: 134464: loss: 0.6897092930:
1: 140864: loss: 0.6887307835:
1: 147264: loss: 0.6877303798:
Dev-Acc: 1: Accuracy: 0.8343189359: precision: 0.0649183493: recall: 0.1372215610: f1: 0.0881389253
Train-Acc: 1: Accuracy: 0.8198606372: precision: 0.1649257834: recall: 0.1972256919: f1: 0.1796353403
2: 6464: loss: 0.6621802372:
2: 12864: loss: 0.6610523337:
2: 19264: loss: 0.6601460157:
2: 25664: loss: 0.6590211490:
2: 32064: loss: 0.6582789561:
2: 38464: loss: 0.6573859018:
2: 44864: loss: 0.6566469970:
2: 51264: loss: 0.6557449967:
2: 57664: loss: 0.6546895483:
2: 64064: loss: 0.6537879000:
2: 70464: loss: 0.6528180683:
2: 76864: loss: 0.6518952147:
2: 83264: loss: 0.6508846533:
2: 89664: loss: 0.6499117038:
2: 96064: loss: 0.6490488523:
2: 102464: loss: 0.6480926296:
2: 108864: loss: 0.6470857361:
2: 115264: loss: 0.6461214911:
2: 121664: loss: 0.6451668797:
2: 128064: loss: 0.6441297138:
2: 134464: loss: 0.6431772817:
2: 140864: loss: 0.6422370407:
2: 147264: loss: 0.6413253669:
Dev-Acc: 2: Accuracy: 0.9393653870: precision: 0.1006944444: recall: 0.0049311342: f1: 0.0094018479
Train-Acc: 2: Accuracy: 0.8980671763: precision: 0.2439024390: recall: 0.0092038656: f1: 0.0177383592
3: 6464: loss: 0.6181995380:
3: 12864: loss: 0.6167504355:
3: 19264: loss: 0.6162159477:
3: 25664: loss: 0.6150235093:
3: 32064: loss: 0.6144187058:
3: 38464: loss: 0.6133448332:
3: 44864: loss: 0.6123938575:
3: 51264: loss: 0.6113636117:
3: 57664: loss: 0.6106864072:
3: 64064: loss: 0.6098219430:
3: 70464: loss: 0.6088130137:
3: 76864: loss: 0.6080163956:
3: 83264: loss: 0.6072078841:
3: 89664: loss: 0.6062824686:
3: 96064: loss: 0.6054073955:
3: 102464: loss: 0.6045155331:
3: 108864: loss: 0.6036526648:
3: 115264: loss: 0.6028280595:
3: 121664: loss: 0.6020360783:
3: 128064: loss: 0.6010793004:
3: 134464: loss: 0.6002045562:
3: 140864: loss: 0.5994029213:
3: 147264: loss: 0.5986003001:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.5000000000: recall: 0.0001700391: f1: 0.0003399626
Train-Acc: 3: Accuracy: 0.9000065923: precision: 0.5454545455: recall: 0.0003944514: f1: 0.0007883327
4: 6464: loss: 0.5760164279:
4: 12864: loss: 0.5751459211:
4: 19264: loss: 0.5740777449:
4: 25664: loss: 0.5734095879:
4: 32064: loss: 0.5731181147:
4: 38464: loss: 0.5724531985:
4: 44864: loss: 0.5713883422:
4: 51264: loss: 0.5703561558:
4: 57664: loss: 0.5694248998:
4: 64064: loss: 0.5686625982:
4: 70464: loss: 0.5680959654:
4: 76864: loss: 0.5671256149:
4: 83264: loss: 0.5661291905:
4: 89664: loss: 0.5653217883:
4: 96064: loss: 0.5644800807:
4: 102464: loss: 0.5636257099:
4: 108864: loss: 0.5627738546:
4: 115264: loss: 0.5618267895:
4: 121664: loss: 0.5608742706:
4: 128064: loss: 0.5599840300:
4: 134464: loss: 0.5593151948:
4: 140864: loss: 0.5586317565:
4: 147264: loss: 0.5578201347:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 6464: loss: 0.5361688432:
5: 12864: loss: 0.5358919477:
5: 19264: loss: 0.5348037536:
5: 25664: loss: 0.5343531337:
5: 32064: loss: 0.5332343321:
5: 38464: loss: 0.5331136008:
5: 44864: loss: 0.5326232831:
5: 51264: loss: 0.5322262345:
5: 57664: loss: 0.5316340129:
5: 64064: loss: 0.5308333424:
5: 70464: loss: 0.5301127536:
5: 76864: loss: 0.5291327920:
5: 83264: loss: 0.5283306455:
5: 89664: loss: 0.5276743135:
5: 96064: loss: 0.5269788953:
5: 102464: loss: 0.5262718238:
5: 108864: loss: 0.5253637271:
5: 115264: loss: 0.5245055265:
5: 121664: loss: 0.5235244574:
5: 128064: loss: 0.5224757849:
5: 134464: loss: 0.5216499412:
5: 140864: loss: 0.5207293767:
5: 147264: loss: 0.5199481851:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 6464: loss: 0.4971549714:
6: 12864: loss: 0.4997183144:
6: 19264: loss: 0.4990434919:
6: 25664: loss: 0.4987636048:
6: 32064: loss: 0.4974894434:
6: 38464: loss: 0.4967866390:
6: 44864: loss: 0.4961707799:
6: 51264: loss: 0.4960364541:
6: 57664: loss: 0.4957045880:
6: 64064: loss: 0.4947415146:
6: 70464: loss: 0.4934744514:
6: 76864: loss: 0.4928099054:
6: 83264: loss: 0.4922001810:
6: 89664: loss: 0.4915001076:
6: 96064: loss: 0.4907536440:
6: 102464: loss: 0.4895716649:
6: 108864: loss: 0.4889953609:
6: 115264: loss: 0.4883618236:
6: 121664: loss: 0.4876741849:
6: 128064: loss: 0.4869143747:
6: 134464: loss: 0.4861243839:
6: 140864: loss: 0.4852997937:
6: 147264: loss: 0.4845845005:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 6464: loss: 0.4681190372:
7: 12864: loss: 0.4671439521:
7: 19264: loss: 0.4662729323:
7: 25664: loss: 0.4653504524:
7: 32064: loss: 0.4658559044:
7: 38464: loss: 0.4638764289:
7: 44864: loss: 0.4628825550:
7: 51264: loss: 0.4624600452:
7: 57664: loss: 0.4617032364:
7: 64064: loss: 0.4610001190:
7: 70464: loss: 0.4601709987:
7: 76864: loss: 0.4591932526:
7: 83264: loss: 0.4588061979:
7: 89664: loss: 0.4581009318:
7: 96064: loss: 0.4574555904:
7: 102464: loss: 0.4567389285:
7: 108864: loss: 0.4558792685:
7: 115264: loss: 0.4554561093:
7: 121664: loss: 0.4547948332:
7: 128064: loss: 0.4542426523:
7: 134464: loss: 0.4534278875:
7: 140864: loss: 0.4527749041:
7: 147264: loss: 0.4521161820:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.4387595627:
8: 12864: loss: 0.4367874891:
8: 19264: loss: 0.4358108544:
8: 25664: loss: 0.4343718939:
8: 32064: loss: 0.4346767333:
8: 38464: loss: 0.4346681497:
8: 44864: loss: 0.4345952914:
8: 51264: loss: 0.4335944900:
8: 57664: loss: 0.4319753147:
8: 64064: loss: 0.4311572122:
8: 70464: loss: 0.4299259806:
8: 76864: loss: 0.4294504257:
8: 83264: loss: 0.4289102788:
8: 89664: loss: 0.4279444519:
8: 96064: loss: 0.4273063903:
8: 102464: loss: 0.4263009592:
8: 108864: loss: 0.4260811777:
8: 115264: loss: 0.4256115832:
8: 121664: loss: 0.4251414835:
8: 128064: loss: 0.4243208888:
8: 134464: loss: 0.4237561604:
8: 140864: loss: 0.4231384183:
8: 147264: loss: 0.4228839749:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.4079076082:
9: 12864: loss: 0.4080186902:
9: 19264: loss: 0.4068326656:
9: 25664: loss: 0.4057258023:
9: 32064: loss: 0.4046074426:
9: 38464: loss: 0.4031750943:
9: 44864: loss: 0.4027662269:
9: 51264: loss: 0.4022746631:
9: 57664: loss: 0.4023660064:
9: 64064: loss: 0.4020127721:
9: 70464: loss: 0.4017006688:
9: 76864: loss: 0.4020191776:
9: 83264: loss: 0.4014797983:
9: 89664: loss: 0.4008554200:
9: 96064: loss: 0.4004766352:
9: 102464: loss: 0.4006014802:
9: 108864: loss: 0.4003460812:
9: 115264: loss: 0.4001226417:
9: 121664: loss: 0.3997814412:
9: 128064: loss: 0.3993993003:
9: 134464: loss: 0.3988933740:
9: 140864: loss: 0.3980178437:
9: 147264: loss: 0.3973976375:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.3863253698:
10: 12864: loss: 0.3820098650:
10: 19264: loss: 0.3813394373:
10: 25664: loss: 0.3826755106:
10: 32064: loss: 0.3829210525:
10: 38464: loss: 0.3808250647:
10: 44864: loss: 0.3802064815:
10: 51264: loss: 0.3800316115:
10: 57664: loss: 0.3800769250:
10: 64064: loss: 0.3799592595:
10: 70464: loss: 0.3798649477:
10: 76864: loss: 0.3786494505:
10: 83264: loss: 0.3787961458:
10: 89664: loss: 0.3785727860:
10: 96064: loss: 0.3786570230:
10: 102464: loss: 0.3778567054:
10: 108864: loss: 0.3776903186:
10: 115264: loss: 0.3773251781:
10: 121664: loss: 0.3769066269:
10: 128064: loss: 0.3765231337:
10: 134464: loss: 0.3761352195:
10: 140864: loss: 0.3753793579:
10: 147264: loss: 0.3753045033:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.3708743635:
11: 12864: loss: 0.3664788677:
11: 19264: loss: 0.3659280575:
11: 25664: loss: 0.3639677801:
11: 32064: loss: 0.3633801884:
11: 38464: loss: 0.3622855853:
11: 44864: loss: 0.3607780904:
11: 51264: loss: 0.3609474485:
11: 57664: loss: 0.3596768504:
11: 64064: loss: 0.3593080049:
11: 70464: loss: 0.3593340668:
11: 76864: loss: 0.3595959906:
11: 83264: loss: 0.3595906812:
11: 89664: loss: 0.3590423299:
11: 96064: loss: 0.3589652799:
11: 102464: loss: 0.3584538222:
11: 108864: loss: 0.3584147214:
11: 115264: loss: 0.3582416803:
11: 121664: loss: 0.3577587618:
11: 128064: loss: 0.3576326673:
11: 134464: loss: 0.3570992219:
11: 140864: loss: 0.3565415989:
11: 147264: loss: 0.3567863567:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.3553563252:
12: 12864: loss: 0.3478299502:
12: 19264: loss: 0.3487106913:
12: 25664: loss: 0.3478624952:
12: 32064: loss: 0.3477858797:
12: 38464: loss: 0.3479429791:
12: 44864: loss: 0.3480089550:
12: 51264: loss: 0.3465698741:
12: 57664: loss: 0.3461709226:
12: 64064: loss: 0.3461610691:
12: 70464: loss: 0.3440625999:
12: 76864: loss: 0.3444231065:
12: 83264: loss: 0.3446309891:
12: 89664: loss: 0.3440191544:
12: 96064: loss: 0.3436178076:
12: 102464: loss: 0.3436573071:
12: 108864: loss: 0.3434929749:
12: 115264: loss: 0.3427444851:
12: 121664: loss: 0.3425879386:
12: 128064: loss: 0.3422470562:
12: 134464: loss: 0.3418252511:
12: 140864: loss: 0.3419089679:
12: 147264: loss: 0.3414246836:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.3417544302:
13: 12864: loss: 0.3347549030:
13: 19264: loss: 0.3359781657:
13: 25664: loss: 0.3353518950:
13: 32064: loss: 0.3328680047:
13: 38464: loss: 0.3322001088:
13: 44864: loss: 0.3319906123:
13: 51264: loss: 0.3321914040:
13: 57664: loss: 0.3317393894:
13: 64064: loss: 0.3311871853:
13: 70464: loss: 0.3315955698:
13: 76864: loss: 0.3316318136:
13: 83264: loss: 0.3315097370:
13: 89664: loss: 0.3307332255:
13: 96064: loss: 0.3307893384:
13: 102464: loss: 0.3306637097:
13: 108864: loss: 0.3305194350:
13: 115264: loss: 0.3303838918:
13: 121664: loss: 0.3295670622:
13: 128064: loss: 0.3295815411:
13: 134464: loss: 0.3294478580:
13: 140864: loss: 0.3291843231:
13: 147264: loss: 0.3283589043:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.3288388340:
14: 12864: loss: 0.3257270944:
14: 19264: loss: 0.3231349791:
14: 25664: loss: 0.3224537957:
14: 32064: loss: 0.3236453422:
14: 38464: loss: 0.3228847844:
14: 44864: loss: 0.3224541158:
14: 51264: loss: 0.3224164995:
14: 57664: loss: 0.3211879516:
14: 64064: loss: 0.3217602779:
14: 70464: loss: 0.3205996727:
14: 76864: loss: 0.3202254518:
14: 83264: loss: 0.3207020488:
14: 89664: loss: 0.3203195483:
14: 96064: loss: 0.3202840340:
14: 102464: loss: 0.3197614296:
14: 108864: loss: 0.3194848879:
14: 115264: loss: 0.3194189985:
14: 121664: loss: 0.3186316836:
14: 128064: loss: 0.3180752431:
14: 134464: loss: 0.3178455815:
14: 140864: loss: 0.3174682403:
14: 147264: loss: 0.3168732686:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.3036046849:
15: 12864: loss: 0.3071806476:
15: 19264: loss: 0.3072752257:
15: 25664: loss: 0.3074493359:
15: 32064: loss: 0.3101492966:
15: 38464: loss: 0.3108012828:
15: 44864: loss: 0.3108550100:
15: 51264: loss: 0.3098090953:
15: 57664: loss: 0.3097104442:
15: 64064: loss: 0.3093271995:
15: 70464: loss: 0.3096873102:
15: 76864: loss: 0.3107172220:
15: 83264: loss: 0.3106184840:
15: 89664: loss: 0.3103134303:
15: 96064: loss: 0.3106970424:
15: 102464: loss: 0.3101504533:
15: 108864: loss: 0.3097931213:
15: 115264: loss: 0.3097070921:
15: 121664: loss: 0.3090261628:
15: 128064: loss: 0.3081359885:
15: 134464: loss: 0.3081544823:
15: 140864: loss: 0.3076396060:
15: 147264: loss: 0.3075018019:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.3046423182:
16: 12864: loss: 0.2986404123:
16: 19264: loss: 0.2982275104:
16: 25664: loss: 0.2985490220:
16: 32064: loss: 0.3005838503:
16: 38464: loss: 0.3004012815:
16: 44864: loss: 0.3016370810:
16: 51264: loss: 0.3005561542:
16: 57664: loss: 0.2997685470:
16: 64064: loss: 0.3001702840:
16: 70464: loss: 0.3001739516:
16: 76864: loss: 0.3007288123:
16: 83264: loss: 0.3009007358:
16: 89664: loss: 0.3016823694:
16: 96064: loss: 0.3013533369:
16: 102464: loss: 0.3005733899:
16: 108864: loss: 0.3005052468:
16: 115264: loss: 0.3000244788:
16: 121664: loss: 0.2998533614:
16: 128064: loss: 0.2999266905:
16: 134464: loss: 0.2995439651:
16: 140864: loss: 0.2989566445:
16: 147264: loss: 0.2992532485:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.3004249200:
17: 12864: loss: 0.2952615345:
17: 19264: loss: 0.2948085088:
17: 25664: loss: 0.2937689892:
17: 32064: loss: 0.2944371136:
17: 38464: loss: 0.2919535640:
17: 44864: loss: 0.2919298820:
17: 51264: loss: 0.2926090395:
17: 57664: loss: 0.2940553734:
17: 64064: loss: 0.2946338928:
17: 70464: loss: 0.2942977428:
17: 76864: loss: 0.2933565275:
17: 83264: loss: 0.2924157328:
17: 89664: loss: 0.2933100270:
17: 96064: loss: 0.2930432385:
17: 102464: loss: 0.2935965826:
17: 108864: loss: 0.2934033012:
17: 115264: loss: 0.2935143626:
17: 121664: loss: 0.2928265283:
17: 128064: loss: 0.2926673677:
17: 134464: loss: 0.2922984016:
17: 140864: loss: 0.2922874048:
17: 147264: loss: 0.2920108028:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.2878292498:
18: 12864: loss: 0.2808023223:
18: 19264: loss: 0.2827822673:
18: 25664: loss: 0.2850368871:
18: 32064: loss: 0.2856704293:
18: 38464: loss: 0.2852313510:
18: 44864: loss: 0.2853479560:
18: 51264: loss: 0.2851395425:
18: 57664: loss: 0.2857284279:
18: 64064: loss: 0.2856220455:
18: 70464: loss: 0.2858234558:
18: 76864: loss: 0.2859291477:
18: 83264: loss: 0.2858712779:
18: 89664: loss: 0.2861693189:
18: 96064: loss: 0.2866695422:
18: 102464: loss: 0.2862607949:
18: 108864: loss: 0.2859417125:
18: 115264: loss: 0.2853038761:
18: 121664: loss: 0.2853043133:
18: 128064: loss: 0.2850335532:
18: 134464: loss: 0.2846564554:
18: 140864: loss: 0.2840965970:
18: 147264: loss: 0.2846070751:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 6464: loss: 0.2826374587:
19: 12864: loss: 0.2807701850:
19: 19264: loss: 0.2848830083:
19: 25664: loss: 0.2845109293:
19: 32064: loss: 0.2845823729:
19: 38464: loss: 0.2817572510:
19: 44864: loss: 0.2813236551:
19: 51264: loss: 0.2795280563:
19: 57664: loss: 0.2784008228:
19: 64064: loss: 0.2780118037:
19: 70464: loss: 0.2784284592:
19: 76864: loss: 0.2783486849:
19: 83264: loss: 0.2773713144:
19: 89664: loss: 0.2777571149:
19: 96064: loss: 0.2784915822:
19: 102464: loss: 0.2789285532:
19: 108864: loss: 0.2789664741:
19: 115264: loss: 0.2783003523:
19: 121664: loss: 0.2786789727:
19: 128064: loss: 0.2789845124:
19: 134464: loss: 0.2784730998:
19: 140864: loss: 0.2787287735:
19: 147264: loss: 0.2790835300:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.9000197053: precision: 1.0000000000: recall: 0.0001972257: f1: 0.0003943736
20: 6464: loss: 0.2749191704:
20: 12864: loss: 0.2748423479:
20: 19264: loss: 0.2752631489:
20: 25664: loss: 0.2732130214:
20: 32064: loss: 0.2739111111:
20: 38464: loss: 0.2739964476:
20: 44864: loss: 0.2743799942:
20: 51264: loss: 0.2753942066:
20: 57664: loss: 0.2762448201:
20: 64064: loss: 0.2755461972:
20: 70464: loss: 0.2761430109:
20: 76864: loss: 0.2762787838:
20: 83264: loss: 0.2757717454:
20: 89664: loss: 0.2756380149:
20: 96064: loss: 0.2755276501:
20: 102464: loss: 0.2750039159:
20: 108864: loss: 0.2756983263:
20: 115264: loss: 0.2749928150:
20: 121664: loss: 0.2741953099:
20: 128064: loss: 0.2737431510:
20: 134464: loss: 0.2736171197:
20: 140864: loss: 0.2730712645:
20: 147264: loss: 0.2732592579:
Dev-Acc: 20: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 20: Accuracy: 0.9003089666: precision: 1.0000000000: recall: 0.0030898692: f1: 0.0061607026
21: 6464: loss: 0.2747270845:
21: 12864: loss: 0.2715936600:
21: 19264: loss: 0.2721269155:
21: 25664: loss: 0.2696315309:
21: 32064: loss: 0.2701466339:
21: 38464: loss: 0.2706088413:
21: 44864: loss: 0.2699092953:
21: 51264: loss: 0.2706114018:
21: 57664: loss: 0.2703260665:
21: 64064: loss: 0.2705700988:
21: 70464: loss: 0.2694847896:
21: 76864: loss: 0.2688842895:
21: 83264: loss: 0.2690929725:
21: 89664: loss: 0.2694050307:
21: 96064: loss: 0.2689801908:
21: 102464: loss: 0.2679201012:
21: 108864: loss: 0.2682776337:
21: 115264: loss: 0.2685083645:
21: 121664: loss: 0.2689056317:
21: 128064: loss: 0.2685585840:
21: 134464: loss: 0.2681420069:
21: 140864: loss: 0.2679757630:
21: 147264: loss: 0.2678328397:
Dev-Acc: 21: Accuracy: 0.9416673183: precision: 0.6000000000: recall: 0.0010202347: f1: 0.0020370056
Train-Acc: 21: Accuracy: 0.9009006619: precision: 0.9659863946: recall: 0.0093353494: f1: 0.0184919911
22: 6464: loss: 0.2586510378:
22: 12864: loss: 0.2571948671:
22: 19264: loss: 0.2617091800:
22: 25664: loss: 0.2615952915:
22: 32064: loss: 0.2638851308:
22: 38464: loss: 0.2637976929:
22: 44864: loss: 0.2652337270:
22: 51264: loss: 0.2647218561:
22: 57664: loss: 0.2644631680:
22: 64064: loss: 0.2648983320:
22: 70464: loss: 0.2640874509:
22: 76864: loss: 0.2632888974:
22: 83264: loss: 0.2626135473:
22: 89664: loss: 0.2629837134:
22: 96064: loss: 0.2629588204:
22: 102464: loss: 0.2634078466:
22: 108864: loss: 0.2629934380:
22: 115264: loss: 0.2625564168:
22: 121664: loss: 0.2630745982:
22: 128064: loss: 0.2629179595:
22: 134464: loss: 0.2627833083:
22: 140864: loss: 0.2633283667:
22: 147264: loss: 0.2633189697:
Dev-Acc: 22: Accuracy: 0.9418458939: precision: 0.6470588235: recall: 0.0074817208: f1: 0.0147924021
Train-Acc: 22: Accuracy: 0.9020971656: precision: 0.8015122873: recall: 0.0278745645: f1: 0.0538754765
23: 6464: loss: 0.2551172715:
23: 12864: loss: 0.2605368566:
23: 19264: loss: 0.2644075098:
23: 25664: loss: 0.2637970105:
23: 32064: loss: 0.2624416298:
23: 38464: loss: 0.2620942649:
23: 44864: loss: 0.2598311565:
23: 51264: loss: 0.2597567171:
23: 57664: loss: 0.2596053745:
23: 64064: loss: 0.2601802185:
23: 70464: loss: 0.2598604879:
23: 76864: loss: 0.2604278951:
23: 83264: loss: 0.2604622982:
23: 89664: loss: 0.2608547906:
23: 96064: loss: 0.2605697348:
23: 102464: loss: 0.2598300301:
23: 108864: loss: 0.2601485463:
23: 115264: loss: 0.2604073557:
23: 121664: loss: 0.2605413543:
23: 128064: loss: 0.2597528655:
23: 134464: loss: 0.2595643611:
23: 140864: loss: 0.2592262378:
23: 147264: loss: 0.2590360763:
Dev-Acc: 23: Accuracy: 0.9422031045: precision: 0.7413793103: recall: 0.0146233634: f1: 0.0286810072
Train-Acc: 23: Accuracy: 0.9026428461: precision: 0.8130841121: recall: 0.0343172704: f1: 0.0658550432
24: 6464: loss: 0.2589362452:
24: 12864: loss: 0.2589108354:
24: 19264: loss: 0.2552622623:
24: 25664: loss: 0.2535136338:
24: 32064: loss: 0.2526053102:
24: 38464: loss: 0.2534548707:
24: 44864: loss: 0.2536420202:
24: 51264: loss: 0.2545220267:
24: 57664: loss: 0.2550298070:
24: 64064: loss: 0.2548205034:
24: 70464: loss: 0.2559464007:
24: 76864: loss: 0.2557632785:
24: 83264: loss: 0.2559007253:
24: 89664: loss: 0.2561238490:
24: 96064: loss: 0.2552703974:
24: 102464: loss: 0.2555020772:
24: 108864: loss: 0.2554502223:
24: 115264: loss: 0.2547636146:
24: 121664: loss: 0.2551737269:
24: 128064: loss: 0.2552898920:
24: 134464: loss: 0.2551413522:
24: 140864: loss: 0.2549103882:
24: 147264: loss: 0.2550611518:
Dev-Acc: 24: Accuracy: 0.9422923923: precision: 0.6245210728: recall: 0.0277163748: f1: 0.0530771736
Train-Acc: 24: Accuracy: 0.9037341475: precision: 0.8471882641: recall: 0.0455591348: f1: 0.0864682762
25: 6464: loss: 0.2485138915:
25: 12864: loss: 0.2502017376:
25: 19264: loss: 0.2533040543:
25: 25664: loss: 0.2522320423:
25: 32064: loss: 0.2531943449:
25: 38464: loss: 0.2515117233:
25: 44864: loss: 0.2524509351:
25: 51264: loss: 0.2538757120:
25: 57664: loss: 0.2538441178:
25: 64064: loss: 0.2535605854:
25: 70464: loss: 0.2528836643:
25: 76864: loss: 0.2516924200:
25: 83264: loss: 0.2517710165:
25: 89664: loss: 0.2520672655:
25: 96064: loss: 0.2517208553:
25: 102464: loss: 0.2514843389:
25: 108864: loss: 0.2510147199:
25: 115264: loss: 0.2510149072:
25: 121664: loss: 0.2508467020:
25: 128064: loss: 0.2505070434:
25: 134464: loss: 0.2508443957:
25: 140864: loss: 0.2507215034:
25: 147264: loss: 0.2508837723:
Dev-Acc: 25: Accuracy: 0.9420145750: precision: 0.5495978552: recall: 0.0348580173: f1: 0.0655580429
Train-Acc: 25: Accuracy: 0.9047465324: precision: 0.8525390625: recall: 0.0573926764: f1: 0.1075454265
26: 6464: loss: 0.2473959892:
26: 12864: loss: 0.2475859695:
26: 19264: loss: 0.2472423529:
26: 25664: loss: 0.2496362952:
26: 32064: loss: 0.2505864408:
26: 38464: loss: 0.2508190180:
26: 44864: loss: 0.2508787837:
26: 51264: loss: 0.2498785880:
26: 57664: loss: 0.2492150333:
26: 64064: loss: 0.2491694133:
26: 70464: loss: 0.2490048354:
26: 76864: loss: 0.2490197915:
26: 83264: loss: 0.2490718318:
26: 89664: loss: 0.2489648061:
26: 96064: loss: 0.2484023547:
26: 102464: loss: 0.2489814686:
26: 108864: loss: 0.2488850778:
26: 115264: loss: 0.2488834547:
26: 121664: loss: 0.2490862766:
26: 128064: loss: 0.2490582765:
26: 134464: loss: 0.2485968424:
26: 140864: loss: 0.2481643061:
26: 147264: loss: 0.2481967428:
Dev-Acc: 26: Accuracy: 0.9419947267: precision: 0.5381263617: recall: 0.0419996599: f1: 0.0779179811
Train-Acc: 26: Accuracy: 0.9056274891: precision: 0.8513957307: recall: 0.0681743475: f1: 0.1262401850
27: 6464: loss: 0.2618151726:
27: 12864: loss: 0.2472486877:
27: 19264: loss: 0.2465298635:
27: 25664: loss: 0.2446318580:
27: 32064: loss: 0.2444665187:
27: 38464: loss: 0.2443618448:
27: 44864: loss: 0.2454185969:
27: 51264: loss: 0.2454774731:
27: 57664: loss: 0.2457973000:
27: 64064: loss: 0.2461223853:
27: 70464: loss: 0.2460112029:
27: 76864: loss: 0.2451785749:
27: 83264: loss: 0.2455308439:
27: 89664: loss: 0.2454356843:
27: 96064: loss: 0.2450451749:
27: 102464: loss: 0.2451828237:
27: 108864: loss: 0.2447562281:
27: 115264: loss: 0.2451521905:
27: 121664: loss: 0.2444468056:
27: 128064: loss: 0.2445106110:
27: 134464: loss: 0.2444428492:
27: 140864: loss: 0.2445589266:
27: 147264: loss: 0.2445242122:
Dev-Acc: 27: Accuracy: 0.9422824979: precision: 0.5533333333: recall: 0.0564529842: f1: 0.1024533251
Train-Acc: 27: Accuracy: 0.9074419737: precision: 0.8618925831: recall: 0.0886200776: f1: 0.1607153502
28: 6464: loss: 0.2483528328:
28: 12864: loss: 0.2457771976:
28: 19264: loss: 0.2450941722:
28: 25664: loss: 0.2430033222:
28: 32064: loss: 0.2426225531:
28: 38464: loss: 0.2428915448:
28: 44864: loss: 0.2421565455:
28: 51264: loss: 0.2432989855:
28: 57664: loss: 0.2435369413:
28: 64064: loss: 0.2429620790:
28: 70464: loss: 0.2427383284:
28: 76864: loss: 0.2419724035:
28: 83264: loss: 0.2418058637:
28: 89664: loss: 0.2418144074:
28: 96064: loss: 0.2413770085:
28: 102464: loss: 0.2414708413:
28: 108864: loss: 0.2413284603:
28: 115264: loss: 0.2411685456:
28: 121664: loss: 0.2409380464:
28: 128064: loss: 0.2413464582:
28: 134464: loss: 0.2416028071:
28: 140864: loss: 0.2413548733:
28: 147264: loss: 0.2412377499:
Dev-Acc: 28: Accuracy: 0.9432548881: precision: 0.5933179724: recall: 0.0875701411: f1: 0.1526152023
Train-Acc: 28: Accuracy: 0.9092432857: precision: 0.8631198347: recall: 0.1098547104: f1: 0.1949028985
29: 6464: loss: 0.2431780771:
29: 12864: loss: 0.2376759616:
29: 19264: loss: 0.2398634980:
29: 25664: loss: 0.2405720262:
29: 32064: loss: 0.2391043770:
29: 38464: loss: 0.2406123256:
29: 44864: loss: 0.2411414415:
29: 51264: loss: 0.2401275328:
29: 57664: loss: 0.2399214923:
29: 64064: loss: 0.2396278498:
29: 70464: loss: 0.2396350988:
29: 76864: loss: 0.2391281306:
29: 83264: loss: 0.2394369509:
29: 89664: loss: 0.2391438272:
29: 96064: loss: 0.2386593599:
29: 102464: loss: 0.2388301407:
29: 108864: loss: 0.2392754324:
29: 115264: loss: 0.2397299535:
29: 121664: loss: 0.2400011335:
29: 128064: loss: 0.2392196052:
29: 134464: loss: 0.2390299254:
29: 140864: loss: 0.2387683851:
29: 147264: loss: 0.2379682935:
Dev-Acc: 29: Accuracy: 0.9430564046: precision: 0.5650183150: recall: 0.1049141302: f1: 0.1769683063
Train-Acc: 29: Accuracy: 0.9112089872: precision: 0.8607702074: recall: 0.1337190191: f1: 0.2314783202
30: 6464: loss: 0.2270919322:
30: 12864: loss: 0.2332228037:
30: 19264: loss: 0.2313854641:
30: 25664: loss: 0.2322226268:
30: 32064: loss: 0.2331966876:
30: 38464: loss: 0.2345943445:
30: 44864: loss: 0.2357554858:
30: 51264: loss: 0.2361153845:
30: 57664: loss: 0.2353379809:
30: 64064: loss: 0.2349974926:
30: 70464: loss: 0.2343951592:
30: 76864: loss: 0.2346833444:
30: 83264: loss: 0.2348740529:
30: 89664: loss: 0.2350545404:
30: 96064: loss: 0.2346871817:
30: 102464: loss: 0.2352090222:
30: 108864: loss: 0.2357951099:
30: 115264: loss: 0.2360417804:
30: 121664: loss: 0.2360078681:
30: 128064: loss: 0.2357942233:
30: 134464: loss: 0.2354184490:
30: 140864: loss: 0.2356376873:
30: 147264: loss: 0.2356084519:
Dev-Acc: 30: Accuracy: 0.9433441758: precision: 0.5680190931: recall: 0.1214079238: f1: 0.2000560381
Train-Acc: 30: Accuracy: 0.9128130674: precision: 0.8537205082: recall: 0.1546249425: f1: 0.2618278971
31: 6464: loss: 0.2297683576:
31: 12864: loss: 0.2355951973:
31: 19264: loss: 0.2372903259:
31: 25664: loss: 0.2347811691:
31: 32064: loss: 0.2312447684:
31: 38464: loss: 0.2305656191:
31: 44864: loss: 0.2305042258:
31: 51264: loss: 0.2310802128:
31: 57664: loss: 0.2321004575:
31: 64064: loss: 0.2330945583:
31: 70464: loss: 0.2328443982:
31: 76864: loss: 0.2318196324:
31: 83264: loss: 0.2314308579:
31: 89664: loss: 0.2317294249:
31: 96064: loss: 0.2320664920:
31: 102464: loss: 0.2318915392:
31: 108864: loss: 0.2318193524:
31: 115264: loss: 0.2317347982:
31: 121664: loss: 0.2316846210:
31: 128064: loss: 0.2326477875:
31: 134464: loss: 0.2325825143:
31: 140864: loss: 0.2326165406:
31: 147264: loss: 0.2329054179:
Dev-Acc: 31: Accuracy: 0.9437311292: precision: 0.5737359551: recall: 0.1389219520: f1: 0.2236824093
Train-Acc: 31: Accuracy: 0.9144303203: precision: 0.8347057030: recall: 0.1799355729: f1: 0.2960519200
32: 6464: loss: 0.2271006222:
32: 12864: loss: 0.2335969641:
32: 19264: loss: 0.2382839222:
32: 25664: loss: 0.2351184504:
32: 32064: loss: 0.2332583584:
32: 38464: loss: 0.2313511248:
32: 44864: loss: 0.2308226089:
32: 51264: loss: 0.2309910051:
32: 57664: loss: 0.2331357272:
32: 64064: loss: 0.2336149975:
32: 70464: loss: 0.2330773520:
32: 76864: loss: 0.2325431149:
32: 83264: loss: 0.2326672185:
32: 89664: loss: 0.2330862831:
32: 96064: loss: 0.2326654915:
32: 102464: loss: 0.2329219594:
32: 108864: loss: 0.2324015893:
32: 115264: loss: 0.2320724530:
32: 121664: loss: 0.2314417928:
32: 128064: loss: 0.2311365169:
32: 134464: loss: 0.2310740130:
32: 140864: loss: 0.2306744263:
32: 147264: loss: 0.2305609480:
Dev-Acc: 32: Accuracy: 0.9437212348: precision: 0.5636806825: recall: 0.1572861758: f1: 0.2459452273
Train-Acc: 32: Accuracy: 0.9158503413: precision: 0.8130355752: recall: 0.2058378805: f1: 0.3285069772
33: 6464: loss: 0.2284936472:
33: 12864: loss: 0.2286951248:
33: 19264: loss: 0.2288499061:
33: 25664: loss: 0.2284288847:
33: 32064: loss: 0.2280508619:
33: 38464: loss: 0.2275259076:
33: 44864: loss: 0.2277465701:
33: 51264: loss: 0.2276011947:
33: 57664: loss: 0.2284521664:
33: 64064: loss: 0.2282060043:
33: 70464: loss: 0.2296959636:
33: 76864: loss: 0.2298007425:
33: 83264: loss: 0.2299486085:
33: 89664: loss: 0.2295502798:
33: 96064: loss: 0.2292477903:
33: 102464: loss: 0.2293768910:
33: 108864: loss: 0.2292389636:
33: 115264: loss: 0.2288786367:
33: 121664: loss: 0.2279021690:
33: 128064: loss: 0.2283148535:
33: 134464: loss: 0.2281258234:
33: 140864: loss: 0.2276687966:
33: 147264: loss: 0.2275444963:
Dev-Acc: 33: Accuracy: 0.9428480864: precision: 0.5302651326: recall: 0.1802414555: f1: 0.2690355330
Train-Acc: 33: Accuracy: 0.9167115688: precision: 0.7989181562: recall: 0.2233252252: f1: 0.3490725993
34: 6464: loss: 0.2251379564:
34: 12864: loss: 0.2255549262:
34: 19264: loss: 0.2272098777:
34: 25664: loss: 0.2267322680:
34: 32064: loss: 0.2283145713:
34: 38464: loss: 0.2280199221:
34: 44864: loss: 0.2275412662:
34: 51264: loss: 0.2271907048:
34: 57664: loss: 0.2259589018:
34: 64064: loss: 0.2257461093:
34: 70464: loss: 0.2269430226:
34: 76864: loss: 0.2273232413:
34: 83264: loss: 0.2266813977:
34: 89664: loss: 0.2263037044:
34: 96064: loss: 0.2267203968:
34: 102464: loss: 0.2263720917:
34: 108864: loss: 0.2254576844:
34: 115264: loss: 0.2251060656:
34: 121664: loss: 0.2260141217:
34: 128064: loss: 0.2260305780:
34: 134464: loss: 0.2258610768:
34: 140864: loss: 0.2257301717:
34: 147264: loss: 0.2258907328:
Dev-Acc: 34: Accuracy: 0.9416871667: precision: 0.5008169935: recall: 0.2084679476: f1: 0.2943930844
Train-Acc: 34: Accuracy: 0.9175925255: precision: 0.7898613518: recall: 0.2396949576: f1: 0.3677813083
35: 6464: loss: 0.2206410948:
35: 12864: loss: 0.2160106004:
35: 19264: loss: 0.2189103015:
35: 25664: loss: 0.2222960046:
35: 32064: loss: 0.2204843609:
35: 38464: loss: 0.2231788854:
35: 44864: loss: 0.2228999680:
35: 51264: loss: 0.2224640262:
35: 57664: loss: 0.2219714691:
35: 64064: loss: 0.2221130100:
35: 70464: loss: 0.2222283740:
35: 76864: loss: 0.2222664800:
35: 83264: loss: 0.2217774510:
35: 89664: loss: 0.2214720477:
35: 96064: loss: 0.2223823143:
35: 102464: loss: 0.2223597979:
35: 108864: loss: 0.2225093655:
35: 115264: loss: 0.2229413592:
35: 121664: loss: 0.2231727558:
35: 128064: loss: 0.2237613856:
35: 134464: loss: 0.2237611978:
35: 140864: loss: 0.2233210723:
35: 147264: loss: 0.2232704851:
Dev-Acc: 35: Accuracy: 0.9409430027: precision: 0.4880752435: recall: 0.2470668254: f1: 0.3280650260
Train-Acc: 35: Accuracy: 0.9188022017: precision: 0.7904142973: recall: 0.2558674643: f1: 0.3865905140
36: 6464: loss: 0.2134976816:
36: 12864: loss: 0.2227431651:
36: 19264: loss: 0.2204240729:
36: 25664: loss: 0.2178337490:
36: 32064: loss: 0.2183474452:
36: 38464: loss: 0.2186861273:
36: 44864: loss: 0.2205674489:
36: 51264: loss: 0.2208496333:
36: 57664: loss: 0.2209129973:
36: 64064: loss: 0.2204357617:
36: 70464: loss: 0.2206051336:
36: 76864: loss: 0.2205834829:
36: 83264: loss: 0.2201794700:
36: 89664: loss: 0.2203002166:
36: 96064: loss: 0.2203192125:
36: 102464: loss: 0.2210875502:
36: 108864: loss: 0.2210520690:
36: 115264: loss: 0.2210549613:
36: 121664: loss: 0.2206786442:
36: 128064: loss: 0.2208068635:
36: 134464: loss: 0.2211751276:
36: 140864: loss: 0.2211207518:
36: 147264: loss: 0.2213952355:
Dev-Acc: 36: Accuracy: 0.9407048821: precision: 0.4855315260: recall: 0.2710423397: f1: 0.3478830205
Train-Acc: 36: Accuracy: 0.9198606014: precision: 0.7930164888: recall: 0.2687528762: f1: 0.4014534027
37: 6464: loss: 0.2131288808:
37: 12864: loss: 0.2187880337:
37: 19264: loss: 0.2218832284:
37: 25664: loss: 0.2222542908:
37: 32064: loss: 0.2222692752:
37: 38464: loss: 0.2211599593:
37: 44864: loss: 0.2215473266:
37: 51264: loss: 0.2206641422:
37: 57664: loss: 0.2194048047:
37: 64064: loss: 0.2194671138:
37: 70464: loss: 0.2200612907:
37: 76864: loss: 0.2203338658:
37: 83264: loss: 0.2204395640:
37: 89664: loss: 0.2206005280:
37: 96064: loss: 0.2204135610:
37: 102464: loss: 0.2204994445:
37: 108864: loss: 0.2205461346:
37: 115264: loss: 0.2205973971:
37: 121664: loss: 0.2205175000:
37: 128064: loss: 0.2202093231:
37: 134464: loss: 0.2198722354:
37: 140864: loss: 0.2199527832:
37: 147264: loss: 0.2199200723:
Dev-Acc: 37: Accuracy: 0.9404072165: precision: 0.4819207405: recall: 0.2832851556: f1: 0.3568215892
Train-Acc: 37: Accuracy: 0.9206100702: precision: 0.7930454291: recall: 0.2788771284: f1: 0.4126459144
38: 6464: loss: 0.2080703274:
38: 12864: loss: 0.2190963193:
38: 19264: loss: 0.2204716105:
38: 25664: loss: 0.2197970695:
38: 32064: loss: 0.2215069742:
38: 38464: loss: 0.2208425909:
38: 44864: loss: 0.2211126814:
38: 51264: loss: 0.2204635187:
38: 57664: loss: 0.2194336274:
38: 64064: loss: 0.2201156272:
38: 70464: loss: 0.2199140436:
38: 76864: loss: 0.2200295090:
38: 83264: loss: 0.2197274981:
38: 89664: loss: 0.2192452135:
38: 96064: loss: 0.2183090206:
38: 102464: loss: 0.2188486660:
38: 108864: loss: 0.2192043799:
38: 115264: loss: 0.2187890930:
38: 121664: loss: 0.2186222119:
38: 128064: loss: 0.2178415032:
38: 134464: loss: 0.2175773067:
38: 140864: loss: 0.2173526118:
38: 147264: loss: 0.2170270647:
Dev-Acc: 38: Accuracy: 0.9400103092: precision: 0.4770769658: recall: 0.2919571501: f1: 0.3622362869
Train-Acc: 38: Accuracy: 0.9212346077: precision: 0.7937431793: recall: 0.2868976399: f1: 0.4214592689
39: 6464: loss: 0.2172808323:
39: 12864: loss: 0.2112567209:
39: 19264: loss: 0.2110731959:
39: 25664: loss: 0.2128615175:
39: 32064: loss: 0.2155243527:
39: 38464: loss: 0.2154058053:
39: 44864: loss: 0.2158391217:
39: 51264: loss: 0.2158021211:
39: 57664: loss: 0.2169509156:
39: 64064: loss: 0.2165502005:
39: 70464: loss: 0.2167653296:
39: 76864: loss: 0.2169627674:
39: 83264: loss: 0.2159444181:
39: 89664: loss: 0.2158130365:
39: 96064: loss: 0.2158646451:
39: 102464: loss: 0.2154673770:
39: 108864: loss: 0.2156083409:
39: 115264: loss: 0.2157565417:
39: 121664: loss: 0.2162127181:
39: 128064: loss: 0.2165789240:
39: 134464: loss: 0.2166355372:
39: 140864: loss: 0.2161449543:
39: 147264: loss: 0.2158809065:
Dev-Acc: 39: Accuracy: 0.9397721887: precision: 0.4746308725: recall: 0.3006291447: f1: 0.3681032688
Train-Acc: 39: Accuracy: 0.9217934608: precision: 0.7918647649: recall: 0.2956413122: f1: 0.4305409287
40: 6464: loss: 0.2114620189:
40: 12864: loss: 0.2077891862:
40: 19264: loss: 0.2093758845:
40: 25664: loss: 0.2108115184:
40: 32064: loss: 0.2120443609:
40: 38464: loss: 0.2119246252:
40: 44864: loss: 0.2123594541:
40: 51264: loss: 0.2133846000:
40: 57664: loss: 0.2147634559:
40: 64064: loss: 0.2160243504:
40: 70464: loss: 0.2156564165:
40: 76864: loss: 0.2150366648:
40: 83264: loss: 0.2148734807:
40: 89664: loss: 0.2148422765:
40: 96064: loss: 0.2150380343:
40: 102464: loss: 0.2151840984:
40: 108864: loss: 0.2147671101:
40: 115264: loss: 0.2145325988:
40: 121664: loss: 0.2145734643:
40: 128064: loss: 0.2145365657:
40: 134464: loss: 0.2141816766:
40: 140864: loss: 0.2142003746:
40: 147264: loss: 0.2143685728:
Dev-Acc: 40: Accuracy: 0.9393554330: precision: 0.4699140401: recall: 0.3067505526: f1: 0.3711934156
Train-Acc: 40: Accuracy: 0.9223850965: precision: 0.7904794404: recall: 0.3045822102: f1: 0.4397304480
41: 6464: loss: 0.2151236576:
41: 12864: loss: 0.2147688272:
41: 19264: loss: 0.2171733692:
41: 25664: loss: 0.2162056647:
41: 32064: loss: 0.2153050394:
41: 38464: loss: 0.2137160168:
41: 44864: loss: 0.2136514629:
41: 51264: loss: 0.2142746831:
41: 57664: loss: 0.2141108346:
41: 64064: loss: 0.2144332782:
41: 70464: loss: 0.2144623246:
41: 76864: loss: 0.2139124776:
41: 83264: loss: 0.2134173107:
41: 89664: loss: 0.2125924931:
41: 96064: loss: 0.2124122579:
41: 102464: loss: 0.2119823077:
41: 108864: loss: 0.2119193163:
41: 115264: loss: 0.2117391460:
41: 121664: loss: 0.2122353367:
41: 128064: loss: 0.2122899453:
41: 134464: loss: 0.2122804102:
41: 140864: loss: 0.2124923054:
41: 147264: loss: 0.2122636224:
Dev-Acc: 41: Accuracy: 0.9391768575: precision: 0.4681992337: recall: 0.3116816868: f1: 0.3742343814
Train-Acc: 41: Accuracy: 0.9231213927: precision: 0.7919641375: recall: 0.3135888502: f1: 0.4492794575
42: 6464: loss: 0.2108743339:
42: 12864: loss: 0.2169895195:
42: 19264: loss: 0.2148161672:
42: 25664: loss: 0.2128718837:
42: 32064: loss: 0.2134478548:
42: 38464: loss: 0.2136215099:
42: 44864: loss: 0.2142095570:
42: 51264: loss: 0.2127024021:
42: 57664: loss: 0.2113861445:
42: 64064: loss: 0.2112642199:
42: 70464: loss: 0.2110177001:
42: 76864: loss: 0.2111032929:
42: 83264: loss: 0.2110577766:
42: 89664: loss: 0.2106303218:
42: 96064: loss: 0.2111128747:
42: 102464: loss: 0.2117256905:
42: 108864: loss: 0.2115297467:
42: 115264: loss: 0.2116287478:
42: 121664: loss: 0.2112806145:
42: 128064: loss: 0.2111842507:
42: 134464: loss: 0.2110183404:
42: 140864: loss: 0.2111504335:
42: 147264: loss: 0.2107463025:
Dev-Acc: 42: Accuracy: 0.9390081763: precision: 0.4665661136: recall: 0.3155925863: f1: 0.3765087737
Train-Acc: 42: Accuracy: 0.9235158563: precision: 0.7892608766: recall: 0.3208204589: f1: 0.4562026736
43: 6464: loss: 0.2161236185:
43: 12864: loss: 0.2116792386:
43: 19264: loss: 0.2125314262:
43: 25664: loss: 0.2127792051:
43: 32064: loss: 0.2108109473:
43: 38464: loss: 0.2104494769:
43: 44864: loss: 0.2091440211:
43: 51264: loss: 0.2082513560:
43: 57664: loss: 0.2098547860:
43: 64064: loss: 0.2095602650:
43: 70464: loss: 0.2097343061:
43: 76864: loss: 0.2098760925:
43: 83264: loss: 0.2102761913:
43: 89664: loss: 0.2098987098:
43: 96064: loss: 0.2098160654:
43: 102464: loss: 0.2100478057:
43: 108864: loss: 0.2094055552:
43: 115264: loss: 0.2097964980:
43: 121664: loss: 0.2095009439:
43: 128064: loss: 0.2093151527:
43: 134464: loss: 0.2091206336:
43: 140864: loss: 0.2093896960:
43: 147264: loss: 0.2099473153:
Dev-Acc: 43: Accuracy: 0.9389089346: precision: 0.4658922392: recall: 0.3205237205: f1: 0.3797723381
Train-Acc: 43: Accuracy: 0.9240615368: precision: 0.7890995261: recall: 0.3283807771: f1: 0.4637667703
44: 6464: loss: 0.2081294822:
44: 12864: loss: 0.2123136012:
44: 19264: loss: 0.2121472676:
44: 25664: loss: 0.2103491854:
44: 32064: loss: 0.2088099490:
44: 38464: loss: 0.2078396569:
44: 44864: loss: 0.2081828842:
44: 51264: loss: 0.2089533149:
44: 57664: loss: 0.2092208441:
44: 64064: loss: 0.2087507466:
44: 70464: loss: 0.2088907055:
44: 76864: loss: 0.2091012198:
44: 83264: loss: 0.2086257343:
44: 89664: loss: 0.2082775922:
44: 96064: loss: 0.2087583704:
44: 102464: loss: 0.2091225861:
44: 108864: loss: 0.2088063471:
44: 115264: loss: 0.2085721505:
44: 121664: loss: 0.2077811002:
44: 128064: loss: 0.2078795574:
44: 134464: loss: 0.2076258679:
44: 140864: loss: 0.2077748379:
44: 147264: loss: 0.2082556701:
Dev-Acc: 44: Accuracy: 0.9388593435: precision: 0.4660053230: recall: 0.3274953239: f1: 0.3846614739
Train-Acc: 44: Accuracy: 0.9246006012: precision: 0.7886454798: recall: 0.3360725791: f1: 0.4713041073
45: 6464: loss: 0.2062311353:
45: 12864: loss: 0.2033167780:
45: 19264: loss: 0.2043650223:
45: 25664: loss: 0.2037200255:
45: 32064: loss: 0.2053604466:
45: 38464: loss: 0.2052099586:
45: 44864: loss: 0.2055768910:
45: 51264: loss: 0.2059364314:
45: 57664: loss: 0.2069194084:
45: 64064: loss: 0.2072200997:
45: 70464: loss: 0.2074036561:
45: 76864: loss: 0.2073034284:
45: 83264: loss: 0.2074231855:
45: 89664: loss: 0.2078209417:
45: 96064: loss: 0.2072637694:
45: 102464: loss: 0.2072909211:
45: 108864: loss: 0.2072081321:
45: 115264: loss: 0.2074691721:
45: 121664: loss: 0.2074315653:
45: 128064: loss: 0.2076914267:
45: 134464: loss: 0.2076682222:
45: 140864: loss: 0.2074495004:
45: 147264: loss: 0.2071954205:
Dev-Acc: 45: Accuracy: 0.9385616779: precision: 0.4628966834: recall: 0.3298758715: f1: 0.3852263701
Train-Acc: 45: Accuracy: 0.9250608087: precision: 0.7865303668: recall: 0.3439616067: f1: 0.4786168412
46: 6464: loss: 0.2098609313:
46: 12864: loss: 0.2057353839:
46: 19264: loss: 0.2067884367:
46: 25664: loss: 0.2046340447:
46: 32064: loss: 0.2058567349:
46: 38464: loss: 0.2051281140:
46: 44864: loss: 0.2059262999:
46: 51264: loss: 0.2068523910:
46: 57664: loss: 0.2080610688:
46: 64064: loss: 0.2078656916:
46: 70464: loss: 0.2078615457:
46: 76864: loss: 0.2076762357:
46: 83264: loss: 0.2070031506:
46: 89664: loss: 0.2064779107:
46: 96064: loss: 0.2054801814:
46: 102464: loss: 0.2054354332:
46: 108864: loss: 0.2047117851:
46: 115264: loss: 0.2045716255:
46: 121664: loss: 0.2043602341:
46: 128064: loss: 0.2051573616:
46: 134464: loss: 0.2054013378:
46: 140864: loss: 0.2055226857:
46: 147264: loss: 0.2052966751:
Dev-Acc: 46: Accuracy: 0.9380853772: precision: 0.4580116959: recall: 0.3329365754: f1: 0.3855848759
Train-Acc: 46: Accuracy: 0.9255670309: precision: 0.7871806232: recall: 0.3504043127: f1: 0.4849422255
47: 6464: loss: 0.2094828857:
47: 12864: loss: 0.2061278950:
47: 19264: loss: 0.2069622878:
47: 25664: loss: 0.2066674192:
47: 32064: loss: 0.2060491527:
47: 38464: loss: 0.2051752952:
47: 44864: loss: 0.2041698133:
47: 51264: loss: 0.2030354655:
47: 57664: loss: 0.2037094374:
47: 64064: loss: 0.2036455712:
47: 70464: loss: 0.2039895232:
47: 76864: loss: 0.2042686819:
47: 83264: loss: 0.2045959021:
47: 89664: loss: 0.2049348059:
47: 96064: loss: 0.2046975373:
47: 102464: loss: 0.2048568620:
47: 108864: loss: 0.2051414123:
47: 115264: loss: 0.2047228686:
47: 121664: loss: 0.2046967922:
47: 128064: loss: 0.2048674392:
47: 134464: loss: 0.2046849546:
47: 140864: loss: 0.2043616223:
47: 147264: loss: 0.2043627977:
Dev-Acc: 47: Accuracy: 0.9376885295: precision: 0.4546075085: recall: 0.3397381398: f1: 0.3888672635
Train-Acc: 47: Accuracy: 0.9260403514: precision: 0.7864892232: recall: 0.3574386957: f1: 0.4915024408
48: 6464: loss: 0.2146705604:
48: 12864: loss: 0.2107085028:
48: 19264: loss: 0.2085871097:
48: 25664: loss: 0.2065964308:
48: 32064: loss: 0.2076743918:
48: 38464: loss: 0.2075743942:
48: 44864: loss: 0.2076662058:
48: 51264: loss: 0.2067808235:
48: 57664: loss: 0.2059971240:
48: 64064: loss: 0.2057828635:
48: 70464: loss: 0.2053827486:
48: 76864: loss: 0.2046939052:
48: 83264: loss: 0.2059112921:
48: 89664: loss: 0.2053791149:
48: 96064: loss: 0.2053740309:
48: 102464: loss: 0.2052482123:
48: 108864: loss: 0.2047830469:
48: 115264: loss: 0.2047899008:
48: 121664: loss: 0.2048576080:
48: 128064: loss: 0.2042393378:
48: 134464: loss: 0.2040457625:
48: 140864: loss: 0.2035293885:
48: 147264: loss: 0.2034051782:
Dev-Acc: 48: Accuracy: 0.9373213649: precision: 0.4511648746: recall: 0.3424587655: f1: 0.3893668439
Train-Acc: 48: Accuracy: 0.9261783957: precision: 0.7844285714: recall: 0.3609887581: f1: 0.4944396920
49: 6464: loss: 0.2088091398:
49: 12864: loss: 0.2074839874:
49: 19264: loss: 0.2074366784:
49: 25664: loss: 0.2074873166:
49: 32064: loss: 0.2045764633:
49: 38464: loss: 0.2043000497:
49: 44864: loss: 0.2028476857:
49: 51264: loss: 0.2036536641:
49: 57664: loss: 0.2032576594:
49: 64064: loss: 0.2027258161:
49: 70464: loss: 0.2023432813:
49: 76864: loss: 0.2017980052:
49: 83264: loss: 0.2026292681:
49: 89664: loss: 0.2026353902:
49: 96064: loss: 0.2027084533:
49: 102464: loss: 0.2023185319:
49: 108864: loss: 0.2016730201:
49: 115264: loss: 0.2015448306:
49: 121664: loss: 0.2014952670:
49: 128064: loss: 0.2016619786:
49: 134464: loss: 0.2024135692:
49: 140864: loss: 0.2021064914:
49: 147264: loss: 0.2022739439:
Dev-Acc: 49: Accuracy: 0.9371725321: precision: 0.4501657459: recall: 0.3463696650: f1: 0.3915049010
Train-Acc: 49: Accuracy: 0.9265860319: precision: 0.7843881857: recall: 0.3666425613: f1: 0.4997087944
50: 6464: loss: 0.1959526630:
50: 12864: loss: 0.1964386914:
50: 19264: loss: 0.1981950033:
50: 25664: loss: 0.1994615747:
50: 32064: loss: 0.1979811411:
50: 38464: loss: 0.1988942755:
50: 44864: loss: 0.1994101663:
50: 51264: loss: 0.2005601477:
50: 57664: loss: 0.1998001925:
50: 64064: loss: 0.2011380077:
50: 70464: loss: 0.2013495486:
50: 76864: loss: 0.2013087914:
50: 83264: loss: 0.2010214435:
50: 89664: loss: 0.2008487262:
50: 96064: loss: 0.2016078765:
50: 102464: loss: 0.2012636096:
50: 108864: loss: 0.2012539010:
50: 115264: loss: 0.2013048709:
50: 121664: loss: 0.2009696236:
50: 128064: loss: 0.2009527355:
50: 134464: loss: 0.2010870772:
50: 140864: loss: 0.2015671313:
50: 147264: loss: 0.2012967713:
Dev-Acc: 50: Accuracy: 0.9368748665: precision: 0.4472703355: recall: 0.3468797823: f1: 0.3907297453
Train-Acc: 50: Accuracy: 0.9268621206: precision: 0.7844611529: recall: 0.3703898495: f1: 0.5031929621
