1: 3232: loss: 0.7015961999:
1: 6432: loss: 0.6930018592:
1: 9632: loss: 0.6847791090:
1: 12832: loss: 0.6767913190:
1: 16032: loss: 0.6688336489:
1: 19232: loss: 0.6606927822:
1: 22432: loss: 0.6527274163:
1: 25632: loss: 0.6447100092:
1: 28832: loss: 0.6374052811:
1: 32032: loss: 0.6303164517:
1: 35232: loss: 0.6228978692:
1: 38432: loss: 0.6159002175:
1: 41632: loss: 0.6085032414:
1: 44832: loss: 0.6015645077:
1: 48032: loss: 0.5946469700:
1: 51232: loss: 0.5875248696:
1: 54432: loss: 0.5813098489:
1: 57632: loss: 0.5750567898:
1: 60832: loss: 0.5687962721:
1: 64032: loss: 0.5625917547:
1: 67232: loss: 0.5564990709:
1: 70432: loss: 0.5501255646:
1: 73632: loss: 0.5440804235:
1: 76832: loss: 0.5379526871:
1: 80032: loss: 0.5322121125:
1: 83232: loss: 0.5271091854:
1: 86432: loss: 0.5213166721:
1: 89632: loss: 0.5156409244:
1: 92832: loss: 0.5108724474:
1: 96032: loss: 0.5059042172:
1: 99232: loss: 0.5013345987:
1: 102432: loss: 0.4969020075:
1: 105632: loss: 0.4921448561:
1: 108832: loss: 0.4882453827:
1: 112032: loss: 0.4841617373:
1: 115232: loss: 0.4798693160:
1: 118432: loss: 0.4760781247:
1: 121632: loss: 0.4726145065:
1: 124832: loss: 0.4691957852:
1: 128032: loss: 0.4657456900:
1: 131232: loss: 0.4624520344:
1: 134432: loss: 0.4591550752:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 3232: loss: 0.3124374622:
2: 6432: loss: 0.3160554215:
2: 9632: loss: 0.3104701934:
2: 12832: loss: 0.3107448006:
2: 16032: loss: 0.3098116855:
2: 19232: loss: 0.3072461584:
2: 22432: loss: 0.3057266008:
2: 25632: loss: 0.3053344755:
2: 28832: loss: 0.3041641036:
2: 32032: loss: 0.3027798151:
2: 35232: loss: 0.3018170260:
2: 38432: loss: 0.3013798391:
2: 41632: loss: 0.3002397571:
2: 44832: loss: 0.2982854988:
2: 48032: loss: 0.2975914525:
2: 51232: loss: 0.2959578726:
2: 54432: loss: 0.2960039526:
2: 57632: loss: 0.2957637526:
2: 60832: loss: 0.2944098772:
2: 64032: loss: 0.2926134468:
2: 67232: loss: 0.2918999164:
2: 70432: loss: 0.2909159836:
2: 73632: loss: 0.2894024728:
2: 76832: loss: 0.2883671106:
2: 80032: loss: 0.2877339404:
2: 83232: loss: 0.2870463792:
2: 86432: loss: 0.2866078909:
2: 89632: loss: 0.2859927006:
2: 92832: loss: 0.2855147351:
2: 96032: loss: 0.2848729614:
2: 99232: loss: 0.2842601274:
2: 102432: loss: 0.2832540168:
2: 105632: loss: 0.2821395981:
2: 108832: loss: 0.2815584895:
2: 112032: loss: 0.2809032536:
2: 115232: loss: 0.2800536909:
2: 118432: loss: 0.2796869285:
2: 121632: loss: 0.2790742843:
2: 124832: loss: 0.2781399060:
2: 128032: loss: 0.2772717999:
2: 131232: loss: 0.2766408178:
2: 134432: loss: 0.2761198079:
Dev-Acc: 2: Accuracy: 0.9428282380: precision: 0.5275080906: recall: 0.1940146234: f1: 0.2836897066
Train-Acc: 2: Accuracy: 0.9074062109: precision: 0.8325898714: recall: 0.2085990402: f1: 0.3336137104
3: 3232: loss: 0.2414983175:
3: 6432: loss: 0.2442971956:
3: 9632: loss: 0.2469754345:
3: 12832: loss: 0.2454707090:
3: 16032: loss: 0.2459652029:
3: 19232: loss: 0.2473941025:
3: 22432: loss: 0.2440659963:
3: 25632: loss: 0.2431227428:
3: 28832: loss: 0.2433801439:
3: 32032: loss: 0.2430739936:
3: 35232: loss: 0.2437130857:
3: 38432: loss: 0.2423319731:
3: 41632: loss: 0.2414374391:
3: 44832: loss: 0.2412343674:
3: 48032: loss: 0.2418808169:
3: 51232: loss: 0.2422853391:
3: 54432: loss: 0.2413906690:
3: 57632: loss: 0.2415539838:
3: 60832: loss: 0.2409877981:
3: 64032: loss: 0.2408401251:
3: 67232: loss: 0.2401925748:
3: 70432: loss: 0.2396571515:
3: 73632: loss: 0.2389064074:
3: 76832: loss: 0.2384087072:
3: 80032: loss: 0.2377270409:
3: 83232: loss: 0.2369767380:
3: 86432: loss: 0.2364828732:
3: 89632: loss: 0.2362235202:
3: 92832: loss: 0.2354834358:
3: 96032: loss: 0.2344170292:
3: 99232: loss: 0.2343531268:
3: 102432: loss: 0.2341504905:
3: 105632: loss: 0.2337602420:
3: 108832: loss: 0.2335291664:
3: 112032: loss: 0.2333697496:
3: 115232: loss: 0.2329446278:
3: 118432: loss: 0.2326870533:
3: 121632: loss: 0.2320354053:
3: 124832: loss: 0.2315744158:
3: 128032: loss: 0.2317714659:
3: 131232: loss: 0.2316979540:
3: 134432: loss: 0.2314671679:
Dev-Acc: 3: Accuracy: 0.9364283681: precision: 0.4434165232: recall: 0.3504506036: f1: 0.3914901700
Train-Acc: 3: Accuracy: 0.9200432301: precision: 0.7993822827: recall: 0.3743343633: f1: 0.5098952270
4: 3232: loss: 0.2141647739:
4: 6432: loss: 0.2138838143:
4: 9632: loss: 0.2156124431:
4: 12832: loss: 0.2193516092:
4: 16032: loss: 0.2163358475:
4: 19232: loss: 0.2154136118:
4: 22432: loss: 0.2141862422:
4: 25632: loss: 0.2162863696:
4: 28832: loss: 0.2175802791:
4: 32032: loss: 0.2173843116:
4: 35232: loss: 0.2161784886:
4: 38432: loss: 0.2156614045:
4: 41632: loss: 0.2151200024:
4: 44832: loss: 0.2148864233:
4: 48032: loss: 0.2148046240:
4: 51232: loss: 0.2148849773:
4: 54432: loss: 0.2144411927:
4: 57632: loss: 0.2134729661:
4: 60832: loss: 0.2135763550:
4: 64032: loss: 0.2129152777:
4: 67232: loss: 0.2130176470:
4: 70432: loss: 0.2132106957:
4: 73632: loss: 0.2133487266:
4: 76832: loss: 0.2132099763:
4: 80032: loss: 0.2133952113:
4: 83232: loss: 0.2129183459:
4: 86432: loss: 0.2128643941:
4: 89632: loss: 0.2127291586:
4: 92832: loss: 0.2123657108:
4: 96032: loss: 0.2124590612:
4: 99232: loss: 0.2117984164:
4: 102432: loss: 0.2114567119:
4: 105632: loss: 0.2111563239:
4: 108832: loss: 0.2107508168:
4: 112032: loss: 0.2102742470:
4: 115232: loss: 0.2100626716:
4: 118432: loss: 0.2099564893:
4: 121632: loss: 0.2098420507:
4: 124832: loss: 0.2096858177:
4: 128032: loss: 0.2096305748:
4: 131232: loss: 0.2096268509:
4: 134432: loss: 0.2099015590:
Dev-Acc: 4: Accuracy: 0.9338089228: precision: 0.4269330374: recall: 0.3924502636: f1: 0.4089660672
Train-Acc: 4: Accuracy: 0.9244114161: precision: 0.7993352210: recall: 0.4268621392: f1: 0.5565269564
5: 3232: loss: 0.2092198846:
5: 6432: loss: 0.2062049328:
5: 9632: loss: 0.2027806897:
5: 12832: loss: 0.2055115647:
5: 16032: loss: 0.2015592551:
5: 19232: loss: 0.2038929570:
5: 22432: loss: 0.2027051113:
5: 25632: loss: 0.2037169179:
5: 28832: loss: 0.2033118855:
5: 32032: loss: 0.2042928970:
5: 35232: loss: 0.2017901663:
5: 38432: loss: 0.2012888208:
5: 41632: loss: 0.2006510835:
5: 44832: loss: 0.2004687189:
5: 48032: loss: 0.2004873434:
5: 51232: loss: 0.2000953379:
5: 54432: loss: 0.2002154709:
5: 57632: loss: 0.1998084165:
5: 60832: loss: 0.1987260491:
5: 64032: loss: 0.1991814804:
5: 67232: loss: 0.1991409545:
5: 70432: loss: 0.1992284019:
5: 73632: loss: 0.1988943373:
5: 76832: loss: 0.1980832790:
5: 80032: loss: 0.1985005826:
5: 83232: loss: 0.1981572889:
5: 86432: loss: 0.1977480582:
5: 89632: loss: 0.1978269533:
5: 92832: loss: 0.1973146628:
5: 96032: loss: 0.1976177432:
5: 99232: loss: 0.1975018977:
5: 102432: loss: 0.1976217828:
5: 105632: loss: 0.1972854450:
5: 108832: loss: 0.1975702107:
5: 112032: loss: 0.1972234352:
5: 115232: loss: 0.1970151108:
5: 118432: loss: 0.1970507967:
5: 121632: loss: 0.1968352685:
5: 124832: loss: 0.1967786370:
5: 128032: loss: 0.1970447423:
5: 131232: loss: 0.1970509334:
5: 134432: loss: 0.1971070414:
Dev-Acc: 5: Accuracy: 0.9320725203: precision: 0.4181787349: recall: 0.4193164428: f1: 0.4187468161
Train-Acc: 5: Accuracy: 0.9271652699: precision: 0.8014265992: recall: 0.4579580567: f1: 0.5828557085
6: 3232: loss: 0.1992354033:
6: 6432: loss: 0.1979915215:
6: 9632: loss: 0.1927514433:
6: 12832: loss: 0.1925445496:
6: 16032: loss: 0.1925665320:
6: 19232: loss: 0.1927871343:
6: 22432: loss: 0.1913538035:
6: 25632: loss: 0.1903616020:
6: 28832: loss: 0.1912171715:
6: 32032: loss: 0.1902455674:
6: 35232: loss: 0.1898858944:
6: 38432: loss: 0.1895338046:
6: 41632: loss: 0.1899664238:
6: 44832: loss: 0.1903732042:
6: 48032: loss: 0.1902797423:
6: 51232: loss: 0.1899720981:
6: 54432: loss: 0.1900042794:
6: 57632: loss: 0.1904367867:
6: 60832: loss: 0.1895642680:
6: 64032: loss: 0.1891466779:
6: 67232: loss: 0.1888457904:
6: 70432: loss: 0.1887522142:
6: 73632: loss: 0.1889659266:
6: 76832: loss: 0.1885831843:
6: 80032: loss: 0.1888774052:
6: 83232: loss: 0.1889213533:
6: 86432: loss: 0.1891228242:
6: 89632: loss: 0.1891405420:
6: 92832: loss: 0.1890902790:
6: 96032: loss: 0.1891031562:
6: 99232: loss: 0.1885151039:
6: 102432: loss: 0.1882855309:
6: 105632: loss: 0.1884201738:
6: 108832: loss: 0.1885899601:
6: 112032: loss: 0.1889809697:
6: 115232: loss: 0.1887551368:
6: 118432: loss: 0.1884957692:
6: 121632: loss: 0.1886519951:
6: 124832: loss: 0.1887294461:
6: 128032: loss: 0.1887950925:
6: 131232: loss: 0.1886923317:
6: 134432: loss: 0.1886407900:
Dev-Acc: 6: Accuracy: 0.9299690127: precision: 0.4069859333: recall: 0.4378507057: f1: 0.4218545216
Train-Acc: 6: Accuracy: 0.9292909503: precision: 0.7968867418: recall: 0.4880021037: f1: 0.6053168067
7: 3232: loss: 0.1877961716:
7: 6432: loss: 0.1849620880:
7: 9632: loss: 0.1852364192:
7: 12832: loss: 0.1844505087:
7: 16032: loss: 0.1822479374:
7: 19232: loss: 0.1821605126:
7: 22432: loss: 0.1808156779:
7: 25632: loss: 0.1815181579:
7: 28832: loss: 0.1823035645:
7: 32032: loss: 0.1830732854:
7: 35232: loss: 0.1832861114:
7: 38432: loss: 0.1834652118:
7: 41632: loss: 0.1828947591:
7: 44832: loss: 0.1826776175:
7: 48032: loss: 0.1828208925:
7: 51232: loss: 0.1828882485:
7: 54432: loss: 0.1823514448:
7: 57632: loss: 0.1812894479:
7: 60832: loss: 0.1811080848:
7: 64032: loss: 0.1814347360:
7: 67232: loss: 0.1817220350:
7: 70432: loss: 0.1823350824:
7: 73632: loss: 0.1829870408:
7: 76832: loss: 0.1833756851:
7: 80032: loss: 0.1828187089:
7: 83232: loss: 0.1824885063:
7: 86432: loss: 0.1819419421:
7: 89632: loss: 0.1824950097:
7: 92832: loss: 0.1824619027:
7: 96032: loss: 0.1821478943:
7: 99232: loss: 0.1821742469:
7: 102432: loss: 0.1821051133:
7: 105632: loss: 0.1819665002:
7: 108832: loss: 0.1824199992:
7: 112032: loss: 0.1821514509:
7: 115232: loss: 0.1821802634:
7: 118432: loss: 0.1822438630:
7: 121632: loss: 0.1825131342:
7: 124832: loss: 0.1822679650:
7: 128032: loss: 0.1821008383:
7: 131232: loss: 0.1820632117:
7: 134432: loss: 0.1819852752:
Dev-Acc: 7: Accuracy: 0.9293339849: precision: 0.4026666667: recall: 0.4364903928: f1: 0.4188968668
Train-Acc: 7: Accuracy: 0.9308979511: precision: 0.8058066574: recall: 0.4981263559: f1: 0.6156658812
8: 3232: loss: 0.1924701833:
8: 6432: loss: 0.1886475425:
8: 9632: loss: 0.1852673676:
8: 12832: loss: 0.1840775772:
8: 16032: loss: 0.1832541212:
8: 19232: loss: 0.1819709225:
8: 22432: loss: 0.1829645208:
8: 25632: loss: 0.1815991268:
8: 28832: loss: 0.1814414102:
8: 32032: loss: 0.1814692946:
8: 35232: loss: 0.1826515128:
8: 38432: loss: 0.1818279862:
8: 41632: loss: 0.1817353737:
8: 44832: loss: 0.1806972573:
8: 48032: loss: 0.1799223792:
8: 51232: loss: 0.1797386134:
8: 54432: loss: 0.1796854819:
8: 57632: loss: 0.1793839504:
8: 60832: loss: 0.1787207770:
8: 64032: loss: 0.1787872904:
8: 67232: loss: 0.1789854443:
8: 70432: loss: 0.1795537085:
8: 73632: loss: 0.1795310683:
8: 76832: loss: 0.1792454820:
8: 80032: loss: 0.1792954136:
8: 83232: loss: 0.1788224564:
8: 86432: loss: 0.1789895874:
8: 89632: loss: 0.1786596270:
8: 92832: loss: 0.1784911457:
8: 96032: loss: 0.1782410653:
8: 99232: loss: 0.1778190496:
8: 102432: loss: 0.1773626886:
8: 105632: loss: 0.1768750575:
8: 108832: loss: 0.1767932954:
8: 112032: loss: 0.1767195820:
8: 115232: loss: 0.1765358941:
8: 118432: loss: 0.1764765205:
8: 121632: loss: 0.1765420181:
8: 124832: loss: 0.1764604395:
8: 128032: loss: 0.1762978403:
8: 131232: loss: 0.1763688253:
8: 134432: loss: 0.1761710246:
Dev-Acc: 8: Accuracy: 0.9261986017: precision: 0.3843411083: recall: 0.4398911750: f1: 0.4102442119
Train-Acc: 8: Accuracy: 0.9336883426: precision: 0.8097787655: recall: 0.5269870488: f1: 0.6384707288
9: 3232: loss: 0.1708234257:
9: 6432: loss: 0.1743736162:
9: 9632: loss: 0.1731032440:
9: 12832: loss: 0.1715490710:
9: 16032: loss: 0.1736872865:
9: 19232: loss: 0.1729479513:
9: 22432: loss: 0.1743559572:
9: 25632: loss: 0.1745327170:
9: 28832: loss: 0.1726934519:
9: 32032: loss: 0.1730431937:
9: 35232: loss: 0.1736961292:
9: 38432: loss: 0.1734429987:
9: 41632: loss: 0.1730671471:
9: 44832: loss: 0.1730390457:
9: 48032: loss: 0.1725428546:
9: 51232: loss: 0.1734018703:
9: 54432: loss: 0.1731063176:
9: 57632: loss: 0.1731345627:
9: 60832: loss: 0.1732005698:
9: 64032: loss: 0.1737484554:
9: 67232: loss: 0.1738883304:
9: 70432: loss: 0.1736755913:
9: 73632: loss: 0.1743093202:
9: 76832: loss: 0.1737361204:
9: 80032: loss: 0.1733496099:
9: 83232: loss: 0.1732256353:
9: 86432: loss: 0.1726904663:
9: 89632: loss: 0.1724142488:
9: 92832: loss: 0.1723366109:
9: 96032: loss: 0.1720141278:
9: 99232: loss: 0.1721983959:
9: 102432: loss: 0.1716357585:
9: 105632: loss: 0.1719339950:
9: 108832: loss: 0.1719186062:
9: 112032: loss: 0.1717517681:
9: 115232: loss: 0.1721223286:
9: 118432: loss: 0.1720557639:
9: 121632: loss: 0.1718382113:
9: 124832: loss: 0.1721312324:
9: 128032: loss: 0.1719623349:
9: 131232: loss: 0.1716250451:
9: 134432: loss: 0.1714764914:
Dev-Acc: 9: Accuracy: 0.9241149426: precision: 0.3726761781: recall: 0.4397211359: f1: 0.4034321373
Train-Acc: 9: Accuracy: 0.9362888336: precision: 0.8214604181: recall: 0.5450660706: f1: 0.6553114132
10: 3232: loss: 0.1737889899:
10: 6432: loss: 0.1734474073:
10: 9632: loss: 0.1704682275:
10: 12832: loss: 0.1661441858:
10: 16032: loss: 0.1645763139:
10: 19232: loss: 0.1621365158:
10: 22432: loss: 0.1610178573:
10: 25632: loss: 0.1604943979:
10: 28832: loss: 0.1621378972:
10: 32032: loss: 0.1625563098:
10: 35232: loss: 0.1624340726:
10: 38432: loss: 0.1630274335:
10: 41632: loss: 0.1631774767:
10: 44832: loss: 0.1630070874:
10: 48032: loss: 0.1637641778:
10: 51232: loss: 0.1639650688:
10: 54432: loss: 0.1640020903:
10: 57632: loss: 0.1632148506:
10: 60832: loss: 0.1639983001:
10: 64032: loss: 0.1646869780:
10: 67232: loss: 0.1646130765:
10: 70432: loss: 0.1642280984:
10: 73632: loss: 0.1648569884:
10: 76832: loss: 0.1648793923:
10: 80032: loss: 0.1646860251:
10: 83232: loss: 0.1644489270:
10: 86432: loss: 0.1643415256:
10: 89632: loss: 0.1648601354:
10: 92832: loss: 0.1651384936:
10: 96032: loss: 0.1645971583:
10: 99232: loss: 0.1646940231:
10: 102432: loss: 0.1646923399:
10: 105632: loss: 0.1648104257:
10: 108832: loss: 0.1647477734:
10: 112032: loss: 0.1649171415:
10: 115232: loss: 0.1655203846:
10: 118432: loss: 0.1655361880:
10: 121632: loss: 0.1652424732:
10: 124832: loss: 0.1655918652:
10: 128032: loss: 0.1657997820:
10: 131232: loss: 0.1660127811:
10: 134432: loss: 0.1664854691:
Dev-Acc: 10: Accuracy: 0.9212176204: precision: 0.3590305354: recall: 0.4458425438: f1: 0.3977548544
Train-Acc: 10: Accuracy: 0.9383268356: precision: 0.8190646804: recall: 0.5710998619: f1: 0.6729674246
11: 3232: loss: 0.1626637276:
11: 6432: loss: 0.1626263374:
11: 9632: loss: 0.1629238435:
11: 12832: loss: 0.1632075586:
11: 16032: loss: 0.1624372386:
11: 19232: loss: 0.1619671115:
11: 22432: loss: 0.1633453174:
11: 25632: loss: 0.1630733581:
11: 28832: loss: 0.1629534440:
11: 32032: loss: 0.1624530740:
11: 35232: loss: 0.1630674126:
11: 38432: loss: 0.1623887673:
11: 41632: loss: 0.1616084082:
11: 44832: loss: 0.1626171864:
11: 48032: loss: 0.1630834463:
11: 51232: loss: 0.1630226865:
11: 54432: loss: 0.1634421035:
11: 57632: loss: 0.1637590071:
11: 60832: loss: 0.1637967617:
11: 64032: loss: 0.1638611906:
11: 67232: loss: 0.1641526210:
11: 70432: loss: 0.1633954510:
11: 73632: loss: 0.1629215680:
11: 76832: loss: 0.1630094218:
11: 80032: loss: 0.1632373848:
11: 83232: loss: 0.1640591942:
11: 86432: loss: 0.1639601346:
11: 89632: loss: 0.1640606063:
11: 92832: loss: 0.1640633804:
11: 96032: loss: 0.1642649221:
11: 99232: loss: 0.1641449317:
11: 102432: loss: 0.1638043765:
11: 105632: loss: 0.1637763160:
11: 108832: loss: 0.1636048311:
11: 112032: loss: 0.1632629348:
11: 115232: loss: 0.1634727605:
11: 118432: loss: 0.1634726974:
11: 121632: loss: 0.1635111168:
11: 124832: loss: 0.1632257335:
11: 128032: loss: 0.1629689279:
11: 131232: loss: 0.1631678664:
11: 134432: loss: 0.1629391249:
Dev-Acc: 11: Accuracy: 0.9211680293: precision: 0.3573403373: recall: 0.4395510968: f1: 0.3942051087
Train-Acc: 11: Accuracy: 0.9396051168: precision: 0.8338943926: recall: 0.5699822497: f1: 0.6771321462
12: 3232: loss: 0.1746515030:
12: 6432: loss: 0.1738220426:
12: 9632: loss: 0.1722601570:
12: 12832: loss: 0.1714529851:
12: 16032: loss: 0.1690730891:
12: 19232: loss: 0.1670904684:
12: 22432: loss: 0.1658481995:
12: 25632: loss: 0.1641697596:
12: 28832: loss: 0.1636220482:
12: 32032: loss: 0.1642424117:
12: 35232: loss: 0.1640242029:
12: 38432: loss: 0.1633895344:
12: 41632: loss: 0.1632995188:
12: 44832: loss: 0.1627164732:
12: 48032: loss: 0.1624145621:
12: 51232: loss: 0.1625503917:
12: 54432: loss: 0.1620060882:
12: 57632: loss: 0.1617553066:
12: 60832: loss: 0.1619610596:
12: 64032: loss: 0.1612907349:
12: 67232: loss: 0.1612435215:
12: 70432: loss: 0.1610214025:
12: 73632: loss: 0.1606773921:
12: 76832: loss: 0.1606540495:
12: 80032: loss: 0.1599387951:
12: 83232: loss: 0.1599866080:
12: 86432: loss: 0.1599243326:
12: 89632: loss: 0.1600059387:
12: 92832: loss: 0.1600950501:
12: 96032: loss: 0.1597736935:
12: 99232: loss: 0.1594843698:
12: 102432: loss: 0.1597620358:
12: 105632: loss: 0.1595180173:
12: 108832: loss: 0.1595434283:
12: 112032: loss: 0.1593442235:
12: 115232: loss: 0.1596989177:
12: 118432: loss: 0.1598344717:
12: 121632: loss: 0.1601860256:
12: 124832: loss: 0.1602655434:
12: 128032: loss: 0.1601537171:
12: 131232: loss: 0.1602664362:
12: 134432: loss: 0.1599621552:
Dev-Acc: 12: Accuracy: 0.9186775684: precision: 0.3459747172: recall: 0.4421016834: f1: 0.3881755748
Train-Acc: 12: Accuracy: 0.9407446384: precision: 0.8330674674: recall: 0.5836565643: f1: 0.6864079171
13: 3232: loss: 0.1551622590:
13: 6432: loss: 0.1597824497:
13: 9632: loss: 0.1623667178:
13: 12832: loss: 0.1618382438:
13: 16032: loss: 0.1626005351:
13: 19232: loss: 0.1602762905:
13: 22432: loss: 0.1598130598:
13: 25632: loss: 0.1597308000:
13: 28832: loss: 0.1609928380:
13: 32032: loss: 0.1602762681:
13: 35232: loss: 0.1580390301:
13: 38432: loss: 0.1574538379:
13: 41632: loss: 0.1577198734:
13: 44832: loss: 0.1572069816:
13: 48032: loss: 0.1568319842:
13: 51232: loss: 0.1561916756:
13: 54432: loss: 0.1571082213:
13: 57632: loss: 0.1570137396:
13: 60832: loss: 0.1567076336:
13: 64032: loss: 0.1564069530:
13: 67232: loss: 0.1569249153:
13: 70432: loss: 0.1567444604:
13: 73632: loss: 0.1574130608:
13: 76832: loss: 0.1576836063:
13: 80032: loss: 0.1576628516:
13: 83232: loss: 0.1581230947:
13: 86432: loss: 0.1576185551:
13: 89632: loss: 0.1569328008:
13: 92832: loss: 0.1575768883:
13: 96032: loss: 0.1572337764:
13: 99232: loss: 0.1573765802:
13: 102432: loss: 0.1572281703:
13: 105632: loss: 0.1577030551:
13: 108832: loss: 0.1574317592:
13: 112032: loss: 0.1572696124:
13: 115232: loss: 0.1573026724:
13: 118432: loss: 0.1569623658:
13: 121632: loss: 0.1568323794:
13: 124832: loss: 0.1564690937:
13: 128032: loss: 0.1566863968:
13: 131232: loss: 0.1566429152:
13: 134432: loss: 0.1565102096:
Dev-Acc: 13: Accuracy: 0.9173182249: precision: 0.3400313152: recall: 0.4431219180: f1: 0.3847914360
Train-Acc: 13: Accuracy: 0.9419426322: precision: 0.8362811371: recall: 0.5937150746: f1: 0.6944252211
14: 3232: loss: 0.1625731859:
14: 6432: loss: 0.1618948178:
14: 9632: loss: 0.1595526352:
14: 12832: loss: 0.1581375463:
14: 16032: loss: 0.1572352342:
14: 19232: loss: 0.1575227292:
14: 22432: loss: 0.1575286832:
14: 25632: loss: 0.1563776382:
14: 28832: loss: 0.1554771867:
14: 32032: loss: 0.1549762824:
14: 35232: loss: 0.1533575972:
14: 38432: loss: 0.1543193600:
14: 41632: loss: 0.1545291017:
14: 44832: loss: 0.1543742328:
14: 48032: loss: 0.1545882745:
14: 51232: loss: 0.1536849011:
14: 54432: loss: 0.1535102162:
14: 57632: loss: 0.1530896187:
14: 60832: loss: 0.1534429111:
14: 64032: loss: 0.1530629832:
14: 67232: loss: 0.1529041756:
14: 70432: loss: 0.1536317092:
14: 73632: loss: 0.1530872358:
14: 76832: loss: 0.1530631083:
14: 80032: loss: 0.1532273785:
14: 83232: loss: 0.1533057470:
14: 86432: loss: 0.1535709798:
14: 89632: loss: 0.1531903583:
14: 92832: loss: 0.1532004386:
14: 96032: loss: 0.1531362624:
14: 99232: loss: 0.1534744982:
14: 102432: loss: 0.1537470348:
14: 105632: loss: 0.1538555177:
14: 108832: loss: 0.1539575831:
14: 112032: loss: 0.1541303837:
14: 115232: loss: 0.1540504330:
14: 118432: loss: 0.1539158059:
14: 121632: loss: 0.1540814315:
14: 124832: loss: 0.1544576303:
14: 128032: loss: 0.1543520854:
14: 131232: loss: 0.1541476711:
14: 134432: loss: 0.1539071226:
Dev-Acc: 14: Accuracy: 0.9159092307: precision: 0.3338883197: recall: 0.4432919572: f1: 0.3808897655
Train-Acc: 14: Accuracy: 0.9429506660: precision: 0.8375444678: recall: 0.6036421011: f1: 0.7016122870
15: 3232: loss: 0.1478498352:
15: 6432: loss: 0.1482887779:
15: 9632: loss: 0.1534424014:
15: 12832: loss: 0.1552835179:
15: 16032: loss: 0.1552104689:
15: 19232: loss: 0.1547478764:
15: 22432: loss: 0.1544921889:
15: 25632: loss: 0.1559123571:
15: 28832: loss: 0.1571305743:
15: 32032: loss: 0.1567606764:
15: 35232: loss: 0.1556141937:
15: 38432: loss: 0.1538239007:
15: 41632: loss: 0.1534376395:
15: 44832: loss: 0.1528413255:
15: 48032: loss: 0.1528749155:
15: 51232: loss: 0.1519251305:
15: 54432: loss: 0.1521476552:
15: 57632: loss: 0.1515395053:
15: 60832: loss: 0.1521513554:
15: 64032: loss: 0.1518112852:
15: 67232: loss: 0.1518838438:
15: 70432: loss: 0.1516124869:
15: 73632: loss: 0.1520781361:
15: 76832: loss: 0.1518027060:
15: 80032: loss: 0.1519933037:
15: 83232: loss: 0.1518802916:
15: 86432: loss: 0.1516214922:
15: 89632: loss: 0.1513537447:
15: 92832: loss: 0.1517952115:
15: 96032: loss: 0.1512417992:
15: 99232: loss: 0.1508903415:
15: 102432: loss: 0.1510209776:
15: 105632: loss: 0.1507581093:
15: 108832: loss: 0.1509212374:
15: 112032: loss: 0.1508428341:
15: 115232: loss: 0.1507624315:
15: 118432: loss: 0.1507545447:
15: 121632: loss: 0.1508411068:
15: 124832: loss: 0.1507431397:
15: 128032: loss: 0.1507091618:
15: 131232: loss: 0.1506693089:
15: 134432: loss: 0.1509673586:
Dev-Acc: 15: Accuracy: 0.9155222774: precision: 0.3318431473: recall: 0.4417616052: f1: 0.3789934354
Train-Acc: 15: Accuracy: 0.9440244436: precision: 0.8480907582: recall: 0.6044967458: f1: 0.7058688059
16: 3232: loss: 0.1334487027:
16: 6432: loss: 0.1439966034:
16: 9632: loss: 0.1471142036:
16: 12832: loss: 0.1461482109:
16: 16032: loss: 0.1464570888:
16: 19232: loss: 0.1506930540:
16: 22432: loss: 0.1515489944:
16: 25632: loss: 0.1504365344:
16: 28832: loss: 0.1498911219:
16: 32032: loss: 0.1498345025:
16: 35232: loss: 0.1498852149:
16: 38432: loss: 0.1496774512:
16: 41632: loss: 0.1486222019:
16: 44832: loss: 0.1489901254:
16: 48032: loss: 0.1484634097:
16: 51232: loss: 0.1478173825:
16: 54432: loss: 0.1477611568:
16: 57632: loss: 0.1487345923:
16: 60832: loss: 0.1489131246:
16: 64032: loss: 0.1489882765:
16: 67232: loss: 0.1489261006:
16: 70432: loss: 0.1491640571:
16: 73632: loss: 0.1494258969:
16: 76832: loss: 0.1493950206:
16: 80032: loss: 0.1491945830:
16: 83232: loss: 0.1490666257:
16: 86432: loss: 0.1492778529:
16: 89632: loss: 0.1498426929:
16: 92832: loss: 0.1497288538:
16: 96032: loss: 0.1495271576:
16: 99232: loss: 0.1497099918:
16: 102432: loss: 0.1498285169:
16: 105632: loss: 0.1498680353:
16: 108832: loss: 0.1495085594:
16: 112032: loss: 0.1493887050:
16: 115232: loss: 0.1491864170:
16: 118432: loss: 0.1487761652:
16: 121632: loss: 0.1484990129:
16: 124832: loss: 0.1487436046:
16: 128032: loss: 0.1487382583:
16: 131232: loss: 0.1486314834:
16: 134432: loss: 0.1482982179:
Dev-Acc: 16: Accuracy: 0.9146491289: precision: 0.3280242700: recall: 0.4412514878: f1: 0.3763051044
Train-Acc: 16: Accuracy: 0.9447987080: precision: 0.8500091458: recall: 0.6110051936: f1: 0.7109581182
17: 3232: loss: 0.1446272038:
17: 6432: loss: 0.1431686827:
17: 9632: loss: 0.1447559363:
17: 12832: loss: 0.1412817272:
17: 16032: loss: 0.1408427779:
17: 19232: loss: 0.1430585088:
17: 22432: loss: 0.1451565490:
17: 25632: loss: 0.1460173239:
17: 28832: loss: 0.1455098670:
17: 32032: loss: 0.1453311739:
17: 35232: loss: 0.1442172826:
17: 38432: loss: 0.1444919907:
17: 41632: loss: 0.1450656974:
17: 44832: loss: 0.1443056017:
17: 48032: loss: 0.1451044643:
17: 51232: loss: 0.1453229201:
17: 54432: loss: 0.1451219972:
17: 57632: loss: 0.1450499311:
17: 60832: loss: 0.1454483696:
17: 64032: loss: 0.1455713748:
17: 67232: loss: 0.1459151512:
17: 70432: loss: 0.1461874006:
17: 73632: loss: 0.1461876846:
17: 76832: loss: 0.1461480896:
17: 80032: loss: 0.1466401394:
17: 83232: loss: 0.1466961893:
17: 86432: loss: 0.1469257015:
17: 89632: loss: 0.1463239548:
17: 92832: loss: 0.1467993102:
17: 96032: loss: 0.1468275319:
17: 99232: loss: 0.1464204755:
17: 102432: loss: 0.1463337803:
17: 105632: loss: 0.1460142864:
17: 108832: loss: 0.1456966222:
17: 112032: loss: 0.1453623005:
17: 115232: loss: 0.1452692504:
17: 118432: loss: 0.1451400789:
17: 121632: loss: 0.1453088292:
17: 124832: loss: 0.1452408578:
17: 128032: loss: 0.1452964332:
17: 131232: loss: 0.1451902837:
17: 134432: loss: 0.1455950261:
Dev-Acc: 17: Accuracy: 0.9120991230: precision: 0.3194713870: recall: 0.4480530522: f1: 0.3729917192
Train-Acc: 17: Accuracy: 0.9459236264: precision: 0.8477021731: recall: 0.6257313786: f1: 0.7199969742
18: 3232: loss: 0.1435446743:
18: 6432: loss: 0.1437287926:
18: 9632: loss: 0.1437505393:
18: 12832: loss: 0.1454008995:
18: 16032: loss: 0.1472579885:
18: 19232: loss: 0.1486861647:
18: 22432: loss: 0.1479826090:
18: 25632: loss: 0.1482476734:
18: 28832: loss: 0.1470917940:
18: 32032: loss: 0.1470976022:
18: 35232: loss: 0.1467744480:
18: 38432: loss: 0.1458739463:
18: 41632: loss: 0.1455294532:
18: 44832: loss: 0.1447395892:
18: 48032: loss: 0.1448521078:
18: 51232: loss: 0.1446148709:
18: 54432: loss: 0.1441285158:
18: 57632: loss: 0.1441164301:
18: 60832: loss: 0.1447006518:
18: 64032: loss: 0.1450475271:
18: 67232: loss: 0.1453339276:
18: 70432: loss: 0.1458563038:
18: 73632: loss: 0.1456939530:
18: 76832: loss: 0.1454044822:
18: 80032: loss: 0.1448107506:
18: 83232: loss: 0.1451103560:
18: 86432: loss: 0.1446781159:
18: 89632: loss: 0.1447728778:
18: 92832: loss: 0.1448377363:
18: 96032: loss: 0.1450472671:
18: 99232: loss: 0.1453105373:
18: 102432: loss: 0.1451726643:
18: 105632: loss: 0.1451847125:
18: 108832: loss: 0.1451492019:
18: 112032: loss: 0.1447188805:
18: 115232: loss: 0.1446839215:
18: 118432: loss: 0.1443936162:
18: 121632: loss: 0.1443629550:
18: 124832: loss: 0.1442279414:
18: 128032: loss: 0.1437562131:
18: 131232: loss: 0.1436344768:
18: 134432: loss: 0.1434170515:
Dev-Acc: 18: Accuracy: 0.9110672474: precision: 0.3150947924: recall: 0.4465227002: f1: 0.3694688709
Train-Acc: 18: Accuracy: 0.9467782974: precision: 0.8509432291: recall: 0.6316481494: f1: 0.7250773527
19: 3232: loss: 0.1497249975:
19: 6432: loss: 0.1454611745:
19: 9632: loss: 0.1465517524:
19: 12832: loss: 0.1470591975:
19: 16032: loss: 0.1496038390:
19: 19232: loss: 0.1498407797:
19: 22432: loss: 0.1501637560:
19: 25632: loss: 0.1501396197:
19: 28832: loss: 0.1476861686:
19: 32032: loss: 0.1472473694:
19: 35232: loss: 0.1459276063:
19: 38432: loss: 0.1463648593:
19: 41632: loss: 0.1458495557:
19: 44832: loss: 0.1448653900:
19: 48032: loss: 0.1443777633:
19: 51232: loss: 0.1440005171:
19: 54432: loss: 0.1439226532:
19: 57632: loss: 0.1429801837:
19: 60832: loss: 0.1426293451:
19: 64032: loss: 0.1425858465:
19: 67232: loss: 0.1426923540:
19: 70432: loss: 0.1424517913:
19: 73632: loss: 0.1426436002:
19: 76832: loss: 0.1422795770:
19: 80032: loss: 0.1426388507:
19: 83232: loss: 0.1426274014:
19: 86432: loss: 0.1426995041:
19: 89632: loss: 0.1427795272:
19: 92832: loss: 0.1423496502:
19: 96032: loss: 0.1423818233:
19: 99232: loss: 0.1421068267:
19: 102432: loss: 0.1419289358:
19: 105632: loss: 0.1417725560:
19: 108832: loss: 0.1419493306:
19: 112032: loss: 0.1423941648:
19: 115232: loss: 0.1422605484:
19: 118432: loss: 0.1420141412:
19: 121632: loss: 0.1417789331:
19: 124832: loss: 0.1416158756:
19: 128032: loss: 0.1415548464:
19: 131232: loss: 0.1419964795:
19: 134432: loss: 0.1417916429:
Dev-Acc: 19: Accuracy: 0.9089240432: precision: 0.3078536472: recall: 0.4492433260: f1: 0.3653460555
Train-Acc: 19: Accuracy: 0.9475087523: precision: 0.8508963708: recall: 0.6396686608: f1: 0.7303159949
20: 3232: loss: 0.1423186563:
20: 6432: loss: 0.1460641139:
20: 9632: loss: 0.1456313395:
20: 12832: loss: 0.1395752858:
20: 16032: loss: 0.1380522668:
20: 19232: loss: 0.1356919614:
20: 22432: loss: 0.1374339221:
20: 25632: loss: 0.1366613726:
20: 28832: loss: 0.1371388462:
20: 32032: loss: 0.1365252380:
20: 35232: loss: 0.1357584713:
20: 38432: loss: 0.1351772755:
20: 41632: loss: 0.1355533929:
20: 44832: loss: 0.1364144573:
20: 48032: loss: 0.1364899475:
20: 51232: loss: 0.1362799560:
20: 54432: loss: 0.1367402045:
20: 57632: loss: 0.1365539983:
20: 60832: loss: 0.1372791720:
20: 64032: loss: 0.1371247199:
20: 67232: loss: 0.1378547185:
20: 70432: loss: 0.1381130569:
20: 73632: loss: 0.1383977761:
20: 76832: loss: 0.1385911491:
20: 80032: loss: 0.1388441194:
20: 83232: loss: 0.1384695124:
20: 86432: loss: 0.1385053177:
20: 89632: loss: 0.1389567619:
20: 92832: loss: 0.1391185602:
20: 96032: loss: 0.1391140731:
20: 99232: loss: 0.1391423093:
20: 102432: loss: 0.1390126115:
20: 105632: loss: 0.1393000035:
20: 108832: loss: 0.1392960978:
20: 112032: loss: 0.1395315391:
20: 115232: loss: 0.1396536100:
20: 118432: loss: 0.1398571551:
20: 121632: loss: 0.1397183051:
20: 124832: loss: 0.1394771323:
20: 128032: loss: 0.1394557235:
20: 131232: loss: 0.1397255143:
20: 134432: loss: 0.1394706053:
Dev-Acc: 20: Accuracy: 0.9085469842: precision: 0.3049122807: recall: 0.4432919572: f1: 0.3613055228
Train-Acc: 20: Accuracy: 0.9480858445: precision: 0.8582036775: recall: 0.6382223391: f1: 0.7320438864
