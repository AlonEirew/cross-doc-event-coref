1: 6464: loss: 0.7056183130:
1: 12864: loss: 0.7049724665:
1: 19264: loss: 0.7038608984:
1: 25664: loss: 0.7028562401:
1: 32064: loss: 0.7017640493:
1: 38464: loss: 0.7010371466:
1: 44864: loss: 0.7002557760:
1: 51264: loss: 0.6995038862:
1: 57664: loss: 0.6986960534:
1: 64064: loss: 0.6979622448:
1: 70464: loss: 0.6972028954:
1: 76864: loss: 0.6962952303:
1: 83264: loss: 0.6955215263:
1: 89664: loss: 0.6946647029:
Dev-Acc: 1: Accuracy: 0.5964438915: precision: 0.0671792192: recall: 0.4591055943: f1: 0.1172078486
Train-Acc: 1: Accuracy: 0.6364253759: precision: 0.2320720398: recall: 0.5116691868: f1: 0.3193156642
2: 6464: loss: 0.6829457700:
2: 12864: loss: 0.6818002099:
2: 19264: loss: 0.6807123156:
2: 25664: loss: 0.6798244724:
2: 32064: loss: 0.6791695578:
2: 38464: loss: 0.6782851582:
2: 44864: loss: 0.6775528664:
2: 51264: loss: 0.6768925223:
2: 57664: loss: 0.6760435239:
2: 64064: loss: 0.6752607513:
2: 70464: loss: 0.6744327219:
2: 76864: loss: 0.6735898064:
2: 83264: loss: 0.6728914425:
2: 89664: loss: 0.6722406872:
Dev-Acc: 2: Accuracy: 0.8367895484: precision: 0.0887937743: recall: 0.1940146234: f1: 0.1218301212
Train-Acc: 2: Accuracy: 0.8127013445: precision: 0.4007380074: recall: 0.2498849517: f1: 0.3078231293
3: 6464: loss: 0.6598911446:
3: 12864: loss: 0.6595269033:
3: 19264: loss: 0.6590351478:
3: 25664: loss: 0.6583277915:
3: 32064: loss: 0.6577664504:
3: 38464: loss: 0.6570001716:
3: 44864: loss: 0.6562814642:
3: 51264: loss: 0.6555832870:
3: 57664: loss: 0.6548863159:
3: 64064: loss: 0.6541678624:
3: 70464: loss: 0.6533138246:
3: 76864: loss: 0.6525793777:
3: 83264: loss: 0.6517366768:
3: 89664: loss: 0.6509627426:
Dev-Acc: 3: Accuracy: 0.9215450883: precision: 0.1667763158: recall: 0.0862098283: f1: 0.1136643874
Train-Acc: 3: Accuracy: 0.8412990570: precision: 0.6302400573: recall: 0.1156399974: f1: 0.1954227308
4: 6464: loss: 0.6403502649:
4: 12864: loss: 0.6398286045:
4: 19264: loss: 0.6392159873:
4: 25664: loss: 0.6386020505:
4: 32064: loss: 0.6373947885:
4: 38464: loss: 0.6367270495:
4: 44864: loss: 0.6359900128:
4: 51264: loss: 0.6352860083:
4: 57664: loss: 0.6345908722:
4: 64064: loss: 0.6338231147:
4: 70464: loss: 0.6331335091:
4: 76864: loss: 0.6323440407:
4: 83264: loss: 0.6315976378:
4: 89664: loss: 0.6308477770:
Dev-Acc: 4: Accuracy: 0.9387700558: precision: 0.1914893617: recall: 0.0153035198: f1: 0.0283419934
Train-Acc: 4: Accuracy: 0.8385488391: precision: 0.7500000000: recall: 0.0469397147: f1: 0.0883499350
5: 6464: loss: 0.6197063899:
5: 12864: loss: 0.6191877455:
5: 19264: loss: 0.6188783624:
5: 25664: loss: 0.6182302444:
5: 32064: loss: 0.6177410362:
5: 38464: loss: 0.6168272147:
5: 44864: loss: 0.6159892154:
5: 51264: loss: 0.6152724479:
5: 57664: loss: 0.6145674370:
5: 64064: loss: 0.6138520072:
5: 70464: loss: 0.6130546501:
5: 76864: loss: 0.6124050785:
5: 83264: loss: 0.6118400035:
5: 89664: loss: 0.6112545397:
Dev-Acc: 5: Accuracy: 0.9413895011: precision: 0.2592592593: recall: 0.0023805475: f1: 0.0047177759
Train-Acc: 5: Accuracy: 0.8353275061: precision: 0.8640000000: recall: 0.0142002498: f1: 0.0279412716
6: 6464: loss: 0.6020046443:
6: 12864: loss: 0.5999456275:
6: 19264: loss: 0.5985549098:
6: 25664: loss: 0.5982244933:
6: 32064: loss: 0.5972834684:
6: 38464: loss: 0.5963482043:
6: 44864: loss: 0.5960451839:
6: 51264: loss: 0.5955937520:
6: 57664: loss: 0.5948548095:
6: 64064: loss: 0.5941429794:
6: 70464: loss: 0.5935852049:
6: 76864: loss: 0.5930866329:
6: 83264: loss: 0.5925665118:
6: 89664: loss: 0.5916712353:
Dev-Acc: 6: Accuracy: 0.9416276217: precision: 0.3333333333: recall: 0.0003400782: f1: 0.0006794632
Train-Acc: 6: Accuracy: 0.8342537284: precision: 1.0000000000: recall: 0.0055223194: f1: 0.0109839817
7: 6464: loss: 0.5840873033:
7: 12864: loss: 0.5835743397:
7: 19264: loss: 0.5824378602:
7: 25664: loss: 0.5808646530:
7: 32064: loss: 0.5800826309:
7: 38464: loss: 0.5791672697:
7: 44864: loss: 0.5783108475:
7: 51264: loss: 0.5776015786:
7: 57664: loss: 0.5767861605:
7: 64064: loss: 0.5759984306:
7: 70464: loss: 0.5754810199:
7: 76864: loss: 0.5748872989:
7: 83264: loss: 0.5742505704:
7: 89664: loss: 0.5735793101:
Dev-Acc: 7: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8336072564: precision: 1.0000000000: recall: 0.0016435474: f1: 0.0032817012
8: 6464: loss: 0.5618839598:
8: 12864: loss: 0.5621407703:
8: 19264: loss: 0.5615554885:
8: 25664: loss: 0.5615960844:
8: 32064: loss: 0.5611868379:
8: 38464: loss: 0.5605016065:
8: 44864: loss: 0.5598686076:
8: 51264: loss: 0.5595124939:
8: 57664: loss: 0.5590965513:
8: 64064: loss: 0.5584149596:
8: 70464: loss: 0.5577547207:
8: 76864: loss: 0.5574421001:
8: 83264: loss: 0.5565590187:
8: 89664: loss: 0.5559571297:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8334319592: precision: 1.0000000000: recall: 0.0005916771: f1: 0.0011826544
9: 6464: loss: 0.5490128887:
9: 12864: loss: 0.5461885221:
9: 19264: loss: 0.5456560015:
9: 25664: loss: 0.5457163230:
9: 32064: loss: 0.5449224947:
9: 38464: loss: 0.5440462151:
9: 44864: loss: 0.5439461199:
9: 51264: loss: 0.5428851285:
9: 57664: loss: 0.5417521226:
9: 64064: loss: 0.5411040635:
9: 70464: loss: 0.5403492949:
9: 76864: loss: 0.5398110133:
9: 83264: loss: 0.5392938995:
9: 89664: loss: 0.5385928942:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8333662152: precision: 1.0000000000: recall: 0.0001972257: f1: 0.0003943736
10: 6464: loss: 0.5249373761:
10: 12864: loss: 0.5276908171:
10: 19264: loss: 0.5270028849:
10: 25664: loss: 0.5261259101:
10: 32064: loss: 0.5255999698:
10: 38464: loss: 0.5255795158:
10: 44864: loss: 0.5250974836:
10: 51264: loss: 0.5246719987:
10: 57664: loss: 0.5242236445:
10: 64064: loss: 0.5240056039:
10: 70464: loss: 0.5240593244:
10: 76864: loss: 0.5235088144:
10: 83264: loss: 0.5227056336:
10: 89664: loss: 0.5217644911:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.5079465398:
11: 12864: loss: 0.5093240204:
11: 19264: loss: 0.5103314919:
11: 25664: loss: 0.5117778175:
11: 32064: loss: 0.5113551205:
11: 38464: loss: 0.5110934735:
11: 44864: loss: 0.5101580042:
11: 51264: loss: 0.5094513621:
11: 57664: loss: 0.5086761681:
11: 64064: loss: 0.5077597990:
11: 70464: loss: 0.5075097388:
11: 76864: loss: 0.5070287870:
11: 83264: loss: 0.5067155441:
11: 89664: loss: 0.5056578727:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.5002078503:
12: 12864: loss: 0.4987060350:
12: 19264: loss: 0.4952704947:
12: 25664: loss: 0.4954094934:
12: 32064: loss: 0.4957344630:
12: 38464: loss: 0.4950588725:
12: 44864: loss: 0.4943423320:
12: 51264: loss: 0.4938081675:
12: 57664: loss: 0.4931569744:
12: 64064: loss: 0.4924072364:
12: 70464: loss: 0.4919418802:
12: 76864: loss: 0.4916584645:
12: 83264: loss: 0.4910740510:
12: 89664: loss: 0.4905670161:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8333333135: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.4771482050:
13: 12864: loss: 0.4813731733:
13: 19264: loss: 0.4801384448:
13: 25664: loss: 0.4799688227:
13: 32064: loss: 0.4805119133:
13: 38464: loss: 0.4806756099:
13: 44864: loss: 0.4794960159:
13: 51264: loss: 0.4785557708:
13: 57664: loss: 0.4781322538:
13: 64064: loss: 0.4779630150:
13: 70464: loss: 0.4776780466:
13: 76864: loss: 0.4770356947:
13: 83264: loss: 0.4765265410:
13: 89664: loss: 0.4761365804:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.8333552480: precision: 1.0000000000: recall: 0.0001314838: f1: 0.0002629330
14: 6464: loss: 0.4605964056:
14: 12864: loss: 0.4628424978:
14: 19264: loss: 0.4633181627:
14: 25664: loss: 0.4647846808:
14: 32064: loss: 0.4654032139:
14: 38464: loss: 0.4656009387:
14: 44864: loss: 0.4653669844:
14: 51264: loss: 0.4645836959:
14: 57664: loss: 0.4641752070:
14: 64064: loss: 0.4648154091:
14: 70464: loss: 0.4635581763:
14: 76864: loss: 0.4637119965:
14: 83264: loss: 0.4630985047:
14: 89664: loss: 0.4623894154:
Dev-Acc: 14: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.8335853219: precision: 1.0000000000: recall: 0.0015120636: f1: 0.0030195615
15: 6464: loss: 0.4551898077:
15: 12864: loss: 0.4520259105:
15: 19264: loss: 0.4520000755:
15: 25664: loss: 0.4521607144:
15: 32064: loss: 0.4523566954:
15: 38464: loss: 0.4519262215:
15: 44864: loss: 0.4508614223:
15: 51264: loss: 0.4500933015:
15: 57664: loss: 0.4493485104:
15: 64064: loss: 0.4490248662:
15: 70464: loss: 0.4490652189:
15: 76864: loss: 0.4486713330:
15: 83264: loss: 0.4487775279:
15: 89664: loss: 0.4487798326:
Dev-Acc: 15: Accuracy: 0.9416673183: precision: 0.7500000000: recall: 0.0005101173: f1: 0.0010195412
Train-Acc: 15: Accuracy: 0.8353494406: precision: 1.0000000000: recall: 0.0120965091: f1: 0.0239038649
16: 6464: loss: 0.4450261050:
16: 12864: loss: 0.4441880651:
16: 19264: loss: 0.4434776571:
16: 25664: loss: 0.4428521997:
16: 32064: loss: 0.4416047288:
16: 38464: loss: 0.4399279632:
16: 44864: loss: 0.4395723957:
16: 51264: loss: 0.4391093976:
16: 57664: loss: 0.4385618404:
16: 64064: loss: 0.4376758325:
16: 70464: loss: 0.4377050909:
16: 76864: loss: 0.4376234929:
16: 83264: loss: 0.4369953556:
16: 89664: loss: 0.4368511844:
Dev-Acc: 16: Accuracy: 0.9416871667: precision: 0.8333333333: recall: 0.0008501955: f1: 0.0016986581
Train-Acc: 16: Accuracy: 0.8373873830: precision: 1.0000000000: recall: 0.0243245020: f1: 0.0474937424
17: 6464: loss: 0.4329012063:
17: 12864: loss: 0.4296091567:
17: 19264: loss: 0.4312702859:
17: 25664: loss: 0.4301918157:
17: 32064: loss: 0.4306048730:
17: 38464: loss: 0.4304330618:
17: 44864: loss: 0.4300859611:
17: 51264: loss: 0.4303899112:
17: 57664: loss: 0.4294320946:
17: 64064: loss: 0.4281663330:
17: 70464: loss: 0.4271539121:
17: 76864: loss: 0.4264225030:
17: 83264: loss: 0.4257681105:
17: 89664: loss: 0.4251383407:
Dev-Acc: 17: Accuracy: 0.9417268634: precision: 0.8333333333: recall: 0.0017003911: f1: 0.0033938571
Train-Acc: 17: Accuracy: 0.8383625746: precision: 1.0000000000: recall: 0.0301755309: f1: 0.0585832802
18: 6464: loss: 0.4199251345:
18: 12864: loss: 0.4165805894:
18: 19264: loss: 0.4172744438:
18: 25664: loss: 0.4168833540:
18: 32064: loss: 0.4157434225:
18: 38464: loss: 0.4156487350:
18: 44864: loss: 0.4147049930:
18: 51264: loss: 0.4145978574:
18: 57664: loss: 0.4147760573:
18: 64064: loss: 0.4148106551:
18: 70464: loss: 0.4146758039:
18: 76864: loss: 0.4140754629:
18: 83264: loss: 0.4137460162:
18: 89664: loss: 0.4135630938:
Dev-Acc: 18: Accuracy: 0.9418062568: precision: 0.6333333333: recall: 0.0064614861: f1: 0.0127924592
Train-Acc: 18: Accuracy: 0.8395130634: precision: 0.9878892734: recall: 0.0375386234: f1: 0.0723288365
19: 6464: loss: 0.4117354521:
19: 12864: loss: 0.4090546562:
19: 19264: loss: 0.4067678900:
19: 25664: loss: 0.4084833556:
19: 32064: loss: 0.4073019493:
19: 38464: loss: 0.4062784847:
19: 44864: loss: 0.4067191076:
19: 51264: loss: 0.4063468990:
19: 57664: loss: 0.4052737710:
19: 64064: loss: 0.4046312958:
19: 70464: loss: 0.4039952276:
19: 76864: loss: 0.4041749539:
19: 83264: loss: 0.4037272403:
19: 89664: loss: 0.4029147833:
Dev-Acc: 19: Accuracy: 0.9419947267: precision: 0.6206896552: recall: 0.0153035198: f1: 0.0298705609
Train-Acc: 19: Accuracy: 0.8420003057: precision: 0.9509692132: recall: 0.0548287424: f1: 0.1036797613
20: 6464: loss: 0.4023242015:
20: 12864: loss: 0.3993062167:
20: 19264: loss: 0.3986854148:
20: 25664: loss: 0.3996854816:
20: 32064: loss: 0.3993641251:
20: 38464: loss: 0.3959685783:
20: 44864: loss: 0.3957262503:
20: 51264: loss: 0.3949328482:
20: 57664: loss: 0.3944890733:
20: 64064: loss: 0.3945536905:
20: 70464: loss: 0.3943104288:
20: 76864: loss: 0.3943524896:
20: 83264: loss: 0.3937076780:
20: 89664: loss: 0.3933513427:
Dev-Acc: 20: Accuracy: 0.9424313307: precision: 0.6525096525: recall: 0.0287366094: f1: 0.0550488599
Train-Acc: 20: Accuracy: 0.8445861340: precision: 0.9117882919: recall: 0.0747485372: f1: 0.1381698870
21: 6464: loss: 0.3854778957:
21: 12864: loss: 0.3869781654:
21: 19264: loss: 0.3890134902:
21: 25664: loss: 0.3900301021:
21: 32064: loss: 0.3879072869:
21: 38464: loss: 0.3859788026:
21: 44864: loss: 0.3852383036:
21: 51264: loss: 0.3845298803:
21: 57664: loss: 0.3851729122:
21: 64064: loss: 0.3853308216:
21: 70464: loss: 0.3849584233:
21: 76864: loss: 0.3847050012:
21: 83264: loss: 0.3842122693:
21: 89664: loss: 0.3836902010:
Dev-Acc: 21: Accuracy: 0.9424908757: precision: 0.5782688766: recall: 0.0533922802: f1: 0.0977584060
Train-Acc: 21: Accuracy: 0.8473473191: precision: 0.9107257547: recall: 0.0932220104: f1: 0.1691316794
22: 6464: loss: 0.3777632263:
22: 12864: loss: 0.3763184316:
22: 19264: loss: 0.3731942552:
22: 25664: loss: 0.3748017487:
22: 32064: loss: 0.3744075829:
22: 38464: loss: 0.3757059673:
22: 44864: loss: 0.3752037159:
22: 51264: loss: 0.3752089446:
22: 57664: loss: 0.3748565479:
22: 64064: loss: 0.3748753239:
22: 70464: loss: 0.3749962216:
22: 76864: loss: 0.3748500064:
22: 83264: loss: 0.3749826975:
22: 89664: loss: 0.3750214277:
Dev-Acc: 22: Accuracy: 0.9423916340: precision: 0.5533428165: recall: 0.0661452134: f1: 0.1181652491
Train-Acc: 22: Accuracy: 0.8492538333: precision: 0.9182498561: recall: 0.1048583262: f1: 0.1882227992
23: 6464: loss: 0.3739083424:
23: 12864: loss: 0.3711002417:
23: 19264: loss: 0.3698377241:
23: 25664: loss: 0.3697178606:
23: 32064: loss: 0.3687342026:
23: 38464: loss: 0.3686747191:
23: 44864: loss: 0.3682129858:
23: 51264: loss: 0.3686621059:
23: 57664: loss: 0.3684130379:
23: 64064: loss: 0.3684257342:
23: 70464: loss: 0.3686897901:
23: 76864: loss: 0.3678925515:
23: 83264: loss: 0.3674488660:
23: 89664: loss: 0.3669700009:
Dev-Acc: 23: Accuracy: 0.9423023462: precision: 0.5412500000: recall: 0.0736269342: f1: 0.1296213142
Train-Acc: 23: Accuracy: 0.8523436785: precision: 0.9237909135: recall: 0.1243179278: f1: 0.2191447445
24: 6464: loss: 0.3683616585:
24: 12864: loss: 0.3655604817:
24: 19264: loss: 0.3618475824:
24: 25664: loss: 0.3642587545:
24: 32064: loss: 0.3655939552:
24: 38464: loss: 0.3631971569:
24: 44864: loss: 0.3611546411:
24: 51264: loss: 0.3599067946:
24: 57664: loss: 0.3607435888:
24: 64064: loss: 0.3603895790:
24: 70464: loss: 0.3596541890:
24: 76864: loss: 0.3592431488:
24: 83264: loss: 0.3590361767:
24: 89664: loss: 0.3587691594:
Dev-Acc: 24: Accuracy: 0.9425900578: precision: 0.5475475475: recall: 0.0930113926: f1: 0.1590116279
Train-Acc: 24: Accuracy: 0.8556746244: precision: 0.9285414040: recall: 0.1452238512: f1: 0.2511654349
25: 6464: loss: 0.3557266515:
25: 12864: loss: 0.3489372425:
25: 19264: loss: 0.3492643102:
25: 25664: loss: 0.3508883814:
25: 32064: loss: 0.3532042210:
25: 38464: loss: 0.3535781693:
25: 44864: loss: 0.3518550553:
25: 51264: loss: 0.3511809940:
25: 57664: loss: 0.3506843117:
25: 64064: loss: 0.3517405765:
25: 70464: loss: 0.3519439796:
25: 76864: loss: 0.3514576788:
25: 83264: loss: 0.3513926074:
25: 89664: loss: 0.3516336219:
Dev-Acc: 25: Accuracy: 0.9426297545: precision: 0.5357400722: recall: 0.1261690189: f1: 0.2042389210
Train-Acc: 25: Accuracy: 0.8586220145: precision: 0.9283593170: recall: 0.1644204852: f1: 0.2793633063
26: 6464: loss: 0.3498683748:
26: 12864: loss: 0.3491124742:
26: 19264: loss: 0.3504170523:
26: 25664: loss: 0.3503626150:
26: 32064: loss: 0.3494254351:
26: 38464: loss: 0.3495594333:
26: 44864: loss: 0.3490015985:
26: 51264: loss: 0.3483100258:
26: 57664: loss: 0.3489357408:
26: 64064: loss: 0.3476649726:
26: 70464: loss: 0.3465107471:
26: 76864: loss: 0.3455167030:
26: 83264: loss: 0.3449956994:
26: 89664: loss: 0.3445165444:
Dev-Acc: 26: Accuracy: 0.9423420429: precision: 0.5200228833: recall: 0.1545655501: f1: 0.2383012190
Train-Acc: 26: Accuracy: 0.8619201183: precision: 0.9315249752: recall: 0.1851291828: f1: 0.3088735330
27: 6464: loss: 0.3447195093:
27: 12864: loss: 0.3424115115:
27: 19264: loss: 0.3428109873:
27: 25664: loss: 0.3422908777:
27: 32064: loss: 0.3432670732:
27: 38464: loss: 0.3424557576:
27: 44864: loss: 0.3407963984:
27: 51264: loss: 0.3400639807:
27: 57664: loss: 0.3392912288:
27: 64064: loss: 0.3384683433:
27: 70464: loss: 0.3379192687:
27: 76864: loss: 0.3380357413:
27: 83264: loss: 0.3377068064:
27: 89664: loss: 0.3373057130:
Dev-Acc: 27: Accuracy: 0.9419748783: precision: 0.5079060853: recall: 0.1802414555: f1: 0.2660642570
Train-Acc: 27: Accuracy: 0.8663905263: precision: 0.9348515422: recall: 0.2132009730: f1: 0.3472162741
28: 6464: loss: 0.3317763828:
28: 12864: loss: 0.3373645771:
28: 19264: loss: 0.3316505975:
28: 25664: loss: 0.3346639599:
28: 32064: loss: 0.3338271106:
28: 38464: loss: 0.3336264120:
28: 44864: loss: 0.3328079396:
28: 51264: loss: 0.3325993983:
28: 57664: loss: 0.3323171805:
28: 64064: loss: 0.3327288163:
28: 70464: loss: 0.3320093794:
28: 76864: loss: 0.3313425500:
28: 83264: loss: 0.3311946770:
28: 89664: loss: 0.3314778692:
Dev-Acc: 28: Accuracy: 0.9405957460: precision: 0.4792968750: recall: 0.2086379867: f1: 0.2907238479
Train-Acc: 28: Accuracy: 0.8710472584: precision: 0.9354757085: recall: 0.2430477944: f1: 0.3858477274
29: 6464: loss: 0.3222456057:
29: 12864: loss: 0.3222305259:
29: 19264: loss: 0.3232372797:
29: 25664: loss: 0.3228324243:
29: 32064: loss: 0.3242857786:
29: 38464: loss: 0.3252588494:
29: 44864: loss: 0.3259951162:
29: 51264: loss: 0.3247812795:
29: 57664: loss: 0.3254311336:
29: 64064: loss: 0.3255089640:
29: 70464: loss: 0.3258395368:
29: 76864: loss: 0.3256139832:
29: 83264: loss: 0.3251057271:
29: 89664: loss: 0.3253972204:
Dev-Acc: 29: Accuracy: 0.9364283681: precision: 0.4349975284: recall: 0.2992688318: f1: 0.3545884960
Train-Acc: 29: Accuracy: 0.8766353130: precision: 0.9243986254: recall: 0.2829531260: f1: 0.4332813208
30: 6464: loss: 0.3169139317:
30: 12864: loss: 0.3229078858:
30: 19264: loss: 0.3199216441:
30: 25664: loss: 0.3191515606:
30: 32064: loss: 0.3191722183:
30: 38464: loss: 0.3180003856:
30: 44864: loss: 0.3181570133:
30: 51264: loss: 0.3193887049:
30: 57664: loss: 0.3191557843:
30: 64064: loss: 0.3199447193:
30: 70464: loss: 0.3196783301:
30: 76864: loss: 0.3191643917:
30: 83264: loss: 0.3197560640:
30: 89664: loss: 0.3197102651:
Dev-Acc: 30: Accuracy: 0.9286692142: precision: 0.3864188954: recall: 0.3783370175: f1: 0.3823352522
Train-Acc: 30: Accuracy: 0.8815768957: precision: 0.8837371448: recall: 0.3333114194: f1: 0.4840557571
31: 6464: loss: 0.3213743138:
31: 12864: loss: 0.3166482189:
31: 19264: loss: 0.3130845491:
31: 25664: loss: 0.3125529047:
31: 32064: loss: 0.3127074979:
31: 38464: loss: 0.3133736737:
31: 44864: loss: 0.3142553027:
31: 51264: loss: 0.3131096254:
31: 57664: loss: 0.3142012326:
31: 64064: loss: 0.3145544563:
31: 70464: loss: 0.3142015418:
31: 76864: loss: 0.3142967413:
31: 83264: loss: 0.3138837819:
31: 89664: loss: 0.3141284347:
Dev-Acc: 31: Accuracy: 0.9229441285: precision: 0.3608034264: recall: 0.4154055433: f1: 0.3861840025
Train-Acc: 31: Accuracy: 0.8830342293: precision: 0.8474264706: recall: 0.3636841759: f1: 0.5089470537
32: 6464: loss: 0.3086554281:
32: 12864: loss: 0.3135645648:
32: 19264: loss: 0.3147213831:
32: 25664: loss: 0.3137012506:
32: 32064: loss: 0.3123439239:
32: 38464: loss: 0.3110750880:
32: 44864: loss: 0.3110675052:
32: 51264: loss: 0.3110898623:
32: 57664: loss: 0.3108157035:
32: 64064: loss: 0.3102060494:
32: 70464: loss: 0.3091326181:
32: 76864: loss: 0.3089995064:
32: 83264: loss: 0.3094387860:
32: 89664: loss: 0.3092148304:
Dev-Acc: 32: Accuracy: 0.9199674726: precision: 0.3494556979: recall: 0.4312191804: f1: 0.3860557162
Train-Acc: 32: Accuracy: 0.8851379156: precision: 0.8384879725: recall: 0.3849845507: f1: 0.5276864159
33: 6464: loss: 0.2992662396:
33: 12864: loss: 0.3043626419:
33: 19264: loss: 0.3086543697:
33: 25664: loss: 0.3092511464:
33: 32064: loss: 0.3081826411:
33: 38464: loss: 0.3074945872:
33: 44864: loss: 0.3067670309:
33: 51264: loss: 0.3065428406:
33: 57664: loss: 0.3057375006:
33: 64064: loss: 0.3055816186:
33: 70464: loss: 0.3051820203:
33: 76864: loss: 0.3047353134:
33: 83264: loss: 0.3039772553:
33: 89664: loss: 0.3043141254:
Dev-Acc: 33: Accuracy: 0.9185584784: precision: 0.3451763140: recall: 0.4410814487: f1: 0.3872797850
Train-Acc: 33: Accuracy: 0.8875156045: precision: 0.8392097681: recall: 0.4021431859: f1: 0.5437333333
34: 6464: loss: 0.3008154868:
34: 12864: loss: 0.3012039272:
34: 19264: loss: 0.3018517508:
34: 25664: loss: 0.3011573651:
34: 32064: loss: 0.3002500229:
34: 38464: loss: 0.2996459910:
34: 44864: loss: 0.2997816908:
34: 51264: loss: 0.2989935335:
34: 57664: loss: 0.2988418476:
34: 64064: loss: 0.2987448718:
34: 70464: loss: 0.2985050126:
34: 76864: loss: 0.2994492704:
34: 83264: loss: 0.2996804377:
34: 89664: loss: 0.2998091985:
Dev-Acc: 34: Accuracy: 0.9173380733: precision: 0.3425854536: recall: 0.4533242646: f1: 0.3902510430
Train-Acc: 34: Accuracy: 0.8892248869: precision: 0.8382177430: recall: 0.4155545329: f1: 0.5556434599
35: 6464: loss: 0.2889080262:
35: 12864: loss: 0.2932649309:
35: 19264: loss: 0.2910525601:
35: 25664: loss: 0.2925118434:
35: 32064: loss: 0.2953254403:
35: 38464: loss: 0.2963243908:
35: 44864: loss: 0.2956009451:
35: 51264: loss: 0.2951563352:
35: 57664: loss: 0.2944031207:
35: 64064: loss: 0.2947582517:
35: 70464: loss: 0.2949595913:
35: 76864: loss: 0.2952512928:
35: 83264: loss: 0.2951509367:
35: 89664: loss: 0.2949687663:
Dev-Acc: 35: Accuracy: 0.9167228937: precision: 0.3440139096: recall: 0.4710083319: f1: 0.3976171679
Train-Acc: 35: Accuracy: 0.8909451365: precision: 0.8370512821: recall: 0.4292288475: f1: 0.5674677328
36: 6464: loss: 0.2828036387:
36: 12864: loss: 0.2886834431:
36: 19264: loss: 0.2916592118:
36: 25664: loss: 0.2917203868:
36: 32064: loss: 0.2900482259:
36: 38464: loss: 0.2895216825:
36: 44864: loss: 0.2897532283:
36: 51264: loss: 0.2905132453:
36: 57664: loss: 0.2915300836:
36: 64064: loss: 0.2914636029:
36: 70464: loss: 0.2907664359:
36: 76864: loss: 0.2912299252:
36: 83264: loss: 0.2908878840:
36: 89664: loss: 0.2913824557:
Dev-Acc: 36: Accuracy: 0.9151849151: precision: 0.3401654081: recall: 0.4825709913: f1: 0.3990438695
Train-Acc: 36: Accuracy: 0.8929502964: precision: 0.8384969516: recall: 0.4430346460: f1: 0.5797487956
37: 6464: loss: 0.2868416825:
37: 12864: loss: 0.2839911176:
37: 19264: loss: 0.2861859534:
37: 25664: loss: 0.2884995991:
37: 32064: loss: 0.2890191420:
37: 38464: loss: 0.2877296807:
37: 44864: loss: 0.2878613200:
37: 51264: loss: 0.2883922354:
37: 57664: loss: 0.2892091617:
37: 64064: loss: 0.2886084084:
37: 70464: loss: 0.2880646776:
37: 76864: loss: 0.2877118003:
37: 83264: loss: 0.2872490032:
37: 89664: loss: 0.2873735575:
Dev-Acc: 37: Accuracy: 0.9136767387: precision: 0.3357034619: recall: 0.4897126339: f1: 0.3983402490
Train-Acc: 37: Accuracy: 0.8942541480: precision: 0.8380350195: recall: 0.4530931563: f1: 0.5881800725
38: 6464: loss: 0.2814221562:
38: 12864: loss: 0.2774718352:
38: 19264: loss: 0.2786524570:
38: 25664: loss: 0.2789922105:
38: 32064: loss: 0.2804539233:
38: 38464: loss: 0.2798907382:
38: 44864: loss: 0.2806776743:
38: 51264: loss: 0.2819758093:
38: 57664: loss: 0.2829756371:
38: 64064: loss: 0.2831624754:
38: 70464: loss: 0.2830073004:
38: 76864: loss: 0.2835571430:
38: 83264: loss: 0.2834168093:
38: 89664: loss: 0.2830602372:
Dev-Acc: 38: Accuracy: 0.9124860764: precision: 0.3327643109: recall: 0.4971943547: f1: 0.3986910281
Train-Acc: 38: Accuracy: 0.8958648443: precision: 0.8377322760: recall: 0.4653211492: f1: 0.5983093829
39: 6464: loss: 0.2900306685:
39: 12864: loss: 0.2853925943:
39: 19264: loss: 0.2852348078:
39: 25664: loss: 0.2846822472:
39: 32064: loss: 0.2814481909:
39: 38464: loss: 0.2796235982:
39: 44864: loss: 0.2798588453:
39: 51264: loss: 0.2803283463:
39: 57664: loss: 0.2793667323:
39: 64064: loss: 0.2793741149:
39: 70464: loss: 0.2802361847:
39: 76864: loss: 0.2807826966:
39: 83264: loss: 0.2812331789:
39: 89664: loss: 0.2801149993:
Dev-Acc: 39: Accuracy: 0.9111267328: precision: 0.3286923591: recall: 0.5017854106: f1: 0.3972003500
Train-Acc: 39: Accuracy: 0.8972892165: precision: 0.8393999302: recall: 0.4745250148: f1: 0.6062998740
40: 6464: loss: 0.2766569881:
40: 12864: loss: 0.2736297558:
40: 19264: loss: 0.2761367003:
40: 25664: loss: 0.2752568516:
40: 32064: loss: 0.2773075904:
40: 38464: loss: 0.2754891273:
40: 44864: loss: 0.2748085157:
40: 51264: loss: 0.2760161050:
40: 57664: loss: 0.2761635779:
40: 64064: loss: 0.2761938857:
40: 70464: loss: 0.2746879984:
40: 76864: loss: 0.2751594282:
40: 83264: loss: 0.2761832115:
40: 89664: loss: 0.2762982062:
Dev-Acc: 40: Accuracy: 0.9095094204: precision: 0.3236415115: recall: 0.5053562319: f1: 0.3945831121
Train-Acc: 40: Accuracy: 0.8988232017: precision: 0.8411140281: recall: 0.4844520413: f1: 0.6148006007
41: 6464: loss: 0.2710990476:
41: 12864: loss: 0.2731926746:
41: 19264: loss: 0.2736052282:
41: 25664: loss: 0.2745979795:
41: 32064: loss: 0.2733910503:
41: 38464: loss: 0.2743860042:
41: 44864: loss: 0.2734093221:
41: 51264: loss: 0.2725262027:
41: 57664: loss: 0.2731074210:
41: 64064: loss: 0.2731770607:
41: 70464: loss: 0.2739696330:
41: 76864: loss: 0.2737000921:
41: 83264: loss: 0.2734743350:
41: 89664: loss: 0.2733279026:
Dev-Acc: 41: Accuracy: 0.9075944424: precision: 0.3179889690: recall: 0.5097772488: f1: 0.3916650336
Train-Acc: 41: Accuracy: 0.9000394344: precision: 0.8418688230: recall: 0.4928012622: f1: 0.6216877462
42: 6464: loss: 0.2617781760:
42: 12864: loss: 0.2640947608:
42: 19264: loss: 0.2681928257:
42: 25664: loss: 0.2684126836:
42: 32064: loss: 0.2705638625:
42: 38464: loss: 0.2681552899:
42: 44864: loss: 0.2683895719:
42: 51264: loss: 0.2688924660:
42: 57664: loss: 0.2687246576:
42: 64064: loss: 0.2693347654:
42: 70464: loss: 0.2703656752:
42: 76864: loss: 0.2697191562:
42: 83264: loss: 0.2695466391:
42: 89664: loss: 0.2696887842:
Dev-Acc: 42: Accuracy: 0.9059572816: precision: 0.3146449552: recall: 0.5191293998: f1: 0.3918121150
Train-Acc: 42: Accuracy: 0.9010036588: precision: 0.8426542388: recall: 0.4992439682: f1: 0.6270073897
43: 6464: loss: 0.2695039339:
43: 12864: loss: 0.2705612463:
43: 19264: loss: 0.2673993783:
43: 25664: loss: 0.2697346861:
43: 32064: loss: 0.2687201750:
43: 38464: loss: 0.2683672661:
43: 44864: loss: 0.2669476995:
43: 51264: loss: 0.2673342285:
43: 57664: loss: 0.2667352891:
43: 64064: loss: 0.2664495440:
43: 70464: loss: 0.2669207678:
43: 76864: loss: 0.2666136635:
43: 83264: loss: 0.2667601426:
43: 89664: loss: 0.2674603484:
Dev-Acc: 43: Accuracy: 0.9043300152: precision: 0.3104907790: recall: 0.5238904948: f1: 0.3899012908
Train-Acc: 43: Accuracy: 0.9019131064: precision: 0.8424335266: recall: 0.5061468674: f1: 0.6323613963
44: 6464: loss: 0.2738074249:
44: 12864: loss: 0.2675348365:
44: 19264: loss: 0.2660644818:
44: 25664: loss: 0.2646408935:
44: 32064: loss: 0.2638573484:
44: 38464: loss: 0.2623046933:
44: 44864: loss: 0.2633422214:
44: 51264: loss: 0.2624432010:
44: 57664: loss: 0.2639085058:
44: 64064: loss: 0.2646455739:
44: 70464: loss: 0.2648819984:
44: 76864: loss: 0.2646855089:
44: 83264: loss: 0.2647332822:
44: 89664: loss: 0.2646037662:
Dev-Acc: 44: Accuracy: 0.9027722478: precision: 0.3062697785: recall: 0.5266111206: f1: 0.3872944413
Train-Acc: 44: Accuracy: 0.9028553963: precision: 0.8429358988: recall: 0.5126553152: f1: 0.6375602976
45: 6464: loss: 0.2615517327:
45: 12864: loss: 0.2593796942:
45: 19264: loss: 0.2626042351:
45: 25664: loss: 0.2624561078:
45: 32064: loss: 0.2627087131:
45: 38464: loss: 0.2607211137:
45: 44864: loss: 0.2622798596:
45: 51264: loss: 0.2619361250:
45: 57664: loss: 0.2625661392:
45: 64064: loss: 0.2624137436:
45: 70464: loss: 0.2620687738:
45: 76864: loss: 0.2618220033:
45: 83264: loss: 0.2625510552:
45: 89664: loss: 0.2621313574:
Dev-Acc: 45: Accuracy: 0.9012343287: precision: 0.3025307864: recall: 0.5305220201: f1: 0.3853278992
Train-Acc: 45: Accuracy: 0.9036552310: precision: 0.8427686392: recall: 0.5187035698: f1: 0.6421682334
46: 6464: loss: 0.2642244846:
46: 12864: loss: 0.2623510566:
46: 19264: loss: 0.2599461351:
46: 25664: loss: 0.2584290726:
46: 32064: loss: 0.2586082633:
46: 38464: loss: 0.2601459577:
46: 44864: loss: 0.2599365237:
46: 51264: loss: 0.2594138864:
46: 57664: loss: 0.2593084347:
46: 64064: loss: 0.2589418207:
46: 70464: loss: 0.2594172823:
46: 76864: loss: 0.2593025187:
46: 83264: loss: 0.2589633989:
46: 89664: loss: 0.2589010667:
Dev-Acc: 46: Accuracy: 0.8997360468: precision: 0.2994682871: recall: 0.5363033498: f1: 0.3843294949
Train-Acc: 46: Accuracy: 0.9045866132: precision: 0.8438193930: recall: 0.5246203405: f1: 0.6469920545
47: 6464: loss: 0.2519224726:
47: 12864: loss: 0.2502583784:
47: 19264: loss: 0.2524713414:
47: 25664: loss: 0.2541178229:
47: 32064: loss: 0.2549601517:
47: 38464: loss: 0.2556630440:
47: 44864: loss: 0.2560518169:
47: 51264: loss: 0.2560724441:
47: 57664: loss: 0.2567542508:
47: 64064: loss: 0.2570932863:
47: 70464: loss: 0.2565474256:
47: 76864: loss: 0.2569883171:
47: 83264: loss: 0.2568564197:
47: 89664: loss: 0.2569589839:
Dev-Acc: 47: Accuracy: 0.8978607655: precision: 0.2951063237: recall: 0.5403842884: f1: 0.3817417417
Train-Acc: 47: Accuracy: 0.9055179358: precision: 0.8436261214: recall: 0.5316547236: f1: 0.6522563213
48: 6464: loss: 0.2646974839:
48: 12864: loss: 0.2590245677:
48: 19264: loss: 0.2570020610:
48: 25664: loss: 0.2580230642:
48: 32064: loss: 0.2570172857:
48: 38464: loss: 0.2548373658:
48: 44864: loss: 0.2553409072:
48: 51264: loss: 0.2548075671:
48: 57664: loss: 0.2546883435:
48: 64064: loss: 0.2549987490:
48: 70464: loss: 0.2552341177:
48: 76864: loss: 0.2548216635:
48: 83264: loss: 0.2550168429:
48: 89664: loss: 0.2546841434:
Dev-Acc: 48: Accuracy: 0.8963128924: precision: 0.2919588380: recall: 0.5451453834: f1: 0.3802633140
Train-Acc: 48: Accuracy: 0.9062191844: precision: 0.8439503619: recall: 0.5365196240: f1: 0.6560025722
49: 6464: loss: 0.2503251316:
49: 12864: loss: 0.2483744561:
49: 19264: loss: 0.2481939584:
49: 25664: loss: 0.2490719903:
49: 32064: loss: 0.2511485766:
49: 38464: loss: 0.2525222129:
49: 44864: loss: 0.2529032208:
49: 51264: loss: 0.2528499972:
49: 57664: loss: 0.2524278039:
49: 64064: loss: 0.2527022805:
49: 70464: loss: 0.2523392890:
49: 76864: loss: 0.2529834524:
49: 83264: loss: 0.2524696844:
49: 89664: loss: 0.2521039770:
Dev-Acc: 49: Accuracy: 0.8947650194: precision: 0.2890436646: recall: 0.5504165958: f1: 0.3790398126
Train-Acc: 49: Accuracy: 0.9069094658: precision: 0.8436188722: recall: 0.5419104595: f1: 0.6599151389
50: 6464: loss: 0.2441247830:
50: 12864: loss: 0.2424449484:
50: 19264: loss: 0.2474319837:
50: 25664: loss: 0.2498574820:
50: 32064: loss: 0.2490720529:
50: 38464: loss: 0.2502770273:
50: 44864: loss: 0.2497574434:
50: 51264: loss: 0.2492801193:
50: 57664: loss: 0.2498313496:
50: 64064: loss: 0.2495088945:
50: 70464: loss: 0.2502045073:
50: 76864: loss: 0.2501519120:
50: 83264: loss: 0.2501120791:
50: 89664: loss: 0.2500149653:
Dev-Acc: 50: Accuracy: 0.8934751153: precision: 0.2863304287: recall: 0.5531372216: f1: 0.3773344160
Train-Acc: 50: Accuracy: 0.9074244499: precision: 0.8428310687: recall: 0.5464466505: f1: 0.6630239700
