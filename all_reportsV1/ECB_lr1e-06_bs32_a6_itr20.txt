1: 3232: loss: 0.6999704921:
1: 6432: loss: 0.6925827914:
1: 9632: loss: 0.6850593644:
1: 12832: loss: 0.6777186960:
1: 16032: loss: 0.6709546374:
1: 19232: loss: 0.6644219589:
1: 22432: loss: 0.6577604039:
1: 25632: loss: 0.6511593169:
1: 28832: loss: 0.6446663569:
1: 32032: loss: 0.6379591812:
1: 35232: loss: 0.6318627103:
1: 38432: loss: 0.6255839093:
1: 41632: loss: 0.6195806578:
1: 44832: loss: 0.6136209718:
1: 48032: loss: 0.6075015334:
1: 51232: loss: 0.6014796732:
1: 54432: loss: 0.5957093507:
1: 57632: loss: 0.5902161965:
1: 60832: loss: 0.5845457193:
1: 64032: loss: 0.5787851975:
1: 67232: loss: 0.5727753607:
1: 70432: loss: 0.5671355291:
1: 73632: loss: 0.5618794854:
1: 76832: loss: 0.5564339427:
1: 80032: loss: 0.5511726970:
1: 83232: loss: 0.5458127701:
1: 86432: loss: 0.5411321836:
1: 89632: loss: 0.5359379423:
1: 92832: loss: 0.5310292282:
1: 96032: loss: 0.5267803960:
1: 99232: loss: 0.5230041456:
1: 102432: loss: 0.5190309345:
1: 105632: loss: 0.5147657969:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8571428657: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 3232: loss: 0.3668548657:
2: 6432: loss: 0.3637332192:
2: 9632: loss: 0.3679705906:
2: 12832: loss: 0.3643414252:
2: 16032: loss: 0.3599905552:
2: 19232: loss: 0.3600703635:
2: 22432: loss: 0.3573609747:
2: 25632: loss: 0.3562175344:
2: 28832: loss: 0.3535419453:
2: 32032: loss: 0.3530354488:
2: 35232: loss: 0.3515705747:
2: 38432: loss: 0.3499815168:
2: 41632: loss: 0.3490215049:
2: 44832: loss: 0.3477273031:
2: 48032: loss: 0.3466042117:
2: 51232: loss: 0.3450450643:
2: 54432: loss: 0.3435169312:
2: 57632: loss: 0.3422886567:
2: 60832: loss: 0.3410775393:
2: 64032: loss: 0.3394754551:
2: 67232: loss: 0.3381015385:
2: 70432: loss: 0.3367039003:
2: 73632: loss: 0.3355687473:
2: 76832: loss: 0.3344136002:
2: 80032: loss: 0.3328971885:
2: 83232: loss: 0.3312503805:
2: 86432: loss: 0.3305988514:
2: 89632: loss: 0.3293889123:
2: 92832: loss: 0.3283573865:
2: 96032: loss: 0.3272558504:
2: 99232: loss: 0.3254054217:
2: 102432: loss: 0.3244150026:
2: 105632: loss: 0.3229782551:
Dev-Acc: 2: Accuracy: 0.9306834340: precision: 0.3986052487: recall: 0.3693249447: f1: 0.3834068844
Train-Acc: 2: Accuracy: 0.8931788206: precision: 0.8349921425: recall: 0.3143777529: f1: 0.4567771516
3: 3232: loss: 0.2735957223:
3: 6432: loss: 0.2766350982:
3: 9632: loss: 0.2781178442:
3: 12832: loss: 0.2748024270:
3: 16032: loss: 0.2751730499:
3: 19232: loss: 0.2757886005:
3: 22432: loss: 0.2757035925:
3: 25632: loss: 0.2750252643:
3: 28832: loss: 0.2739219804:
3: 32032: loss: 0.2741583589:
3: 35232: loss: 0.2724261648:
3: 38432: loss: 0.2727271395:
3: 41632: loss: 0.2705911841:
3: 44832: loss: 0.2703423926:
3: 48032: loss: 0.2699975699:
3: 51232: loss: 0.2694555523:
3: 54432: loss: 0.2695284772:
3: 57632: loss: 0.2688441921:
3: 60832: loss: 0.2683392569:
3: 64032: loss: 0.2676637315:
3: 67232: loss: 0.2667182886:
3: 70432: loss: 0.2663758648:
3: 73632: loss: 0.2658784589:
3: 76832: loss: 0.2651232243:
3: 80032: loss: 0.2644647627:
3: 83232: loss: 0.2638773181:
3: 86432: loss: 0.2635408125:
3: 89632: loss: 0.2631197168:
3: 92832: loss: 0.2621478513:
3: 96032: loss: 0.2613268705:
3: 99232: loss: 0.2610657372:
3: 102432: loss: 0.2602722532:
3: 105632: loss: 0.2600468223:
Dev-Acc: 3: Accuracy: 0.9105512500: precision: 0.3292656352: recall: 0.5138581874: f1: 0.4013546716
Train-Acc: 3: Accuracy: 0.9087221026: precision: 0.7891743892: recall: 0.4926697784: f1: 0.6066297001
4: 3232: loss: 0.2441950476:
4: 6432: loss: 0.2344202752:
4: 9632: loss: 0.2375069050:
4: 12832: loss: 0.2392898282:
4: 16032: loss: 0.2379448347:
4: 19232: loss: 0.2351980062:
4: 22432: loss: 0.2355132099:
4: 25632: loss: 0.2366048048:
4: 28832: loss: 0.2384454820:
4: 32032: loss: 0.2373992563:
4: 35232: loss: 0.2374093254:
4: 38432: loss: 0.2374251407:
4: 41632: loss: 0.2363614674:
4: 44832: loss: 0.2359339170:
4: 48032: loss: 0.2355750939:
4: 51232: loss: 0.2351884489:
4: 54432: loss: 0.2354727161:
4: 57632: loss: 0.2340717189:
4: 60832: loss: 0.2331965439:
4: 64032: loss: 0.2335549079:
4: 67232: loss: 0.2327834201:
4: 70432: loss: 0.2323727736:
4: 73632: loss: 0.2319310826:
4: 76832: loss: 0.2318528411:
4: 80032: loss: 0.2315671507:
4: 83232: loss: 0.2316522173:
4: 86432: loss: 0.2312279900:
4: 89632: loss: 0.2310363202:
4: 92832: loss: 0.2309730225:
4: 96032: loss: 0.2304865310:
4: 99232: loss: 0.2305860186:
4: 102432: loss: 0.2304395300:
4: 105632: loss: 0.2300520510:
Dev-Acc: 4: Accuracy: 0.9014427066: precision: 0.3079984837: recall: 0.5526271042: f1: 0.3955455486
Train-Acc: 4: Accuracy: 0.9146200418: precision: 0.7955950541: recall: 0.5414502663: f1: 0.6443688143
5: 3232: loss: 0.2284093735:
5: 6432: loss: 0.2233326380:
5: 9632: loss: 0.2225119215:
5: 12832: loss: 0.2221061206:
5: 16032: loss: 0.2221348898:
5: 19232: loss: 0.2242748014:
5: 22432: loss: 0.2235281608:
5: 25632: loss: 0.2233583332:
5: 28832: loss: 0.2240294675:
5: 32032: loss: 0.2225655391:
5: 35232: loss: 0.2213383920:
5: 38432: loss: 0.2210149050:
5: 41632: loss: 0.2207155632:
5: 44832: loss: 0.2192598831:
5: 48032: loss: 0.2182398462:
5: 51232: loss: 0.2178767523:
5: 54432: loss: 0.2182547577:
5: 57632: loss: 0.2183346722:
5: 60832: loss: 0.2179788874:
5: 64032: loss: 0.2175007543:
5: 67232: loss: 0.2178319283:
5: 70432: loss: 0.2181676791:
5: 73632: loss: 0.2182712786:
5: 76832: loss: 0.2174750799:
5: 80032: loss: 0.2165626762:
5: 83232: loss: 0.2162777020:
5: 86432: loss: 0.2158615901:
5: 89632: loss: 0.2157528664:
5: 92832: loss: 0.2157930333:
5: 96032: loss: 0.2146291584:
5: 99232: loss: 0.2139421996:
5: 102432: loss: 0.2134881177:
5: 105632: loss: 0.2128989950:
Dev-Acc: 5: Accuracy: 0.9004206657: precision: 0.3061129258: recall: 0.5577282775: f1: 0.3952759701
Train-Acc: 5: Accuracy: 0.9195694923: precision: 0.8158920255: recall: 0.5643284465: f1: 0.6671848282
6: 3232: loss: 0.2126150791:
6: 6432: loss: 0.2102262194:
6: 9632: loss: 0.2126420241:
6: 12832: loss: 0.2102312297:
6: 16032: loss: 0.2114150692:
6: 19232: loss: 0.2092088109:
6: 22432: loss: 0.2075596129:
6: 25632: loss: 0.2072789254:
6: 28832: loss: 0.2071376956:
6: 32032: loss: 0.2078042483:
6: 35232: loss: 0.2070649939:
6: 38432: loss: 0.2066986420:
6: 41632: loss: 0.2071626607:
6: 44832: loss: 0.2058297532:
6: 48032: loss: 0.2048734038:
6: 51232: loss: 0.2050438345:
6: 54432: loss: 0.2045145133:
6: 57632: loss: 0.2034695077:
6: 60832: loss: 0.2031864865:
6: 64032: loss: 0.2028417230:
6: 67232: loss: 0.2024754019:
6: 70432: loss: 0.2028569884:
6: 73632: loss: 0.2030388456:
6: 76832: loss: 0.2021571130:
6: 80032: loss: 0.2023166963:
6: 83232: loss: 0.2017631222:
6: 86432: loss: 0.2018790422:
6: 89632: loss: 0.2015648299:
6: 92832: loss: 0.2015150721:
6: 96032: loss: 0.2014843028:
6: 99232: loss: 0.2007998646:
6: 102432: loss: 0.2013942720:
6: 105632: loss: 0.2012112367:
Dev-Acc: 6: Accuracy: 0.8958168030: precision: 0.2929256702: recall: 0.5555177691: f1: 0.3835857696
Train-Acc: 6: Accuracy: 0.9251481891: precision: 0.8275581290: recall: 0.6013411347: f1: 0.6965427962
7: 3232: loss: 0.1939375030:
7: 6432: loss: 0.1900221799:
7: 9632: loss: 0.1945862477:
7: 12832: loss: 0.1929406929:
7: 16032: loss: 0.1905933606:
7: 19232: loss: 0.1925339786:
7: 22432: loss: 0.1942838757:
7: 25632: loss: 0.1942184843:
7: 28832: loss: 0.1940178051:
7: 32032: loss: 0.1947414795:
7: 35232: loss: 0.1955576461:
7: 38432: loss: 0.1958604589:
7: 41632: loss: 0.1963017088:
7: 44832: loss: 0.1958736328:
7: 48032: loss: 0.1959053896:
7: 51232: loss: 0.1955409327:
7: 54432: loss: 0.1954474445:
7: 57632: loss: 0.1951768928:
7: 60832: loss: 0.1944776201:
7: 64032: loss: 0.1948655622:
7: 67232: loss: 0.1945608965:
7: 70432: loss: 0.1946476995:
7: 73632: loss: 0.1938086838:
7: 76832: loss: 0.1936596404:
7: 80032: loss: 0.1932609046:
7: 83232: loss: 0.1935036323:
7: 86432: loss: 0.1935305115:
7: 89632: loss: 0.1931048468:
7: 92832: loss: 0.1924850102:
7: 96032: loss: 0.1920775113:
7: 99232: loss: 0.1922954759:
7: 102432: loss: 0.1920506047:
7: 105632: loss: 0.1918822380:
Dev-Acc: 7: Accuracy: 0.8954694867: precision: 0.2832526080: recall: 0.5170889305: f1: 0.3660107119
Train-Acc: 7: Accuracy: 0.9286794662: precision: 0.8547080190: recall: 0.6033133916: f1: 0.7073377524
8: 3232: loss: 0.1925001546:
8: 6432: loss: 0.1905080653:
8: 9632: loss: 0.1890281496:
8: 12832: loss: 0.1893282569:
8: 16032: loss: 0.1883075593:
8: 19232: loss: 0.1858730551:
8: 22432: loss: 0.1844930837:
8: 25632: loss: 0.1857974939:
8: 28832: loss: 0.1858269617:
8: 32032: loss: 0.1860625047:
8: 35232: loss: 0.1844996703:
8: 38432: loss: 0.1835954652:
8: 41632: loss: 0.1849800452:
8: 44832: loss: 0.1859801485:
8: 48032: loss: 0.1856162132:
8: 51232: loss: 0.1852321621:
8: 54432: loss: 0.1851962288:
8: 57632: loss: 0.1862398806:
8: 60832: loss: 0.1864171781:
8: 64032: loss: 0.1858709655:
8: 67232: loss: 0.1858465763:
8: 70432: loss: 0.1856654893:
8: 73632: loss: 0.1854438504:
8: 76832: loss: 0.1854279318:
8: 80032: loss: 0.1853342272:
8: 83232: loss: 0.1854146876:
8: 86432: loss: 0.1850043953:
8: 89632: loss: 0.1849084569:
8: 92832: loss: 0.1844933355:
8: 96032: loss: 0.1840795997:
8: 99232: loss: 0.1843367276:
8: 102432: loss: 0.1841520814:
8: 105632: loss: 0.1842377495:
Dev-Acc: 8: Accuracy: 0.8871645927: precision: 0.2637059988: recall: 0.5209998300: f1: 0.3501714286
Train-Acc: 8: Accuracy: 0.9327648282: precision: 0.8590155163: recall: 0.6332916968: f1: 0.7290823084
9: 3232: loss: 0.1643758629:
9: 6432: loss: 0.1758087814:
9: 9632: loss: 0.1725748609:
9: 12832: loss: 0.1723563057:
9: 16032: loss: 0.1715301832:
9: 19232: loss: 0.1722113612:
9: 22432: loss: 0.1729980938:
9: 25632: loss: 0.1747568492:
9: 28832: loss: 0.1741418726:
9: 32032: loss: 0.1756622441:
9: 35232: loss: 0.1741156114:
9: 38432: loss: 0.1761349869:
9: 41632: loss: 0.1763493716:
9: 44832: loss: 0.1768368538:
9: 48032: loss: 0.1756589333:
9: 51232: loss: 0.1759479970:
9: 54432: loss: 0.1762309876:
9: 57632: loss: 0.1762807417:
9: 60832: loss: 0.1767619041:
9: 64032: loss: 0.1761606978:
9: 67232: loss: 0.1767164978:
9: 70432: loss: 0.1766584969:
9: 73632: loss: 0.1763014711:
9: 76832: loss: 0.1766242409:
9: 80032: loss: 0.1765533712:
9: 83232: loss: 0.1772338542:
9: 86432: loss: 0.1773235867:
9: 89632: loss: 0.1774438694:
9: 92832: loss: 0.1773276146:
9: 96032: loss: 0.1775351634:
9: 99232: loss: 0.1775538109:
9: 102432: loss: 0.1777828204:
9: 105632: loss: 0.1779765242:
Dev-Acc: 9: Accuracy: 0.8769248724: precision: 0.2456126667: recall: 0.5354531542: f1: 0.3367554272
Train-Acc: 9: Accuracy: 0.9363430738: precision: 0.8618381533: recall: 0.6602458747: f1: 0.7476920786
10: 3232: loss: 0.1809276786:
10: 6432: loss: 0.1724756902:
10: 9632: loss: 0.1742012124:
10: 12832: loss: 0.1726745497:
10: 16032: loss: 0.1773757124:
10: 19232: loss: 0.1758992118:
10: 22432: loss: 0.1782732387:
10: 25632: loss: 0.1779489813:
10: 28832: loss: 0.1783084423:
10: 32032: loss: 0.1788301572:
10: 35232: loss: 0.1779579457:
10: 38432: loss: 0.1765913878:
10: 41632: loss: 0.1773452994:
10: 44832: loss: 0.1770646028:
10: 48032: loss: 0.1762121413:
10: 51232: loss: 0.1762496240:
10: 54432: loss: 0.1753189202:
10: 57632: loss: 0.1751871070:
10: 60832: loss: 0.1757505893:
10: 64032: loss: 0.1753992939:
10: 67232: loss: 0.1748809594:
10: 70432: loss: 0.1744491713:
10: 73632: loss: 0.1744434474:
10: 76832: loss: 0.1742738034:
10: 80032: loss: 0.1747604482:
10: 83232: loss: 0.1748967664:
10: 86432: loss: 0.1747457812:
10: 89632: loss: 0.1745964778:
10: 92832: loss: 0.1740430866:
10: 96032: loss: 0.1736485689:
10: 99232: loss: 0.1734743567:
10: 102432: loss: 0.1734444759:
10: 105632: loss: 0.1731170145:
Dev-Acc: 10: Accuracy: 0.8720828891: precision: 0.2338470883: recall: 0.5237204557: f1: 0.3233256351
Train-Acc: 10: Accuracy: 0.9379866123: precision: 0.8700773861: recall: 0.6652422589: f1: 0.7539957528
11: 3232: loss: 0.1730088364:
11: 6432: loss: 0.1678863819:
11: 9632: loss: 0.1737587942:
11: 12832: loss: 0.1734915054:
11: 16032: loss: 0.1710970415:
11: 19232: loss: 0.1694549550:
11: 22432: loss: 0.1708659891:
11: 25632: loss: 0.1718690906:
11: 28832: loss: 0.1708867692:
11: 32032: loss: 0.1707732245:
11: 35232: loss: 0.1714366919:
11: 38432: loss: 0.1712818917:
11: 41632: loss: 0.1707439079:
11: 44832: loss: 0.1701071957:
11: 48032: loss: 0.1691398584:
11: 51232: loss: 0.1686463661:
11: 54432: loss: 0.1698027670:
11: 57632: loss: 0.1695701060:
11: 60832: loss: 0.1691144675:
11: 64032: loss: 0.1692584594:
11: 67232: loss: 0.1691395371:
11: 70432: loss: 0.1693810839:
11: 73632: loss: 0.1686736937:
11: 76832: loss: 0.1686776419:
11: 80032: loss: 0.1685199043:
11: 83232: loss: 0.1685966816:
11: 86432: loss: 0.1688143714:
11: 89632: loss: 0.1689674386:
11: 92832: loss: 0.1688859865:
11: 96032: loss: 0.1684195222:
11: 99232: loss: 0.1683828155:
11: 102432: loss: 0.1683608024:
11: 105632: loss: 0.1680129999:
Dev-Acc: 11: Accuracy: 0.8705548644: precision: 0.2287834053: recall: 0.5138581874: f1: 0.3166055526
Train-Acc: 11: Accuracy: 0.9387191534: precision: 0.8791026536: recall: 0.6620866478: f1: 0.7553155586
12: 3232: loss: 0.1711097157:
12: 6432: loss: 0.1724672564:
12: 9632: loss: 0.1734788095:
12: 12832: loss: 0.1723054560:
12: 16032: loss: 0.1712967814:
12: 19232: loss: 0.1694885692:
12: 22432: loss: 0.1701641254:
12: 25632: loss: 0.1713353632:
12: 28832: loss: 0.1701126422:
12: 32032: loss: 0.1699879762:
12: 35232: loss: 0.1686084682:
12: 38432: loss: 0.1679241545:
12: 41632: loss: 0.1666490310:
12: 44832: loss: 0.1658243148:
12: 48032: loss: 0.1658283240:
12: 51232: loss: 0.1655425258:
12: 54432: loss: 0.1655054426:
12: 57632: loss: 0.1652927503:
12: 60832: loss: 0.1645623258:
12: 64032: loss: 0.1650829273:
12: 67232: loss: 0.1643448354:
12: 70432: loss: 0.1639958284:
12: 73632: loss: 0.1641638125:
12: 76832: loss: 0.1638087520:
12: 80032: loss: 0.1639054265:
12: 83232: loss: 0.1633178090:
12: 86432: loss: 0.1633653152:
12: 89632: loss: 0.1632823108:
12: 92832: loss: 0.1635355093:
12: 96032: loss: 0.1639107870:
12: 99232: loss: 0.1638056291:
12: 102432: loss: 0.1637444651:
12: 105632: loss: 0.1637658648:
Dev-Acc: 12: Accuracy: 0.8628254533: precision: 0.2188561721: recall: 0.5257609250: f1: 0.3090609226
Train-Acc: 12: Accuracy: 0.9410389066: precision: 0.8754939050: recall: 0.6846361186: f1: 0.7683907622
13: 3232: loss: 0.1639818097:
13: 6432: loss: 0.1658138102:
13: 9632: loss: 0.1685733556:
13: 12832: loss: 0.1706745709:
13: 16032: loss: 0.1674633929:
13: 19232: loss: 0.1678440194:
13: 22432: loss: 0.1655722589:
13: 25632: loss: 0.1656604593:
13: 28832: loss: 0.1660305506:
13: 32032: loss: 0.1648009583:
13: 35232: loss: 0.1646848507:
13: 38432: loss: 0.1630790363:
13: 41632: loss: 0.1617016326:
13: 44832: loss: 0.1614879296:
13: 48032: loss: 0.1606140067:
13: 51232: loss: 0.1601179765:
13: 54432: loss: 0.1592394412:
13: 57632: loss: 0.1586095835:
13: 60832: loss: 0.1585666901:
13: 64032: loss: 0.1587196794:
13: 67232: loss: 0.1584449501:
13: 70432: loss: 0.1590067611:
13: 73632: loss: 0.1589441081:
13: 76832: loss: 0.1594029862:
13: 80032: loss: 0.1592238752:
13: 83232: loss: 0.1594833131:
13: 86432: loss: 0.1591344345:
13: 89632: loss: 0.1597228355:
13: 92832: loss: 0.1596090562:
13: 96032: loss: 0.1595543079:
13: 99232: loss: 0.1598827606:
13: 102432: loss: 0.1597725956:
13: 105632: loss: 0.1597177358:
Dev-Acc: 13: Accuracy: 0.8539649248: precision: 0.2084845286: recall: 0.5373235844: f1: 0.3004087841
Train-Acc: 13: Accuracy: 0.9429360628: precision: 0.8733142624: recall: 0.7024521728: f1: 0.7786198353
14: 3232: loss: 0.1613970158:
14: 6432: loss: 0.1564085471:
14: 9632: loss: 0.1574798027:
14: 12832: loss: 0.1554328871:
14: 16032: loss: 0.1554565748:
14: 19232: loss: 0.1541208377:
14: 22432: loss: 0.1564038888:
14: 25632: loss: 0.1569375608:
14: 28832: loss: 0.1593770339:
14: 32032: loss: 0.1582946371:
14: 35232: loss: 0.1559150260:
14: 38432: loss: 0.1555939654:
14: 41632: loss: 0.1557668749:
14: 44832: loss: 0.1550153566:
14: 48032: loss: 0.1553644715:
14: 51232: loss: 0.1556110215:
14: 54432: loss: 0.1557135531:
14: 57632: loss: 0.1550705345:
14: 60832: loss: 0.1558368148:
14: 64032: loss: 0.1568022592:
14: 67232: loss: 0.1565888786:
14: 70432: loss: 0.1563847398:
14: 73632: loss: 0.1559181986:
14: 76832: loss: 0.1560728727:
14: 80032: loss: 0.1563592682:
14: 83232: loss: 0.1564235263:
14: 86432: loss: 0.1563569807:
14: 89632: loss: 0.1565096938:
14: 92832: loss: 0.1567147881:
14: 96032: loss: 0.1564701974:
14: 99232: loss: 0.1560105573:
14: 102432: loss: 0.1561774699:
14: 105632: loss: 0.1562751656:
Dev-Acc: 14: Accuracy: 0.8545503020: precision: 0.2054362416: recall: 0.5204897126: f1: 0.2945960252
Train-Acc: 14: Accuracy: 0.9431144595: precision: 0.8828843902: recall: 0.6938399842: f1: 0.7770292656
15: 3232: loss: 0.1549696338:
15: 6432: loss: 0.1556670825:
15: 9632: loss: 0.1574489127:
15: 12832: loss: 0.1568434066:
15: 16032: loss: 0.1546150806:
15: 19232: loss: 0.1540694907:
15: 22432: loss: 0.1519216783:
15: 25632: loss: 0.1530038373:
15: 28832: loss: 0.1532537371:
15: 32032: loss: 0.1547363580:
15: 35232: loss: 0.1547508970:
15: 38432: loss: 0.1549588416:
15: 41632: loss: 0.1545837876:
15: 44832: loss: 0.1541306714:
15: 48032: loss: 0.1543564854:
15: 51232: loss: 0.1537130584:
15: 54432: loss: 0.1533991355:
15: 57632: loss: 0.1534349078:
15: 60832: loss: 0.1533167944:
15: 64032: loss: 0.1531449116:
15: 67232: loss: 0.1529490942:
15: 70432: loss: 0.1529220297:
15: 73632: loss: 0.1526976400:
15: 76832: loss: 0.1529630910:
15: 80032: loss: 0.1528955048:
15: 83232: loss: 0.1535893539:
15: 86432: loss: 0.1535566106:
15: 89632: loss: 0.1534350145:
15: 92832: loss: 0.1533335303:
15: 96032: loss: 0.1533174084:
15: 99232: loss: 0.1532232593:
15: 102432: loss: 0.1534367755:
15: 105632: loss: 0.1533333093:
Dev-Acc: 15: Accuracy: 0.8501051664: precision: 0.2007266122: recall: 0.5261010032: f1: 0.2905846443
Train-Acc: 15: Accuracy: 0.9439973235: precision: 0.8834798474: recall: 0.7003484321: f1: 0.7813267813
16: 3232: loss: 0.1540682824:
16: 6432: loss: 0.1487805341:
16: 9632: loss: 0.1520957979:
16: 12832: loss: 0.1519912688:
16: 16032: loss: 0.1502434382:
16: 19232: loss: 0.1506801906:
16: 22432: loss: 0.1489244662:
16: 25632: loss: 0.1500350104:
16: 28832: loss: 0.1504017899:
16: 32032: loss: 0.1503440404:
16: 35232: loss: 0.1497884700:
16: 38432: loss: 0.1495767002:
16: 41632: loss: 0.1489235804:
16: 44832: loss: 0.1499511506:
16: 48032: loss: 0.1504161065:
16: 51232: loss: 0.1503174957:
16: 54432: loss: 0.1504004717:
16: 57632: loss: 0.1500003386:
16: 60832: loss: 0.1497483722:
16: 64032: loss: 0.1498061587:
16: 67232: loss: 0.1501921310:
16: 70432: loss: 0.1498461792:
16: 73632: loss: 0.1497842749:
16: 76832: loss: 0.1493007641:
16: 80032: loss: 0.1497320075:
16: 83232: loss: 0.1496016078:
16: 86432: loss: 0.1497544872:
16: 89632: loss: 0.1494784184:
16: 92832: loss: 0.1494564152:
16: 96032: loss: 0.1492674863:
16: 99232: loss: 0.1494404022:
16: 102432: loss: 0.1493591151:
16: 105632: loss: 0.1498858022:
Dev-Acc: 16: Accuracy: 0.8384862542: precision: 0.1912821426: recall: 0.5476959701: f1: 0.2835387324
Train-Acc: 16: Accuracy: 0.9457723498: precision: 0.8737425743: recall: 0.7251988692: f1: 0.7925707717
17: 3232: loss: 0.1403936164:
17: 6432: loss: 0.1423258483:
17: 9632: loss: 0.1423449715:
17: 12832: loss: 0.1442564214:
17: 16032: loss: 0.1455887798:
17: 19232: loss: 0.1451713671:
17: 22432: loss: 0.1448641503:
17: 25632: loss: 0.1460920248:
17: 28832: loss: 0.1465558233:
17: 32032: loss: 0.1461229649:
17: 35232: loss: 0.1455602908:
17: 38432: loss: 0.1460309268:
17: 41632: loss: 0.1460870739:
17: 44832: loss: 0.1463132345:
17: 48032: loss: 0.1466312389:
17: 51232: loss: 0.1469986963:
17: 54432: loss: 0.1473751059:
17: 57632: loss: 0.1480284196:
17: 60832: loss: 0.1474659176:
17: 64032: loss: 0.1469851806:
17: 67232: loss: 0.1471819475:
17: 70432: loss: 0.1466764025:
17: 73632: loss: 0.1466362998:
17: 76832: loss: 0.1472098971:
17: 80032: loss: 0.1474597853:
17: 83232: loss: 0.1474156100:
17: 86432: loss: 0.1473801770:
17: 89632: loss: 0.1474583432:
17: 92832: loss: 0.1474668789:
17: 96032: loss: 0.1473620297:
17: 99232: loss: 0.1472997070:
17: 102432: loss: 0.1473140570:
17: 105632: loss: 0.1473967583:
Dev-Acc: 17: Accuracy: 0.8367995024: precision: 0.1893338037: recall: 0.5475259310: f1: 0.2813701503
Train-Acc: 17: Accuracy: 0.9464579225: precision: 0.8778607756: recall: 0.7262507396: f1: 0.7948911675
18: 3232: loss: 0.1396244085:
18: 6432: loss: 0.1490812089:
18: 9632: loss: 0.1419997968:
18: 12832: loss: 0.1442000757:
18: 16032: loss: 0.1455807195:
18: 19232: loss: 0.1454341749:
18: 22432: loss: 0.1430980222:
18: 25632: loss: 0.1434714049:
18: 28832: loss: 0.1429455200:
18: 32032: loss: 0.1436961802:
18: 35232: loss: 0.1439110761:
18: 38432: loss: 0.1431688512:
18: 41632: loss: 0.1428361754:
18: 44832: loss: 0.1425931088:
18: 48032: loss: 0.1421470749:
18: 51232: loss: 0.1428840170:
18: 54432: loss: 0.1425012559:
18: 57632: loss: 0.1430382692:
18: 60832: loss: 0.1431198762:
18: 64032: loss: 0.1435753591:
18: 67232: loss: 0.1429750537:
18: 70432: loss: 0.1424971791:
18: 73632: loss: 0.1430443950:
18: 76832: loss: 0.1431845110:
18: 80032: loss: 0.1433487245:
18: 83232: loss: 0.1435173313:
18: 86432: loss: 0.1441611275:
18: 89632: loss: 0.1444253756:
18: 92832: loss: 0.1446158806:
18: 96032: loss: 0.1449281961:
18: 99232: loss: 0.1447943786:
18: 102432: loss: 0.1446112618:
18: 105632: loss: 0.1445913532:
Dev-Acc: 18: Accuracy: 0.8356385827: precision: 0.1866494603: recall: 0.5410644448: f1: 0.2775524445
Train-Acc: 18: Accuracy: 0.9471904635: precision: 0.8822356881: recall: 0.7274340937: f1: 0.7973912730
19: 3232: loss: 0.1583198213:
19: 6432: loss: 0.1544083508:
19: 9632: loss: 0.1517434973:
19: 12832: loss: 0.1510389901:
19: 16032: loss: 0.1490996725:
19: 19232: loss: 0.1468504512:
19: 22432: loss: 0.1464170038:
19: 25632: loss: 0.1459038670:
19: 28832: loss: 0.1457418135:
19: 32032: loss: 0.1450045432:
19: 35232: loss: 0.1433413706:
19: 38432: loss: 0.1429323452:
19: 41632: loss: 0.1435478996:
19: 44832: loss: 0.1434074068:
19: 48032: loss: 0.1429776820:
19: 51232: loss: 0.1423469576:
19: 54432: loss: 0.1417046455:
19: 57632: loss: 0.1415696045:
19: 60832: loss: 0.1417982839:
19: 64032: loss: 0.1415264201:
19: 67232: loss: 0.1412394252:
19: 70432: loss: 0.1416333063:
19: 73632: loss: 0.1418975893:
19: 76832: loss: 0.1417333956:
19: 80032: loss: 0.1420112090:
19: 83232: loss: 0.1422602256:
19: 86432: loss: 0.1424575088:
19: 89632: loss: 0.1422965268:
19: 92832: loss: 0.1418072100:
19: 96032: loss: 0.1420001813:
19: 99232: loss: 0.1419941124:
19: 102432: loss: 0.1414628565:
19: 105632: loss: 0.1418452456:
Dev-Acc: 19: Accuracy: 0.8372558951: precision: 0.1869681642: recall: 0.5342628805: f1: 0.2769990302
Train-Acc: 19: Accuracy: 0.9475473762: precision: 0.8904120701: recall: 0.7216488068: f1: 0.7971967029
20: 3232: loss: 0.1413659977:
20: 6432: loss: 0.1416250277:
20: 9632: loss: 0.1435518651:
20: 12832: loss: 0.1409157905:
20: 16032: loss: 0.1403017092:
20: 19232: loss: 0.1428439491:
20: 22432: loss: 0.1430460872:
20: 25632: loss: 0.1424013135:
20: 28832: loss: 0.1425157195:
20: 32032: loss: 0.1421408568:
20: 35232: loss: 0.1409767564:
20: 38432: loss: 0.1409505789:
20: 41632: loss: 0.1416662807:
20: 44832: loss: 0.1411628308:
20: 48032: loss: 0.1405150736:
20: 51232: loss: 0.1404895415:
20: 54432: loss: 0.1405771468:
20: 57632: loss: 0.1401899737:
20: 60832: loss: 0.1405284964:
20: 64032: loss: 0.1401189874:
20: 67232: loss: 0.1404721525:
20: 70432: loss: 0.1401894049:
20: 73632: loss: 0.1401717883:
20: 76832: loss: 0.1405322254:
20: 80032: loss: 0.1401395377:
20: 83232: loss: 0.1400175411:
20: 86432: loss: 0.1395741445:
20: 89632: loss: 0.1394960789:
20: 92832: loss: 0.1397429713:
20: 96032: loss: 0.1396675192:
20: 99232: loss: 0.1397246004:
20: 102432: loss: 0.1394754324:
20: 105632: loss: 0.1392923838:
Dev-Acc: 20: Accuracy: 0.8311735988: precision: 0.1818857143: recall: 0.5412344839: f1: 0.2722723579
Train-Acc: 20: Accuracy: 0.9485146999: precision: 0.8891911353: recall: 0.7306554467: f1: 0.8021652833
