1: 3232: loss: 0.6910700315:
1: 6432: loss: 0.6868744823:
1: 9632: loss: 0.6830287196:
1: 12832: loss: 0.6788614470:
1: 16032: loss: 0.6743687278:
1: 19232: loss: 0.6701097767:
1: 22432: loss: 0.6661020786:
1: 25632: loss: 0.6615664387:
1: 28832: loss: 0.6574002038:
1: 32032: loss: 0.6527563255:
1: 35232: loss: 0.6483278499:
1: 38432: loss: 0.6441088675:
1: 41632: loss: 0.6399038557:
1: 44832: loss: 0.6351322594:
Dev-Acc: 1: Accuracy: 0.8405203223: precision: 0.1937131867: recall: 0.5480360483: f1: 0.2862471691
Train-Acc: 1: Accuracy: 0.8417592645: precision: 0.9696684693: recall: 0.5422391690: f1: 0.6955348484
2: 3232: loss: 0.5640625912:
2: 6432: loss: 0.5566599600:
2: 9632: loss: 0.5518291425:
2: 12832: loss: 0.5468508866:
2: 16032: loss: 0.5413335202:
2: 19232: loss: 0.5366419076:
2: 22432: loss: 0.5310501878:
2: 25632: loss: 0.5256468798:
2: 28832: loss: 0.5213258815:
2: 32032: loss: 0.5162161384:
2: 35232: loss: 0.5116607411:
2: 38432: loss: 0.5062730278:
2: 41632: loss: 0.5009469856:
2: 44832: loss: 0.4956059916:
Dev-Acc: 2: Accuracy: 0.7721860409: precision: 0.1511784649: recall: 0.6293147424: f1: 0.2437915816
Train-Acc: 2: Accuracy: 0.8743234277: precision: 0.9663385827: recall: 0.6454539478: f1: 0.7739545150
3: 3232: loss: 0.4210299045:
3: 6432: loss: 0.4166903213:
3: 9632: loss: 0.4123040927:
3: 12832: loss: 0.4086089955:
3: 16032: loss: 0.4042506608:
3: 19232: loss: 0.3994346831:
3: 22432: loss: 0.3951188460:
3: 25632: loss: 0.3902843338:
3: 28832: loss: 0.3860074550:
3: 32032: loss: 0.3813927793:
3: 35232: loss: 0.3774125124:
3: 38432: loss: 0.3737254667:
3: 41632: loss: 0.3700350279:
3: 44832: loss: 0.3659639325:
Dev-Acc: 3: Accuracy: 0.6868748665: precision: 0.1197014130: recall: 0.6871280394: f1: 0.2038849647
Train-Acc: 3: Accuracy: 0.9052001834: precision: 0.9667266958: recall: 0.7411084084: f1: 0.8390145877
4: 3232: loss: 0.3149709210:
4: 6432: loss: 0.3121246731:
4: 9632: loss: 0.3113597336:
4: 12832: loss: 0.3077718632:
4: 16032: loss: 0.3040224460:
4: 19232: loss: 0.2998805038:
4: 22432: loss: 0.2984267376:
4: 25632: loss: 0.2956051392:
4: 28832: loss: 0.2944667798:
4: 32032: loss: 0.2937542911:
4: 35232: loss: 0.2908104964:
4: 38432: loss: 0.2882682427:
4: 41632: loss: 0.2863037158:
4: 44832: loss: 0.2839305244:
Dev-Acc: 4: Accuracy: 0.5988748074: precision: 0.1017843969: recall: 0.7507226662: f1: 0.1792638610
Train-Acc: 4: Accuracy: 0.9243310690: precision: 0.9659216992: recall: 0.8012622444: f1: 0.8759208020
5: 3232: loss: 0.2562888086:
5: 6432: loss: 0.2543802828:
5: 9632: loss: 0.2540987386:
5: 12832: loss: 0.2512535848:
5: 16032: loss: 0.2478907397:
5: 19232: loss: 0.2465738819:
5: 22432: loss: 0.2464631032:
5: 25632: loss: 0.2446906783:
5: 28832: loss: 0.2428494828:
5: 32032: loss: 0.2419272499:
5: 35232: loss: 0.2404702336:
5: 38432: loss: 0.2395898114:
5: 41632: loss: 0.2377740098:
5: 44832: loss: 0.2361628427:
Dev-Acc: 5: Accuracy: 0.5498194098: precision: 0.0949245035: recall: 0.7867709573: f1: 0.1694096110
Train-Acc: 5: Accuracy: 0.9331623912: precision: 0.9649766766: recall: 0.8295970022: f1: 0.8921804299
6: 3232: loss: 0.2173227820:
6: 6432: loss: 0.2172428428:
6: 9632: loss: 0.2188188887:
6: 12832: loss: 0.2148296560:
6: 16032: loss: 0.2135377624:
6: 19232: loss: 0.2111635502:
6: 22432: loss: 0.2117605416:
6: 25632: loss: 0.2126088093:
6: 28832: loss: 0.2105677568:
6: 32032: loss: 0.2092980831:
6: 35232: loss: 0.2073090870:
6: 38432: loss: 0.2064519436:
6: 41632: loss: 0.2063150926:
6: 44832: loss: 0.2049858257:
Dev-Acc: 6: Accuracy: 0.5241109729: precision: 0.0914228013: recall: 0.8005441251: f1: 0.1641047091
Train-Acc: 6: Accuracy: 0.9388599992: precision: 0.9662887604: recall: 0.8460982184: f1: 0.9022082019
7: 3232: loss: 0.1931270049:
7: 6432: loss: 0.1927472553:
7: 9632: loss: 0.1916316857:
7: 12832: loss: 0.1905483276:
7: 16032: loss: 0.1911677772:
7: 19232: loss: 0.1915533458:
7: 22432: loss: 0.1899396824:
7: 25632: loss: 0.1888256384:
7: 28832: loss: 0.1887988065:
7: 32032: loss: 0.1880746178:
7: 35232: loss: 0.1872674993:
7: 38432: loss: 0.1860833833:
7: 41632: loss: 0.1852725690:
7: 44832: loss: 0.1840264819:
Dev-Acc: 7: Accuracy: 0.4932132065: precision: 0.0878549673: recall: 0.8190783880: f1: 0.1586888486
Train-Acc: 7: Accuracy: 0.9442289472: precision: 0.9614543865: recall: 0.8674643350: f1: 0.9120442371
8: 3232: loss: 0.1827670678:
8: 6432: loss: 0.1694151838:
8: 9632: loss: 0.1683850248:
8: 12832: loss: 0.1642062217:
8: 16032: loss: 0.1668902518:
8: 19232: loss: 0.1670743376:
8: 22432: loss: 0.1698495544:
8: 25632: loss: 0.1716255816:
8: 28832: loss: 0.1707805155:
8: 32032: loss: 0.1710003822:
8: 35232: loss: 0.1704628984:
8: 38432: loss: 0.1693191256:
8: 41632: loss: 0.1692918315:
8: 44832: loss: 0.1688821928:
Dev-Acc: 8: Accuracy: 0.4711164534: precision: 0.0855444852: recall: 0.8321713994: f1: 0.1551409868
Train-Acc: 8: Accuracy: 0.9486774802: precision: 0.9617509867: recall: 0.8810729078: f1: 0.9196459205
9: 3232: loss: 0.1702626478:
9: 6432: loss: 0.1665004646:
9: 9632: loss: 0.1648069475:
9: 12832: loss: 0.1632885526:
9: 16032: loss: 0.1619775865:
9: 19232: loss: 0.1596406141:
9: 22432: loss: 0.1609264545:
9: 25632: loss: 0.1607996679:
9: 28832: loss: 0.1616886029:
9: 32032: loss: 0.1603950596:
9: 35232: loss: 0.1594907641:
9: 38432: loss: 0.1589600540:
9: 41632: loss: 0.1581835369:
9: 44832: loss: 0.1575321938:
Dev-Acc: 9: Accuracy: 0.4549333155: precision: 0.0841203900: recall: 0.8435640197: f1: 0.1529850746
Train-Acc: 9: Accuracy: 0.9520522356: precision: 0.9626945214: recall: 0.8906712248: f1: 0.9252834312
10: 3232: loss: 0.1529733687:
10: 6432: loss: 0.1558272964:
10: 9632: loss: 0.1513150342:
10: 12832: loss: 0.1495218172:
10: 16032: loss: 0.1478805741:
10: 19232: loss: 0.1485295316:
10: 22432: loss: 0.1485373404:
10: 25632: loss: 0.1484562569:
10: 28832: loss: 0.1476841938:
10: 32032: loss: 0.1489509021:
10: 35232: loss: 0.1481811049:
10: 38432: loss: 0.1477612610:
10: 41632: loss: 0.1473946413:
10: 44832: loss: 0.1476638037:
Dev-Acc: 10: Accuracy: 0.4399210215: precision: 0.0829814607: recall: 0.8554667574: f1: 0.1512877956
Train-Acc: 10: Accuracy: 0.9545065761: precision: 0.9633810767: recall: 0.8976398659: f1: 0.9293493057
11: 3232: loss: 0.1502892057:
11: 6432: loss: 0.1495297936:
11: 9632: loss: 0.1443285374:
11: 12832: loss: 0.1453528836:
11: 16032: loss: 0.1442112517:
11: 19232: loss: 0.1441842408:
11: 22432: loss: 0.1417325218:
11: 25632: loss: 0.1405923420:
11: 28832: loss: 0.1422625364:
11: 32032: loss: 0.1410430247:
11: 35232: loss: 0.1413384784:
11: 38432: loss: 0.1419961186:
11: 41632: loss: 0.1413199494:
11: 44832: loss: 0.1408777988:
Dev-Acc: 11: Accuracy: 0.4267542362: precision: 0.0822828624: recall: 0.8690698861: f1: 0.1503323725
Train-Acc: 11: Accuracy: 0.9563912153: precision: 0.9634719204: recall: 0.9034251528: f1: 0.9324828663
12: 3232: loss: 0.1421782624:
12: 6432: loss: 0.1349161287:
12: 9632: loss: 0.1376899168:
12: 12832: loss: 0.1352047700:
12: 16032: loss: 0.1338168976:
12: 19232: loss: 0.1334289098:
12: 22432: loss: 0.1328054035:
12: 25632: loss: 0.1339609619:
12: 28832: loss: 0.1339484800:
12: 32032: loss: 0.1339423852:
12: 35232: loss: 0.1341305474:
12: 38432: loss: 0.1347344142:
12: 41632: loss: 0.1349913120:
12: 44832: loss: 0.1346991513:
Dev-Acc: 12: Accuracy: 0.4129723012: precision: 0.0812743619: recall: 0.8792722326: f1: 0.1487950507
Train-Acc: 12: Accuracy: 0.9581224322: precision: 0.9632854953: recall: 0.9090132141: f1: 0.9353627600
13: 3232: loss: 0.1369865114:
13: 6432: loss: 0.1376243604:
13: 9632: loss: 0.1353857505:
13: 12832: loss: 0.1313901113:
13: 16032: loss: 0.1303947489:
13: 19232: loss: 0.1278024261:
13: 22432: loss: 0.1280818082:
13: 25632: loss: 0.1272241212:
13: 28832: loss: 0.1278242433:
13: 32032: loss: 0.1277684586:
13: 35232: loss: 0.1278911644:
13: 38432: loss: 0.1285480005:
13: 41632: loss: 0.1282493185:
13: 44832: loss: 0.1288223089:
Dev-Acc: 13: Accuracy: 0.4072174132: precision: 0.0806315986: recall: 0.8804625064: f1: 0.1477339192
Train-Acc: 13: Accuracy: 0.9591523409: precision: 0.9641143334: recall: 0.9113799224: f1: 0.9370057452
14: 3232: loss: 0.1158467391:
14: 6432: loss: 0.1264811322:
14: 9632: loss: 0.1328085114:
14: 12832: loss: 0.1295561475:
14: 16032: loss: 0.1302732291:
14: 19232: loss: 0.1298369833:
14: 22432: loss: 0.1268148656:
14: 25632: loss: 0.1258689576:
14: 28832: loss: 0.1251916061:
14: 32032: loss: 0.1256622340:
14: 35232: loss: 0.1265463454:
14: 38432: loss: 0.1263126902:
14: 41632: loss: 0.1255712063:
14: 44832: loss: 0.1251835928:
Dev-Acc: 14: Accuracy: 0.4005695283: precision: 0.0801872267: recall: 0.8855636796: f1: 0.1470584083
Train-Acc: 14: Accuracy: 0.9607740045: precision: 0.9653283406: recall: 0.9151929525: f1: 0.9395923326
15: 3232: loss: 0.1228835834:
15: 6432: loss: 0.1268437011:
15: 9632: loss: 0.1235658384:
15: 12832: loss: 0.1236709616:
15: 16032: loss: 0.1219347862:
15: 19232: loss: 0.1218394510:
15: 22432: loss: 0.1222178324:
15: 25632: loss: 0.1221462908:
15: 28832: loss: 0.1214831120:
15: 32032: loss: 0.1221834685:
15: 35232: loss: 0.1223262645:
15: 38432: loss: 0.1222082497:
15: 41632: loss: 0.1213449233:
15: 44832: loss: 0.1208986297:
Dev-Acc: 15: Accuracy: 0.3889704645: precision: 0.0792276662: recall: 0.8916850876: f1: 0.1455251838
Train-Acc: 15: Accuracy: 0.9621107578: precision: 0.9646401985: recall: 0.9200578529: f1: 0.9418217302
16: 3232: loss: 0.1070575324:
16: 6432: loss: 0.1153965513:
16: 9632: loss: 0.1149309528:
16: 12832: loss: 0.1151009776:
16: 16032: loss: 0.1157125215:
16: 19232: loss: 0.1159379078:
16: 22432: loss: 0.1164518834:
16: 25632: loss: 0.1163096961:
16: 28832: loss: 0.1173364896:
16: 32032: loss: 0.1167245379:
16: 35232: loss: 0.1169553918:
16: 38432: loss: 0.1168503813:
16: 41632: loss: 0.1172098467:
16: 44832: loss: 0.1171014905:
Dev-Acc: 16: Accuracy: 0.3831461370: precision: 0.0786952486: recall: 0.8938955960: f1: 0.1446555591
Train-Acc: 16: Accuracy: 0.9630968571: precision: 0.9654531691: recall: 0.9222930774: f1: 0.9433797324
17: 3232: loss: 0.1082292750:
17: 6432: loss: 0.1149638209:
17: 9632: loss: 0.1131531348:
17: 12832: loss: 0.1131115862:
17: 16032: loss: 0.1119936585:
17: 19232: loss: 0.1124898133:
17: 22432: loss: 0.1140252949:
17: 25632: loss: 0.1136976835:
17: 28832: loss: 0.1125627005:
17: 32032: loss: 0.1125273055:
17: 35232: loss: 0.1119021799:
17: 38432: loss: 0.1123216158:
17: 41632: loss: 0.1127947866:
17: 44832: loss: 0.1121360666:
Dev-Acc: 17: Accuracy: 0.3792566359: precision: 0.0782863605: recall: 0.8945757524: f1: 0.1439732906
Train-Acc: 17: Accuracy: 0.9638638496: precision: 0.9674617400: recall: 0.9226217869: f1: 0.9445098765
18: 3232: loss: 0.1080944659:
18: 6432: loss: 0.1054011014:
18: 9632: loss: 0.1052752812:
18: 12832: loss: 0.1067865493:
18: 16032: loss: 0.1080970097:
18: 19232: loss: 0.1101143460:
18: 22432: loss: 0.1084959981:
18: 25632: loss: 0.1072216535:
18: 28832: loss: 0.1075578662:
18: 32032: loss: 0.1077413897:
18: 35232: loss: 0.1083221132:
18: 38432: loss: 0.1098572619:
18: 41632: loss: 0.1099613575:
18: 44832: loss: 0.1094249908:
Dev-Acc: 18: Accuracy: 0.3695130050: precision: 0.0775801442: recall: 0.9003570821: f1: 0.1428513617
Train-Acc: 18: Accuracy: 0.9643678665: precision: 0.9655905134: recall: 0.9261061074: f1: 0.9454362416
19: 3232: loss: 0.1002591338:
19: 6432: loss: 0.1026369767:
19: 9632: loss: 0.1053257038:
19: 12832: loss: 0.1055649951:
19: 16032: loss: 0.1062980326:
19: 19232: loss: 0.1074278688:
19: 22432: loss: 0.1078751549:
19: 25632: loss: 0.1083293878:
19: 28832: loss: 0.1097921141:
19: 32032: loss: 0.1096387009:
19: 35232: loss: 0.1083790328:
19: 38432: loss: 0.1079116653:
19: 41632: loss: 0.1073957207:
19: 44832: loss: 0.1075073187:
Dev-Acc: 19: Accuracy: 0.3737299442: precision: 0.0777674501: recall: 0.8962761435: f1: 0.1431170242
Train-Acc: 19: Accuracy: 0.9651348591: precision: 0.9702389173: recall: 0.9237393991: f1: 0.9464183478
20: 3232: loss: 0.1212660644:
20: 6432: loss: 0.1208011114:
20: 9632: loss: 0.1169670828:
20: 12832: loss: 0.1137207378:
20: 16032: loss: 0.1124680908:
20: 19232: loss: 0.1116341949:
20: 22432: loss: 0.1098327590:
20: 25632: loss: 0.1086294939:
20: 28832: loss: 0.1089958839:
20: 32032: loss: 0.1080836232:
20: 35232: loss: 0.1073261417:
20: 38432: loss: 0.1057593273:
20: 41632: loss: 0.1048730570:
20: 44832: loss: 0.1044018656:
Dev-Acc: 20: Accuracy: 0.3653357625: precision: 0.0771661110: recall: 0.9012072777: f1: 0.1421597554
Train-Acc: 20: Accuracy: 0.9659895301: precision: 0.9698658411: recall: 0.9267635264: f1: 0.9478249176
