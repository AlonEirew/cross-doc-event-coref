1: 6464: loss: 0.7063823676:
1: 12864: loss: 0.7065202567:
1: 19264: loss: 0.7061468814:
1: 25664: loss: 0.7058704497:
1: 32064: loss: 0.7055238624:
1: 38464: loss: 0.7055309180:
1: 44864: loss: 0.7054527378:
1: 51264: loss: 0.7054465774:
1: 57664: loss: 0.7053580643:
1: 64064: loss: 0.7053540854:
1: 70464: loss: 0.7053128753:
1: 76864: loss: 0.7050834258:
1: 83264: loss: 0.7050336668:
1: 89664: loss: 0.7048922691:
Dev-Acc: 1: Accuracy: 0.2813442647: precision: 0.0637226622: recall: 0.8263900697: f1: 0.1183215864
Train-Acc: 1: Accuracy: 0.3637279868: precision: 0.1853369161: recall: 0.8297942279: f1: 0.3029982956
2: 6464: loss: 0.7041548127:
2: 12864: loss: 0.7037975559:
2: 19264: loss: 0.7035091652:
2: 25664: loss: 0.7032651867:
2: 32064: loss: 0.7033645332:
2: 38464: loss: 0.7032030010:
2: 44864: loss: 0.7030875358:
2: 51264: loss: 0.7029582268:
2: 57664: loss: 0.7028123810:
2: 64064: loss: 0.7027278822:
2: 70464: loss: 0.7026246875:
2: 76864: loss: 0.7025394522:
2: 83264: loss: 0.7024706244:
2: 89664: loss: 0.7024249592:
Dev-Acc: 2: Accuracy: 0.3101087511: precision: 0.0644596204: recall: 0.8008842034: f1: 0.1193160228
Train-Acc: 2: Accuracy: 0.3889181018: precision: 0.1881439336: recall: 0.8043521136: f1: 0.3049563191
3: 6464: loss: 0.7007042104:
3: 12864: loss: 0.7011998060:
3: 19264: loss: 0.7008985523:
3: 25664: loss: 0.7007755376:
3: 32064: loss: 0.7006076802:
3: 38464: loss: 0.7004586273:
3: 44864: loss: 0.7005027443:
3: 51264: loss: 0.7004503618:
3: 57664: loss: 0.7004652750:
3: 64064: loss: 0.7004198667:
3: 70464: loss: 0.7003073909:
3: 76864: loss: 0.7002092141:
3: 83264: loss: 0.7000157979:
3: 89664: loss: 0.6998961626:
Dev-Acc: 3: Accuracy: 0.3422071040: precision: 0.0656169111: recall: 0.7758884543: f1: 0.1210007823
Train-Acc: 3: Accuracy: 0.4168912768: precision: 0.1916467897: recall: 0.7764775491: f1: 0.3074180115
4: 6464: loss: 0.6992731833:
4: 12864: loss: 0.6995478234:
4: 19264: loss: 0.6990878214:
4: 25664: loss: 0.6989589603:
4: 32064: loss: 0.6986905440:
4: 38464: loss: 0.6986926312:
4: 44864: loss: 0.6986367704:
4: 51264: loss: 0.6983875643:
4: 57664: loss: 0.6982899459:
4: 64064: loss: 0.6982045680:
4: 70464: loss: 0.6981286487:
4: 76864: loss: 0.6980504462:
4: 83264: loss: 0.6979074739:
4: 89664: loss: 0.6977755639:
Dev-Acc: 4: Accuracy: 0.3772821128: precision: 0.0668210135: recall: 0.7459615712: f1: 0.1226549613
Train-Acc: 4: Accuracy: 0.4452041388: precision: 0.1956734652: recall: 0.7486687266: f1: 0.3102574581
5: 6464: loss: 0.6964771479:
5: 12864: loss: 0.6963914803:
5: 19264: loss: 0.6963680824:
5: 25664: loss: 0.6964657910:
5: 32064: loss: 0.6963952863:
5: 38464: loss: 0.6964387348:
5: 44864: loss: 0.6961776014:
5: 51264: loss: 0.6960780256:
5: 57664: loss: 0.6959494086:
5: 64064: loss: 0.6958820037:
5: 70464: loss: 0.6957162236:
5: 76864: loss: 0.6956312248:
5: 83264: loss: 0.6955260855:
5: 89664: loss: 0.6954270858:
Dev-Acc: 5: Accuracy: 0.4130913615: precision: 0.0673748498: recall: 0.7053222241: f1: 0.1230002817
Train-Acc: 5: Accuracy: 0.4758398533: precision: 0.1996280680: recall: 0.7127736506: f1: 0.3119012687
6: 6464: loss: 0.6935864109:
6: 12864: loss: 0.6932818347:
6: 19264: loss: 0.6932693074:
6: 25664: loss: 0.6932533994:
6: 32064: loss: 0.6933537172:
6: 38464: loss: 0.6932153674:
6: 44864: loss: 0.6931849561:
6: 51264: loss: 0.6930778457:
6: 57664: loss: 0.6930883422:
6: 64064: loss: 0.6929862760:
6: 70464: loss: 0.6930052267:
6: 76864: loss: 0.6928750415:
6: 83264: loss: 0.6928371586:
6: 89664: loss: 0.6928027509:
Dev-Acc: 6: Accuracy: 0.4499325156: precision: 0.0674673137: recall: 0.6572011563: f1: 0.1223720871
Train-Acc: 6: Accuracy: 0.5093463063: precision: 0.2049885264: recall: 0.6753665111: f1: 0.3145148945
7: 6464: loss: 0.6910485047:
7: 12864: loss: 0.6911565992:
7: 19264: loss: 0.6914372579:
7: 25664: loss: 0.6913873985:
7: 32064: loss: 0.6911056789:
7: 38464: loss: 0.6911748031:
7: 44864: loss: 0.6911618123:
7: 51264: loss: 0.6910182122:
7: 57664: loss: 0.6910093407:
7: 64064: loss: 0.6910605440:
7: 70464: loss: 0.6909359516:
7: 76864: loss: 0.6908644673:
7: 83264: loss: 0.6908027202:
7: 89664: loss: 0.6906416964:
Dev-Acc: 7: Accuracy: 0.4859898388: precision: 0.0671153592: recall: 0.6053392280: f1: 0.1208336162
Train-Acc: 7: Accuracy: 0.5417570472: precision: 0.2096453901: recall: 0.6315824075: f1: 0.3147978242
8: 6464: loss: 0.6907318449:
8: 12864: loss: 0.6899578080:
8: 19264: loss: 0.6893976897:
8: 25664: loss: 0.6893681802:
8: 32064: loss: 0.6892485366:
8: 38464: loss: 0.6890745150:
8: 44864: loss: 0.6889078973:
8: 51264: loss: 0.6888664107:
8: 57664: loss: 0.6888032813:
8: 64064: loss: 0.6887277224:
8: 70464: loss: 0.6886324672:
8: 76864: loss: 0.6885620847:
8: 83264: loss: 0.6885189541:
8: 89664: loss: 0.6884549226:
Dev-Acc: 8: Accuracy: 0.5231286883: precision: 0.0663349235: recall: 0.5485461656: f1: 0.1183570891
Train-Acc: 8: Accuracy: 0.5737624168: precision: 0.2147982279: recall: 0.5864834659: f1: 0.3144352607
9: 6464: loss: 0.6871279228:
9: 12864: loss: 0.6874967942:
9: 19264: loss: 0.6873463009:
9: 25664: loss: 0.6873572777:
9: 32064: loss: 0.6872480303:
9: 38464: loss: 0.6870944667:
9: 44864: loss: 0.6869946602:
9: 51264: loss: 0.6868976740:
9: 57664: loss: 0.6867492584:
9: 64064: loss: 0.6866241238:
9: 70464: loss: 0.6865462211:
9: 76864: loss: 0.6864268599:
9: 83264: loss: 0.6863440711:
9: 89664: loss: 0.6862799152:
Dev-Acc: 9: Accuracy: 0.5612795353: precision: 0.0657468452: recall: 0.4934534943: f1: 0.1160335866
Train-Acc: 9: Accuracy: 0.6062169671: precision: 0.2222936763: recall: 0.5453947801: f1: 0.3158515924
10: 6464: loss: 0.6856358904:
10: 12864: loss: 0.6855285463:
10: 19264: loss: 0.6850774312:
10: 25664: loss: 0.6848820256:
10: 32064: loss: 0.6848444047:
10: 38464: loss: 0.6847955621:
10: 44864: loss: 0.6847517811:
10: 51264: loss: 0.6845480780:
10: 57664: loss: 0.6844656909:
10: 64064: loss: 0.6843377587:
10: 70464: loss: 0.6842615776:
10: 76864: loss: 0.6842556532:
10: 83264: loss: 0.6841585955:
10: 89664: loss: 0.6839924168:
Dev-Acc: 10: Accuracy: 0.5986465812: precision: 0.0672275220: recall: 0.4565550077: f1: 0.1171977302
Train-Acc: 10: Accuracy: 0.6373348236: precision: 0.2320390676: recall: 0.5091709947: f1: 0.3187964354
11: 6464: loss: 0.6826868188:
11: 12864: loss: 0.6826256648:
11: 19264: loss: 0.6826334288:
11: 25664: loss: 0.6827584493:
11: 32064: loss: 0.6824590595:
11: 38464: loss: 0.6822542681:
11: 44864: loss: 0.6821097227:
11: 51264: loss: 0.6820388619:
11: 57664: loss: 0.6819767198:
11: 64064: loss: 0.6819247768:
11: 70464: loss: 0.6817999512:
11: 76864: loss: 0.6817659407:
11: 83264: loss: 0.6817162949:
11: 89664: loss: 0.6816038769:
Dev-Acc: 11: Accuracy: 0.6326202750: precision: 0.0680185303: recall: 0.4169358953: f1: 0.1169568328
Train-Acc: 11: Accuracy: 0.6665461063: precision: 0.2435646900: recall: 0.4752481757: f1: 0.3220690116
12: 6464: loss: 0.6812764198:
12: 12864: loss: 0.6810793182:
12: 19264: loss: 0.6804719841:
12: 25664: loss: 0.6804709876:
12: 32064: loss: 0.6802558939:
12: 38464: loss: 0.6800680129:
12: 44864: loss: 0.6799065768:
12: 51264: loss: 0.6798269710:
12: 57664: loss: 0.6797345603:
12: 64064: loss: 0.6796574333:
12: 70464: loss: 0.6795638964:
12: 76864: loss: 0.6795108005:
12: 83264: loss: 0.6794861767:
12: 89664: loss: 0.6794285473:
Dev-Acc: 12: Accuracy: 0.6639049649: precision: 0.0695922258: recall: 0.3847985037: f1: 0.1178676528
Train-Acc: 12: Accuracy: 0.6918896437: precision: 0.2544790597: recall: 0.4398132930: f1: 0.3224096386
13: 6464: loss: 0.6781340730:
13: 12864: loss: 0.6783956361:
13: 19264: loss: 0.6782563307:
13: 25664: loss: 0.6782942981:
13: 32064: loss: 0.6783300877:
13: 38464: loss: 0.6783316852:
13: 44864: loss: 0.6780986467:
13: 51264: loss: 0.6779331000:
13: 57664: loss: 0.6777483579:
13: 64064: loss: 0.6776575371:
13: 70464: loss: 0.6775209462:
13: 76864: loss: 0.6774614621:
13: 83264: loss: 0.6774007534:
13: 89664: loss: 0.6773003471:
Dev-Acc: 13: Accuracy: 0.6963704228: precision: 0.0718145916: recall: 0.3524910729: f1: 0.1193196535
Train-Acc: 13: Accuracy: 0.7148445249: precision: 0.2676407391: recall: 0.4094405365: f1: 0.3236922117
14: 6464: loss: 0.6757788771:
14: 12864: loss: 0.6756629056:
14: 19264: loss: 0.6756043114:
14: 25664: loss: 0.6756769350:
14: 32064: loss: 0.6756662537:
14: 38464: loss: 0.6757259009:
14: 44864: loss: 0.6756439106:
14: 51264: loss: 0.6755695648:
14: 57664: loss: 0.6754855497:
14: 64064: loss: 0.6755023235:
14: 70464: loss: 0.6753747840:
14: 76864: loss: 0.6753234975:
14: 83264: loss: 0.6752141689:
14: 89664: loss: 0.6751588921:
Dev-Acc: 14: Accuracy: 0.7240831852: precision: 0.0741834000: recall: 0.3247746982: f1: 0.1207790565
Train-Acc: 14: Accuracy: 0.7373611331: precision: 0.2850974042: recall: 0.3819604234: f1: 0.3264962068
15: 6464: loss: 0.6736228532:
15: 12864: loss: 0.6735008425:
15: 19264: loss: 0.6736372878:
15: 25664: loss: 0.6736396928:
15: 32064: loss: 0.6734953130:
15: 38464: loss: 0.6735233560:
15: 44864: loss: 0.6734457997:
15: 51264: loss: 0.6733028372:
15: 57664: loss: 0.6732077590:
15: 64064: loss: 0.6732061133:
15: 70464: loss: 0.6731435318:
15: 76864: loss: 0.6730898795:
15: 83264: loss: 0.6730099061:
15: 89664: loss: 0.6729177045:
Dev-Acc: 15: Accuracy: 0.7485612631: precision: 0.0759239888: recall: 0.2962081279: f1: 0.1208673027
Train-Acc: 15: Accuracy: 0.7562618852: precision: 0.3034756370: recall: 0.3570442443: f1: 0.3280877156
16: 6464: loss: 0.6718618894:
16: 12864: loss: 0.6720603827:
16: 19264: loss: 0.6720280999:
16: 25664: loss: 0.6719446096:
16: 32064: loss: 0.6719335730:
16: 38464: loss: 0.6717133528:
16: 44864: loss: 0.6716069640:
16: 51264: loss: 0.6714907517:
16: 57664: loss: 0.6712593655:
16: 64064: loss: 0.6711037616:
16: 70464: loss: 0.6709468095:
16: 76864: loss: 0.6709137353:
16: 83264: loss: 0.6708501518:
16: 89664: loss: 0.6708164146:
Dev-Acc: 16: Accuracy: 0.7716799974: precision: 0.0794874313: recall: 0.2752933175: f1: 0.1233570803
Train-Acc: 16: Accuracy: 0.7714483142: precision: 0.3196449100: recall: 0.3290381960: f1: 0.3242735430
17: 6464: loss: 0.6699123615:
17: 12864: loss: 0.6693269661:
17: 19264: loss: 0.6692556357:
17: 25664: loss: 0.6691733404:
17: 32064: loss: 0.6692075788:
17: 38464: loss: 0.6690467163:
17: 44864: loss: 0.6690340628:
17: 51264: loss: 0.6691258959:
17: 57664: loss: 0.6690505301:
17: 64064: loss: 0.6688705984:
17: 70464: loss: 0.6688252523:
17: 76864: loss: 0.6687316169:
17: 83264: loss: 0.6686802649:
17: 89664: loss: 0.6685356932:
Dev-Acc: 17: Accuracy: 0.7909191847: precision: 0.0812613705: recall: 0.2506376467: f1: 0.1227310575
Train-Acc: 17: Accuracy: 0.7849911451: precision: 0.3394468705: recall: 0.3066202091: f1: 0.3221995786
18: 6464: loss: 0.6669085890:
18: 12864: loss: 0.6665968305:
18: 19264: loss: 0.6667105414:
18: 25664: loss: 0.6667054683:
18: 32064: loss: 0.6664776334:
18: 38464: loss: 0.6665507338:
18: 44864: loss: 0.6665019677:
18: 51264: loss: 0.6665090270:
18: 57664: loss: 0.6665526344:
18: 64064: loss: 0.6665019206:
18: 70464: loss: 0.6664742597:
18: 76864: loss: 0.6664984723:
18: 83264: loss: 0.6664652745:
18: 89664: loss: 0.6663394513:
Dev-Acc: 18: Accuracy: 0.8080250621: precision: 0.0832972337: recall: 0.2288726407: f1: 0.1221415608
Train-Acc: 18: Accuracy: 0.7958385348: precision: 0.3584546658: recall: 0.2848596410: f1: 0.3174475256
19: 6464: loss: 0.6651987839:
19: 12864: loss: 0.6649710646:
19: 19264: loss: 0.6650775033:
19: 25664: loss: 0.6650432996:
19: 32064: loss: 0.6647555611:
19: 38464: loss: 0.6648002236:
19: 44864: loss: 0.6647061505:
19: 51264: loss: 0.6646240482:
19: 57664: loss: 0.6645031387:
19: 64064: loss: 0.6644236203:
19: 70464: loss: 0.6643272732:
19: 76864: loss: 0.6643366011:
19: 83264: loss: 0.6642555516:
19: 89664: loss: 0.6642151265:
Dev-Acc: 19: Accuracy: 0.8244364262: precision: 0.0868713716: recall: 0.2111885734: f1: 0.1231043711
Train-Acc: 19: Accuracy: 0.8054916263: precision: 0.3802431897: recall: 0.2652028138: f1: 0.3124709527
20: 6464: loss: 0.6626625729:
20: 12864: loss: 0.6630170116:
20: 19264: loss: 0.6629078992:
20: 25664: loss: 0.6628023107:
20: 32064: loss: 0.6627487248:
20: 38464: loss: 0.6625670344:
20: 44864: loss: 0.6625156271:
20: 51264: loss: 0.6624745743:
20: 57664: loss: 0.6624545543:
20: 64064: loss: 0.6624556041:
20: 70464: loss: 0.6624331617:
20: 76864: loss: 0.6623404589:
20: 83264: loss: 0.6622462442:
20: 89664: loss: 0.6621789845:
Dev-Acc: 20: Accuracy: 0.8392998576: precision: 0.0895344210: recall: 0.1912939976: f1: 0.1219776645
Train-Acc: 20: Accuracy: 0.8135888577: precision: 0.4028885536: recall: 0.2457432121: f1: 0.3052799216
