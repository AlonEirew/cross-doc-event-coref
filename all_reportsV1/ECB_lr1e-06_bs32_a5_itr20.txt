1: 3232: loss: 0.6992058444:
1: 6432: loss: 0.6915017542:
1: 9632: loss: 0.6847373285:
1: 12832: loss: 0.6783000991:
1: 16032: loss: 0.6718814509:
1: 19232: loss: 0.6655601977:
1: 22432: loss: 0.6592906528:
1: 25632: loss: 0.6531549409:
1: 28832: loss: 0.6469246857:
1: 32032: loss: 0.6407429550:
1: 35232: loss: 0.6350338838:
1: 38432: loss: 0.6291892455:
1: 41632: loss: 0.6234046387:
1: 44832: loss: 0.6178816839:
1: 48032: loss: 0.6119273889:
1: 51232: loss: 0.6062576774:
1: 54432: loss: 0.6008974339:
1: 57632: loss: 0.5953120849:
1: 60832: loss: 0.5900940133:
1: 64032: loss: 0.5846728652:
1: 67232: loss: 0.5794704906:
1: 70432: loss: 0.5744248042:
1: 73632: loss: 0.5698708439:
1: 76832: loss: 0.5650898299:
1: 80032: loss: 0.5602953135:
1: 83232: loss: 0.5553530621:
1: 86432: loss: 0.5507330600:
1: 89632: loss: 0.5458478846:
Dev-Acc: 1: Accuracy: 0.9416871667: precision: 0.8333333333: recall: 0.0008501955: f1: 0.0016986581
Train-Acc: 1: Accuracy: 0.8368833661: precision: 1.0000000000: recall: 0.0213003747: f1: 0.0417122626
2: 3232: loss: 0.4212847728:
2: 6432: loss: 0.4132142489:
2: 9632: loss: 0.4116860984:
2: 12832: loss: 0.4062069056:
2: 16032: loss: 0.4020149110:
2: 19232: loss: 0.3978774793:
2: 22432: loss: 0.3951679739:
2: 25632: loss: 0.3928842731:
2: 28832: loss: 0.3905650349:
2: 32032: loss: 0.3872479244:
2: 35232: loss: 0.3839523901:
2: 38432: loss: 0.3817701066:
2: 41632: loss: 0.3796798051:
2: 44832: loss: 0.3779825586:
2: 48032: loss: 0.3766866186:
2: 51232: loss: 0.3757160357:
2: 54432: loss: 0.3732036604:
2: 57632: loss: 0.3709364720:
2: 60832: loss: 0.3686897171:
2: 64032: loss: 0.3668053975:
2: 67232: loss: 0.3650347929:
2: 70432: loss: 0.3632914015:
2: 73632: loss: 0.3611498573:
2: 76832: loss: 0.3591668894:
2: 80032: loss: 0.3578083607:
2: 83232: loss: 0.3559294464:
2: 86432: loss: 0.3538319878:
2: 89632: loss: 0.3528654982:
Dev-Acc: 2: Accuracy: 0.9190744162: precision: 0.3467600701: recall: 0.4376806666: f1: 0.3869512928
Train-Acc: 2: Accuracy: 0.8865295053: precision: 0.8401289057: recall: 0.3941884163: f1: 0.5366028280
3: 3232: loss: 0.2956578395:
3: 6432: loss: 0.2962044494:
3: 9632: loss: 0.2962242510:
3: 12832: loss: 0.2953397680:
3: 16032: loss: 0.2964503734:
3: 19232: loss: 0.2957490063:
3: 22432: loss: 0.2942520502:
3: 25632: loss: 0.2936703385:
3: 28832: loss: 0.2938988757:
3: 32032: loss: 0.2928751959:
3: 35232: loss: 0.2922664173:
3: 38432: loss: 0.2911934325:
3: 41632: loss: 0.2905468992:
3: 44832: loss: 0.2894323422:
3: 48032: loss: 0.2885603840:
3: 51232: loss: 0.2875533731:
3: 54432: loss: 0.2867550382:
3: 57632: loss: 0.2852524786:
3: 60832: loss: 0.2838109610:
3: 64032: loss: 0.2830185372:
3: 67232: loss: 0.2823357189:
3: 70432: loss: 0.2813945866:
3: 73632: loss: 0.2811410025:
3: 76832: loss: 0.2801889255:
3: 80032: loss: 0.2791917693:
3: 83232: loss: 0.2779943809:
3: 86432: loss: 0.2773599501:
3: 89632: loss: 0.2763208412:
Dev-Acc: 3: Accuracy: 0.8966205120: precision: 0.2928610553: recall: 0.5454854617: f1: 0.3811107811
Train-Acc: 3: Accuracy: 0.9061315060: precision: 0.8438211550: recall: 0.5359936888: f1: 0.6555702971
4: 3232: loss: 0.2460160168:
4: 6432: loss: 0.2486548211:
4: 9632: loss: 0.2472796763:
4: 12832: loss: 0.2485651396:
4: 16032: loss: 0.2489077009:
4: 19232: loss: 0.2509070533:
4: 22432: loss: 0.2503201441:
4: 25632: loss: 0.2493589742:
4: 28832: loss: 0.2469969878:
4: 32032: loss: 0.2459961864:
4: 35232: loss: 0.2454726222:
4: 38432: loss: 0.2459926510:
4: 41632: loss: 0.2453689633:
4: 44832: loss: 0.2446750929:
4: 48032: loss: 0.2445867432:
4: 51232: loss: 0.2445369036:
4: 54432: loss: 0.2442578777:
4: 57632: loss: 0.2444073800:
4: 60832: loss: 0.2440946857:
4: 64032: loss: 0.2434377075:
4: 67232: loss: 0.2432232938:
4: 70432: loss: 0.2427471202:
4: 73632: loss: 0.2418692848:
4: 76832: loss: 0.2413764334:
4: 80032: loss: 0.2416041610:
4: 83232: loss: 0.2405683284:
4: 86432: loss: 0.2402471325:
4: 89632: loss: 0.2395802379:
Dev-Acc: 4: Accuracy: 0.8764783740: precision: 0.2544122046: recall: 0.5784730488: f1: 0.3533994702
Train-Acc: 4: Accuracy: 0.9133850336: precision: 0.8411468061: recall: 0.5921372691: f1: 0.6950113816
5: 3232: loss: 0.2266136882:
5: 6432: loss: 0.2260071407:
5: 9632: loss: 0.2238543698:
5: 12832: loss: 0.2239804552:
5: 16032: loss: 0.2233719741:
5: 19232: loss: 0.2231152676:
5: 22432: loss: 0.2232298173:
5: 25632: loss: 0.2222676373:
5: 28832: loss: 0.2213915934:
5: 32032: loss: 0.2215117922:
5: 35232: loss: 0.2206343799:
5: 38432: loss: 0.2212840888:
5: 41632: loss: 0.2210279727:
5: 44832: loss: 0.2204598963:
5: 48032: loss: 0.2197165089:
5: 51232: loss: 0.2190765044:
5: 54432: loss: 0.2189761920:
5: 57632: loss: 0.2187590062:
5: 60832: loss: 0.2186319473:
5: 64032: loss: 0.2182729495:
5: 67232: loss: 0.2184159001:
5: 70432: loss: 0.2182361377:
5: 73632: loss: 0.2178223147:
5: 76832: loss: 0.2178773759:
5: 80032: loss: 0.2180662080:
5: 83232: loss: 0.2178006800:
5: 86432: loss: 0.2175777758:
5: 89632: loss: 0.2178194960:
Dev-Acc: 5: Accuracy: 0.8646014929: precision: 0.2364401602: recall: 0.5922462166: f1: 0.3379584708
Train-Acc: 5: Accuracy: 0.9196962714: precision: 0.8453987730: recall: 0.6341463415: f1: 0.7246910334
6: 3232: loss: 0.2115460230:
6: 6432: loss: 0.2118166238:
6: 9632: loss: 0.2122192426:
6: 12832: loss: 0.2100209588:
6: 16032: loss: 0.2062819858:
6: 19232: loss: 0.2069845982:
6: 22432: loss: 0.2093797443:
6: 25632: loss: 0.2098071909:
6: 28832: loss: 0.2088322283:
6: 32032: loss: 0.2085319474:
6: 35232: loss: 0.2080076322:
6: 38432: loss: 0.2075058142:
6: 41632: loss: 0.2084622342:
6: 44832: loss: 0.2069053955:
6: 48032: loss: 0.2063514565:
6: 51232: loss: 0.2062537726:
6: 54432: loss: 0.2060823893:
6: 57632: loss: 0.2059969479:
6: 60832: loss: 0.2054102771:
6: 64032: loss: 0.2048023105:
6: 67232: loss: 0.2042610047:
6: 70432: loss: 0.2045706646:
6: 73632: loss: 0.2048210921:
6: 76832: loss: 0.2051489109:
6: 80032: loss: 0.2045173515:
6: 83232: loss: 0.2042686203:
6: 86432: loss: 0.2040507255:
6: 89632: loss: 0.2034815659:
Dev-Acc: 6: Accuracy: 0.8520598412: precision: 0.2204124605: recall: 0.6051691889: f1: 0.3231341928
Train-Acc: 6: Accuracy: 0.9262047410: precision: 0.8545256818: recall: 0.6715534810: f1: 0.7520706792
7: 3232: loss: 0.1978365918:
7: 6432: loss: 0.2040149572:
7: 9632: loss: 0.2067521981:
7: 12832: loss: 0.2043346392:
7: 16032: loss: 0.2004884554:
7: 19232: loss: 0.2014840307:
7: 22432: loss: 0.2004020291:
7: 25632: loss: 0.1990599145:
7: 28832: loss: 0.1986368985:
7: 32032: loss: 0.1973565153:
7: 35232: loss: 0.1974571484:
7: 38432: loss: 0.1965746864:
7: 41632: loss: 0.1949571602:
7: 44832: loss: 0.1945714407:
7: 48032: loss: 0.1941815436:
7: 51232: loss: 0.1941262371:
7: 54432: loss: 0.1934151540:
7: 57632: loss: 0.1923956548:
7: 60832: loss: 0.1918676962:
7: 64032: loss: 0.1925128921:
7: 67232: loss: 0.1924486921:
7: 70432: loss: 0.1927943594:
7: 73632: loss: 0.1927630727:
7: 76832: loss: 0.1928954590:
7: 80032: loss: 0.1931056429:
7: 83232: loss: 0.1929167091:
7: 86432: loss: 0.1926498954:
7: 89632: loss: 0.1926157318:
Dev-Acc: 7: Accuracy: 0.8408477306: precision: 0.2079236387: recall: 0.6148614181: f1: 0.3107597112
Train-Acc: 7: Accuracy: 0.9309271574: precision: 0.8647718896: recall: 0.6941029518: f1: 0.7700948213
8: 3232: loss: 0.1775029470:
8: 6432: loss: 0.1830318639:
8: 9632: loss: 0.1821419340:
8: 12832: loss: 0.1834319409:
8: 16032: loss: 0.1826055212:
8: 19232: loss: 0.1839295962:
8: 22432: loss: 0.1819005942:
8: 25632: loss: 0.1822279921:
8: 28832: loss: 0.1830531333:
8: 32032: loss: 0.1828097999:
8: 35232: loss: 0.1839861973:
8: 38432: loss: 0.1835787367:
8: 41632: loss: 0.1840800537:
8: 44832: loss: 0.1836661618:
8: 48032: loss: 0.1840341934:
8: 51232: loss: 0.1841727017:
8: 54432: loss: 0.1845308447:
8: 57632: loss: 0.1847032804:
8: 60832: loss: 0.1841255132:
8: 64032: loss: 0.1840284001:
8: 67232: loss: 0.1833811699:
8: 70432: loss: 0.1834884497:
8: 73632: loss: 0.1839902494:
8: 76832: loss: 0.1843595657:
8: 80032: loss: 0.1839775673:
8: 83232: loss: 0.1844465365:
8: 86432: loss: 0.1850018104:
8: 89632: loss: 0.1843455237:
Dev-Acc: 8: Accuracy: 0.8331084251: precision: 0.1988658261: recall: 0.6141812617: f1: 0.3004491765
Train-Acc: 8: Accuracy: 0.9352551699: precision: 0.8771488810: recall: 0.7111301032: f1: 0.7854627310
9: 3232: loss: 0.1956053100:
9: 6432: loss: 0.1836348106:
9: 9632: loss: 0.1789221338:
9: 12832: loss: 0.1784818464:
9: 16032: loss: 0.1813720019:
9: 19232: loss: 0.1811284597:
9: 22432: loss: 0.1823676057:
9: 25632: loss: 0.1820694404:
9: 28832: loss: 0.1817824732:
9: 32032: loss: 0.1812473666:
9: 35232: loss: 0.1802213650:
9: 38432: loss: 0.1802876655:
9: 41632: loss: 0.1795039863:
9: 44832: loss: 0.1794200117:
9: 48032: loss: 0.1791772598:
9: 51232: loss: 0.1788668759:
9: 54432: loss: 0.1782226995:
9: 57632: loss: 0.1775942917:
9: 60832: loss: 0.1770408476:
9: 64032: loss: 0.1773218777:
9: 67232: loss: 0.1769902427:
9: 70432: loss: 0.1775066348:
9: 73632: loss: 0.1776839049:
9: 76832: loss: 0.1773208087:
9: 80032: loss: 0.1770763876:
9: 83232: loss: 0.1771099879:
9: 86432: loss: 0.1769802962:
9: 89632: loss: 0.1766497475:
Dev-Acc: 9: Accuracy: 0.8213704228: precision: 0.1882201646: recall: 0.6221730998: f1: 0.2890091229
Train-Acc: 9: Accuracy: 0.9378300905: precision: 0.8768075859: recall: 0.7294720926: f1: 0.7963826886
10: 3232: loss: 0.1555597500:
10: 6432: loss: 0.1569290331:
10: 9632: loss: 0.1657371227:
10: 12832: loss: 0.1698459312:
10: 16032: loss: 0.1709362941:
10: 19232: loss: 0.1704645966:
10: 22432: loss: 0.1693210114:
10: 25632: loss: 0.1699805185:
10: 28832: loss: 0.1689363956:
10: 32032: loss: 0.1678695968:
10: 35232: loss: 0.1688214375:
10: 38432: loss: 0.1697796124:
10: 41632: loss: 0.1701243332:
10: 44832: loss: 0.1702778051:
10: 48032: loss: 0.1707014040:
10: 51232: loss: 0.1707631576:
10: 54432: loss: 0.1705993838:
10: 57632: loss: 0.1713012888:
10: 60832: loss: 0.1711675626:
10: 64032: loss: 0.1713062137:
10: 67232: loss: 0.1714453320:
10: 70432: loss: 0.1718559509:
10: 73632: loss: 0.1715930234:
10: 76832: loss: 0.1715619840:
10: 80032: loss: 0.1715098585:
10: 83232: loss: 0.1713395043:
10: 86432: loss: 0.1707548974:
10: 89632: loss: 0.1709593198:
Dev-Acc: 10: Accuracy: 0.8149805665: precision: 0.1800180469: recall: 0.6106104404: f1: 0.2780595455
Train-Acc: 10: Accuracy: 0.9404488206: precision: 0.8912277893: recall: 0.7320360266: f1: 0.8038260242
11: 3232: loss: 0.1657147954:
11: 6432: loss: 0.1620476650:
11: 9632: loss: 0.1619144579:
11: 12832: loss: 0.1643540743:
11: 16032: loss: 0.1665058304:
11: 19232: loss: 0.1663751126:
11: 22432: loss: 0.1670629173:
11: 25632: loss: 0.1678026769:
11: 28832: loss: 0.1683566964:
11: 32032: loss: 0.1683360193:
11: 35232: loss: 0.1675430895:
11: 38432: loss: 0.1685989910:
11: 41632: loss: 0.1683763826:
11: 44832: loss: 0.1692162109:
11: 48032: loss: 0.1685821188:
11: 51232: loss: 0.1677146651:
11: 54432: loss: 0.1671395202:
11: 57632: loss: 0.1671176368:
11: 60832: loss: 0.1670717252:
11: 64032: loss: 0.1663810671:
11: 67232: loss: 0.1672275179:
11: 70432: loss: 0.1674766683:
11: 73632: loss: 0.1673816386:
11: 76832: loss: 0.1672722849:
11: 80032: loss: 0.1673343808:
11: 83232: loss: 0.1668812993:
11: 86432: loss: 0.1666623380:
11: 89632: loss: 0.1661633731:
Dev-Acc: 11: Accuracy: 0.8089577556: precision: 0.1757117222: recall: 0.6160516919: f1: 0.2734339623
Train-Acc: 11: Accuracy: 0.9420594573: precision: 0.8914398422: recall: 0.7428176977: f1: 0.8103707954
12: 3232: loss: 0.1823016024:
12: 6432: loss: 0.1735161802:
12: 9632: loss: 0.1707679511:
12: 12832: loss: 0.1695004253:
12: 16032: loss: 0.1664798797:
12: 19232: loss: 0.1665517324:
12: 22432: loss: 0.1667153874:
12: 25632: loss: 0.1647773177:
12: 28832: loss: 0.1646575329:
12: 32032: loss: 0.1645616523:
12: 35232: loss: 0.1650368164:
12: 38432: loss: 0.1645476041:
12: 41632: loss: 0.1646802393:
12: 44832: loss: 0.1648604728:
12: 48032: loss: 0.1645541037:
12: 51232: loss: 0.1638836911:
12: 54432: loss: 0.1636555685:
12: 57632: loss: 0.1632141531:
12: 60832: loss: 0.1627485045:
12: 64032: loss: 0.1624362442:
12: 67232: loss: 0.1625931433:
12: 70432: loss: 0.1624629478:
12: 73632: loss: 0.1624697411:
12: 76832: loss: 0.1627860957:
12: 80032: loss: 0.1625683323:
12: 83232: loss: 0.1624362950:
12: 86432: loss: 0.1622834576:
12: 89632: loss: 0.1618827845:
Dev-Acc: 12: Accuracy: 0.8010993600: precision: 0.1698289124: recall: 0.6194524741: f1: 0.2665739792
Train-Acc: 12: Accuracy: 0.9433962107: precision: 0.8913738019: recall: 0.7520215633: f1: 0.8157894737
13: 3232: loss: 0.1489669634:
13: 6432: loss: 0.1472282098:
13: 9632: loss: 0.1483222872:
13: 12832: loss: 0.1542304184:
13: 16032: loss: 0.1528994909:
13: 19232: loss: 0.1536869071:
13: 22432: loss: 0.1545251298:
13: 25632: loss: 0.1557298677:
13: 28832: loss: 0.1568573081:
13: 32032: loss: 0.1573583093:
13: 35232: loss: 0.1577656102:
13: 38432: loss: 0.1588914790:
13: 41632: loss: 0.1592609612:
13: 44832: loss: 0.1581738904:
13: 48032: loss: 0.1575359404:
13: 51232: loss: 0.1580321575:
13: 54432: loss: 0.1578515205:
13: 57632: loss: 0.1580272976:
13: 60832: loss: 0.1582627068:
13: 64032: loss: 0.1581940713:
13: 67232: loss: 0.1576167962:
13: 70432: loss: 0.1578924418:
13: 73632: loss: 0.1574463277:
13: 76832: loss: 0.1573671415:
13: 80032: loss: 0.1575965080:
13: 83232: loss: 0.1577581845:
13: 86432: loss: 0.1578162211:
13: 89632: loss: 0.1580068675:
Dev-Acc: 13: Accuracy: 0.7953147292: precision: 0.1660628566: recall: 0.6235334127: f1: 0.2622751493
Train-Acc: 13: Accuracy: 0.9445138574: precision: 0.8927769606: recall: 0.7581355598: f1: 0.8199658703
14: 3232: loss: 0.1465764760:
14: 6432: loss: 0.1530083983:
14: 9632: loss: 0.1537186506:
14: 12832: loss: 0.1540081888:
14: 16032: loss: 0.1585068231:
14: 19232: loss: 0.1558045567:
14: 22432: loss: 0.1562694937:
14: 25632: loss: 0.1564458694:
14: 28832: loss: 0.1566298566:
14: 32032: loss: 0.1573298825:
14: 35232: loss: 0.1568731553:
14: 38432: loss: 0.1576202266:
14: 41632: loss: 0.1573319468:
14: 44832: loss: 0.1568841140:
14: 48032: loss: 0.1564925055:
14: 51232: loss: 0.1572141739:
14: 54432: loss: 0.1570487199:
14: 57632: loss: 0.1568106017:
14: 60832: loss: 0.1564962169:
14: 64032: loss: 0.1564871042:
14: 67232: loss: 0.1558949421:
14: 70432: loss: 0.1552298044:
14: 73632: loss: 0.1546768861:
14: 76832: loss: 0.1552310089:
14: 80032: loss: 0.1548124063:
14: 83232: loss: 0.1545166128:
14: 86432: loss: 0.1540730122:
14: 89632: loss: 0.1540010808:
Dev-Acc: 14: Accuracy: 0.7893812656: precision: 0.1622204613: recall: 0.6265941166: f1: 0.2577193412
Train-Acc: 14: Accuracy: 0.9455547333: precision: 0.8942870342: recall: 0.7635921373: f1: 0.8237880776
15: 3232: loss: 0.1572425821:
15: 6432: loss: 0.1524514624:
15: 9632: loss: 0.1545603295:
15: 12832: loss: 0.1523498176:
15: 16032: loss: 0.1533136303:
15: 19232: loss: 0.1538011576:
15: 22432: loss: 0.1516114474:
15: 25632: loss: 0.1512482949:
15: 28832: loss: 0.1502320800:
15: 32032: loss: 0.1496419196:
15: 35232: loss: 0.1516660355:
15: 38432: loss: 0.1522880680:
15: 41632: loss: 0.1521483550:
15: 44832: loss: 0.1521449172:
15: 48032: loss: 0.1520430567:
15: 51232: loss: 0.1513733754:
15: 54432: loss: 0.1508733676:
15: 57632: loss: 0.1508401085:
15: 60832: loss: 0.1508824924:
15: 64032: loss: 0.1498957287:
15: 67232: loss: 0.1498565220:
15: 70432: loss: 0.1497479128:
15: 73632: loss: 0.1494200659:
15: 76832: loss: 0.1494709892:
15: 80032: loss: 0.1497858095:
15: 83232: loss: 0.1501041839:
15: 86432: loss: 0.1499943888:
15: 89632: loss: 0.1499839783:
Dev-Acc: 15: Accuracy: 0.7806100249: precision: 0.1574504010: recall: 0.6342458766: f1: 0.2522741875
Train-Acc: 15: Accuracy: 0.9467271566: precision: 0.8923345212: recall: 0.7737163895: f1: 0.8288028169
16: 3232: loss: 0.1574259316:
16: 6432: loss: 0.1530315906:
16: 9632: loss: 0.1481876027:
16: 12832: loss: 0.1450747894:
16: 16032: loss: 0.1458766064:
16: 19232: loss: 0.1463166219:
16: 22432: loss: 0.1478698116:
16: 25632: loss: 0.1488292274:
16: 28832: loss: 0.1495595886:
16: 32032: loss: 0.1487734742:
16: 35232: loss: 0.1482572308:
16: 38432: loss: 0.1475297329:
16: 41632: loss: 0.1477526760:
16: 44832: loss: 0.1471986683:
16: 48032: loss: 0.1478467798:
16: 51232: loss: 0.1472853066:
16: 54432: loss: 0.1478410268:
16: 57632: loss: 0.1471135450:
16: 60832: loss: 0.1469264479:
16: 64032: loss: 0.1468292252:
16: 67232: loss: 0.1472296404:
16: 70432: loss: 0.1474206421:
16: 73632: loss: 0.1475507830:
16: 76832: loss: 0.1481006491:
16: 80032: loss: 0.1476936201:
16: 83232: loss: 0.1473815937:
16: 86432: loss: 0.1472126829:
16: 89632: loss: 0.1468339733:
Dev-Acc: 16: Accuracy: 0.7789827585: precision: 0.1561949501: recall: 0.6332256419: f1: 0.2505803586
Train-Acc: 16: Accuracy: 0.9474284053: precision: 0.8947009325: recall: 0.7758858721: f1: 0.8310682346
17: 3232: loss: 0.1416295513:
17: 6432: loss: 0.1494825940:
17: 9632: loss: 0.1468264999:
17: 12832: loss: 0.1441594164:
17: 16032: loss: 0.1458845977:
17: 19232: loss: 0.1473093094:
17: 22432: loss: 0.1463366437:
17: 25632: loss: 0.1451514103:
17: 28832: loss: 0.1472240568:
17: 32032: loss: 0.1474598934:
17: 35232: loss: 0.1472632211:
17: 38432: loss: 0.1478516870:
17: 41632: loss: 0.1478534434:
17: 44832: loss: 0.1482734169:
17: 48032: loss: 0.1487139384:
17: 51232: loss: 0.1491649501:
17: 54432: loss: 0.1486120414:
17: 57632: loss: 0.1479350696:
17: 60832: loss: 0.1472822356:
17: 64032: loss: 0.1464127498:
17: 67232: loss: 0.1456808653:
17: 70432: loss: 0.1452841518:
17: 73632: loss: 0.1453647019:
17: 76832: loss: 0.1450227527:
17: 80032: loss: 0.1444992002:
17: 83232: loss: 0.1442276486:
17: 86432: loss: 0.1440738788:
17: 89632: loss: 0.1441004796:
Dev-Acc: 17: Accuracy: 0.7810068727: precision: 0.1548857435: recall: 0.6177520830: f1: 0.2476735863
Train-Acc: 17: Accuracy: 0.9488856792: precision: 0.9102224988: recall: 0.7691801985: f1: 0.8337787280
18: 3232: loss: 0.1483172744:
18: 6432: loss: 0.1485856708:
18: 9632: loss: 0.1482869372:
18: 12832: loss: 0.1463985713:
18: 16032: loss: 0.1455525123:
18: 19232: loss: 0.1443757937:
18: 22432: loss: 0.1438134851:
18: 25632: loss: 0.1426387693:
18: 28832: loss: 0.1402742930:
18: 32032: loss: 0.1400434289:
18: 35232: loss: 0.1402138579:
18: 38432: loss: 0.1412716215:
18: 41632: loss: 0.1417832176:
18: 44832: loss: 0.1414963435:
18: 48032: loss: 0.1414287146:
18: 51232: loss: 0.1411652184:
18: 54432: loss: 0.1414511444:
18: 57632: loss: 0.1422148141:
18: 60832: loss: 0.1418977978:
18: 64032: loss: 0.1426994886:
18: 67232: loss: 0.1422626274:
18: 70432: loss: 0.1423815935:
18: 73632: loss: 0.1422535901:
18: 76832: loss: 0.1422276673:
18: 80032: loss: 0.1421804301:
18: 83232: loss: 0.1418779620:
18: 86432: loss: 0.1415953332:
18: 89632: loss: 0.1413154633:
Dev-Acc: 18: Accuracy: 0.7740315795: precision: 0.1525646827: recall: 0.6306750553: f1: 0.2456942236
Train-Acc: 18: Accuracy: 0.9496526718: precision: 0.9034047728: recall: 0.7814739333: f1: 0.8380274243
19: 3232: loss: 0.1310440013:
19: 6432: loss: 0.1429177833:
19: 9632: loss: 0.1469934447:
19: 12832: loss: 0.1441788412:
19: 16032: loss: 0.1435817871:
19: 19232: loss: 0.1408235447:
19: 22432: loss: 0.1422593926:
19: 25632: loss: 0.1428601297:
19: 28832: loss: 0.1421445902:
19: 32032: loss: 0.1422905223:
19: 35232: loss: 0.1417099014:
19: 38432: loss: 0.1413462190:
19: 41632: loss: 0.1407383693:
19: 44832: loss: 0.1405318349:
19: 48032: loss: 0.1407560548:
19: 51232: loss: 0.1402303031:
19: 54432: loss: 0.1396842633:
19: 57632: loss: 0.1394098783:
19: 60832: loss: 0.1394319622:
19: 64032: loss: 0.1394646357:
19: 67232: loss: 0.1391372711:
19: 70432: loss: 0.1395703768:
19: 73632: loss: 0.1398424269:
19: 76832: loss: 0.1402863691:
19: 80032: loss: 0.1403604178:
19: 83232: loss: 0.1397968926:
19: 86432: loss: 0.1394294953:
19: 89632: loss: 0.1393424248:
Dev-Acc: 19: Accuracy: 0.7715411186: precision: 0.1514030094: recall: 0.6330556028: f1: 0.2443634932
Train-Acc: 19: Accuracy: 0.9505620599: precision: 0.9059725279: recall: 0.7848267701: f1: 0.8410596026
20: 3232: loss: 0.1409163101:
20: 6432: loss: 0.1404554607:
20: 9632: loss: 0.1434392318:
20: 12832: loss: 0.1400698176:
20: 16032: loss: 0.1409694114:
20: 19232: loss: 0.1388827072:
20: 22432: loss: 0.1398705095:
20: 25632: loss: 0.1395284874:
20: 28832: loss: 0.1386935950:
20: 32032: loss: 0.1400202486:
20: 35232: loss: 0.1393342362:
20: 38432: loss: 0.1381570637:
20: 41632: loss: 0.1386451746:
20: 44832: loss: 0.1377290891:
20: 48032: loss: 0.1370240545:
20: 51232: loss: 0.1377606682:
20: 54432: loss: 0.1376139633:
20: 57632: loss: 0.1372330939:
20: 60832: loss: 0.1373695808:
20: 64032: loss: 0.1369440694:
20: 67232: loss: 0.1368419373:
20: 70432: loss: 0.1373155950:
20: 73632: loss: 0.1367949118:
20: 76832: loss: 0.1373230075:
20: 80032: loss: 0.1365542345:
20: 83232: loss: 0.1366261338:
20: 86432: loss: 0.1366321788:
20: 89632: loss: 0.1363612534:
Dev-Acc: 20: Accuracy: 0.7680484653: precision: 0.1471442401: recall: 0.6203026696: f1: 0.2378639194
Train-Acc: 20: Accuracy: 0.9516797066: precision: 0.9136729223: recall: 0.7841693511: f1: 0.8439821694
