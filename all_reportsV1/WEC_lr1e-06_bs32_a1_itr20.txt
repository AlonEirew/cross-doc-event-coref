1: 3232: loss: 0.6884522563:
1: 6432: loss: 0.6857254136:
1: 9632: loss: 0.6832677811:
1: 12832: loss: 0.6809095348:
1: 16032: loss: 0.6785124962:
1: 19232: loss: 0.6763909270:
1: 22432: loss: 0.6740411116:
1: 25632: loss: 0.6717623948:
1: 28832: loss: 0.6695845340:
1: 32032: loss: 0.6672648422:
1: 35232: loss: 0.6648818030:
1: 38432: loss: 0.6622364962:
1: 41632: loss: 0.6597136418:
1: 44832: loss: 0.6572995333:
1: 48032: loss: 0.6549980902:
1: 51232: loss: 0.6523388124:
1: 54432: loss: 0.6496061905:
1: 57632: loss: 0.6470099071:
1: 60832: loss: 0.6442579038:
1: 64032: loss: 0.6415940289:
1: 67232: loss: 0.6387738631:
1: 70432: loss: 0.6356256409:
1: 73632: loss: 0.6324932865:
1: 76832: loss: 0.6294020007:
1: 80032: loss: 0.6264452098:
1: 83232: loss: 0.6232175105:
1: 86432: loss: 0.6201562569:
1: 89632: loss: 0.6170290857:
1: 92832: loss: 0.6137613383:
1: 96032: loss: 0.6105404245:
1: 99232: loss: 0.6073824934:
1: 102432: loss: 0.6041749957:
1: 105632: loss: 0.6007576749:
1: 108832: loss: 0.5975855817:
1: 112032: loss: 0.5942781788:
1: 115232: loss: 0.5907198647:
1: 118432: loss: 0.5873051814:
1: 121632: loss: 0.5838759867:
1: 124832: loss: 0.5803573109:
1: 128032: loss: 0.5769357425:
1: 131232: loss: 0.5733903768:
1: 134432: loss: 0.5700381304:
1: 137632: loss: 0.5666514341:
1: 140832: loss: 0.5631376352:
1: 144032: loss: 0.5596714271:
1: 147232: loss: 0.5563264049:
1: 150432: loss: 0.5527323119:
1: 153632: loss: 0.5493000388:
1: 156832: loss: 0.5458403078:
1: 160032: loss: 0.5423497567:
1: 163232: loss: 0.5387929894:
1: 166432: loss: 0.5354211250:
1: 169632: loss: 0.5320241872:
1: 172832: loss: 0.5287892061:
1: 176032: loss: 0.5253523115:
1: 179232: loss: 0.5218891325:
1: 182432: loss: 0.5183237521:
1: 185632: loss: 0.5149084579:
1: 188832: loss: 0.5115314208:
1: 192032: loss: 0.5082348021:
1: 195232: loss: 0.5049070717:
1: 198432: loss: 0.5015913331:
1: 201632: loss: 0.4983124779:
1: 204832: loss: 0.4950121456:
1: 208032: loss: 0.4917750309:
1: 211232: loss: 0.4885415978:
1: 214432: loss: 0.4854350714:
1: 217632: loss: 0.4823016565:
1: 220832: loss: 0.4793341766:
1: 224032: loss: 0.4763244146:
1: 227232: loss: 0.4732666839:
1: 230432: loss: 0.4702596668:
1: 233632: loss: 0.4674249294:
1: 236832: loss: 0.4645037947:
1: 240032: loss: 0.4617721192:
1: 243232: loss: 0.4589549658:
1: 246432: loss: 0.4561262098:
1: 249632: loss: 0.4533410320:
1: 252832: loss: 0.4504840727:
1: 256032: loss: 0.4476995273:
1: 259232: loss: 0.4450025023:
1: 262432: loss: 0.4421882150:
1: 265632: loss: 0.4395263666:
1: 268832: loss: 0.4368631529:
1: 272032: loss: 0.4342913473:
1: 275232: loss: 0.4316971457:
1: 278432: loss: 0.4291399003:
1: 281632: loss: 0.4266140191:
1: 284832: loss: 0.4240677707:
1: 288032: loss: 0.4216490946:
1: 291232: loss: 0.4192641391:
1: 294432: loss: 0.4167782388:
1: 297632: loss: 0.4143579894:
1: 300832: loss: 0.4119872082:
1: 304032: loss: 0.4095861862:
1: 307232: loss: 0.4073830431:
1: 310432: loss: 0.4050997982:
1: 313632: loss: 0.4028268103:
1: 316832: loss: 0.4006184529:
1: 320032: loss: 0.3984517259:
1: 323232: loss: 0.3962726201:
1: 326432: loss: 0.3940751053:
1: 329632: loss: 0.3918577553:
Dev-Acc: 1: Accuracy: 0.1544590443: precision: 0.0594108892: recall: 0.9095391940: f1: 0.1115362561
Train-Acc: 1: Accuracy: 0.9513230920: precision: 0.9819260228: recall: 0.9165213601: f1: 0.9480970418
2: 3232: loss: 0.1765669506:
2: 6432: loss: 0.1701537516:
2: 9632: loss: 0.1697297026:
2: 12832: loss: 0.1699525850:
2: 16032: loss: 0.1691544996:
2: 19232: loss: 0.1678408811:
2: 22432: loss: 0.1679891849:
2: 25632: loss: 0.1671195937:
2: 28832: loss: 0.1671170859:
2: 32032: loss: 0.1657187058:
2: 35232: loss: 0.1650495003:
2: 38432: loss: 0.1646981319:
2: 41632: loss: 0.1643393765:
2: 44832: loss: 0.1627229091:
2: 48032: loss: 0.1622018304:
2: 51232: loss: 0.1613020924:
2: 54432: loss: 0.1603258116:
2: 57632: loss: 0.1598704274:
2: 60832: loss: 0.1591690873:
2: 64032: loss: 0.1583312787:
2: 67232: loss: 0.1577735956:
2: 70432: loss: 0.1571776863:
2: 73632: loss: 0.1566647312:
2: 76832: loss: 0.1556882098:
2: 80032: loss: 0.1550331522:
2: 83232: loss: 0.1547221242:
2: 86432: loss: 0.1542168929:
2: 89632: loss: 0.1532510017:
2: 92832: loss: 0.1523601182:
2: 96032: loss: 0.1515606453:
2: 99232: loss: 0.1508212777:
2: 102432: loss: 0.1502425456:
2: 105632: loss: 0.1495307182:
2: 108832: loss: 0.1489052058:
2: 112032: loss: 0.1484567668:
2: 115232: loss: 0.1477482678:
2: 118432: loss: 0.1473565046:
2: 121632: loss: 0.1467012553:
2: 124832: loss: 0.1462717135:
2: 128032: loss: 0.1457240742:
2: 131232: loss: 0.1450905284:
2: 134432: loss: 0.1448463658:
2: 137632: loss: 0.1443422720:
2: 140832: loss: 0.1440037926:
2: 144032: loss: 0.1435927743:
2: 147232: loss: 0.1432337524:
2: 150432: loss: 0.1426622930:
2: 153632: loss: 0.1422666781:
2: 156832: loss: 0.1415191604:
2: 160032: loss: 0.1411131579:
2: 163232: loss: 0.1406068927:
2: 166432: loss: 0.1401614684:
2: 169632: loss: 0.1396669608:
2: 172832: loss: 0.1392941117:
2: 176032: loss: 0.1387958745:
2: 179232: loss: 0.1383194188:
2: 182432: loss: 0.1378504558:
2: 185632: loss: 0.1373653808:
2: 188832: loss: 0.1370094940:
2: 192032: loss: 0.1368379219:
2: 195232: loss: 0.1366240294:
2: 198432: loss: 0.1360954284:
2: 201632: loss: 0.1356387459:
2: 204832: loss: 0.1351782583:
2: 208032: loss: 0.1347452982:
2: 211232: loss: 0.1342582705:
2: 214432: loss: 0.1338671241:
2: 217632: loss: 0.1335207100:
2: 220832: loss: 0.1332375431:
2: 224032: loss: 0.1330054545:
2: 227232: loss: 0.1326246466:
2: 230432: loss: 0.1321761407:
2: 233632: loss: 0.1318939057:
2: 236832: loss: 0.1315241067:
2: 240032: loss: 0.1312620247:
2: 243232: loss: 0.1308254977:
2: 246432: loss: 0.1305083513:
2: 249632: loss: 0.1300982229:
2: 252832: loss: 0.1297300129:
2: 256032: loss: 0.1293154728:
2: 259232: loss: 0.1289636546:
2: 262432: loss: 0.1286627072:
2: 265632: loss: 0.1282758594:
2: 268832: loss: 0.1278802521:
2: 272032: loss: 0.1276123082:
2: 275232: loss: 0.1272921486:
2: 278432: loss: 0.1269345305:
2: 281632: loss: 0.1265889657:
2: 284832: loss: 0.1262065224:
2: 288032: loss: 0.1258693539:
2: 291232: loss: 0.1255990973:
2: 294432: loss: 0.1252185423:
2: 297632: loss: 0.1248659762:
2: 300832: loss: 0.1246102553:
2: 304032: loss: 0.1243156032:
2: 307232: loss: 0.1240012129:
2: 310432: loss: 0.1237321001:
2: 313632: loss: 0.1233826693:
2: 316832: loss: 0.1230492733:
2: 320032: loss: 0.1227370737:
2: 323232: loss: 0.1224482883:
2: 326432: loss: 0.1220539340:
2: 329632: loss: 0.1217865775:
Dev-Acc: 2: Accuracy: 0.1213684678: precision: 0.0573362319: recall: 0.9103893896: f1: 0.1078782994
Train-Acc: 2: Accuracy: 0.9748006463: precision: 0.9966139513: recall: 0.9512828497: f1: 0.9734209319
3: 3232: loss: 0.0922066581:
3: 6432: loss: 0.0935938294:
3: 9632: loss: 0.0914858817:
3: 12832: loss: 0.0903679202:
3: 16032: loss: 0.0919992438:
3: 19232: loss: 0.0911083732:
3: 22432: loss: 0.0914877604:
3: 25632: loss: 0.0904153897:
3: 28832: loss: 0.0897042112:
3: 32032: loss: 0.0896351718:
3: 35232: loss: 0.0895928489:
3: 38432: loss: 0.0895586536:
3: 41632: loss: 0.0899268320:
3: 44832: loss: 0.0894971674:
3: 48032: loss: 0.0897859879:
3: 51232: loss: 0.0893896506:
3: 54432: loss: 0.0891928655:
3: 57632: loss: 0.0894176969:
3: 60832: loss: 0.0889293228:
3: 64032: loss: 0.0887686606:
3: 67232: loss: 0.0886086775:
3: 70432: loss: 0.0886280489:
3: 73632: loss: 0.0889178948:
3: 76832: loss: 0.0887033630:
3: 80032: loss: 0.0885079383:
3: 83232: loss: 0.0884491700:
3: 86432: loss: 0.0880033908:
3: 89632: loss: 0.0881070689:
3: 92832: loss: 0.0880454349:
3: 96032: loss: 0.0879259474:
3: 99232: loss: 0.0878172668:
3: 102432: loss: 0.0877755701:
3: 105632: loss: 0.0877529485:
3: 108832: loss: 0.0877746465:
3: 112032: loss: 0.0878616674:
3: 115232: loss: 0.0877427743:
3: 118432: loss: 0.0875685041:
3: 121632: loss: 0.0874434960:
3: 124832: loss: 0.0874322672:
3: 128032: loss: 0.0874078946:
3: 131232: loss: 0.0874320602:
3: 134432: loss: 0.0872870348:
3: 137632: loss: 0.0870853882:
3: 140832: loss: 0.0866663565:
3: 144032: loss: 0.0863702412:
3: 147232: loss: 0.0860807560:
3: 150432: loss: 0.0858971506:
3: 153632: loss: 0.0857271978:
3: 156832: loss: 0.0856385744:
3: 160032: loss: 0.0855607260:
3: 163232: loss: 0.0853249239:
3: 166432: loss: 0.0849683489:
3: 169632: loss: 0.0849350788:
3: 172832: loss: 0.0847581881:
3: 176032: loss: 0.0844536779:
3: 179232: loss: 0.0843002779:
3: 182432: loss: 0.0840987987:
3: 185632: loss: 0.0840416076:
3: 188832: loss: 0.0840551841:
3: 192032: loss: 0.0837975779:
3: 195232: loss: 0.0835899155:
3: 198432: loss: 0.0836866019:
3: 201632: loss: 0.0834839754:
3: 204832: loss: 0.0833367837:
3: 208032: loss: 0.0831376862:
3: 211232: loss: 0.0830241705:
3: 214432: loss: 0.0828796180:
3: 217632: loss: 0.0827849340:
3: 220832: loss: 0.0825280884:
3: 224032: loss: 0.0823746509:
3: 227232: loss: 0.0822459611:
3: 230432: loss: 0.0820593039:
3: 233632: loss: 0.0818939135:
3: 236832: loss: 0.0818208838:
3: 240032: loss: 0.0816196363:
3: 243232: loss: 0.0814320615:
3: 246432: loss: 0.0813683492:
3: 249632: loss: 0.0812522123:
3: 252832: loss: 0.0810910125:
3: 256032: loss: 0.0809429079:
3: 259232: loss: 0.0809514845:
3: 262432: loss: 0.0808231210:
3: 265632: loss: 0.0807886888:
3: 268832: loss: 0.0807110386:
3: 272032: loss: 0.0806288283:
3: 275232: loss: 0.0805078163:
3: 278432: loss: 0.0803681958:
3: 281632: loss: 0.0802876613:
3: 284832: loss: 0.0802451741:
3: 288032: loss: 0.0802972995:
3: 291232: loss: 0.0802148611:
3: 294432: loss: 0.0802104181:
3: 297632: loss: 0.0801055637:
3: 300832: loss: 0.0799853386:
3: 304032: loss: 0.0798038046:
3: 307232: loss: 0.0796639008:
3: 310432: loss: 0.0795591688:
3: 313632: loss: 0.0794278183:
3: 316832: loss: 0.0792993735:
3: 320032: loss: 0.0791535155:
3: 323232: loss: 0.0790593172:
3: 326432: loss: 0.0789716597:
3: 329632: loss: 0.0788153827:
Dev-Acc: 3: Accuracy: 0.1081421673: precision: 0.0565011720: recall: 0.9098792722: f1: 0.1063954587
Train-Acc: 3: Accuracy: 0.9811895490: precision: 0.9986109586: recall: 0.9625607174: f1: 0.9802545004
4: 3232: loss: 0.0697338247:
4: 6432: loss: 0.0685342495:
4: 9632: loss: 0.0679163581:
4: 12832: loss: 0.0674649869:
4: 16032: loss: 0.0672658461:
4: 19232: loss: 0.0683382362:
4: 22432: loss: 0.0679395920:
4: 25632: loss: 0.0680122611:
4: 28832: loss: 0.0684601559:
4: 32032: loss: 0.0687982072:
4: 35232: loss: 0.0685584329:
4: 38432: loss: 0.0680903199:
4: 41632: loss: 0.0683196634:
4: 44832: loss: 0.0687801941:
4: 48032: loss: 0.0686908416:
4: 51232: loss: 0.0691769552:
4: 54432: loss: 0.0685913252:
4: 57632: loss: 0.0688644557:
4: 60832: loss: 0.0686861012:
4: 64032: loss: 0.0681237145:
4: 67232: loss: 0.0677490768:
4: 70432: loss: 0.0673800627:
4: 73632: loss: 0.0672619699:
4: 76832: loss: 0.0671418190:
4: 80032: loss: 0.0667120638:
4: 83232: loss: 0.0666794416:
4: 86432: loss: 0.0664563628:
4: 89632: loss: 0.0661639387:
4: 92832: loss: 0.0659986853:
4: 96032: loss: 0.0659215701:
4: 99232: loss: 0.0661504717:
4: 102432: loss: 0.0661284273:
4: 105632: loss: 0.0661971898:
4: 108832: loss: 0.0658316954:
4: 112032: loss: 0.0658115867:
4: 115232: loss: 0.0656514015:
4: 118432: loss: 0.0655249063:
4: 121632: loss: 0.0652005891:
4: 124832: loss: 0.0652198644:
4: 128032: loss: 0.0652225638:
4: 131232: loss: 0.0651069061:
4: 134432: loss: 0.0651041277:
4: 137632: loss: 0.0650328685:
4: 140832: loss: 0.0650054137:
4: 144032: loss: 0.0647786498:
4: 147232: loss: 0.0645371543:
4: 150432: loss: 0.0642864266:
4: 153632: loss: 0.0641668039:
4: 156832: loss: 0.0643084515:
4: 160032: loss: 0.0641718012:
4: 163232: loss: 0.0640819828:
4: 166432: loss: 0.0639682692:
4: 169632: loss: 0.0638568700:
4: 172832: loss: 0.0637588815:
4: 176032: loss: 0.0638739053:
4: 179232: loss: 0.0639956259:
4: 182432: loss: 0.0640104176:
4: 185632: loss: 0.0641032054:
4: 188832: loss: 0.0639446547:
4: 192032: loss: 0.0638035621:
4: 195232: loss: 0.0636401240:
4: 198432: loss: 0.0635776090:
4: 201632: loss: 0.0634620503:
4: 204832: loss: 0.0633684788:
4: 208032: loss: 0.0632868279:
4: 211232: loss: 0.0632497999:
4: 214432: loss: 0.0632880322:
4: 217632: loss: 0.0631936658:
4: 220832: loss: 0.0631975989:
4: 224032: loss: 0.0631717385:
4: 227232: loss: 0.0630720767:
4: 230432: loss: 0.0629542344:
4: 233632: loss: 0.0627657052:
4: 236832: loss: 0.0628167266:
4: 240032: loss: 0.0628048232:
4: 243232: loss: 0.0627834139:
4: 246432: loss: 0.0626500854:
4: 249632: loss: 0.0625865892:
4: 252832: loss: 0.0625523490:
4: 256032: loss: 0.0625368290:
4: 259232: loss: 0.0625117694:
4: 262432: loss: 0.0623877093:
4: 265632: loss: 0.0622812261:
4: 268832: loss: 0.0622739605:
4: 272032: loss: 0.0621902560:
4: 275232: loss: 0.0621408437:
4: 278432: loss: 0.0619741949:
4: 281632: loss: 0.0619245916:
4: 284832: loss: 0.0618091357:
4: 288032: loss: 0.0617881554:
4: 291232: loss: 0.0616962620:
4: 294432: loss: 0.0616225267:
4: 297632: loss: 0.0615269983:
4: 300832: loss: 0.0614939170:
4: 304032: loss: 0.0615422594:
4: 307232: loss: 0.0615489113:
4: 310432: loss: 0.0615569044:
4: 313632: loss: 0.0615761866:
4: 316832: loss: 0.0615697464:
4: 320032: loss: 0.0615159789:
4: 323232: loss: 0.0613817459:
4: 326432: loss: 0.0614072559:
4: 329632: loss: 0.0614321634:
Dev-Acc: 4: Accuracy: 0.0985969976: precision: 0.0565356270: recall: 0.9209318143: f1: 0.1065313388
Train-Acc: 4: Accuracy: 0.9849141836: precision: 0.9989353378: recall: 0.9699339893: f1: 0.9842210693
5: 3232: loss: 0.0557826995:
5: 6432: loss: 0.0574207679:
5: 9632: loss: 0.0576463844:
5: 12832: loss: 0.0567335803:
5: 16032: loss: 0.0548955796:
5: 19232: loss: 0.0547295088:
5: 22432: loss: 0.0547429579:
5: 25632: loss: 0.0544563533:
5: 28832: loss: 0.0546398810:
5: 32032: loss: 0.0551884349:
5: 35232: loss: 0.0543953863:
5: 38432: loss: 0.0547181449:
5: 41632: loss: 0.0545380119:
5: 44832: loss: 0.0543550242:
5: 48032: loss: 0.0540183864:
5: 51232: loss: 0.0540258431:
5: 54432: loss: 0.0541556815:
5: 57632: loss: 0.0545657420:
5: 60832: loss: 0.0544239529:
5: 64032: loss: 0.0543922809:
5: 67232: loss: 0.0538945773:
5: 70432: loss: 0.0537326369:
5: 73632: loss: 0.0537915924:
5: 76832: loss: 0.0538172865:
5: 80032: loss: 0.0535347546:
5: 83232: loss: 0.0535903047:
5: 86432: loss: 0.0534656251:
5: 89632: loss: 0.0533917856:
5: 92832: loss: 0.0533125041:
5: 96032: loss: 0.0535553362:
5: 99232: loss: 0.0534692658:
5: 102432: loss: 0.0533768103:
5: 105632: loss: 0.0530507398:
5: 108832: loss: 0.0530611934:
5: 112032: loss: 0.0529460247:
5: 115232: loss: 0.0528542556:
5: 118432: loss: 0.0526904960:
5: 121632: loss: 0.0525777753:
5: 124832: loss: 0.0526629929:
5: 128032: loss: 0.0528655643:
5: 131232: loss: 0.0528910443:
5: 134432: loss: 0.0528472761:
5: 137632: loss: 0.0528729871:
5: 140832: loss: 0.0527635403:
5: 144032: loss: 0.0528192604:
5: 147232: loss: 0.0528169691:
5: 150432: loss: 0.0527522441:
5: 153632: loss: 0.0528584031:
5: 156832: loss: 0.0529723238:
5: 160032: loss: 0.0529765064:
5: 163232: loss: 0.0528682699:
5: 166432: loss: 0.0529885702:
5: 169632: loss: 0.0529289488:
5: 172832: loss: 0.0529531264:
5: 176032: loss: 0.0530424174:
5: 179232: loss: 0.0529903190:
5: 182432: loss: 0.0531014009:
5: 185632: loss: 0.0529590419:
5: 188832: loss: 0.0529054137:
5: 192032: loss: 0.0528914272:
5: 195232: loss: 0.0528519542:
5: 198432: loss: 0.0527285731:
5: 201632: loss: 0.0526763402:
5: 204832: loss: 0.0527032671:
5: 208032: loss: 0.0525898496:
5: 211232: loss: 0.0526758130:
5: 214432: loss: 0.0525559937:
5: 217632: loss: 0.0526682717:
5: 220832: loss: 0.0527382427:
5: 224032: loss: 0.0527881467:
5: 227232: loss: 0.0527632036:
5: 230432: loss: 0.0527100380:
5: 233632: loss: 0.0526862873:
5: 236832: loss: 0.0525463974:
5: 240032: loss: 0.0525056191:
5: 243232: loss: 0.0524766062:
5: 246432: loss: 0.0524901489:
5: 249632: loss: 0.0523642051:
5: 252832: loss: 0.0523082614:
5: 256032: loss: 0.0522705804:
5: 259232: loss: 0.0521920366:
5: 262432: loss: 0.0522169454:
5: 265632: loss: 0.0522594074:
5: 268832: loss: 0.0521764546:
5: 272032: loss: 0.0522465740:
5: 275232: loss: 0.0522572985:
5: 278432: loss: 0.0521884671:
5: 281632: loss: 0.0522004297:
5: 284832: loss: 0.0521824028:
5: 288032: loss: 0.0521227386:
5: 291232: loss: 0.0520539854:
5: 294432: loss: 0.0519934419:
5: 297632: loss: 0.0520420106:
5: 300832: loss: 0.0519863449:
5: 304032: loss: 0.0518644585:
5: 307232: loss: 0.0518193259:
5: 310432: loss: 0.0518192556:
5: 313632: loss: 0.0517453144:
5: 316832: loss: 0.0517353278:
5: 320032: loss: 0.0517039291:
5: 323232: loss: 0.0516536792:
5: 326432: loss: 0.0516550779:
5: 329632: loss: 0.0516317033:
Dev-Acc: 5: Accuracy: 0.0933481529: precision: 0.0565197996: recall: 0.9263730658: f1: 0.1065394243
Train-Acc: 5: Accuracy: 0.9872280955: precision: 0.9994186566: recall: 0.9742371404: f1: 0.9866672553
6: 3232: loss: 0.0523863721:
6: 6432: loss: 0.0523904201:
6: 9632: loss: 0.0506028189:
6: 12832: loss: 0.0494414848:
6: 16032: loss: 0.0491949168:
6: 19232: loss: 0.0496701710:
6: 22432: loss: 0.0502957557:
6: 25632: loss: 0.0490015424:
6: 28832: loss: 0.0482788031:
6: 32032: loss: 0.0484865192:
6: 35232: loss: 0.0486394611:
6: 38432: loss: 0.0484912454:
6: 41632: loss: 0.0485267611:
6: 44832: loss: 0.0485490817:
6: 48032: loss: 0.0481699323:
6: 51232: loss: 0.0479288384:
6: 54432: loss: 0.0479148501:
6: 57632: loss: 0.0480996612:
6: 60832: loss: 0.0475300627:
6: 64032: loss: 0.0477391914:
6: 67232: loss: 0.0476965904:
6: 70432: loss: 0.0476303906:
6: 73632: loss: 0.0473889411:
6: 76832: loss: 0.0473695529:
6: 80032: loss: 0.0473974048:
6: 83232: loss: 0.0474409069:
6: 86432: loss: 0.0470583011:
6: 89632: loss: 0.0470235333:
6: 92832: loss: 0.0468530466:
6: 96032: loss: 0.0470490419:
6: 99232: loss: 0.0470381554:
6: 102432: loss: 0.0469567759:
6: 105632: loss: 0.0467584901:
6: 108832: loss: 0.0470117832:
6: 112032: loss: 0.0468822513:
6: 115232: loss: 0.0467691937:
6: 118432: loss: 0.0466786144:
6: 121632: loss: 0.0465304496:
6: 124832: loss: 0.0464918711:
6: 128032: loss: 0.0463919146:
6: 131232: loss: 0.0463049083:
6: 134432: loss: 0.0462502410:
6: 137632: loss: 0.0464298158:
6: 140832: loss: 0.0463337372:
6: 144032: loss: 0.0465426137:
6: 147232: loss: 0.0465680301:
6: 150432: loss: 0.0464613283:
6: 153632: loss: 0.0463803125:
6: 156832: loss: 0.0462067445:
6: 160032: loss: 0.0460791679:
6: 163232: loss: 0.0460532223:
6: 166432: loss: 0.0459819423:
6: 169632: loss: 0.0458148136:
6: 172832: loss: 0.0458726024:
6: 176032: loss: 0.0459509886:
6: 179232: loss: 0.0459118666:
6: 182432: loss: 0.0458577665:
6: 185632: loss: 0.0459831484:
6: 188832: loss: 0.0457982546:
6: 192032: loss: 0.0457992299:
6: 195232: loss: 0.0457406270:
6: 198432: loss: 0.0457239674:
6: 201632: loss: 0.0457146324:
6: 204832: loss: 0.0457061416:
6: 208032: loss: 0.0457417516:
6: 211232: loss: 0.0457717217:
6: 214432: loss: 0.0457390659:
6: 217632: loss: 0.0457510250:
6: 220832: loss: 0.0457359446:
6: 224032: loss: 0.0456462582:
6: 227232: loss: 0.0456518337:
6: 230432: loss: 0.0457037186:
6: 233632: loss: 0.0456090818:
6: 236832: loss: 0.0455794772:
6: 240032: loss: 0.0454903506:
6: 243232: loss: 0.0454259158:
6: 246432: loss: 0.0453038612:
6: 249632: loss: 0.0451814089:
6: 252832: loss: 0.0451953944:
6: 256032: loss: 0.0451619956:
6: 259232: loss: 0.0451922632:
6: 262432: loss: 0.0451980020:
6: 265632: loss: 0.0451722653:
6: 268832: loss: 0.0451088604:
6: 272032: loss: 0.0451090495:
6: 275232: loss: 0.0450816160:
6: 278432: loss: 0.0450588336:
6: 281632: loss: 0.0449864381:
6: 284832: loss: 0.0448932687:
6: 288032: loss: 0.0448235591:
6: 291232: loss: 0.0448468724:
6: 294432: loss: 0.0448551311:
6: 297632: loss: 0.0448121079:
6: 300832: loss: 0.0448766316:
6: 304032: loss: 0.0449004965:
6: 307232: loss: 0.0449318293:
6: 310432: loss: 0.0449471442:
6: 313632: loss: 0.0448754417:
6: 316832: loss: 0.0448427233:
6: 320032: loss: 0.0447999187:
6: 323232: loss: 0.0447672944:
6: 326432: loss: 0.0447691934:
6: 329632: loss: 0.0447362501:
Dev-Acc: 6: Accuracy: 0.0900440514: precision: 0.0564811178: recall: 0.9292637307: f1: 0.1064897359
Train-Acc: 6: Accuracy: 0.9884696603: precision: 0.9996239188: recall: 0.9765973347: f1: 0.9879764759
7: 3232: loss: 0.0459519640:
7: 6432: loss: 0.0444983792:
7: 9632: loss: 0.0435280833:
7: 12832: loss: 0.0430284785:
7: 16032: loss: 0.0438318463:
7: 19232: loss: 0.0436792737:
7: 22432: loss: 0.0442582080:
7: 25632: loss: 0.0435001259:
7: 28832: loss: 0.0423951005:
7: 32032: loss: 0.0431039500:
7: 35232: loss: 0.0430618843:
7: 38432: loss: 0.0428415987:
7: 41632: loss: 0.0422702242:
7: 44832: loss: 0.0423523477:
7: 48032: loss: 0.0424698878:
7: 51232: loss: 0.0422799054:
7: 54432: loss: 0.0417171334:
7: 57632: loss: 0.0416006903:
7: 60832: loss: 0.0417673821:
7: 64032: loss: 0.0414825152:
7: 67232: loss: 0.0412715059:
7: 70432: loss: 0.0413033211:
7: 73632: loss: 0.0413072826:
7: 76832: loss: 0.0411759491:
7: 80032: loss: 0.0410865521:
7: 83232: loss: 0.0408548671:
7: 86432: loss: 0.0409162502:
7: 89632: loss: 0.0405236987:
7: 92832: loss: 0.0407273076:
7: 96032: loss: 0.0408817408:
7: 99232: loss: 0.0410791867:
7: 102432: loss: 0.0413260135:
7: 105632: loss: 0.0410866049:
7: 108832: loss: 0.0411817996:
7: 112032: loss: 0.0410182306:
7: 115232: loss: 0.0409586164:
7: 118432: loss: 0.0407454964:
7: 121632: loss: 0.0408646989:
7: 124832: loss: 0.0409131973:
7: 128032: loss: 0.0408550445:
7: 131232: loss: 0.0409144066:
7: 134432: loss: 0.0409398530:
7: 137632: loss: 0.0408746715:
7: 140832: loss: 0.0408592545:
7: 144032: loss: 0.0407353512:
7: 147232: loss: 0.0406481100:
7: 150432: loss: 0.0407264713:
7: 153632: loss: 0.0408866186:
7: 156832: loss: 0.0410481014:
7: 160032: loss: 0.0408984573:
7: 163232: loss: 0.0407316364:
7: 166432: loss: 0.0407435331:
7: 169632: loss: 0.0406544765:
7: 172832: loss: 0.0406538659:
7: 176032: loss: 0.0405538794:
7: 179232: loss: 0.0405640233:
7: 182432: loss: 0.0405755485:
7: 185632: loss: 0.0405180285:
7: 188832: loss: 0.0404719830:
7: 192032: loss: 0.0405042888:
7: 195232: loss: 0.0404273992:
7: 198432: loss: 0.0403225645:
7: 201632: loss: 0.0404306918:
7: 204832: loss: 0.0404371506:
7: 208032: loss: 0.0404785805:
7: 211232: loss: 0.0404838601:
7: 214432: loss: 0.0405042598:
7: 217632: loss: 0.0404559564:
7: 220832: loss: 0.0403770454:
7: 224032: loss: 0.0403090570:
7: 227232: loss: 0.0403275752:
7: 230432: loss: 0.0402676516:
7: 233632: loss: 0.0402626407:
7: 236832: loss: 0.0402888898:
7: 240032: loss: 0.0402566076:
7: 243232: loss: 0.0403297513:
7: 246432: loss: 0.0403294950:
7: 249632: loss: 0.0403236764:
7: 252832: loss: 0.0402652019:
7: 256032: loss: 0.0402186467:
7: 259232: loss: 0.0401674909:
7: 262432: loss: 0.0401199961:
7: 265632: loss: 0.0401689803:
7: 268832: loss: 0.0402714185:
7: 272032: loss: 0.0402396836:
7: 275232: loss: 0.0402283109:
7: 278432: loss: 0.0401540341:
7: 281632: loss: 0.0400655475:
7: 284832: loss: 0.0400327421:
7: 288032: loss: 0.0401058128:
7: 291232: loss: 0.0400974082:
7: 294432: loss: 0.0400930794:
7: 297632: loss: 0.0400814114:
7: 300832: loss: 0.0401040516:
7: 304032: loss: 0.0400718246:
7: 307232: loss: 0.0400367680:
7: 310432: loss: 0.0400068153:
7: 313632: loss: 0.0400123391:
7: 316832: loss: 0.0399920742:
7: 320032: loss: 0.0399963039:
7: 323232: loss: 0.0400309668:
7: 326432: loss: 0.0399866822:
7: 329632: loss: 0.0399643676:
Dev-Acc: 7: Accuracy: 0.0853806138: precision: 0.0566913925: recall: 0.9382758034: f1: 0.1069224434
Train-Acc: 7: Accuracy: 0.9905721545: precision: 0.9994607591: recall: 0.9810935359: f1: 0.9901919807
8: 3232: loss: 0.0381286964:
8: 6432: loss: 0.0397110227:
8: 9632: loss: 0.0362414941:
8: 12832: loss: 0.0355075030:
8: 16032: loss: 0.0355753596:
8: 19232: loss: 0.0354897130:
8: 22432: loss: 0.0354021320:
8: 25632: loss: 0.0355469845:
8: 28832: loss: 0.0364205934:
8: 32032: loss: 0.0365047805:
8: 35232: loss: 0.0362319862:
8: 38432: loss: 0.0365146253:
8: 41632: loss: 0.0363585139:
8: 44832: loss: 0.0367201936:
8: 48032: loss: 0.0365260579:
8: 51232: loss: 0.0364751092:
8: 54432: loss: 0.0360617610:
8: 57632: loss: 0.0358324878:
8: 60832: loss: 0.0359172777:
8: 64032: loss: 0.0361812258:
8: 67232: loss: 0.0360901833:
8: 70432: loss: 0.0362736815:
8: 73632: loss: 0.0361055525:
8: 76832: loss: 0.0361309097:
8: 80032: loss: 0.0362362569:
8: 83232: loss: 0.0361679806:
8: 86432: loss: 0.0361258078:
8: 89632: loss: 0.0364036844:
8: 92832: loss: 0.0362930094:
8: 96032: loss: 0.0364544736:
8: 99232: loss: 0.0363305203:
8: 102432: loss: 0.0363497832:
8: 105632: loss: 0.0361357511:
8: 108832: loss: 0.0362418995:
8: 112032: loss: 0.0362820700:
8: 115232: loss: 0.0364298820:
8: 118432: loss: 0.0363654917:
8: 121632: loss: 0.0361801573:
8: 124832: loss: 0.0361846614:
8: 128032: loss: 0.0361237621:
8: 131232: loss: 0.0361260364:
8: 134432: loss: 0.0360593276:
8: 137632: loss: 0.0360674459:
8: 140832: loss: 0.0358879389:
8: 144032: loss: 0.0358942559:
8: 147232: loss: 0.0359542697:
8: 150432: loss: 0.0360120036:
8: 153632: loss: 0.0360027514:
8: 156832: loss: 0.0358211814:
8: 160032: loss: 0.0359603573:
8: 163232: loss: 0.0360744075:
8: 166432: loss: 0.0361596845:
8: 169632: loss: 0.0361465631:
8: 172832: loss: 0.0361610586:
8: 176032: loss: 0.0361366018:
8: 179232: loss: 0.0360350625:
8: 182432: loss: 0.0359901992:
8: 185632: loss: 0.0359193114:
8: 188832: loss: 0.0358742268:
8: 192032: loss: 0.0358324567:
8: 195232: loss: 0.0359057326:
8: 198432: loss: 0.0357996847:
8: 201632: loss: 0.0356949200:
8: 204832: loss: 0.0357009716:
8: 208032: loss: 0.0358773853:
8: 211232: loss: 0.0358916109:
8: 214432: loss: 0.0359433921:
8: 217632: loss: 0.0359187915:
8: 220832: loss: 0.0359276895:
8: 224032: loss: 0.0360119323:
8: 227232: loss: 0.0359979132:
8: 230432: loss: 0.0359572247:
8: 233632: loss: 0.0360261337:
8: 236832: loss: 0.0360002367:
8: 240032: loss: 0.0359812733:
8: 243232: loss: 0.0361777713:
8: 246432: loss: 0.0361264552:
8: 249632: loss: 0.0360985714:
8: 252832: loss: 0.0360760376:
8: 256032: loss: 0.0360694257:
8: 259232: loss: 0.0360937555:
8: 262432: loss: 0.0361114671:
8: 265632: loss: 0.0361282541:
8: 268832: loss: 0.0361255681:
8: 272032: loss: 0.0360957010:
8: 275232: loss: 0.0360485909:
8: 278432: loss: 0.0360152914:
8: 281632: loss: 0.0359865235:
8: 284832: loss: 0.0359817554:
8: 288032: loss: 0.0359601967:
8: 291232: loss: 0.0359597686:
8: 294432: loss: 0.0360288491:
8: 297632: loss: 0.0360164938:
8: 300832: loss: 0.0359469180:
8: 304032: loss: 0.0359728261:
8: 307232: loss: 0.0359867868:
8: 310432: loss: 0.0360082346:
8: 313632: loss: 0.0360509326:
8: 316832: loss: 0.0359776849:
8: 320032: loss: 0.0359365052:
8: 323232: loss: 0.0359745038:
8: 326432: loss: 0.0359850395:
8: 329632: loss: 0.0359462160:
Dev-Acc: 8: Accuracy: 0.0827512294: precision: 0.0572542120: recall: 0.9517088930: f1: 0.1080105753
Train-Acc: 8: Accuracy: 0.9916868210: precision: 0.9995505476: recall: 0.9833042720: f1: 0.9913608539
9: 3232: loss: 0.0363180891:
9: 6432: loss: 0.0345288929:
9: 9632: loss: 0.0334432264:
9: 12832: loss: 0.0325361009:
9: 16032: loss: 0.0320029035:
9: 19232: loss: 0.0328534754:
9: 22432: loss: 0.0319078236:
9: 25632: loss: 0.0335171808:
9: 28832: loss: 0.0338005114:
9: 32032: loss: 0.0340270081:
9: 35232: loss: 0.0338500761:
9: 38432: loss: 0.0332498900:
9: 41632: loss: 0.0330584645:
9: 44832: loss: 0.0328569336:
9: 48032: loss: 0.0330270059:
9: 51232: loss: 0.0326087367:
9: 54432: loss: 0.0330377569:
9: 57632: loss: 0.0330559242:
9: 60832: loss: 0.0332134693:
9: 64032: loss: 0.0333700553:
9: 67232: loss: 0.0334075489:
9: 70432: loss: 0.0335879146:
9: 73632: loss: 0.0331480001:
9: 76832: loss: 0.0331651191:
9: 80032: loss: 0.0331585689:
9: 83232: loss: 0.0331629119:
9: 86432: loss: 0.0332081845:
9: 89632: loss: 0.0334215798:
9: 92832: loss: 0.0336717732:
9: 96032: loss: 0.0335145518:
9: 99232: loss: 0.0336461153:
9: 102432: loss: 0.0336101859:
9: 105632: loss: 0.0335654321:
9: 108832: loss: 0.0333858945:
9: 112032: loss: 0.0336318390:
9: 115232: loss: 0.0337903871:
9: 118432: loss: 0.0337465838:
9: 121632: loss: 0.0335927994:
9: 124832: loss: 0.0334958531:
9: 128032: loss: 0.0335412542:
9: 131232: loss: 0.0335562911:
9: 134432: loss: 0.0334756516:
9: 137632: loss: 0.0332953484:
9: 140832: loss: 0.0334434468:
9: 144032: loss: 0.0334623690:
9: 147232: loss: 0.0335730722:
9: 150432: loss: 0.0337768107:
9: 153632: loss: 0.0336956461:
9: 156832: loss: 0.0336686267:
9: 160032: loss: 0.0336126767:
9: 163232: loss: 0.0335942433:
9: 166432: loss: 0.0335864769:
9: 169632: loss: 0.0336520255:
9: 172832: loss: 0.0336014642:
9: 176032: loss: 0.0335318741:
9: 179232: loss: 0.0335565134:
9: 182432: loss: 0.0335555506:
9: 185632: loss: 0.0336702644:
9: 188832: loss: 0.0336185328:
9: 192032: loss: 0.0336181936:
9: 195232: loss: 0.0335611055:
9: 198432: loss: 0.0335417024:
9: 201632: loss: 0.0333979703:
9: 204832: loss: 0.0333487529:
9: 208032: loss: 0.0332903706:
9: 211232: loss: 0.0333698714:
9: 214432: loss: 0.0335039550:
9: 217632: loss: 0.0334307296:
9: 220832: loss: 0.0334796457:
9: 224032: loss: 0.0334882350:
9: 227232: loss: 0.0334357196:
9: 230432: loss: 0.0333393696:
9: 233632: loss: 0.0332514917:
9: 236832: loss: 0.0332807131:
9: 240032: loss: 0.0332491432:
9: 243232: loss: 0.0332549874:
9: 246432: loss: 0.0332450817:
9: 249632: loss: 0.0332243029:
9: 252832: loss: 0.0331450873:
9: 256032: loss: 0.0331398164:
9: 259232: loss: 0.0330918965:
9: 262432: loss: 0.0330747078:
9: 265632: loss: 0.0330777445:
9: 268832: loss: 0.0330435995:
9: 272032: loss: 0.0330181026:
9: 275232: loss: 0.0330651955:
9: 278432: loss: 0.0330908893:
9: 281632: loss: 0.0330762996:
9: 284832: loss: 0.0330643721:
9: 288032: loss: 0.0329942049:
9: 291232: loss: 0.0329167648:
9: 294432: loss: 0.0328092225:
9: 297632: loss: 0.0327803309:
9: 300832: loss: 0.0328899384:
9: 304032: loss: 0.0329249360:
9: 307232: loss: 0.0329053718:
9: 310432: loss: 0.0328589803:
9: 313632: loss: 0.0328954700:
9: 316832: loss: 0.0328818114:
9: 320032: loss: 0.0328758502:
9: 323232: loss: 0.0328947257:
9: 326432: loss: 0.0328737976:
9: 329632: loss: 0.0328195398:
Dev-Acc: 9: Accuracy: 0.0819376111: precision: 0.0571247483: recall: 0.9501785411: f1: 0.1077703419
Train-Acc: 9: Accuracy: 0.9920855165: precision: 0.9997152800: recall: 0.9839643791: f1: 0.9917772966
10: 3232: loss: 0.0279648643:
10: 6432: loss: 0.0265625377:
10: 9632: loss: 0.0286078460:
10: 12832: loss: 0.0315693155:
10: 16032: loss: 0.0321849688:
10: 19232: loss: 0.0325056770:
10: 22432: loss: 0.0318939674:
10: 25632: loss: 0.0317484378:
10: 28832: loss: 0.0319133560:
10: 32032: loss: 0.0314443457:
10: 35232: loss: 0.0314851545:
10: 38432: loss: 0.0313350899:
10: 41632: loss: 0.0317267965:
10: 44832: loss: 0.0316434199:
10: 48032: loss: 0.0317051569:
10: 51232: loss: 0.0309751493:
10: 54432: loss: 0.0307326578:
10: 57632: loss: 0.0310836977:
10: 60832: loss: 0.0310579088:
10: 64032: loss: 0.0312888057:
10: 67232: loss: 0.0312917307:
10: 70432: loss: 0.0308446166:
10: 73632: loss: 0.0306887592:
10: 76832: loss: 0.0308051356:
10: 80032: loss: 0.0306906090:
10: 83232: loss: 0.0305722535:
10: 86432: loss: 0.0304097067:
10: 89632: loss: 0.0302910521:
10: 92832: loss: 0.0301794208:
10: 96032: loss: 0.0302200115:
10: 99232: loss: 0.0303779485:
10: 102432: loss: 0.0302091376:
10: 105632: loss: 0.0302224204:
10: 108832: loss: 0.0299926394:
10: 112032: loss: 0.0298179078:
10: 115232: loss: 0.0299815650:
10: 118432: loss: 0.0301203545:
10: 121632: loss: 0.0300161566:
10: 124832: loss: 0.0302438968:
10: 128032: loss: 0.0304039002:
10: 131232: loss: 0.0304738742:
10: 134432: loss: 0.0304789951:
10: 137632: loss: 0.0303857118:
10: 140832: loss: 0.0304068134:
10: 144032: loss: 0.0304216807:
10: 147232: loss: 0.0304839887:
10: 150432: loss: 0.0306560232:
10: 153632: loss: 0.0306234257:
10: 156832: loss: 0.0305850226:
10: 160032: loss: 0.0306757521:
10: 163232: loss: 0.0306954760:
10: 166432: loss: 0.0305991688:
10: 169632: loss: 0.0305485834:
10: 172832: loss: 0.0305943760:
10: 176032: loss: 0.0305987163:
10: 179232: loss: 0.0305747824:
10: 182432: loss: 0.0306096256:
10: 185632: loss: 0.0306128398:
10: 188832: loss: 0.0306653096:
10: 192032: loss: 0.0305878400:
10: 195232: loss: 0.0305121140:
10: 198432: loss: 0.0306044454:
10: 201632: loss: 0.0306009968:
10: 204832: loss: 0.0305338338:
10: 208032: loss: 0.0305601577:
10: 211232: loss: 0.0306185048:
10: 214432: loss: 0.0305542058:
10: 217632: loss: 0.0305020985:
10: 220832: loss: 0.0303800492:
10: 224032: loss: 0.0302792820:
10: 227232: loss: 0.0304329544:
10: 230432: loss: 0.0304111731:
10: 233632: loss: 0.0304349384:
10: 236832: loss: 0.0303573037:
10: 240032: loss: 0.0303311678:
10: 243232: loss: 0.0304136305:
10: 246432: loss: 0.0304232546:
10: 249632: loss: 0.0303802101:
10: 252832: loss: 0.0304109712:
10: 256032: loss: 0.0304006220:
10: 259232: loss: 0.0304774531:
10: 262432: loss: 0.0304634595:
10: 265632: loss: 0.0303891704:
10: 268832: loss: 0.0303537011:
10: 272032: loss: 0.0303309837:
10: 275232: loss: 0.0302951516:
10: 278432: loss: 0.0302790197:
10: 281632: loss: 0.0302362582:
10: 284832: loss: 0.0302812467:
10: 288032: loss: 0.0302697044:
10: 291232: loss: 0.0302799481:
10: 294432: loss: 0.0303808250:
10: 297632: loss: 0.0305308077:
10: 300832: loss: 0.0305011524:
10: 304032: loss: 0.0306132243:
10: 307232: loss: 0.0305414544:
10: 310432: loss: 0.0304701539:
10: 313632: loss: 0.0304982266:
10: 316832: loss: 0.0304471102:
10: 320032: loss: 0.0304188114:
10: 323232: loss: 0.0304161513:
10: 326432: loss: 0.0304212579:
10: 329632: loss: 0.0303799103:
Dev-Acc: 10: Accuracy: 0.0817292407: precision: 0.0571939505: recall: 0.9517088930: f1: 0.1079033362
Train-Acc: 10: Accuracy: 0.9923785329: precision: 0.9997976233: recall: 0.9844874829: f1: 0.9920834889
11: 3232: loss: 0.0239457626:
11: 6432: loss: 0.0261877809:
11: 9632: loss: 0.0271969844:
11: 12832: loss: 0.0281688506:
11: 16032: loss: 0.0288072880:
11: 19232: loss: 0.0293222539:
11: 22432: loss: 0.0293845663:
11: 25632: loss: 0.0296938477:
11: 28832: loss: 0.0291841214:
11: 32032: loss: 0.0294414003:
11: 35232: loss: 0.0290295333:
11: 38432: loss: 0.0284861622:
11: 41632: loss: 0.0288122249:
11: 44832: loss: 0.0282153730:
11: 48032: loss: 0.0284678428:
11: 51232: loss: 0.0288729043:
11: 54432: loss: 0.0294434401:
11: 57632: loss: 0.0295168721:
11: 60832: loss: 0.0293881971:
11: 64032: loss: 0.0293543064:
11: 67232: loss: 0.0294340762:
11: 70432: loss: 0.0293167630:
11: 73632: loss: 0.0293695711:
11: 76832: loss: 0.0292425755:
11: 80032: loss: 0.0291078812:
11: 83232: loss: 0.0291444200:
11: 86432: loss: 0.0293016973:
11: 89632: loss: 0.0290766488:
11: 92832: loss: 0.0289516465:
11: 96032: loss: 0.0289361060:
11: 99232: loss: 0.0291189715:
11: 102432: loss: 0.0293433709:
11: 105632: loss: 0.0291807965:
11: 108832: loss: 0.0291113907:
11: 112032: loss: 0.0291599127:
11: 115232: loss: 0.0291408707:
11: 118432: loss: 0.0290748303:
11: 121632: loss: 0.0289956371:
11: 124832: loss: 0.0289629701:
11: 128032: loss: 0.0290630191:
11: 131232: loss: 0.0291154286:
11: 134432: loss: 0.0291715726:
11: 137632: loss: 0.0290694804:
11: 140832: loss: 0.0291035503:
11: 144032: loss: 0.0291036825:
11: 147232: loss: 0.0291102290:
11: 150432: loss: 0.0290763876:
11: 153632: loss: 0.0291315478:
11: 156832: loss: 0.0291694148:
11: 160032: loss: 0.0291642407:
11: 163232: loss: 0.0292977049:
11: 166432: loss: 0.0291650302:
11: 169632: loss: 0.0291196894:
11: 172832: loss: 0.0292051909:
11: 176032: loss: 0.0291311927:
11: 179232: loss: 0.0290439110:
11: 182432: loss: 0.0289840322:
11: 185632: loss: 0.0289639307:
11: 188832: loss: 0.0290260425:
11: 192032: loss: 0.0290304868:
11: 195232: loss: 0.0290989353:
11: 198432: loss: 0.0290529079:
11: 201632: loss: 0.0291698204:
11: 204832: loss: 0.0291434080:
11: 208032: loss: 0.0291192416:
11: 211232: loss: 0.0289888444:
11: 214432: loss: 0.0289400205:
11: 217632: loss: 0.0288783984:
11: 220832: loss: 0.0288356275:
11: 224032: loss: 0.0288227938:
11: 227232: loss: 0.0288303541:
11: 230432: loss: 0.0287181660:
11: 233632: loss: 0.0287360810:
11: 236832: loss: 0.0287778953:
11: 240032: loss: 0.0288398639:
11: 243232: loss: 0.0288107044:
11: 246432: loss: 0.0288214404:
11: 249632: loss: 0.0287933656:
11: 252832: loss: 0.0287464582:
11: 256032: loss: 0.0286842509:
11: 259232: loss: 0.0287364561:
11: 262432: loss: 0.0287336906:
11: 265632: loss: 0.0286109116:
11: 268832: loss: 0.0286157411:
11: 272032: loss: 0.0285195709:
11: 275232: loss: 0.0284780777:
11: 278432: loss: 0.0284155418:
11: 281632: loss: 0.0283954026:
11: 284832: loss: 0.0283297690:
11: 288032: loss: 0.0283040448:
11: 291232: loss: 0.0283243279:
11: 294432: loss: 0.0283194324:
11: 297632: loss: 0.0282778290:
11: 300832: loss: 0.0283338518:
11: 304032: loss: 0.0283689186:
11: 307232: loss: 0.0283487385:
11: 310432: loss: 0.0282819154:
11: 313632: loss: 0.0282428706:
11: 316832: loss: 0.0282286932:
11: 320032: loss: 0.0282183296:
11: 323232: loss: 0.0281752405:
11: 326432: loss: 0.0281325295:
11: 329632: loss: 0.0281279269:
Dev-Acc: 11: Accuracy: 0.0786533579: precision: 0.0571576954: recall: 0.9544295188: f1: 0.1078562301
Train-Acc: 11: Accuracy: 0.9931760430: precision: 0.9997727172: recall: 0.9861564329: f1: 0.9929178959
12: 3232: loss: 0.0272976857:
12: 6432: loss: 0.0266454285:
12: 9632: loss: 0.0259771093:
12: 12832: loss: 0.0265185081:
12: 16032: loss: 0.0264932001:
12: 19232: loss: 0.0270343679:
12: 22432: loss: 0.0260372319:
12: 25632: loss: 0.0262296915:
12: 28832: loss: 0.0263362526:
12: 32032: loss: 0.0262510730:
12: 35232: loss: 0.0267554994:
12: 38432: loss: 0.0266426502:
12: 41632: loss: 0.0270445504:
12: 44832: loss: 0.0269565212:
12: 48032: loss: 0.0269416203:
12: 51232: loss: 0.0265926145:
12: 54432: loss: 0.0264208357:
12: 57632: loss: 0.0264561404:
12: 60832: loss: 0.0267147754:
12: 64032: loss: 0.0266053169:
12: 67232: loss: 0.0264486059:
12: 70432: loss: 0.0263964270:
12: 73632: loss: 0.0262028350:
12: 76832: loss: 0.0261171366:
12: 80032: loss: 0.0261444505:
12: 83232: loss: 0.0258633383:
12: 86432: loss: 0.0259136026:
12: 89632: loss: 0.0261839710:
12: 92832: loss: 0.0261393851:
12: 96032: loss: 0.0264109094:
12: 99232: loss: 0.0264290983:
12: 102432: loss: 0.0263330824:
12: 105632: loss: 0.0262655712:
12: 108832: loss: 0.0262171436:
12: 112032: loss: 0.0261245920:
12: 115232: loss: 0.0262838068:
12: 118432: loss: 0.0264095982:
12: 121632: loss: 0.0265627733:
12: 124832: loss: 0.0264817238:
12: 128032: loss: 0.0266668843:
12: 131232: loss: 0.0265776779:
12: 134432: loss: 0.0265372787:
12: 137632: loss: 0.0264904702:
12: 140832: loss: 0.0262879644:
12: 144032: loss: 0.0264349695:
12: 147232: loss: 0.0265976075:
12: 150432: loss: 0.0267924638:
12: 153632: loss: 0.0267899181:
12: 156832: loss: 0.0268630257:
12: 160032: loss: 0.0268194633:
12: 163232: loss: 0.0267185431:
12: 166432: loss: 0.0267136613:
12: 169632: loss: 0.0267665575:
12: 172832: loss: 0.0268685014:
12: 176032: loss: 0.0269969560:
12: 179232: loss: 0.0270695625:
12: 182432: loss: 0.0271366721:
12: 185632: loss: 0.0271179658:
12: 188832: loss: 0.0272593716:
12: 192032: loss: 0.0272507124:
12: 195232: loss: 0.0271534733:
12: 198432: loss: 0.0270818674:
12: 201632: loss: 0.0270515292:
12: 204832: loss: 0.0271007499:
12: 208032: loss: 0.0270563847:
12: 211232: loss: 0.0270083930:
12: 214432: loss: 0.0269587739:
12: 217632: loss: 0.0268517176:
12: 220832: loss: 0.0270051196:
12: 224032: loss: 0.0270148182:
12: 227232: loss: 0.0269782774:
12: 230432: loss: 0.0268942373:
12: 233632: loss: 0.0268205681:
12: 236832: loss: 0.0268191143:
12: 240032: loss: 0.0267910786:
12: 243232: loss: 0.0267783114:
12: 246432: loss: 0.0268249335:
12: 249632: loss: 0.0268150281:
12: 252832: loss: 0.0267864501:
12: 256032: loss: 0.0267253429:
12: 259232: loss: 0.0267626300:
12: 262432: loss: 0.0268080657:
12: 265632: loss: 0.0267568656:
12: 268832: loss: 0.0268149363:
12: 272032: loss: 0.0268353067:
12: 275232: loss: 0.0268250112:
12: 278432: loss: 0.0268214100:
12: 281632: loss: 0.0267566193:
12: 284832: loss: 0.0267362306:
12: 288032: loss: 0.0266477663:
12: 291232: loss: 0.0266639391:
12: 294432: loss: 0.0266230692:
12: 297632: loss: 0.0265314662:
12: 300832: loss: 0.0264637826:
12: 304032: loss: 0.0265242225:
12: 307232: loss: 0.0265419194:
12: 310432: loss: 0.0265512664:
12: 313632: loss: 0.0265650623:
12: 316832: loss: 0.0265417522:
12: 320032: loss: 0.0264645822:
12: 323232: loss: 0.0264236982:
12: 326432: loss: 0.0264293283:
12: 329632: loss: 0.0263792914:
Dev-Acc: 12: Accuracy: 0.0787724257: precision: 0.0572097760: recall: 0.9552797143: f1: 0.1079543817
Train-Acc: 12: Accuracy: 0.9933482409: precision: 0.9998421996: recall: 0.9864428945: f1: 0.9930973518
13: 3232: loss: 0.0308501779:
13: 6432: loss: 0.0250174628:
13: 9632: loss: 0.0265316016:
13: 12832: loss: 0.0257429383:
13: 16032: loss: 0.0254273273:
13: 19232: loss: 0.0250630073:
13: 22432: loss: 0.0259445129:
13: 25632: loss: 0.0260266414:
13: 28832: loss: 0.0255942544:
13: 32032: loss: 0.0259961332:
13: 35232: loss: 0.0259158987:
13: 38432: loss: 0.0259842214:
13: 41632: loss: 0.0258643964:
13: 44832: loss: 0.0256357509:
13: 48032: loss: 0.0255977007:
13: 51232: loss: 0.0251853062:
13: 54432: loss: 0.0256392182:
13: 57632: loss: 0.0258674779:
13: 60832: loss: 0.0262385058:
13: 64032: loss: 0.0258645564:
13: 67232: loss: 0.0254485364:
13: 70432: loss: 0.0255976335:
13: 73632: loss: 0.0255241128:
13: 76832: loss: 0.0254065502:
13: 80032: loss: 0.0255375759:
13: 83232: loss: 0.0255651266:
13: 86432: loss: 0.0254364031:
13: 89632: loss: 0.0254772337:
13: 92832: loss: 0.0255713677:
13: 96032: loss: 0.0256259859:
13: 99232: loss: 0.0256754446:
13: 102432: loss: 0.0256934529:
13: 105632: loss: 0.0257632578:
13: 108832: loss: 0.0254638439:
13: 112032: loss: 0.0254474602:
13: 115232: loss: 0.0256032943:
13: 118432: loss: 0.0254029225:
13: 121632: loss: 0.0255357443:
13: 124832: loss: 0.0253806436:
13: 128032: loss: 0.0252382471:
13: 131232: loss: 0.0252368381:
13: 134432: loss: 0.0251289765:
13: 137632: loss: 0.0249713331:
13: 140832: loss: 0.0249307185:
13: 144032: loss: 0.0249566218:
13: 147232: loss: 0.0249750299:
13: 150432: loss: 0.0249245527:
13: 153632: loss: 0.0251081077:
13: 156832: loss: 0.0250056208:
13: 160032: loss: 0.0249346809:
13: 163232: loss: 0.0249887650:
13: 166432: loss: 0.0249546416:
13: 169632: loss: 0.0250753095:
13: 172832: loss: 0.0250572547:
13: 176032: loss: 0.0249920668:
13: 179232: loss: 0.0250718044:
13: 182432: loss: 0.0250197694:
13: 185632: loss: 0.0249417453:
13: 188832: loss: 0.0250559114:
13: 192032: loss: 0.0251645799:
13: 195232: loss: 0.0250944321:
13: 198432: loss: 0.0251173326:
13: 201632: loss: 0.0250882010:
13: 204832: loss: 0.0250159912:
13: 208032: loss: 0.0249080177:
13: 211232: loss: 0.0249580315:
13: 214432: loss: 0.0251152557:
13: 217632: loss: 0.0250556783:
13: 220832: loss: 0.0250656020:
13: 224032: loss: 0.0252478337:
13: 227232: loss: 0.0251607657:
13: 230432: loss: 0.0252391966:
13: 233632: loss: 0.0251699887:
13: 236832: loss: 0.0251514848:
13: 240032: loss: 0.0251201673:
13: 243232: loss: 0.0250747964:
13: 246432: loss: 0.0251154662:
13: 249632: loss: 0.0250594154:
13: 252832: loss: 0.0250109294:
13: 256032: loss: 0.0249829643:
13: 259232: loss: 0.0248617512:
13: 262432: loss: 0.0249201737:
13: 265632: loss: 0.0249941643:
13: 268832: loss: 0.0249347050:
13: 272032: loss: 0.0249531735:
13: 275232: loss: 0.0249299823:
13: 278432: loss: 0.0248619472:
13: 281632: loss: 0.0248766012:
13: 284832: loss: 0.0248312065:
13: 288032: loss: 0.0248199559:
13: 291232: loss: 0.0248516971:
13: 294432: loss: 0.0248899179:
13: 297632: loss: 0.0248597165:
13: 300832: loss: 0.0248279814:
13: 304032: loss: 0.0247730660:
13: 307232: loss: 0.0247843596:
13: 310432: loss: 0.0247866192:
13: 313632: loss: 0.0247329260:
13: 316832: loss: 0.0246919680:
13: 320032: loss: 0.0247434821:
13: 323232: loss: 0.0247462914:
13: 326432: loss: 0.0247037654:
13: 329632: loss: 0.0246245863:
Dev-Acc: 13: Accuracy: 0.0781671703: precision: 0.0572103105: recall: 0.9559598708: f1: 0.1079596735
Train-Acc: 13: Accuracy: 0.9936684370: precision: 0.9998738361: recall: 0.9870718645: f1: 0.9934316085
14: 3232: loss: 0.0204985181:
14: 6432: loss: 0.0227039901:
14: 9632: loss: 0.0250935204:
14: 12832: loss: 0.0253171162:
14: 16032: loss: 0.0238944640:
14: 19232: loss: 0.0240197920:
14: 22432: loss: 0.0245991331:
14: 25632: loss: 0.0239903810:
14: 28832: loss: 0.0240392837:
14: 32032: loss: 0.0248657748:
14: 35232: loss: 0.0248850282:
14: 38432: loss: 0.0247408262:
14: 41632: loss: 0.0247074476:
14: 44832: loss: 0.0245683491:
14: 48032: loss: 0.0246983475:
14: 51232: loss: 0.0249454707:
14: 54432: loss: 0.0250668000:
14: 57632: loss: 0.0248694191:
14: 60832: loss: 0.0250564777:
14: 64032: loss: 0.0249195609:
14: 67232: loss: 0.0244009511:
14: 70432: loss: 0.0245482022:
14: 73632: loss: 0.0244656264:
14: 76832: loss: 0.0244849664:
14: 80032: loss: 0.0245727524:
14: 83232: loss: 0.0244182249:
14: 86432: loss: 0.0243927017:
14: 89632: loss: 0.0245461375:
14: 92832: loss: 0.0246615965:
14: 96032: loss: 0.0247719522:
14: 99232: loss: 0.0247514974:
14: 102432: loss: 0.0248206872:
14: 105632: loss: 0.0248213714:
14: 108832: loss: 0.0248197521:
14: 112032: loss: 0.0249093158:
14: 115232: loss: 0.0249039601:
14: 118432: loss: 0.0249400037:
14: 121632: loss: 0.0249255006:
14: 124832: loss: 0.0250720322:
14: 128032: loss: 0.0248720602:
14: 131232: loss: 0.0246187881:
14: 134432: loss: 0.0245480437:
14: 137632: loss: 0.0245943443:
14: 140832: loss: 0.0247063088:
14: 144032: loss: 0.0247793321:
14: 147232: loss: 0.0246231438:
14: 150432: loss: 0.0245237702:
14: 153632: loss: 0.0244692046:
14: 156832: loss: 0.0244871161:
14: 160032: loss: 0.0245262181:
14: 163232: loss: 0.0244355289:
14: 166432: loss: 0.0244657854:
14: 169632: loss: 0.0243429457:
14: 172832: loss: 0.0242467875:
14: 176032: loss: 0.0241353154:
14: 179232: loss: 0.0239940971:
14: 182432: loss: 0.0239284582:
14: 185632: loss: 0.0238749794:
14: 188832: loss: 0.0239061421:
14: 192032: loss: 0.0238158016:
14: 195232: loss: 0.0238004457:
14: 198432: loss: 0.0239218297:
14: 201632: loss: 0.0238726727:
14: 204832: loss: 0.0237321977:
14: 208032: loss: 0.0237480121:
14: 211232: loss: 0.0237872641:
14: 214432: loss: 0.0237780223:
14: 217632: loss: 0.0236476995:
14: 220832: loss: 0.0236135562:
14: 224032: loss: 0.0235773528:
14: 227232: loss: 0.0235851032:
14: 230432: loss: 0.0235282834:
14: 233632: loss: 0.0234888756:
14: 236832: loss: 0.0234624069:
14: 240032: loss: 0.0234904204:
14: 243232: loss: 0.0234237809:
14: 246432: loss: 0.0234005913:
14: 249632: loss: 0.0234351858:
14: 252832: loss: 0.0233876146:
14: 256032: loss: 0.0234129000:
14: 259232: loss: 0.0233751914:
14: 262432: loss: 0.0233796336:
14: 265632: loss: 0.0233696043:
14: 268832: loss: 0.0233944422:
14: 272032: loss: 0.0233569688:
14: 275232: loss: 0.0232860280:
14: 278432: loss: 0.0232905260:
14: 281632: loss: 0.0232884443:
14: 284832: loss: 0.0232507529:
14: 288032: loss: 0.0232437018:
14: 291232: loss: 0.0232357593:
14: 294432: loss: 0.0232535098:
14: 297632: loss: 0.0233083081:
14: 300832: loss: 0.0232331760:
14: 304032: loss: 0.0232777799:
14: 307232: loss: 0.0232177537:
14: 310432: loss: 0.0232164893:
14: 313632: loss: 0.0232186007:
14: 316832: loss: 0.0231924858:
14: 320032: loss: 0.0231026406:
14: 323232: loss: 0.0231228749:
14: 326432: loss: 0.0230970498:
14: 329632: loss: 0.0230778225:
Dev-Acc: 14: Accuracy: 0.0761827230: precision: 0.0572829430: recall: 0.9595306921: f1: 0.1081117327
Train-Acc: 14: Accuracy: 0.9942876697: precision: 0.9998236165: recall: 0.9883983061: f1: 0.9940781336
15: 3232: loss: 0.0218926051:
15: 6432: loss: 0.0206771113:
15: 9632: loss: 0.0209889893:
15: 12832: loss: 0.0214552629:
15: 16032: loss: 0.0206215071:
15: 19232: loss: 0.0220788648:
15: 22432: loss: 0.0217799552:
15: 25632: loss: 0.0218981020:
15: 28832: loss: 0.0214133161:
15: 32032: loss: 0.0216303734:
15: 35232: loss: 0.0218534612:
15: 38432: loss: 0.0214291885:
15: 41632: loss: 0.0216658805:
15: 44832: loss: 0.0213600952:
15: 48032: loss: 0.0217201238:
15: 51232: loss: 0.0219990419:
15: 54432: loss: 0.0222025750:
15: 57632: loss: 0.0221609612:
15: 60832: loss: 0.0222118284:
15: 64032: loss: 0.0221235279:
15: 67232: loss: 0.0225407477:
15: 70432: loss: 0.0224207498:
15: 73632: loss: 0.0222824793:
15: 76832: loss: 0.0223632678:
15: 80032: loss: 0.0223841668:
15: 83232: loss: 0.0221242394:
15: 86432: loss: 0.0222098350:
15: 89632: loss: 0.0222870435:
15: 92832: loss: 0.0223555272:
15: 96032: loss: 0.0222550932:
15: 99232: loss: 0.0222661622:
15: 102432: loss: 0.0221515319:
15: 105632: loss: 0.0219693792:
15: 108832: loss: 0.0217513751:
15: 112032: loss: 0.0216274843:
15: 115232: loss: 0.0216784200:
15: 118432: loss: 0.0219555445:
15: 121632: loss: 0.0219244333:
15: 124832: loss: 0.0220418311:
15: 128032: loss: 0.0221862579:
15: 131232: loss: 0.0220507371:
15: 134432: loss: 0.0220972843:
15: 137632: loss: 0.0221322220:
15: 140832: loss: 0.0220983835:
15: 144032: loss: 0.0221090165:
15: 147232: loss: 0.0221193066:
15: 150432: loss: 0.0219970882:
15: 153632: loss: 0.0220962352:
15: 156832: loss: 0.0222422923:
15: 160032: loss: 0.0222591782:
15: 163232: loss: 0.0221977305:
15: 166432: loss: 0.0222082999:
15: 169632: loss: 0.0220959257:
15: 172832: loss: 0.0221310825:
15: 176032: loss: 0.0222276140:
15: 179232: loss: 0.0222059026:
15: 182432: loss: 0.0221615988:
15: 185632: loss: 0.0220972233:
15: 188832: loss: 0.0221109413:
15: 192032: loss: 0.0221268272:
15: 195232: loss: 0.0220660632:
15: 198432: loss: 0.0220677487:
15: 201632: loss: 0.0221104446:
15: 204832: loss: 0.0221021381:
15: 208032: loss: 0.0220861715:
15: 211232: loss: 0.0221277592:
15: 214432: loss: 0.0221000630:
15: 217632: loss: 0.0222050726:
15: 220832: loss: 0.0222624088:
15: 224032: loss: 0.0222678892:
15: 227232: loss: 0.0222517509:
15: 230432: loss: 0.0222782026:
15: 233632: loss: 0.0222082452:
15: 236832: loss: 0.0221906146:
15: 240032: loss: 0.0221909756:
15: 243232: loss: 0.0221387627:
15: 246432: loss: 0.0220879647:
15: 249632: loss: 0.0220957548:
15: 252832: loss: 0.0220419304:
15: 256032: loss: 0.0219860691:
15: 259232: loss: 0.0220394073:
15: 262432: loss: 0.0220337572:
15: 265632: loss: 0.0220391144:
15: 268832: loss: 0.0220462276:
15: 272032: loss: 0.0220832140:
15: 275232: loss: 0.0220972572:
15: 278432: loss: 0.0220755692:
15: 281632: loss: 0.0220963530:
15: 284832: loss: 0.0220815391:
15: 288032: loss: 0.0220570477:
15: 291232: loss: 0.0219933804:
15: 294432: loss: 0.0220017516:
15: 297632: loss: 0.0220148820:
15: 300832: loss: 0.0220002058:
15: 304032: loss: 0.0220220723:
15: 307232: loss: 0.0219588451:
15: 310432: loss: 0.0218726634:
15: 313632: loss: 0.0217943790:
15: 316832: loss: 0.0217833236:
15: 320032: loss: 0.0217386860:
15: 323232: loss: 0.0217651968:
15: 326432: loss: 0.0218147122:
15: 329632: loss: 0.0217457955:
Dev-Acc: 15: Accuracy: 0.0755179375: precision: 0.0573607562: recall: 0.9617412005: f1: 0.1082643442
Train-Acc: 15: Accuracy: 0.9946532249: precision: 0.9998426287: recall: 0.9891331424: f1: 0.9944590533
16: 3232: loss: 0.0258809716:
16: 6432: loss: 0.0195937110:
16: 9632: loss: 0.0209225810:
16: 12832: loss: 0.0216950243:
16: 16032: loss: 0.0220900284:
16: 19232: loss: 0.0222177166:
16: 22432: loss: 0.0217649465:
16: 25632: loss: 0.0206712484:
16: 28832: loss: 0.0205956227:
16: 32032: loss: 0.0206485096:
16: 35232: loss: 0.0204242937:
16: 38432: loss: 0.0203941543:
16: 41632: loss: 0.0211798564:
16: 44832: loss: 0.0212281966:
16: 48032: loss: 0.0209265774:
16: 51232: loss: 0.0208595356:
16: 54432: loss: 0.0210474897:
16: 57632: loss: 0.0209970354:
16: 60832: loss: 0.0210960628:
16: 64032: loss: 0.0211882930:
16: 67232: loss: 0.0212785945:
16: 70432: loss: 0.0211866018:
16: 73632: loss: 0.0209741655:
16: 76832: loss: 0.0210196905:
16: 80032: loss: 0.0209607907:
16: 83232: loss: 0.0212517013:
16: 86432: loss: 0.0211794639:
16: 89632: loss: 0.0209469425:
16: 92832: loss: 0.0209748088:
16: 96032: loss: 0.0210583335:
16: 99232: loss: 0.0211435130:
16: 102432: loss: 0.0210908274:
16: 105632: loss: 0.0212066487:
16: 108832: loss: 0.0212804214:
16: 112032: loss: 0.0213520492:
16: 115232: loss: 0.0214342960:
16: 118432: loss: 0.0212999855:
16: 121632: loss: 0.0213642876:
16: 124832: loss: 0.0213861686:
16: 128032: loss: 0.0215029139:
16: 131232: loss: 0.0215036361:
16: 134432: loss: 0.0213741973:
16: 137632: loss: 0.0213083420:
16: 140832: loss: 0.0212626058:
16: 144032: loss: 0.0212311582:
16: 147232: loss: 0.0211278722:
16: 150432: loss: 0.0210866249:
16: 153632: loss: 0.0211476464:
16: 156832: loss: 0.0210767213:
16: 160032: loss: 0.0212301947:
16: 163232: loss: 0.0212898703:
16: 166432: loss: 0.0212760851:
16: 169632: loss: 0.0213861071:
16: 172832: loss: 0.0214150389:
16: 176032: loss: 0.0212728193:
16: 179232: loss: 0.0212901088:
16: 182432: loss: 0.0210987180:
16: 185632: loss: 0.0212119913:
16: 188832: loss: 0.0212619816:
16: 192032: loss: 0.0213323256:
16: 195232: loss: 0.0212588076:
16: 198432: loss: 0.0212158818:
16: 201632: loss: 0.0212518366:
16: 204832: loss: 0.0212756483:
16: 208032: loss: 0.0214017761:
16: 211232: loss: 0.0214456609:
16: 214432: loss: 0.0214238264:
16: 217632: loss: 0.0213667321:
16: 220832: loss: 0.0212373354:
16: 224032: loss: 0.0212490141:
16: 227232: loss: 0.0212521463:
16: 230432: loss: 0.0212379549:
16: 233632: loss: 0.0212063971:
16: 236832: loss: 0.0212211740:
16: 240032: loss: 0.0212241587:
16: 243232: loss: 0.0211396389:
16: 246432: loss: 0.0211275021:
16: 249632: loss: 0.0210917020:
16: 252832: loss: 0.0210335434:
16: 256032: loss: 0.0209607085:
16: 259232: loss: 0.0209319369:
16: 262432: loss: 0.0209287954:
16: 265632: loss: 0.0209578469:
16: 268832: loss: 0.0209579342:
16: 272032: loss: 0.0209336371:
16: 275232: loss: 0.0209382139:
16: 278432: loss: 0.0209070759:
16: 281632: loss: 0.0208935493:
16: 284832: loss: 0.0209044911:
16: 288032: loss: 0.0209769016:
16: 291232: loss: 0.0210428512:
16: 294432: loss: 0.0210732522:
16: 297632: loss: 0.0210524780:
16: 300832: loss: 0.0211022873:
16: 304032: loss: 0.0210818240:
16: 307232: loss: 0.0210571121:
16: 310432: loss: 0.0210240473:
16: 313632: loss: 0.0209978670:
16: 316832: loss: 0.0209212449:
16: 320032: loss: 0.0209024319:
16: 323232: loss: 0.0209087904:
16: 326432: loss: 0.0208557720:
16: 329632: loss: 0.0208859308:
Dev-Acc: 16: Accuracy: 0.0743570402: precision: 0.0575975058: recall: 0.9675225302: f1: 0.1087226521
Train-Acc: 16: Accuracy: 0.9951516390: precision: 0.9997422308: recall: 0.9902603064: f1: 0.9949786789
17: 3232: loss: 0.0184468358:
17: 6432: loss: 0.0213525578:
17: 9632: loss: 0.0213448771:
17: 12832: loss: 0.0211442639:
17: 16032: loss: 0.0205313386:
17: 19232: loss: 0.0197746515:
17: 22432: loss: 0.0193285426:
17: 25632: loss: 0.0192163678:
17: 28832: loss: 0.0195546150:
17: 32032: loss: 0.0198380528:
17: 35232: loss: 0.0197111073:
17: 38432: loss: 0.0201380850:
17: 41632: loss: 0.0202956802:
17: 44832: loss: 0.0204356266:
17: 48032: loss: 0.0205182449:
17: 51232: loss: 0.0207176606:
17: 54432: loss: 0.0203724866:
17: 57632: loss: 0.0201982248:
17: 60832: loss: 0.0206323113:
17: 64032: loss: 0.0205157525:
17: 67232: loss: 0.0204027380:
17: 70432: loss: 0.0204822414:
17: 73632: loss: 0.0205486944:
17: 76832: loss: 0.0207240336:
17: 80032: loss: 0.0207641528:
17: 83232: loss: 0.0205588248:
17: 86432: loss: 0.0203253379:
17: 89632: loss: 0.0203058239:
17: 92832: loss: 0.0200215748:
17: 96032: loss: 0.0199801168:
17: 99232: loss: 0.0198110026:
17: 102432: loss: 0.0197420461:
17: 105632: loss: 0.0199884887:
17: 108832: loss: 0.0199020106:
17: 112032: loss: 0.0199019487:
17: 115232: loss: 0.0198694675:
17: 118432: loss: 0.0199592119:
17: 121632: loss: 0.0199615571:
17: 124832: loss: 0.0199681684:
17: 128032: loss: 0.0201228597:
17: 131232: loss: 0.0201099516:
17: 134432: loss: 0.0199887395:
17: 137632: loss: 0.0200189072:
17: 140832: loss: 0.0199222518:
17: 144032: loss: 0.0198095067:
17: 147232: loss: 0.0199032938:
17: 150432: loss: 0.0198682426:
17: 153632: loss: 0.0197751238:
17: 156832: loss: 0.0197393232:
17: 160032: loss: 0.0198433446:
17: 163232: loss: 0.0198547265:
17: 166432: loss: 0.0197046723:
17: 169632: loss: 0.0196879171:
17: 172832: loss: 0.0196631657:
17: 176032: loss: 0.0196341781:
17: 179232: loss: 0.0197297999:
17: 182432: loss: 0.0197231010:
17: 185632: loss: 0.0197159906:
17: 188832: loss: 0.0196430324:
17: 192032: loss: 0.0196196485:
17: 195232: loss: 0.0195767967:
17: 198432: loss: 0.0195807085:
17: 201632: loss: 0.0196238435:
17: 204832: loss: 0.0195780926:
17: 208032: loss: 0.0196174829:
17: 211232: loss: 0.0195754907:
17: 214432: loss: 0.0195370712:
17: 217632: loss: 0.0194669645:
17: 220832: loss: 0.0194846086:
17: 224032: loss: 0.0195218291:
17: 227232: loss: 0.0195194836:
17: 230432: loss: 0.0195120668:
17: 233632: loss: 0.0195362179:
17: 236832: loss: 0.0195759645:
17: 240032: loss: 0.0195471189:
17: 243232: loss: 0.0195502732:
17: 246432: loss: 0.0195613212:
17: 249632: loss: 0.0195408592:
17: 252832: loss: 0.0195297930:
17: 256032: loss: 0.0195407925:
17: 259232: loss: 0.0196311004:
17: 262432: loss: 0.0195324417:
17: 265632: loss: 0.0195265483:
17: 268832: loss: 0.0195556724:
17: 272032: loss: 0.0195510444:
17: 275232: loss: 0.0195307712:
17: 278432: loss: 0.0195036602:
17: 281632: loss: 0.0194986072:
17: 284832: loss: 0.0195367863:
17: 288032: loss: 0.0195505526:
17: 291232: loss: 0.0195765320:
17: 294432: loss: 0.0195766241:
17: 297632: loss: 0.0195760980:
17: 300832: loss: 0.0195965070:
17: 304032: loss: 0.0195897332:
17: 307232: loss: 0.0195319266:
17: 310432: loss: 0.0195361625:
17: 313632: loss: 0.0195462463:
17: 316832: loss: 0.0195504193:
17: 320032: loss: 0.0195717720:
17: 323232: loss: 0.0195807332:
17: 326432: loss: 0.0196000807:
17: 329632: loss: 0.0196250784:
Dev-Acc: 17: Accuracy: 0.0742280483: precision: 0.0575451454: recall: 0.9666723346: f1: 0.1086240005
Train-Acc: 17: Accuracy: 0.9951395392: precision: 0.9997862160: recall: 0.9901918047: f1: 0.9949658813
18: 3232: loss: 0.0206092897:
18: 6432: loss: 0.0184794785:
18: 9632: loss: 0.0199489085:
18: 12832: loss: 0.0196551459:
18: 16032: loss: 0.0184001030:
18: 19232: loss: 0.0180939655:
18: 22432: loss: 0.0183496355:
18: 25632: loss: 0.0186618954:
18: 28832: loss: 0.0185649902:
18: 32032: loss: 0.0180573325:
18: 35232: loss: 0.0172417828:
18: 38432: loss: 0.0175837797:
18: 41632: loss: 0.0175622825:
18: 44832: loss: 0.0172961328:
18: 48032: loss: 0.0176575873:
18: 51232: loss: 0.0177021759:
18: 54432: loss: 0.0177561757:
18: 57632: loss: 0.0176598955:
18: 60832: loss: 0.0176187659:
18: 64032: loss: 0.0177009179:
18: 67232: loss: 0.0176865912:
18: 70432: loss: 0.0177375203:
18: 73632: loss: 0.0180785400:
18: 76832: loss: 0.0179565751:
18: 80032: loss: 0.0179688992:
18: 83232: loss: 0.0181267424:
18: 86432: loss: 0.0181644739:
18: 89632: loss: 0.0181178686:
18: 92832: loss: 0.0182897551:
18: 96032: loss: 0.0183105200:
18: 99232: loss: 0.0181671574:
18: 102432: loss: 0.0181660888:
18: 105632: loss: 0.0181321997:
18: 108832: loss: 0.0182155701:
18: 112032: loss: 0.0182716789:
18: 115232: loss: 0.0182982949:
18: 118432: loss: 0.0183905057:
18: 121632: loss: 0.0185410228:
18: 124832: loss: 0.0184710397:
18: 128032: loss: 0.0184403958:
18: 131232: loss: 0.0185112591:
18: 134432: loss: 0.0187686099:
18: 137632: loss: 0.0187896196:
18: 140832: loss: 0.0187448752:
18: 144032: loss: 0.0187906097:
18: 147232: loss: 0.0186838389:
18: 150432: loss: 0.0186928814:
18: 153632: loss: 0.0185471610:
18: 156832: loss: 0.0185964238:
18: 160032: loss: 0.0185844068:
18: 163232: loss: 0.0185910941:
18: 166432: loss: 0.0186549823:
18: 169632: loss: 0.0187054778:
18: 172832: loss: 0.0186353921:
18: 176032: loss: 0.0187062610:
18: 179232: loss: 0.0186942984:
18: 182432: loss: 0.0187844996:
18: 185632: loss: 0.0186698063:
18: 188832: loss: 0.0186302936:
18: 192032: loss: 0.0186816734:
18: 195232: loss: 0.0186369667:
18: 198432: loss: 0.0185750069:
18: 201632: loss: 0.0185856534:
18: 204832: loss: 0.0186479335:
18: 208032: loss: 0.0188008144:
18: 211232: loss: 0.0187635632:
18: 214432: loss: 0.0187963339:
18: 217632: loss: 0.0187751271:
18: 220832: loss: 0.0186606816:
18: 224032: loss: 0.0186904261:
18: 227232: loss: 0.0187600628:
18: 230432: loss: 0.0186744745:
18: 233632: loss: 0.0186896343:
18: 236832: loss: 0.0186858275:
18: 240032: loss: 0.0186596709:
18: 243232: loss: 0.0186486815:
18: 246432: loss: 0.0186354023:
18: 249632: loss: 0.0186908781:
18: 252832: loss: 0.0186322394:
18: 256032: loss: 0.0185545847:
18: 259232: loss: 0.0185668014:
18: 262432: loss: 0.0186643600:
18: 265632: loss: 0.0186809842:
18: 268832: loss: 0.0187268380:
18: 272032: loss: 0.0187198254:
18: 275232: loss: 0.0187106552:
18: 278432: loss: 0.0187052832:
18: 281632: loss: 0.0186788380:
18: 284832: loss: 0.0186424448:
18: 288032: loss: 0.0187046099:
18: 291232: loss: 0.0187037359:
18: 294432: loss: 0.0186905163:
18: 297632: loss: 0.0187206775:
18: 300832: loss: 0.0187410858:
18: 304032: loss: 0.0187372505:
18: 307232: loss: 0.0187877239:
18: 310432: loss: 0.0187420514:
18: 313632: loss: 0.0186968516:
18: 316832: loss: 0.0186565955:
18: 320032: loss: 0.0186712427:
18: 323232: loss: 0.0186532677:
18: 326432: loss: 0.0186689172:
18: 329632: loss: 0.0186756043:
Dev-Acc: 18: Accuracy: 0.0739799961: precision: 0.0576022179: recall: 0.9680326475: f1: 0.1087342667
Train-Acc: 18: Accuracy: 0.9953691363: precision: 0.9998428632: recall: 0.9906090422: f1: 0.9952045346
19: 3232: loss: 0.0191828965:
19: 6432: loss: 0.0187837639:
19: 9632: loss: 0.0186509210:
19: 12832: loss: 0.0188596913:
19: 16032: loss: 0.0194043390:
19: 19232: loss: 0.0204280968:
19: 22432: loss: 0.0200288517:
19: 25632: loss: 0.0197791133:
19: 28832: loss: 0.0197145317:
19: 32032: loss: 0.0192313613:
19: 35232: loss: 0.0193317359:
19: 38432: loss: 0.0195954557:
19: 41632: loss: 0.0194304394:
19: 44832: loss: 0.0193071525:
19: 48032: loss: 0.0190278188:
19: 51232: loss: 0.0189831532:
19: 54432: loss: 0.0190529459:
19: 57632: loss: 0.0190943909:
19: 60832: loss: 0.0188246843:
19: 64032: loss: 0.0188963415:
19: 67232: loss: 0.0187199220:
19: 70432: loss: 0.0186959678:
19: 73632: loss: 0.0186018462:
19: 76832: loss: 0.0186800114:
19: 80032: loss: 0.0186798476:
19: 83232: loss: 0.0188298622:
19: 86432: loss: 0.0187612843:
19: 89632: loss: 0.0187334919:
19: 92832: loss: 0.0187434897:
19: 96032: loss: 0.0188480189:
19: 99232: loss: 0.0186949073:
19: 102432: loss: 0.0186374404:
19: 105632: loss: 0.0185616312:
19: 108832: loss: 0.0187675778:
19: 112032: loss: 0.0188295922:
19: 115232: loss: 0.0187759203:
19: 118432: loss: 0.0187397333:
19: 121632: loss: 0.0185063881:
19: 124832: loss: 0.0184789130:
19: 128032: loss: 0.0184256673:
19: 131232: loss: 0.0183609376:
19: 134432: loss: 0.0182636605:
19: 137632: loss: 0.0183220410:
19: 140832: loss: 0.0182585419:
19: 144032: loss: 0.0181116492:
19: 147232: loss: 0.0181792078:
19: 150432: loss: 0.0180909815:
19: 153632: loss: 0.0180660914:
19: 156832: loss: 0.0181470664:
19: 160032: loss: 0.0182365531:
19: 163232: loss: 0.0182316134:
19: 166432: loss: 0.0182180575:
19: 169632: loss: 0.0183748011:
19: 172832: loss: 0.0183129724:
19: 176032: loss: 0.0183817389:
19: 179232: loss: 0.0184227269:
19: 182432: loss: 0.0183695081:
19: 185632: loss: 0.0183599250:
19: 188832: loss: 0.0183489788:
19: 192032: loss: 0.0183165518:
19: 195232: loss: 0.0183046641:
19: 198432: loss: 0.0183491389:
19: 201632: loss: 0.0183195450:
19: 204832: loss: 0.0182522266:
19: 208032: loss: 0.0182531544:
19: 211232: loss: 0.0182873700:
19: 214432: loss: 0.0182745210:
19: 217632: loss: 0.0182084394:
19: 220832: loss: 0.0181395544:
19: 224032: loss: 0.0181942533:
19: 227232: loss: 0.0181584630:
19: 230432: loss: 0.0181159251:
19: 233632: loss: 0.0180954000:
19: 236832: loss: 0.0181076178:
19: 240032: loss: 0.0180644211:
19: 243232: loss: 0.0180104657:
19: 246432: loss: 0.0180147145:
19: 249632: loss: 0.0179727923:
19: 252832: loss: 0.0178769507:
19: 256032: loss: 0.0178320450:
19: 259232: loss: 0.0178024657:
19: 262432: loss: 0.0178896528:
19: 265632: loss: 0.0178512824:
19: 268832: loss: 0.0177939372:
19: 272032: loss: 0.0177865915:
19: 275232: loss: 0.0178108894:
19: 278432: loss: 0.0178209957:
19: 281632: loss: 0.0178551773:
19: 284832: loss: 0.0179519814:
19: 288032: loss: 0.0179646370:
19: 291232: loss: 0.0179658118:
19: 294432: loss: 0.0178969361:
19: 297632: loss: 0.0179257533:
19: 300832: loss: 0.0179123648:
19: 304032: loss: 0.0178725790:
19: 307232: loss: 0.0178427005:
19: 310432: loss: 0.0177531896:
19: 313632: loss: 0.0177544504:
19: 316832: loss: 0.0177898699:
19: 320032: loss: 0.0177792712:
19: 323232: loss: 0.0177886832:
19: 326432: loss: 0.0177530619:
19: 329632: loss: 0.0177491997:
Dev-Acc: 19: Accuracy: 0.0730076209: precision: 0.0577061041: recall: 0.9710933515: f1: 0.1089386541
Train-Acc: 19: Accuracy: 0.9957225919: precision: 0.9998304212: recall: 0.9913501059: f1: 0.9955722050
20: 3232: loss: 0.0156406119:
20: 6432: loss: 0.0168155453:
20: 9632: loss: 0.0181739745:
20: 12832: loss: 0.0175891204:
20: 16032: loss: 0.0169744875:
20: 19232: loss: 0.0168373074:
20: 22432: loss: 0.0173917247:
20: 25632: loss: 0.0173664424:
20: 28832: loss: 0.0182474164:
20: 32032: loss: 0.0179188023:
20: 35232: loss: 0.0182754430:
20: 38432: loss: 0.0185529628:
20: 41632: loss: 0.0186928457:
20: 44832: loss: 0.0181269950:
20: 48032: loss: 0.0181762337:
20: 51232: loss: 0.0178370539:
20: 54432: loss: 0.0176345460:
20: 57632: loss: 0.0174637753:
20: 60832: loss: 0.0173027355:
20: 64032: loss: 0.0173671419:
20: 67232: loss: 0.0171431657:
20: 70432: loss: 0.0172514057:
20: 73632: loss: 0.0171103161:
20: 76832: loss: 0.0169048501:
20: 80032: loss: 0.0168670659:
20: 83232: loss: 0.0167414378:
20: 86432: loss: 0.0165337961:
20: 89632: loss: 0.0166754289:
20: 92832: loss: 0.0165844354:
20: 96032: loss: 0.0164722730:
20: 99232: loss: 0.0164749146:
20: 102432: loss: 0.0164836552:
20: 105632: loss: 0.0165376269:
20: 108832: loss: 0.0166264200:
20: 112032: loss: 0.0166199019:
20: 115232: loss: 0.0166153738:
20: 118432: loss: 0.0166967317:
20: 121632: loss: 0.0165540773:
20: 124832: loss: 0.0165477189:
20: 128032: loss: 0.0166568055:
20: 131232: loss: 0.0168366756:
20: 134432: loss: 0.0168451756:
20: 137632: loss: 0.0168375710:
20: 140832: loss: 0.0168646384:
20: 144032: loss: 0.0168587926:
20: 147232: loss: 0.0168378499:
20: 150432: loss: 0.0168559638:
20: 153632: loss: 0.0170031207:
20: 156832: loss: 0.0170049378:
20: 160032: loss: 0.0170031820:
20: 163232: loss: 0.0170272051:
20: 166432: loss: 0.0169667578:
20: 169632: loss: 0.0170033223:
20: 172832: loss: 0.0168883378:
20: 176032: loss: 0.0168707750:
20: 179232: loss: 0.0167846953:
20: 182432: loss: 0.0167642928:
20: 185632: loss: 0.0167962574:
20: 188832: loss: 0.0167558749:
20: 192032: loss: 0.0167920630:
20: 195232: loss: 0.0167492236:
20: 198432: loss: 0.0168489891:
20: 201632: loss: 0.0167548132:
20: 204832: loss: 0.0167596646:
20: 208032: loss: 0.0166711394:
20: 211232: loss: 0.0166235911:
20: 214432: loss: 0.0165439198:
20: 217632: loss: 0.0166166744:
20: 220832: loss: 0.0166297243:
20: 224032: loss: 0.0167079086:
20: 227232: loss: 0.0167755562:
20: 230432: loss: 0.0168608935:
20: 233632: loss: 0.0168611525:
20: 236832: loss: 0.0169771952:
20: 240032: loss: 0.0169674498:
20: 243232: loss: 0.0170132273:
20: 246432: loss: 0.0169880511:
20: 249632: loss: 0.0170200067:
20: 252832: loss: 0.0170440020:
20: 256032: loss: 0.0171299280:
20: 259232: loss: 0.0171399766:
20: 262432: loss: 0.0171412408:
20: 265632: loss: 0.0171711684:
20: 268832: loss: 0.0171563685:
20: 272032: loss: 0.0171456432:
20: 275232: loss: 0.0172152925:
20: 278432: loss: 0.0172193742:
20: 281632: loss: 0.0172314340:
20: 284832: loss: 0.0171928213:
20: 288032: loss: 0.0171947604:
20: 291232: loss: 0.0171600576:
20: 294432: loss: 0.0170951682:
20: 297632: loss: 0.0170998166:
20: 300832: loss: 0.0170679616:
20: 304032: loss: 0.0170380127:
20: 307232: loss: 0.0170586382:
20: 310432: loss: 0.0170332899:
20: 313632: loss: 0.0170194875:
20: 316832: loss: 0.0170032275:
20: 320032: loss: 0.0170156198:
20: 323232: loss: 0.0169683389:
20: 326432: loss: 0.0169474223:
20: 329632: loss: 0.0169317003:
Dev-Acc: 20: Accuracy: 0.0720352456: precision: 0.0576758083: recall: 0.9716034688: f1: 0.1088878726
Train-Acc: 20: Accuracy: 0.9959249496: precision: 0.9998681509: recall: 0.9917299788: f1: 0.9957824376
