1: 6464: loss: 0.7100279701:
1: 12864: loss: 0.7096620494:
1: 19264: loss: 0.7098836211:
1: 25664: loss: 0.7098582672:
1: 32064: loss: 0.7095823038:
1: 38464: loss: 0.7097231104:
1: 44864: loss: 0.7096137567:
1: 51264: loss: 0.7093812502:
1: 57664: loss: 0.7092319168:
1: 64064: loss: 0.7091310694:
1: 70464: loss: 0.7091336514:
1: 76864: loss: 0.7090976625:
1: 83264: loss: 0.7089944206:
1: 89664: loss: 0.7088795895:
1: 96064: loss: 0.7087773816:
1: 102464: loss: 0.7085623071:
1: 108864: loss: 0.7084063684:
1: 115264: loss: 0.7082988046:
1: 121664: loss: 0.7081981244:
1: 128064: loss: 0.7080065243:
1: 134464: loss: 0.7078968239:
Dev-Acc: 1: Accuracy: 0.3009604812: precision: 0.0640570356: recall: 0.8066655331: f1: 0.1186890168
Train-Acc: 1: Accuracy: 0.3399805725: precision: 0.1230726016: recall: 0.8065215962: f1: 0.2135570797
2: 6464: loss: 0.7055156910:
2: 12864: loss: 0.7058699030:
2: 19264: loss: 0.7053386792:
2: 25664: loss: 0.7050555117:
2: 32064: loss: 0.7050834936:
2: 38464: loss: 0.7050623249:
2: 44864: loss: 0.7049458138:
2: 51264: loss: 0.7047879575:
2: 57664: loss: 0.7046331710:
2: 64064: loss: 0.7044065175:
2: 70464: loss: 0.7043166054:
2: 76864: loss: 0.7042622805:
2: 83264: loss: 0.7041652436:
2: 89664: loss: 0.7040232933:
2: 96064: loss: 0.7039422928:
2: 102464: loss: 0.7038852662:
2: 108864: loss: 0.7038468959:
2: 115264: loss: 0.7037832130:
2: 121664: loss: 0.7036940036:
2: 128064: loss: 0.7035947300:
2: 134464: loss: 0.7034217402:
Dev-Acc: 2: Accuracy: 0.3584001362: precision: 0.0651447002: recall: 0.7486821969: f1: 0.1198600770
Train-Acc: 2: Accuracy: 0.3919166625: precision: 0.1257577258: recall: 0.7514956282: f1: 0.2154597203
3: 6464: loss: 0.7003425866:
3: 12864: loss: 0.7006479779:
3: 19264: loss: 0.7006514072:
3: 25664: loss: 0.7005332129:
3: 32064: loss: 0.7004244529:
3: 38464: loss: 0.7003622793:
3: 44864: loss: 0.7003357516:
3: 51264: loss: 0.7001422500:
3: 57664: loss: 0.7001468168:
3: 64064: loss: 0.7001274699:
3: 70464: loss: 0.7000313376:
3: 76864: loss: 0.6998830901:
3: 83264: loss: 0.6997475516:
3: 89664: loss: 0.6997078511:
3: 96064: loss: 0.6996510043:
3: 102464: loss: 0.6994892406:
3: 108864: loss: 0.6994042384:
3: 115264: loss: 0.6992817341:
3: 121664: loss: 0.6992226397:
3: 128064: loss: 0.6991160596:
3: 134464: loss: 0.6990140000:
Dev-Acc: 3: Accuracy: 0.4211481810: precision: 0.0647216967: recall: 0.6631525251: f1: 0.1179334432
Train-Acc: 3: Accuracy: 0.4465627968: precision: 0.1273324799: recall: 0.6800999277: f1: 0.2145041729
4: 6464: loss: 0.6959373194:
4: 12864: loss: 0.6963596565:
4: 19264: loss: 0.6966139217:
4: 25664: loss: 0.6965326762:
4: 32064: loss: 0.6963776314:
4: 38464: loss: 0.6960731218:
4: 44864: loss: 0.6961016980:
4: 51264: loss: 0.6960267373:
4: 57664: loss: 0.6959426049:
4: 64064: loss: 0.6958527578:
4: 70464: loss: 0.6957603609:
4: 76864: loss: 0.6957011567:
4: 83264: loss: 0.6955425288:
4: 89664: loss: 0.6953856471:
4: 96064: loss: 0.6952936351:
4: 102464: loss: 0.6952144663:
4: 108864: loss: 0.6951634655:
4: 115264: loss: 0.6950900673:
4: 121664: loss: 0.6949668294:
4: 128064: loss: 0.6948752657:
4: 134464: loss: 0.6947663304:
Dev-Acc: 4: Accuracy: 0.4855135679: precision: 0.0614876853: recall: 0.5480360483: f1: 0.1105698309
Train-Acc: 4: Accuracy: 0.5056501627: precision: 0.1288711571: recall: 0.5988429426: f1: 0.2120986332
5: 6464: loss: 0.6923194122:
5: 12864: loss: 0.6922725496:
5: 19264: loss: 0.6925245871:
5: 25664: loss: 0.6924245209:
5: 32064: loss: 0.6923369710:
5: 38464: loss: 0.6921361477:
5: 44864: loss: 0.6919213856:
5: 51264: loss: 0.6919956528:
5: 57664: loss: 0.6920057126:
5: 64064: loss: 0.6919338636:
5: 70464: loss: 0.6917939877:
5: 76864: loss: 0.6917059260:
5: 83264: loss: 0.6915201780:
5: 89664: loss: 0.6914045985:
5: 96064: loss: 0.6912735432:
5: 102464: loss: 0.6911891699:
5: 108864: loss: 0.6910836147:
5: 115264: loss: 0.6910098995:
5: 121664: loss: 0.6909531848:
5: 128064: loss: 0.6908454144:
5: 134464: loss: 0.6907198680:
Dev-Acc: 5: Accuracy: 0.5510795116: precision: 0.0605532856: recall: 0.4611460636: f1: 0.1070498145
Train-Acc: 5: Accuracy: 0.5638317466: precision: 0.1321462818: recall: 0.5254749852: f1: 0.2111840628
6: 6464: loss: 0.6885248911:
6: 12864: loss: 0.6883276033:
6: 19264: loss: 0.6881069507:
6: 25664: loss: 0.6881316368:
6: 32064: loss: 0.6880121408:
6: 38464: loss: 0.6879026283:
6: 44864: loss: 0.6879116953:
6: 51264: loss: 0.6877153380:
6: 57664: loss: 0.6875795898:
6: 64064: loss: 0.6874397557:
6: 70464: loss: 0.6873992935:
6: 76864: loss: 0.6873166498:
6: 83264: loss: 0.6872744078:
6: 89664: loss: 0.6871072544:
6: 96064: loss: 0.6870560868:
6: 102464: loss: 0.6869056410:
6: 108864: loss: 0.6868429265:
6: 115264: loss: 0.6867641251:
6: 121664: loss: 0.6867182435:
6: 128064: loss: 0.6866192618:
6: 134464: loss: 0.6864893738:
Dev-Acc: 6: Accuracy: 0.6130536795: precision: 0.0621182071: recall: 0.3994218670: f1: 0.1075155621
Train-Acc: 6: Accuracy: 0.6186166406: precision: 0.1379647750: recall: 0.4634803760: f1: 0.2126344045
7: 6464: loss: 0.6836920547:
7: 12864: loss: 0.6837090763:
7: 19264: loss: 0.6838715009:
7: 25664: loss: 0.6838717391:
7: 32064: loss: 0.6837707349:
7: 38464: loss: 0.6837440413:
7: 44864: loss: 0.6837356514:
7: 51264: loss: 0.6836083754:
7: 57664: loss: 0.6835659861:
7: 64064: loss: 0.6835064689:
7: 70464: loss: 0.6834317854:
7: 76864: loss: 0.6833683209:
7: 83264: loss: 0.6832771899:
7: 89664: loss: 0.6831583506:
7: 96064: loss: 0.6830129696:
7: 102464: loss: 0.6829248966:
7: 108864: loss: 0.6828200862:
7: 115264: loss: 0.6827361614:
7: 121664: loss: 0.6826832849:
7: 128064: loss: 0.6825237914:
7: 134464: loss: 0.6824582362:
Dev-Acc: 7: Accuracy: 0.6708406210: precision: 0.0638563073: recall: 0.3397381398: f1: 0.1075060533
Train-Acc: 7: Accuracy: 0.6686535478: precision: 0.1453441867: recall: 0.4061534416: f1: 0.2140790408
8: 6464: loss: 0.6801608998:
8: 12864: loss: 0.6804055732:
8: 19264: loss: 0.6800700249:
8: 25664: loss: 0.6798266263:
8: 32064: loss: 0.6800843097:
8: 38464: loss: 0.6799287903:
8: 44864: loss: 0.6798577804:
8: 51264: loss: 0.6797537967:
8: 57664: loss: 0.6796559656:
8: 64064: loss: 0.6794061786:
8: 70464: loss: 0.6793618253:
8: 76864: loss: 0.6792539103:
8: 83264: loss: 0.6791568881:
8: 89664: loss: 0.6790256040:
8: 96064: loss: 0.6788914419:
8: 102464: loss: 0.6788280560:
8: 108864: loss: 0.6787674229:
8: 115264: loss: 0.6786860890:
8: 121664: loss: 0.6785388176:
8: 128064: loss: 0.6784157557:
8: 134464: loss: 0.6783156243:
Dev-Acc: 8: Accuracy: 0.7215728760: precision: 0.0657111529: recall: 0.2853256249: f1: 0.1068211478
Train-Acc: 8: Accuracy: 0.7149723768: precision: 0.1551918899: recall: 0.3522450858: f1: 0.2154576162
9: 6464: loss: 0.6764260167:
9: 12864: loss: 0.6760401809:
9: 19264: loss: 0.6756247199:
9: 25664: loss: 0.6758177993:
9: 32064: loss: 0.6756513054:
9: 38464: loss: 0.6757366062:
9: 44864: loss: 0.6755781657:
9: 51264: loss: 0.6755219552:
9: 57664: loss: 0.6754392347:
9: 64064: loss: 0.6753640072:
9: 70464: loss: 0.6751424121:
9: 76864: loss: 0.6750560971:
9: 83264: loss: 0.6749322777:
9: 89664: loss: 0.6748214040:
9: 96064: loss: 0.6746863625:
9: 102464: loss: 0.6745925085:
9: 108864: loss: 0.6745086526:
9: 115264: loss: 0.6743538872:
9: 121664: loss: 0.6743248425:
9: 128064: loss: 0.6742934507:
9: 134464: loss: 0.6742332154:
Dev-Acc: 9: Accuracy: 0.7637918591: precision: 0.0670080680: recall: 0.2358442442: f1: 0.1043641836
Train-Acc: 9: Accuracy: 0.7538550496: precision: 0.1658109769: recall: 0.3014923411: f1: 0.2139541394
10: 6464: loss: 0.6717975605:
10: 12864: loss: 0.6716166249:
10: 19264: loss: 0.6712455821:
10: 25664: loss: 0.6713056836:
10: 32064: loss: 0.6713464261:
10: 38464: loss: 0.6712068100:
10: 44864: loss: 0.6711084986:
10: 51264: loss: 0.6712848140:
10: 57664: loss: 0.6711537688:
10: 64064: loss: 0.6710272241:
10: 70464: loss: 0.6709063620:
10: 76864: loss: 0.6708057978:
10: 83264: loss: 0.6706604075:
10: 89664: loss: 0.6705610374:
10: 96064: loss: 0.6704496688:
10: 102464: loss: 0.6703939427:
10: 108864: loss: 0.6703612063:
10: 115264: loss: 0.6703095521:
10: 121664: loss: 0.6702297032:
10: 128064: loss: 0.6701470442:
10: 134464: loss: 0.6701052533:
Dev-Acc: 10: Accuracy: 0.7974876761: precision: 0.0668932212: recall: 0.1907838803: f1: 0.0990553545
Train-Acc: 10: Accuracy: 0.7869743705: precision: 0.1801760499: recall: 0.2583656564: f1: 0.2123004619
11: 6464: loss: 0.6689407355:
11: 12864: loss: 0.6682205063:
11: 19264: loss: 0.6679926765:
11: 25664: loss: 0.6679277627:
11: 32064: loss: 0.6678577563:
11: 38464: loss: 0.6676280166:
11: 44864: loss: 0.6676199850:
11: 51264: loss: 0.6674390578:
11: 57664: loss: 0.6674614237:
11: 64064: loss: 0.6675246438:
11: 70464: loss: 0.6672691235:
11: 76864: loss: 0.6671802992:
11: 83264: loss: 0.6671729506:
11: 89664: loss: 0.6670347054:
11: 96064: loss: 0.6669684870:
11: 102464: loss: 0.6668274274:
11: 108864: loss: 0.6667257340:
11: 115264: loss: 0.6666455524:
11: 121664: loss: 0.6666004395:
11: 128064: loss: 0.6664612966:
11: 134464: loss: 0.6663286616:
Dev-Acc: 11: Accuracy: 0.8262124658: precision: 0.0684718101: recall: 0.1569460976: f1: 0.0953463148
Train-Acc: 11: Accuracy: 0.8127890229: precision: 0.1938043734: recall: 0.2167510354: f1: 0.2046364398
12: 6464: loss: 0.6640131372:
12: 12864: loss: 0.6638197303:
12: 19264: loss: 0.6637827156:
12: 25664: loss: 0.6636102431:
12: 32064: loss: 0.6635772512:
12: 38464: loss: 0.6634689296:
12: 44864: loss: 0.6634194873:
12: 51264: loss: 0.6632241067:
12: 57664: loss: 0.6630505778:
12: 64064: loss: 0.6629553038:
12: 70464: loss: 0.6629773989:
12: 76864: loss: 0.6628960658:
12: 83264: loss: 0.6627169937:
12: 89664: loss: 0.6626224634:
12: 96064: loss: 0.6625679260:
12: 102464: loss: 0.6624425070:
12: 108864: loss: 0.6623869972:
12: 115264: loss: 0.6623128896:
12: 121664: loss: 0.6622582206:
12: 128064: loss: 0.6621662729:
12: 134464: loss: 0.6621218887:
Dev-Acc: 12: Accuracy: 0.8513950706: precision: 0.0701323251: recall: 0.1261690189: f1: 0.0901524816
Train-Acc: 12: Accuracy: 0.8325554132: precision: 0.2074355083: recall: 0.1797383472: f1: 0.1925962453
13: 6464: loss: 0.6595102614:
13: 12864: loss: 0.6597950861:
13: 19264: loss: 0.6598576532:
13: 25664: loss: 0.6597074291:
13: 32064: loss: 0.6596160196:
13: 38464: loss: 0.6595423890:
13: 44864: loss: 0.6594480648:
13: 51264: loss: 0.6593798710:
13: 57664: loss: 0.6593181600:
13: 64064: loss: 0.6591288413:
13: 70464: loss: 0.6590733704:
13: 76864: loss: 0.6590783276:
13: 83264: loss: 0.6589317508:
13: 89664: loss: 0.6588216336:
13: 96064: loss: 0.6587897571:
13: 102464: loss: 0.6586905352:
13: 108864: loss: 0.6586481053:
13: 115264: loss: 0.6585899163:
13: 121664: loss: 0.6584845287:
13: 128064: loss: 0.6583919812:
13: 134464: loss: 0.6582913735:
Dev-Acc: 13: Accuracy: 0.8722813129: precision: 0.0739792809: recall: 0.1032137392: f1: 0.0861848644
Train-Acc: 13: Accuracy: 0.8474934101: precision: 0.2193164933: recall: 0.1455525606: f1: 0.1749782660
14: 6464: loss: 0.6573072267:
14: 12864: loss: 0.6563939127:
14: 19264: loss: 0.6561828119:
14: 25664: loss: 0.6559813453:
14: 32064: loss: 0.6558459365:
14: 38464: loss: 0.6558381872:
14: 44864: loss: 0.6557228544:
14: 51264: loss: 0.6555971181:
14: 57664: loss: 0.6555674175:
14: 64064: loss: 0.6553303362:
14: 70464: loss: 0.6552272771:
14: 76864: loss: 0.6551609965:
14: 83264: loss: 0.6550584058:
14: 89664: loss: 0.6549902780:
14: 96064: loss: 0.6549192457:
14: 102464: loss: 0.6548735885:
14: 108864: loss: 0.6547082687:
14: 115264: loss: 0.6546206603:
14: 121664: loss: 0.6545263310:
14: 128064: loss: 0.6544288865:
14: 134464: loss: 0.6543173760:
Dev-Acc: 14: Accuracy: 0.8897444010: precision: 0.0823886316: recall: 0.0877401802: f1: 0.0849802372
Train-Acc: 14: Accuracy: 0.8595826030: precision: 0.2367454068: recall: 0.1185983827: f1: 0.1580307477
15: 6464: loss: 0.6523252046:
15: 12864: loss: 0.6525291237:
15: 19264: loss: 0.6526019810:
15: 25664: loss: 0.6525577272:
15: 32064: loss: 0.6526757969:
15: 38464: loss: 0.6524753494:
15: 44864: loss: 0.6521139048:
15: 51264: loss: 0.6519480459:
15: 57664: loss: 0.6516847979:
15: 64064: loss: 0.6515989941:
15: 70464: loss: 0.6516186521:
15: 76864: loss: 0.6515397241:
15: 83264: loss: 0.6514663054:
15: 89664: loss: 0.6513561822:
15: 96064: loss: 0.6512554309:
15: 102464: loss: 0.6511101030:
15: 108864: loss: 0.6509706923:
15: 115264: loss: 0.6509081642:
15: 121664: loss: 0.6507823369:
15: 128064: loss: 0.6506275527:
15: 134464: loss: 0.6505589581:
Dev-Acc: 15: Accuracy: 0.9039827585: precision: 0.0769059296: recall: 0.0586634926: f1: 0.0665573454
Train-Acc: 15: Accuracy: 0.8681217432: precision: 0.2479162972: recall: 0.0919071724: f1: 0.1341007194
16: 6464: loss: 0.6474061954:
16: 12864: loss: 0.6480937320:
16: 19264: loss: 0.6479856165:
16: 25664: loss: 0.6480541870:
16: 32064: loss: 0.6480970929:
16: 38464: loss: 0.6480779936:
16: 44864: loss: 0.6479159425:
16: 51264: loss: 0.6477369437:
16: 57664: loss: 0.6476529257:
16: 64064: loss: 0.6475585039:
16: 70464: loss: 0.6474683160:
16: 76864: loss: 0.6473809395:
16: 83264: loss: 0.6472968282:
16: 89664: loss: 0.6472884465:
16: 96064: loss: 0.6472131427:
16: 102464: loss: 0.6471467414:
16: 108864: loss: 0.6470341710:
16: 115264: loss: 0.6469288647:
16: 121664: loss: 0.6468399157:
16: 128064: loss: 0.6467786834:
16: 134464: loss: 0.6466445835:
Dev-Acc: 16: Accuracy: 0.9148178101: precision: 0.0775000000: recall: 0.0421696990: f1: 0.0546195353
Train-Acc: 16: Accuracy: 0.8742868900: precision: 0.2574617811: recall: 0.0697521530: f1: 0.1097661908
17: 6464: loss: 0.6446118093:
17: 12864: loss: 0.6445549813:
17: 19264: loss: 0.6443541251:
17: 25664: loss: 0.6443187825:
17: 32064: loss: 0.6442254664:
17: 38464: loss: 0.6442275508:
17: 44864: loss: 0.6440236353:
17: 51264: loss: 0.6441127869:
17: 57664: loss: 0.6439468900:
17: 64064: loss: 0.6438641354:
17: 70464: loss: 0.6436765799:
17: 76864: loss: 0.6435607645:
17: 83264: loss: 0.6434974621:
17: 89664: loss: 0.6433907952:
17: 96064: loss: 0.6432879425:
17: 102464: loss: 0.6431293181:
17: 108864: loss: 0.6430592225:
17: 115264: loss: 0.6429657191:
17: 121664: loss: 0.6429585549:
17: 128064: loss: 0.6428944118:
17: 134464: loss: 0.6428449600:
Dev-Acc: 17: Accuracy: 0.9231227040: precision: 0.0774105930: recall: 0.0290766876: f1: 0.0422744129
Train-Acc: 17: Accuracy: 0.8791371584: precision: 0.2736520855: recall: 0.0530537111: f1: 0.0888766520
18: 6464: loss: 0.6399420333:
18: 12864: loss: 0.6402792996:
18: 19264: loss: 0.6406429718:
18: 25664: loss: 0.6403217579:
18: 32064: loss: 0.6404134374:
18: 38464: loss: 0.6403267842:
18: 44864: loss: 0.6400518898:
18: 51264: loss: 0.6399952487:
18: 57664: loss: 0.6399128121:
18: 64064: loss: 0.6399678273:
18: 70464: loss: 0.6398423626:
18: 76864: loss: 0.6397632493:
18: 83264: loss: 0.6396003005:
18: 89664: loss: 0.6394190406:
18: 96064: loss: 0.6394429831:
18: 102464: loss: 0.6393946958:
18: 108864: loss: 0.6393116695:
18: 115264: loss: 0.6392724005:
18: 121664: loss: 0.6391871398:
18: 128064: loss: 0.6391203976:
18: 134464: loss: 0.6390685042:
Dev-Acc: 18: Accuracy: 0.9284013510: precision: 0.0617202889: recall: 0.0159836762: f1: 0.0253916802
Train-Acc: 18: Accuracy: 0.8820152283: precision: 0.2862335302: recall: 0.0414173953: f1: 0.0723638870
19: 6464: loss: 0.6376414108:
19: 12864: loss: 0.6376576695:
19: 19264: loss: 0.6376880459:
19: 25664: loss: 0.6375738798:
19: 32064: loss: 0.6371646945:
19: 38464: loss: 0.6369980197:
19: 44864: loss: 0.6367971746:
19: 51264: loss: 0.6367317211:
19: 57664: loss: 0.6364175697:
19: 64064: loss: 0.6364913085:
19: 70464: loss: 0.6364118228:
19: 76864: loss: 0.6361687659:
19: 83264: loss: 0.6361462355:
19: 89664: loss: 0.6360058514:
19: 96064: loss: 0.6358575873:
19: 102464: loss: 0.6357087122:
19: 108864: loss: 0.6355483241:
19: 115264: loss: 0.6354255510:
19: 121664: loss: 0.6354133742:
19: 128064: loss: 0.6352896450:
19: 134464: loss: 0.6352232014:
Dev-Acc: 19: Accuracy: 0.9321519136: precision: 0.0597976081: recall: 0.0110525421: f1: 0.0186567164
Train-Acc: 19: Accuracy: 0.8840166926: precision: 0.2975106254: recall: 0.0322135297: f1: 0.0581326373
20: 6464: loss: 0.6335914838:
20: 12864: loss: 0.6335864279:
20: 19264: loss: 0.6333370197:
20: 25664: loss: 0.6329865912:
20: 32064: loss: 0.6327863665:
20: 38464: loss: 0.6325990326:
20: 44864: loss: 0.6327703908:
20: 51264: loss: 0.6325406639:
20: 57664: loss: 0.6325177966:
20: 64064: loss: 0.6323211694:
20: 70464: loss: 0.6324034475:
20: 76864: loss: 0.6323649139:
20: 83264: loss: 0.6321860927:
20: 89664: loss: 0.6321379811:
20: 96064: loss: 0.6321224524:
20: 102464: loss: 0.6321034671:
20: 108864: loss: 0.6320242591:
20: 115264: loss: 0.6319691701:
20: 121664: loss: 0.6318718210:
20: 128064: loss: 0.6317052799:
20: 134464: loss: 0.6316172907:
Dev-Acc: 20: Accuracy: 0.9350293875: precision: 0.0696774194: recall: 0.0091821119: f1: 0.0162259615
Train-Acc: 20: Accuracy: 0.8857187033: precision: 0.3230016313: recall: 0.0260337913: f1: 0.0481839752
