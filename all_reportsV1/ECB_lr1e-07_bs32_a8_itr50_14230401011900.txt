1: 3232: loss: 0.7096930283:
1: 6432: loss: 0.7078628597:
1: 9632: loss: 0.7069279067:
1: 12832: loss: 0.7062362909:
1: 16032: loss: 0.7052760290:
1: 19232: loss: 0.7045255835:
1: 22432: loss: 0.7036874715:
1: 25632: loss: 0.7027131055:
1: 28832: loss: 0.7017498285:
1: 32032: loss: 0.7008981225:
1: 35232: loss: 0.7002181942:
1: 38432: loss: 0.6994278524:
1: 41632: loss: 0.6984843575:
1: 44832: loss: 0.6976051316:
1: 48032: loss: 0.6967114698:
1: 51232: loss: 0.6958339429:
1: 54432: loss: 0.6949826915:
1: 57632: loss: 0.6941746781:
1: 60832: loss: 0.6933586607:
1: 64032: loss: 0.6924716136:
1: 67232: loss: 0.6916188921:
1: 70432: loss: 0.6907760879:
1: 73632: loss: 0.6899620375:
1: 76832: loss: 0.6891404515:
1: 80032: loss: 0.6882947471:
1: 83232: loss: 0.6875416143:
1: 86432: loss: 0.6866985631:
1: 89632: loss: 0.6858446520:
1: 92832: loss: 0.6850062666:
1: 96032: loss: 0.6841868732:
1: 99232: loss: 0.6833186960:
1: 102432: loss: 0.6824609211:
1: 105632: loss: 0.6815928219:
1: 108832: loss: 0.6807794347:
1: 112032: loss: 0.6799604088:
1: 115232: loss: 0.6791194355:
1: 118432: loss: 0.6783471288:
1: 121632: loss: 0.6775537842:
1: 124832: loss: 0.6767594661:
1: 128032: loss: 0.6759561829:
1: 131232: loss: 0.6751875513:
1: 134432: loss: 0.6743764614:
Dev-Acc: 1: Accuracy: 0.9261489511: precision: 0.0732240437: recall: 0.0227852406: f1: 0.0347555440
Train-Acc: 1: Accuracy: 0.8804958463: precision: 0.2794625720: recall: 0.0478601012: f1: 0.0817242928
2: 3232: loss: 0.6390621430:
2: 6432: loss: 0.6386736158:
2: 9632: loss: 0.6374958318:
2: 12832: loss: 0.6369918601:
2: 16032: loss: 0.6363665794:
2: 19232: loss: 0.6355602546:
2: 22432: loss: 0.6347459753:
2: 25632: loss: 0.6339892333:
2: 28832: loss: 0.6331794568:
2: 32032: loss: 0.6323603097:
2: 35232: loss: 0.6317440234:
2: 38432: loss: 0.6309006256:
2: 41632: loss: 0.6300367703:
2: 44832: loss: 0.6291673681:
2: 48032: loss: 0.6284449434:
2: 51232: loss: 0.6275349003:
2: 54432: loss: 0.6269300078:
2: 57632: loss: 0.6262906156:
2: 60832: loss: 0.6254587282:
2: 64032: loss: 0.6245692623:
2: 67232: loss: 0.6238249668:
2: 70432: loss: 0.6230462943:
2: 73632: loss: 0.6221465456:
2: 76832: loss: 0.6213943606:
2: 80032: loss: 0.6207044325:
2: 83232: loss: 0.6200044564:
2: 86432: loss: 0.6192618045:
2: 89632: loss: 0.6185784915:
2: 92832: loss: 0.6178772105:
2: 96032: loss: 0.6171444389:
2: 99232: loss: 0.6164326098:
2: 102432: loss: 0.6156602170:
2: 105632: loss: 0.6148855207:
2: 108832: loss: 0.6141852817:
2: 112032: loss: 0.6134690967:
2: 115232: loss: 0.6127552092:
2: 118432: loss: 0.6120462090:
2: 121632: loss: 0.6113757065:
2: 124832: loss: 0.6105948366:
2: 128032: loss: 0.6098899366:
2: 131232: loss: 0.6091698900:
2: 134432: loss: 0.6084814271:
Dev-Acc: 2: Accuracy: 0.9416574240: precision: 1.0000000000: recall: 0.0001700391: f1: 0.0003400204
Train-Acc: 2: Accuracy: 0.8889254332: precision: 1.0000000000: recall: 0.0003287095: f1: 0.0006572029
3: 3232: loss: 0.5769263446:
3: 6432: loss: 0.5763994446:
3: 9632: loss: 0.5768054658:
3: 12832: loss: 0.5758752549:
3: 16032: loss: 0.5749626234:
3: 19232: loss: 0.5743066004:
3: 22432: loss: 0.5729382902:
3: 25632: loss: 0.5721619212:
3: 28832: loss: 0.5714162088:
3: 32032: loss: 0.5706531818:
3: 35232: loss: 0.5700815767:
3: 38432: loss: 0.5691067080:
3: 41632: loss: 0.5681047982:
3: 44832: loss: 0.5671895605:
3: 48032: loss: 0.5667579549:
3: 51232: loss: 0.5663800164:
3: 54432: loss: 0.5656145104:
3: 57632: loss: 0.5650436662:
3: 60832: loss: 0.5642246154:
3: 64032: loss: 0.5636060400:
3: 67232: loss: 0.5629878085:
3: 70432: loss: 0.5624485094:
3: 73632: loss: 0.5617158353:
3: 76832: loss: 0.5608602242:
3: 80032: loss: 0.5600802123:
3: 83232: loss: 0.5593794117:
3: 86432: loss: 0.5586478697:
3: 89632: loss: 0.5579599473:
3: 92832: loss: 0.5572659789:
3: 96032: loss: 0.5563205374:
3: 99232: loss: 0.5556812028:
3: 102432: loss: 0.5551092768:
3: 105632: loss: 0.5544517773:
3: 108832: loss: 0.5538487857:
3: 112032: loss: 0.5531461269:
3: 115232: loss: 0.5524846982:
3: 118432: loss: 0.5518039084:
3: 121632: loss: 0.5511079196:
3: 124832: loss: 0.5504718941:
3: 128032: loss: 0.5500130976:
3: 131232: loss: 0.5494708357:
3: 134432: loss: 0.5489077067:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.5212054276:
4: 6432: loss: 0.5163943148:
4: 9632: loss: 0.5170160579:
4: 12832: loss: 0.5176221374:
4: 16032: loss: 0.5160315592:
4: 19232: loss: 0.5150181082:
4: 22432: loss: 0.5141108670:
4: 25632: loss: 0.5143524878:
4: 28832: loss: 0.5142224475:
4: 32032: loss: 0.5138601762:
4: 35232: loss: 0.5128337133:
4: 38432: loss: 0.5122690112:
4: 41632: loss: 0.5117764894:
4: 44832: loss: 0.5112927606:
4: 48032: loss: 0.5108120409:
4: 51232: loss: 0.5101411369:
4: 54432: loss: 0.5096297689:
4: 57632: loss: 0.5088078601:
4: 60832: loss: 0.5085088208:
4: 64032: loss: 0.5078648579:
4: 67232: loss: 0.5074501137:
4: 70432: loss: 0.5069983401:
4: 73632: loss: 0.5063965923:
4: 76832: loss: 0.5059845165:
4: 80032: loss: 0.5056697088:
4: 83232: loss: 0.5051050111:
4: 86432: loss: 0.5045581393:
4: 89632: loss: 0.5040665390:
4: 92832: loss: 0.5033170463:
4: 96032: loss: 0.5028334227:
4: 99232: loss: 0.5022211682:
4: 102432: loss: 0.5014365906:
4: 105632: loss: 0.5007396439:
4: 108832: loss: 0.5003201196:
4: 112032: loss: 0.4994318633:
4: 115232: loss: 0.4988949712:
4: 118432: loss: 0.4982731933:
4: 121632: loss: 0.4975784065:
4: 124832: loss: 0.4969155378:
4: 128032: loss: 0.4963803221:
4: 131232: loss: 0.4956737832:
4: 134432: loss: 0.4953624544:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4789079276:
5: 6432: loss: 0.4747523233:
5: 9632: loss: 0.4725700838:
5: 12832: loss: 0.4716378047:
5: 16032: loss: 0.4697150254:
5: 19232: loss: 0.4687345765:
5: 22432: loss: 0.4681843971:
5: 25632: loss: 0.4687271750:
5: 28832: loss: 0.4674717393:
5: 32032: loss: 0.4675737520:
5: 35232: loss: 0.4659673002:
5: 38432: loss: 0.4649952428:
5: 41632: loss: 0.4641635543:
5: 44832: loss: 0.4632999139:
5: 48032: loss: 0.4627981151:
5: 51232: loss: 0.4620683211:
5: 54432: loss: 0.4617070440:
5: 57632: loss: 0.4609316978:
5: 60832: loss: 0.4601801060:
5: 64032: loss: 0.4599679393:
5: 67232: loss: 0.4593015381:
5: 70432: loss: 0.4588752664:
5: 73632: loss: 0.4581597254:
5: 76832: loss: 0.4572134343:
5: 80032: loss: 0.4570101961:
5: 83232: loss: 0.4563842744:
5: 86432: loss: 0.4558783766:
5: 89632: loss: 0.4554996382:
5: 92832: loss: 0.4550045245:
5: 96032: loss: 0.4546661279:
5: 99232: loss: 0.4539516878:
5: 102432: loss: 0.4535772150:
5: 105632: loss: 0.4527251727:
5: 108832: loss: 0.4523581437:
5: 112032: loss: 0.4516052954:
5: 115232: loss: 0.4510900293:
5: 118432: loss: 0.4507870288:
5: 121632: loss: 0.4502923135:
5: 124832: loss: 0.4498918969:
5: 128032: loss: 0.4497543789:
5: 131232: loss: 0.4494351149:
5: 134432: loss: 0.4489215843:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.4379089352:
6: 6432: loss: 0.4299750403:
6: 9632: loss: 0.4250047163:
6: 12832: loss: 0.4244258198:
6: 16032: loss: 0.4262945448:
6: 19232: loss: 0.4253626161:
6: 22432: loss: 0.4243018157:
6: 25632: loss: 0.4240138066:
6: 28832: loss: 0.4232830271:
6: 32032: loss: 0.4225387489:
6: 35232: loss: 0.4221151354:
6: 38432: loss: 0.4213061915:
6: 41632: loss: 0.4212479456:
6: 44832: loss: 0.4209983254:
6: 48032: loss: 0.4199085501:
6: 51232: loss: 0.4198797299:
6: 54432: loss: 0.4196151129:
6: 57632: loss: 0.4194735270:
6: 60832: loss: 0.4186528075:
6: 64032: loss: 0.4176808494:
6: 67232: loss: 0.4175199366:
6: 70432: loss: 0.4174469362:
6: 73632: loss: 0.4175114677:
6: 76832: loss: 0.4171940715:
6: 80032: loss: 0.4171516970:
6: 83232: loss: 0.4168632997:
6: 86432: loss: 0.4167273199:
6: 89632: loss: 0.4163060060:
6: 92832: loss: 0.4158663472:
6: 96032: loss: 0.4153899037:
6: 99232: loss: 0.4149590774:
6: 102432: loss: 0.4144089562:
6: 105632: loss: 0.4139634939:
6: 108832: loss: 0.4135060168:
6: 112032: loss: 0.4131370403:
6: 115232: loss: 0.4127680471:
6: 118432: loss: 0.4123406730:
6: 121632: loss: 0.4120570953:
6: 124832: loss: 0.4115252915:
6: 128032: loss: 0.4112925955:
6: 131232: loss: 0.4109492850:
6: 134432: loss: 0.4105521289:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.4000214601:
7: 6432: loss: 0.3969339809:
7: 9632: loss: 0.3941231573:
7: 12832: loss: 0.3925554966:
7: 16032: loss: 0.3907344448:
7: 19232: loss: 0.3910211843:
7: 22432: loss: 0.3904143899:
7: 25632: loss: 0.3903668718:
7: 28832: loss: 0.3901731663:
7: 32032: loss: 0.3907122966:
7: 35232: loss: 0.3900517260:
7: 38432: loss: 0.3901455287:
7: 41632: loss: 0.3897592869:
7: 44832: loss: 0.3889447200:
7: 48032: loss: 0.3888920538:
7: 51232: loss: 0.3880680078:
7: 54432: loss: 0.3880470272:
7: 57632: loss: 0.3878377761:
7: 60832: loss: 0.3873768365:
7: 64032: loss: 0.3870378271:
7: 67232: loss: 0.3874610174:
7: 70432: loss: 0.3876285812:
7: 73632: loss: 0.3876518394:
7: 76832: loss: 0.3870325019:
7: 80032: loss: 0.3865828741:
7: 83232: loss: 0.3858018400:
7: 86432: loss: 0.3854353025:
7: 89632: loss: 0.3852551357:
7: 92832: loss: 0.3850831052:
7: 96032: loss: 0.3840896467:
7: 99232: loss: 0.3837358306:
7: 102432: loss: 0.3837302961:
7: 105632: loss: 0.3833461302:
7: 108832: loss: 0.3832945965:
7: 112032: loss: 0.3826839591:
7: 115232: loss: 0.3822147879:
7: 118432: loss: 0.3820430776:
7: 121632: loss: 0.3817910132:
7: 124832: loss: 0.3814651541:
7: 128032: loss: 0.3809160822:
7: 131232: loss: 0.3806107309:
7: 134432: loss: 0.3804303625:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3635316381:
8: 6432: loss: 0.3721704490:
8: 9632: loss: 0.3686372123:
8: 12832: loss: 0.3690531770:
8: 16032: loss: 0.3682251856:
8: 19232: loss: 0.3661326795:
8: 22432: loss: 0.3668606788:
8: 25632: loss: 0.3661362049:
8: 28832: loss: 0.3663751284:
8: 32032: loss: 0.3653124530:
8: 35232: loss: 0.3648682830:
8: 38432: loss: 0.3643624539:
8: 41632: loss: 0.3640638093:
8: 44832: loss: 0.3637317033:
8: 48032: loss: 0.3633730156:
8: 51232: loss: 0.3633258418:
8: 54432: loss: 0.3633609921:
8: 57632: loss: 0.3632124894:
8: 60832: loss: 0.3629474613:
8: 64032: loss: 0.3627165798:
8: 67232: loss: 0.3625664968:
8: 70432: loss: 0.3626300175:
8: 73632: loss: 0.3622170203:
8: 76832: loss: 0.3619222472:
8: 80032: loss: 0.3616902035:
8: 83232: loss: 0.3608571176:
8: 86432: loss: 0.3609141146:
8: 89632: loss: 0.3604640868:
8: 92832: loss: 0.3602330655:
8: 96032: loss: 0.3598387010:
8: 99232: loss: 0.3594937616:
8: 102432: loss: 0.3593259530:
8: 105632: loss: 0.3588655790:
8: 108832: loss: 0.3586150675:
8: 112032: loss: 0.3584895948:
8: 115232: loss: 0.3581625658:
8: 118432: loss: 0.3577619036:
8: 121632: loss: 0.3571548155:
8: 124832: loss: 0.3567818406:
8: 128032: loss: 0.3564353096:
8: 131232: loss: 0.3563265381:
8: 134432: loss: 0.3563817778:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3446514875:
9: 6432: loss: 0.3463664839:
9: 9632: loss: 0.3448845216:
9: 12832: loss: 0.3425830401:
9: 16032: loss: 0.3423963949:
9: 19232: loss: 0.3424303767:
9: 22432: loss: 0.3436105946:
9: 25632: loss: 0.3443954876:
9: 28832: loss: 0.3426983670:
9: 32032: loss: 0.3419429564:
9: 35232: loss: 0.3421149364:
9: 38432: loss: 0.3432015869:
9: 41632: loss: 0.3428589453:
9: 44832: loss: 0.3431214524:
9: 48032: loss: 0.3434921286:
9: 51232: loss: 0.3444705915:
9: 54432: loss: 0.3446208040:
9: 57632: loss: 0.3447941989:
9: 60832: loss: 0.3447374951:
9: 64032: loss: 0.3447431000:
9: 67232: loss: 0.3440817244:
9: 70432: loss: 0.3437676569:
9: 73632: loss: 0.3443307015:
9: 76832: loss: 0.3436665792:
9: 80032: loss: 0.3430435898:
9: 83232: loss: 0.3425977217:
9: 86432: loss: 0.3415934742:
9: 89632: loss: 0.3413555326:
9: 92832: loss: 0.3412506811:
9: 96032: loss: 0.3409489444:
9: 99232: loss: 0.3409665871:
9: 102432: loss: 0.3405070838:
9: 105632: loss: 0.3402864354:
9: 108832: loss: 0.3401750786:
9: 112032: loss: 0.3400843566:
9: 115232: loss: 0.3400763123:
9: 118432: loss: 0.3401103324:
9: 121632: loss: 0.3399286645:
9: 124832: loss: 0.3396242748:
9: 128032: loss: 0.3394087371:
9: 131232: loss: 0.3390357059:
9: 134432: loss: 0.3387963303:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.3394562635:
10: 6432: loss: 0.3399997818:
10: 9632: loss: 0.3360729469:
10: 12832: loss: 0.3314115148:
10: 16032: loss: 0.3301592436:
10: 19232: loss: 0.3291013955:
10: 22432: loss: 0.3265528291:
10: 25632: loss: 0.3244947599:
10: 28832: loss: 0.3259730046:
10: 32032: loss: 0.3252279285:
10: 35232: loss: 0.3243219388:
10: 38432: loss: 0.3241330864:
10: 41632: loss: 0.3236223748:
10: 44832: loss: 0.3239123917:
10: 48032: loss: 0.3243119031:
10: 51232: loss: 0.3252536669:
10: 54432: loss: 0.3250050327:
10: 57632: loss: 0.3250917678:
10: 60832: loss: 0.3256004834:
10: 64032: loss: 0.3254467757:
10: 67232: loss: 0.3257777824:
10: 70432: loss: 0.3253397685:
10: 73632: loss: 0.3254355478:
10: 76832: loss: 0.3250670910:
10: 80032: loss: 0.3246772003:
10: 83232: loss: 0.3242648009:
10: 86432: loss: 0.3242775161:
10: 89632: loss: 0.3239657160:
10: 92832: loss: 0.3238495249:
10: 96032: loss: 0.3231193563:
10: 99232: loss: 0.3230032154:
10: 102432: loss: 0.3228599014:
10: 105632: loss: 0.3228847727:
10: 108832: loss: 0.3227614919:
10: 112032: loss: 0.3228382086:
10: 115232: loss: 0.3232647599:
10: 118432: loss: 0.3233184019:
10: 121632: loss: 0.3231325880:
10: 124832: loss: 0.3234550374:
10: 128032: loss: 0.3233074215:
10: 131232: loss: 0.3237152214:
10: 134432: loss: 0.3239867982:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.3197711286:
11: 6432: loss: 0.3185363071:
11: 9632: loss: 0.3147252452:
11: 12832: loss: 0.3170658660:
11: 16032: loss: 0.3173668551:
11: 19232: loss: 0.3155893014:
11: 22432: loss: 0.3169975292:
11: 25632: loss: 0.3163351540:
11: 28832: loss: 0.3156608581:
11: 32032: loss: 0.3146349393:
11: 35232: loss: 0.3162392548:
11: 38432: loss: 0.3154094576:
11: 41632: loss: 0.3147674382:
11: 44832: loss: 0.3159726032:
11: 48032: loss: 0.3159624193:
11: 51232: loss: 0.3152028901:
11: 54432: loss: 0.3154986740:
11: 57632: loss: 0.3152483749:
11: 60832: loss: 0.3155083912:
11: 64032: loss: 0.3156052438:
11: 67232: loss: 0.3157649949:
11: 70432: loss: 0.3154832324:
11: 73632: loss: 0.3151087906:
11: 76832: loss: 0.3147290075:
11: 80032: loss: 0.3146938590:
11: 83232: loss: 0.3152406526:
11: 86432: loss: 0.3150193915:
11: 89632: loss: 0.3150322930:
11: 92832: loss: 0.3147090249:
11: 96032: loss: 0.3148748702:
11: 99232: loss: 0.3146578994:
11: 102432: loss: 0.3142307340:
11: 105632: loss: 0.3138375056:
11: 108832: loss: 0.3135416663:
11: 112032: loss: 0.3130873778:
11: 115232: loss: 0.3133993199:
11: 118432: loss: 0.3132242718:
11: 121632: loss: 0.3131528753:
11: 124832: loss: 0.3126349611:
11: 128032: loss: 0.3123169255:
11: 131232: loss: 0.3122462041:
11: 134432: loss: 0.3119522080:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.3090593450:
12: 6432: loss: 0.3117762492:
12: 9632: loss: 0.3129316822:
12: 12832: loss: 0.3118967273:
12: 16032: loss: 0.3116264709:
12: 19232: loss: 0.3075871713:
12: 22432: loss: 0.3059215939:
12: 25632: loss: 0.3060833276:
12: 28832: loss: 0.3059539562:
12: 32032: loss: 0.3058484950:
12: 35232: loss: 0.3045376159:
12: 38432: loss: 0.3044667419:
12: 41632: loss: 0.3046843248:
12: 44832: loss: 0.3045464278:
12: 48032: loss: 0.3043007734:
12: 51232: loss: 0.3040927323:
12: 54432: loss: 0.3040913352:
12: 57632: loss: 0.3039123106:
12: 60832: loss: 0.3042712354:
12: 64032: loss: 0.3032885311:
12: 67232: loss: 0.3031355759:
12: 70432: loss: 0.3035367029:
12: 73632: loss: 0.3032627992:
12: 76832: loss: 0.3030260557:
12: 80032: loss: 0.3024228272:
12: 83232: loss: 0.3024830556:
12: 86432: loss: 0.3021391603:
12: 89632: loss: 0.3019898961:
12: 92832: loss: 0.3022800647:
12: 96032: loss: 0.3019424380:
12: 99232: loss: 0.3015778456:
12: 102432: loss: 0.3013582485:
12: 105632: loss: 0.3015459116:
12: 108832: loss: 0.3015859058:
12: 112032: loss: 0.3017450623:
12: 115232: loss: 0.3020257605:
12: 118432: loss: 0.3019360333:
12: 121632: loss: 0.3019117535:
12: 124832: loss: 0.3017799509:
12: 128032: loss: 0.3015916278:
12: 131232: loss: 0.3017964313:
12: 134432: loss: 0.3016380232:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8888888955: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2940458944:
13: 6432: loss: 0.2944374941:
13: 9632: loss: 0.2962472975:
13: 12832: loss: 0.2981727892:
13: 16032: loss: 0.2994268970:
13: 19232: loss: 0.2994323084:
13: 22432: loss: 0.2976682745:
13: 25632: loss: 0.2962311165:
13: 28832: loss: 0.2971885605:
13: 32032: loss: 0.2967963166:
13: 35232: loss: 0.2939538192:
13: 38432: loss: 0.2935500767:
13: 41632: loss: 0.2938456719:
13: 44832: loss: 0.2938659031:
13: 48032: loss: 0.2941120696:
13: 51232: loss: 0.2932447847:
13: 54432: loss: 0.2941071936:
13: 57632: loss: 0.2939028139:
13: 60832: loss: 0.2937998471:
13: 64032: loss: 0.2935918969:
13: 67232: loss: 0.2934006943:
13: 70432: loss: 0.2933111152:
13: 73632: loss: 0.2939278967:
13: 76832: loss: 0.2937161608:
13: 80032: loss: 0.2939760350:
13: 83232: loss: 0.2940849809:
13: 86432: loss: 0.2937459768:
13: 89632: loss: 0.2931376475:
13: 92832: loss: 0.2937867560:
13: 96032: loss: 0.2940551653:
13: 99232: loss: 0.2939935109:
13: 102432: loss: 0.2938960699:
13: 105632: loss: 0.2942359436:
13: 108832: loss: 0.2935981477:
13: 112032: loss: 0.2933139135:
13: 115232: loss: 0.2933957811:
13: 118432: loss: 0.2928786790:
13: 121632: loss: 0.2928263182:
13: 124832: loss: 0.2926520540:
13: 128032: loss: 0.2928264652:
13: 131232: loss: 0.2929148460:
13: 134432: loss: 0.2926802560:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.8892102838: precision: 1.0000000000: recall: 0.0028926435: f1: 0.0057686005
14: 3232: loss: 0.2931587488:
14: 6432: loss: 0.3013044717:
14: 9632: loss: 0.2966512394:
14: 12832: loss: 0.2918143024:
14: 16032: loss: 0.2901179874:
14: 19232: loss: 0.2925304252:
14: 22432: loss: 0.2920866781:
14: 25632: loss: 0.2904382074:
14: 28832: loss: 0.2897275966:
14: 32032: loss: 0.2899487618:
14: 35232: loss: 0.2880703455:
14: 38432: loss: 0.2889528954:
14: 41632: loss: 0.2887610392:
14: 44832: loss: 0.2884577157:
14: 48032: loss: 0.2885022954:
14: 51232: loss: 0.2874488822:
14: 54432: loss: 0.2871216536:
14: 57632: loss: 0.2865723201:
14: 60832: loss: 0.2866915493:
14: 64032: loss: 0.2861343727:
14: 67232: loss: 0.2861997291:
14: 70432: loss: 0.2864548840:
14: 73632: loss: 0.2859364946:
14: 76832: loss: 0.2859214498:
14: 80032: loss: 0.2858361479:
14: 83232: loss: 0.2858671695:
14: 86432: loss: 0.2862929772:
14: 89632: loss: 0.2858105834:
14: 92832: loss: 0.2856861654:
14: 96032: loss: 0.2855418821:
14: 99232: loss: 0.2860129690:
14: 102432: loss: 0.2861284978:
14: 105632: loss: 0.2858085112:
14: 108832: loss: 0.2859937642:
14: 112032: loss: 0.2857164456:
14: 115232: loss: 0.2855413401:
14: 118432: loss: 0.2855203569:
14: 121632: loss: 0.2856445290:
14: 124832: loss: 0.2858305603:
14: 128032: loss: 0.2855585509:
14: 131232: loss: 0.2852609571:
14: 134432: loss: 0.2849642593:
Dev-Acc: 14: Accuracy: 0.9416574240: precision: 0.5200000000: recall: 0.0022105084: f1: 0.0044023027
Train-Acc: 14: Accuracy: 0.8905689716: precision: 0.8506097561: recall: 0.0183419893: f1: 0.0359096467
15: 3232: loss: 0.2849934416:
15: 6432: loss: 0.2823551174:
15: 9632: loss: 0.2860842915:
15: 12832: loss: 0.2865023580:
15: 16032: loss: 0.2860474414:
15: 19232: loss: 0.2872862669:
15: 22432: loss: 0.2858781622:
15: 25632: loss: 0.2890552754:
15: 28832: loss: 0.2893908783:
15: 32032: loss: 0.2899844620:
15: 35232: loss: 0.2883348008:
15: 38432: loss: 0.2861091997:
15: 41632: loss: 0.2851549603:
15: 44832: loss: 0.2841388916:
15: 48032: loss: 0.2839134338:
15: 51232: loss: 0.2824402548:
15: 54432: loss: 0.2825120370:
15: 57632: loss: 0.2814658238:
15: 60832: loss: 0.2818704128:
15: 64032: loss: 0.2811021539:
15: 67232: loss: 0.2813631373:
15: 70432: loss: 0.2810609389:
15: 73632: loss: 0.2815056322:
15: 76832: loss: 0.2808563939:
15: 80032: loss: 0.2814200103:
15: 83232: loss: 0.2813687417:
15: 86432: loss: 0.2804731546:
15: 89632: loss: 0.2801408053:
15: 92832: loss: 0.2804203710:
15: 96032: loss: 0.2795918374:
15: 99232: loss: 0.2791073552:
15: 102432: loss: 0.2791191668:
15: 105632: loss: 0.2787149933:
15: 108832: loss: 0.2786482801:
15: 112032: loss: 0.2784668534:
15: 115232: loss: 0.2784961875:
15: 118432: loss: 0.2781478246:
15: 121632: loss: 0.2779777257:
15: 124832: loss: 0.2777926651:
15: 128032: loss: 0.2777462753:
15: 131232: loss: 0.2778649720:
15: 134432: loss: 0.2779649120:
Dev-Acc: 15: Accuracy: 0.9421138167: precision: 0.7117117117: recall: 0.0134330896: f1: 0.0263684913
Train-Acc: 15: Accuracy: 0.8918983936: precision: 0.8159509202: recall: 0.0349746894: f1: 0.0670743239
16: 3232: loss: 0.2659997483:
16: 6432: loss: 0.2692243816:
16: 9632: loss: 0.2725569406:
16: 12832: loss: 0.2709818121:
16: 16032: loss: 0.2709203175:
16: 19232: loss: 0.2745033663:
16: 22432: loss: 0.2763733033:
16: 25632: loss: 0.2754143880:
16: 28832: loss: 0.2745138295:
16: 32032: loss: 0.2743047053:
16: 35232: loss: 0.2749840269:
16: 38432: loss: 0.2754760648:
16: 41632: loss: 0.2748642412:
16: 44832: loss: 0.2747195542:
16: 48032: loss: 0.2748001718:
16: 51232: loss: 0.2736752365:
16: 54432: loss: 0.2735078702:
16: 57632: loss: 0.2742115841:
16: 60832: loss: 0.2740065027:
16: 64032: loss: 0.2738513172:
16: 67232: loss: 0.2735964638:
16: 70432: loss: 0.2738059277:
16: 73632: loss: 0.2739727257:
16: 76832: loss: 0.2739853641:
16: 80032: loss: 0.2740420804:
16: 83232: loss: 0.2739527327:
16: 86432: loss: 0.2740064886:
16: 89632: loss: 0.2746333592:
16: 92832: loss: 0.2741999109:
16: 96032: loss: 0.2740579997:
16: 99232: loss: 0.2741711617:
16: 102432: loss: 0.2744388849:
16: 105632: loss: 0.2740549229:
16: 108832: loss: 0.2738084236:
16: 112032: loss: 0.2735116474:
16: 115232: loss: 0.2734090367:
16: 118432: loss: 0.2729841040:
16: 121632: loss: 0.2726251624:
16: 124832: loss: 0.2726843372:
16: 128032: loss: 0.2726626426:
16: 131232: loss: 0.2722078167:
16: 134432: loss: 0.2717531922:
Dev-Acc: 16: Accuracy: 0.9424015880: precision: 0.6187500000: recall: 0.0336677436: f1: 0.0638606676
Train-Acc: 16: Accuracy: 0.8936807513: precision: 0.8588621444: recall: 0.0516073894: f1: 0.0973643411
17: 3232: loss: 0.2568382264:
17: 6432: loss: 0.2601134314:
17: 9632: loss: 0.2650369607:
17: 12832: loss: 0.2613631846:
17: 16032: loss: 0.2597989820:
17: 19232: loss: 0.2641706257:
17: 22432: loss: 0.2635395826:
17: 25632: loss: 0.2656649681:
17: 28832: loss: 0.2657640681:
17: 32032: loss: 0.2665345109:
17: 35232: loss: 0.2658400421:
17: 38432: loss: 0.2666700153:
17: 41632: loss: 0.2669614900:
17: 44832: loss: 0.2665046925:
17: 48032: loss: 0.2671524108:
17: 51232: loss: 0.2673273488:
17: 54432: loss: 0.2667104143:
17: 57632: loss: 0.2665900488:
17: 60832: loss: 0.2669117412:
17: 64032: loss: 0.2667977250:
17: 67232: loss: 0.2666436940:
17: 70432: loss: 0.2664726931:
17: 73632: loss: 0.2668825499:
17: 76832: loss: 0.2667902413:
17: 80032: loss: 0.2668436770:
17: 83232: loss: 0.2668508018:
17: 86432: loss: 0.2669942862:
17: 89632: loss: 0.2664289524:
17: 92832: loss: 0.2669543020:
17: 96032: loss: 0.2669950967:
17: 99232: loss: 0.2662580085:
17: 102432: loss: 0.2661293974:
17: 105632: loss: 0.2659308820:
17: 108832: loss: 0.2658425251:
17: 112032: loss: 0.2655062909:
17: 115232: loss: 0.2653073556:
17: 118432: loss: 0.2649293618:
17: 121632: loss: 0.2652094362:
17: 124832: loss: 0.2650618154:
17: 128032: loss: 0.2650494025:
17: 131232: loss: 0.2649183056:
17: 134432: loss: 0.2651026496:
Dev-Acc: 17: Accuracy: 0.9420542717: precision: 0.5448577681: recall: 0.0423397381: f1: 0.0785736825
Train-Acc: 17: Accuracy: 0.8958356380: precision: 0.8666152660: recall: 0.0738938926: f1: 0.1361763993
18: 3232: loss: 0.2565662131:
18: 6432: loss: 0.2515549009:
18: 9632: loss: 0.2545225963:
18: 12832: loss: 0.2583683324:
18: 16032: loss: 0.2612956274:
18: 19232: loss: 0.2623720191:
18: 22432: loss: 0.2607745142:
18: 25632: loss: 0.2609411064:
18: 28832: loss: 0.2614242855:
18: 32032: loss: 0.2620764438:
18: 35232: loss: 0.2620369292:
18: 38432: loss: 0.2603923055:
18: 41632: loss: 0.2597806123:
18: 44832: loss: 0.2587941802:
18: 48032: loss: 0.2599050764:
18: 51232: loss: 0.2598370958:
18: 54432: loss: 0.2596453650:
18: 57632: loss: 0.2593715138:
18: 60832: loss: 0.2601926442:
18: 64032: loss: 0.2601153597:
18: 67232: loss: 0.2607241305:
18: 70432: loss: 0.2608635024:
18: 73632: loss: 0.2611387429:
18: 76832: loss: 0.2606175040:
18: 80032: loss: 0.2599480894:
18: 83232: loss: 0.2601467237:
18: 86432: loss: 0.2600071592:
18: 89632: loss: 0.2599482693:
18: 92832: loss: 0.2604419932:
18: 96032: loss: 0.2607401621:
18: 99232: loss: 0.2610572820:
18: 102432: loss: 0.2608337431:
18: 105632: loss: 0.2609475958:
18: 108832: loss: 0.2607168368:
18: 112032: loss: 0.2604109291:
18: 115232: loss: 0.2605824056:
18: 118432: loss: 0.2605115802:
18: 121632: loss: 0.2604281623:
18: 124832: loss: 0.2604054366:
18: 128032: loss: 0.2600909608:
18: 131232: loss: 0.2601250929:
18: 134432: loss: 0.2600210765:
Dev-Acc: 18: Accuracy: 0.9425107241: precision: 0.5568627451: recall: 0.0724366604: f1: 0.1281974120
Train-Acc: 18: Accuracy: 0.8985748887: precision: 0.8622950820: recall: 0.1037407140: f1: 0.1852003990
19: 3232: loss: 0.2717392243:
19: 6432: loss: 0.2686866765:
19: 9632: loss: 0.2688135324:
19: 12832: loss: 0.2669736604:
19: 16032: loss: 0.2682579708:
19: 19232: loss: 0.2662059549:
19: 22432: loss: 0.2651909486:
19: 25632: loss: 0.2647781434:
19: 28832: loss: 0.2624707319:
19: 32032: loss: 0.2614617598:
19: 35232: loss: 0.2606742983:
19: 38432: loss: 0.2599836866:
19: 41632: loss: 0.2601447773:
19: 44832: loss: 0.2596149001:
19: 48032: loss: 0.2589088539:
19: 51232: loss: 0.2588771874:
19: 54432: loss: 0.2580636328:
19: 57632: loss: 0.2573553115:
19: 60832: loss: 0.2576534387:
19: 64032: loss: 0.2577324338:
19: 67232: loss: 0.2574987026:
19: 70432: loss: 0.2576669485:
19: 73632: loss: 0.2575439181:
19: 76832: loss: 0.2572673947:
19: 80032: loss: 0.2577366318:
19: 83232: loss: 0.2578969251:
19: 86432: loss: 0.2578133006:
19: 89632: loss: 0.2575834419:
19: 92832: loss: 0.2570243283:
19: 96032: loss: 0.2567028819:
19: 99232: loss: 0.2564667718:
19: 102432: loss: 0.2562377344:
19: 105632: loss: 0.2558852806:
19: 108832: loss: 0.2555771673:
19: 112032: loss: 0.2558702418:
19: 115232: loss: 0.2555510431:
19: 118432: loss: 0.2556816210:
19: 121632: loss: 0.2557377643:
19: 124832: loss: 0.2555182472:
19: 128032: loss: 0.2553945995:
19: 131232: loss: 0.2559984372:
19: 134432: loss: 0.2557018113:
Dev-Acc: 19: Accuracy: 0.9436616898: precision: 0.5834018077: recall: 0.1207277674: f1: 0.2000563539
Train-Acc: 19: Accuracy: 0.9019861221: precision: 0.8712215321: recall: 0.1383209519: f1: 0.2387382276
20: 3232: loss: 0.2550795195:
20: 6432: loss: 0.2537631021:
20: 9632: loss: 0.2565130097:
20: 12832: loss: 0.2510244006:
20: 16032: loss: 0.2507806347:
20: 19232: loss: 0.2492968253:
20: 22432: loss: 0.2514435294:
20: 25632: loss: 0.2498674174:
20: 28832: loss: 0.2505403849:
20: 32032: loss: 0.2495483844:
20: 35232: loss: 0.2487237903:
20: 38432: loss: 0.2481542194:
20: 41632: loss: 0.2487829336:
20: 44832: loss: 0.2498610257:
20: 48032: loss: 0.2499170974:
20: 51232: loss: 0.2489790758:
20: 54432: loss: 0.2496745058:
20: 57632: loss: 0.2500993389:
20: 60832: loss: 0.2501206650:
20: 64032: loss: 0.2495785838:
20: 67232: loss: 0.2508063623:
20: 70432: loss: 0.2509380258:
20: 73632: loss: 0.2514145118:
20: 76832: loss: 0.2511528394:
20: 80032: loss: 0.2509196768:
20: 83232: loss: 0.2503980431:
20: 86432: loss: 0.2502947732:
20: 89632: loss: 0.2508494510:
20: 92832: loss: 0.2513783905:
20: 96032: loss: 0.2512496282:
20: 99232: loss: 0.2514291500:
20: 102432: loss: 0.2511946984:
20: 105632: loss: 0.2513778056:
20: 108832: loss: 0.2517185495:
20: 112032: loss: 0.2519345537:
20: 115232: loss: 0.2521342129:
20: 118432: loss: 0.2522341828:
20: 121632: loss: 0.2519845968:
20: 124832: loss: 0.2514402399:
20: 128032: loss: 0.2511622693:
20: 131232: loss: 0.2514968656:
20: 134432: loss: 0.2511867649:
Dev-Acc: 20: Accuracy: 0.9437609315: precision: 0.5760171306: recall: 0.1372215610: f1: 0.2216424059
Train-Acc: 20: Accuracy: 0.9052367210: precision: 0.8644951140: recall: 0.1744789955: f1: 0.2903561074
21: 3232: loss: 0.2400305362:
21: 6432: loss: 0.2395796851:
21: 9632: loss: 0.2414975500:
21: 12832: loss: 0.2439969688:
21: 16032: loss: 0.2456268598:
21: 19232: loss: 0.2433940009:
21: 22432: loss: 0.2437216672:
21: 25632: loss: 0.2465876938:
21: 28832: loss: 0.2469019107:
21: 32032: loss: 0.2467491545:
21: 35232: loss: 0.2461015921:
21: 38432: loss: 0.2452509091:
21: 41632: loss: 0.2460006939:
21: 44832: loss: 0.2450245791:
21: 48032: loss: 0.2459250556:
21: 51232: loss: 0.2462077201:
21: 54432: loss: 0.2460263810:
21: 57632: loss: 0.2455599544:
21: 60832: loss: 0.2453493022:
21: 64032: loss: 0.2464926913:
21: 67232: loss: 0.2467469327:
21: 70432: loss: 0.2468282419:
21: 73632: loss: 0.2470057232:
21: 76832: loss: 0.2467592802:
21: 80032: loss: 0.2467750199:
21: 83232: loss: 0.2468113067:
21: 86432: loss: 0.2468108291:
21: 89632: loss: 0.2472125641:
21: 92832: loss: 0.2470924562:
21: 96032: loss: 0.2472520700:
21: 99232: loss: 0.2470293613:
21: 102432: loss: 0.2470579874:
21: 105632: loss: 0.2472109035:
21: 108832: loss: 0.2470380001:
21: 112032: loss: 0.2474790714:
21: 115232: loss: 0.2472389430:
21: 118432: loss: 0.2472522158:
21: 121632: loss: 0.2472692550:
21: 124832: loss: 0.2471111873:
21: 128032: loss: 0.2472989080:
21: 131232: loss: 0.2471764950:
21: 134432: loss: 0.2469488262:
Dev-Acc: 21: Accuracy: 0.9431656003: precision: 0.5365155131: recall: 0.1911239585: f1: 0.2818455366
Train-Acc: 21: Accuracy: 0.9073988795: precision: 0.8357180710: recall: 0.2073499441: f1: 0.3322623124
22: 3232: loss: 0.2433128108:
22: 6432: loss: 0.2505348771:
22: 9632: loss: 0.2495612887:
22: 12832: loss: 0.2481414793:
22: 16032: loss: 0.2475067260:
22: 19232: loss: 0.2475962230:
22: 22432: loss: 0.2449325738:
22: 25632: loss: 0.2457232699:
22: 28832: loss: 0.2445399072:
22: 32032: loss: 0.2442628027:
22: 35232: loss: 0.2443575866:
22: 38432: loss: 0.2442679566:
22: 41632: loss: 0.2444016370:
22: 44832: loss: 0.2430738591:
22: 48032: loss: 0.2430390915:
22: 51232: loss: 0.2423273814:
22: 54432: loss: 0.2430057860:
22: 57632: loss: 0.2423263566:
22: 60832: loss: 0.2424052172:
22: 64032: loss: 0.2427846225:
22: 67232: loss: 0.2429987219:
22: 70432: loss: 0.2428353488:
22: 73632: loss: 0.2428497124:
22: 76832: loss: 0.2429127612:
22: 80032: loss: 0.2430529761:
22: 83232: loss: 0.2430315242:
22: 86432: loss: 0.2430386337:
22: 89632: loss: 0.2429085895:
22: 92832: loss: 0.2426508002:
22: 96032: loss: 0.2427613562:
22: 99232: loss: 0.2425891485:
22: 102432: loss: 0.2424149834:
22: 105632: loss: 0.2430564801:
22: 108832: loss: 0.2431099303:
22: 112032: loss: 0.2430862276:
22: 115232: loss: 0.2434612951:
22: 118432: loss: 0.2442181986:
22: 121632: loss: 0.2438022900:
22: 124832: loss: 0.2440079672:
22: 128032: loss: 0.2439642983:
22: 131232: loss: 0.2437391907:
22: 134432: loss: 0.2435026725:
Dev-Acc: 22: Accuracy: 0.9419352412: precision: 0.5053723601: recall: 0.2319333447: f1: 0.3179487179
Train-Acc: 22: Accuracy: 0.9093127251: precision: 0.8171506352: recall: 0.2368023141: f1: 0.3671950660
23: 3232: loss: 0.2263022549:
23: 6432: loss: 0.2326796892:
23: 9632: loss: 0.2367544180:
23: 12832: loss: 0.2394381000:
23: 16032: loss: 0.2395855570:
23: 19232: loss: 0.2412948359:
23: 22432: loss: 0.2425137017:
23: 25632: loss: 0.2411042215:
23: 28832: loss: 0.2404624477:
23: 32032: loss: 0.2415169883:
23: 35232: loss: 0.2422110183:
23: 38432: loss: 0.2436937506:
23: 41632: loss: 0.2433522266:
23: 44832: loss: 0.2429308489:
23: 48032: loss: 0.2433830027:
23: 51232: loss: 0.2428695828:
23: 54432: loss: 0.2420385657:
23: 57632: loss: 0.2402062342:
23: 60832: loss: 0.2401392195:
23: 64032: loss: 0.2399302307:
23: 67232: loss: 0.2400479059:
23: 70432: loss: 0.2399526770:
23: 73632: loss: 0.2396104698:
23: 76832: loss: 0.2399875044:
23: 80032: loss: 0.2408608108:
23: 83232: loss: 0.2409276708:
23: 86432: loss: 0.2407243375:
23: 89632: loss: 0.2406448890:
23: 92832: loss: 0.2404891838:
23: 96032: loss: 0.2405890968:
23: 99232: loss: 0.2403305914:
23: 102432: loss: 0.2399624291:
23: 105632: loss: 0.2402396641:
23: 108832: loss: 0.2395811585:
23: 112032: loss: 0.2391457275:
23: 115232: loss: 0.2390413574:
23: 118432: loss: 0.2390803360:
23: 121632: loss: 0.2392542820:
23: 124832: loss: 0.2395658950:
23: 128032: loss: 0.2397138757:
23: 131232: loss: 0.2400718234:
23: 134432: loss: 0.2398554640:
Dev-Acc: 23: Accuracy: 0.9408537149: precision: 0.4870967742: recall: 0.2567590546: f1: 0.3362654493
Train-Acc: 23: Accuracy: 0.9113141894: precision: 0.8084807074: recall: 0.2644796529: f1: 0.3985733393
24: 3232: loss: 0.2530676587:
24: 6432: loss: 0.2475747374:
24: 9632: loss: 0.2406097081:
24: 12832: loss: 0.2417223884:
24: 16032: loss: 0.2409988593:
24: 19232: loss: 0.2418837843:
24: 22432: loss: 0.2395757637:
24: 25632: loss: 0.2376985123:
24: 28832: loss: 0.2380314360:
24: 32032: loss: 0.2371053907:
24: 35232: loss: 0.2375085946:
24: 38432: loss: 0.2385097652:
24: 41632: loss: 0.2382731163:
24: 44832: loss: 0.2377924221:
24: 48032: loss: 0.2363990910:
24: 51232: loss: 0.2366084876:
24: 54432: loss: 0.2356001106:
24: 57632: loss: 0.2349975313:
24: 60832: loss: 0.2358734423:
24: 64032: loss: 0.2361375548:
24: 67232: loss: 0.2353592637:
24: 70432: loss: 0.2355553589:
24: 73632: loss: 0.2356175250:
24: 76832: loss: 0.2357010577:
24: 80032: loss: 0.2358905065:
24: 83232: loss: 0.2364608272:
24: 86432: loss: 0.2372992837:
24: 89632: loss: 0.2374025675:
24: 92832: loss: 0.2371651128:
24: 96032: loss: 0.2370510526:
24: 99232: loss: 0.2368377407:
24: 102432: loss: 0.2368181638:
24: 105632: loss: 0.2364854248:
24: 108832: loss: 0.2365844099:
24: 112032: loss: 0.2363848040:
24: 115232: loss: 0.2366424917:
24: 118432: loss: 0.2365326021:
24: 121632: loss: 0.2363210484:
24: 124832: loss: 0.2360519300:
24: 128032: loss: 0.2360987259:
24: 131232: loss: 0.2360337012:
24: 134432: loss: 0.2363579665:
Dev-Acc: 24: Accuracy: 0.9401492476: precision: 0.4780968959: recall: 0.2802244516: f1: 0.3533447684
Train-Acc: 24: Accuracy: 0.9130015969: precision: 0.8058180471: recall: 0.2859115114: f1: 0.4220690994
25: 3232: loss: 0.2348058191:
25: 6432: loss: 0.2313792652:
25: 9632: loss: 0.2316070566:
25: 12832: loss: 0.2338592621:
25: 16032: loss: 0.2353364740:
25: 19232: loss: 0.2344199310:
25: 22432: loss: 0.2351673768:
25: 25632: loss: 0.2338790751:
25: 28832: loss: 0.2342331179:
25: 32032: loss: 0.2339305150:
25: 35232: loss: 0.2347096839:
25: 38432: loss: 0.2343025223:
25: 41632: loss: 0.2351571446:
25: 44832: loss: 0.2342794715:
25: 48032: loss: 0.2354180541:
25: 51232: loss: 0.2346692981:
25: 54432: loss: 0.2341536024:
25: 57632: loss: 0.2345065814:
25: 60832: loss: 0.2343202078:
25: 64032: loss: 0.2346023363:
25: 67232: loss: 0.2351713617:
25: 70432: loss: 0.2347149751:
25: 73632: loss: 0.2345888425:
25: 76832: loss: 0.2342925474:
25: 80032: loss: 0.2345314225:
25: 83232: loss: 0.2350202751:
25: 86432: loss: 0.2346431719:
25: 89632: loss: 0.2333230763:
25: 92832: loss: 0.2335136283:
25: 96032: loss: 0.2332564179:
25: 99232: loss: 0.2336552831:
25: 102432: loss: 0.2333460163:
25: 105632: loss: 0.2333929717:
25: 108832: loss: 0.2334290926:
25: 112032: loss: 0.2337885103:
25: 115232: loss: 0.2341673881:
25: 118432: loss: 0.2338674137:
25: 121632: loss: 0.2339376458:
25: 124832: loss: 0.2336692327:
25: 128032: loss: 0.2337035565:
25: 131232: loss: 0.2337796046:
25: 134432: loss: 0.2336538863:
Dev-Acc: 25: Accuracy: 0.9394149780: precision: 0.4701828783: recall: 0.3016493794: f1: 0.3675160555
Train-Acc: 25: Accuracy: 0.9142360687: precision: 0.8043859649: recall: 0.3014265992: f1: 0.4385251781
26: 3232: loss: 0.2424643070:
26: 6432: loss: 0.2316645246:
26: 9632: loss: 0.2290040650:
26: 12832: loss: 0.2298672759:
26: 16032: loss: 0.2296627963:
26: 19232: loss: 0.2294211674:
26: 22432: loss: 0.2315727148:
26: 25632: loss: 0.2321851069:
26: 28832: loss: 0.2317517783:
26: 32032: loss: 0.2314158189:
26: 35232: loss: 0.2317812476:
26: 38432: loss: 0.2331253133:
26: 41632: loss: 0.2324389187:
26: 44832: loss: 0.2320188336:
26: 48032: loss: 0.2321313740:
26: 51232: loss: 0.2313738100:
26: 54432: loss: 0.2309694274:
26: 57632: loss: 0.2296322763:
26: 60832: loss: 0.2303073305:
26: 64032: loss: 0.2299146390:
26: 67232: loss: 0.2299653702:
26: 70432: loss: 0.2299267521:
26: 73632: loss: 0.2292455464:
26: 76832: loss: 0.2302410897:
26: 80032: loss: 0.2305341472:
26: 83232: loss: 0.2298064143:
26: 86432: loss: 0.2302368636:
26: 89632: loss: 0.2299027172:
26: 92832: loss: 0.2298452446:
26: 96032: loss: 0.2297997083:
26: 99232: loss: 0.2299441584:
26: 102432: loss: 0.2300445110:
26: 105632: loss: 0.2301737530:
26: 108832: loss: 0.2299719396:
26: 112032: loss: 0.2303960685:
26: 115232: loss: 0.2299468905:
26: 118432: loss: 0.2301966548:
26: 121632: loss: 0.2304532879:
26: 124832: loss: 0.2303268023:
26: 128032: loss: 0.2305784715:
26: 131232: loss: 0.2304217114:
26: 134432: loss: 0.2303489279:
Dev-Acc: 26: Accuracy: 0.9384425879: precision: 0.4598358617: recall: 0.3144023125: f1: 0.3734599071
Train-Acc: 26: Accuracy: 0.9151052833: precision: 0.7996326599: recall: 0.3148379462: f1: 0.4517924528
27: 3232: loss: 0.2285873876:
27: 6432: loss: 0.2374202204:
27: 9632: loss: 0.2351723704:
27: 12832: loss: 0.2322506076:
27: 16032: loss: 0.2327144749:
27: 19232: loss: 0.2347245433:
27: 22432: loss: 0.2327569983:
27: 25632: loss: 0.2334673348:
27: 28832: loss: 0.2318425460:
27: 32032: loss: 0.2320622602:
27: 35232: loss: 0.2319048253:
27: 38432: loss: 0.2320686038:
27: 41632: loss: 0.2323134005:
27: 44832: loss: 0.2318437359:
27: 48032: loss: 0.2313518375:
27: 51232: loss: 0.2319051850:
27: 54432: loss: 0.2312204826:
27: 57632: loss: 0.2306016120:
27: 60832: loss: 0.2307153144:
27: 64032: loss: 0.2314929122:
27: 67232: loss: 0.2320717232:
27: 70432: loss: 0.2320219089:
27: 73632: loss: 0.2314490243:
27: 76832: loss: 0.2314408253:
27: 80032: loss: 0.2309174832:
27: 83232: loss: 0.2307261048:
27: 86432: loss: 0.2302400490:
27: 89632: loss: 0.2298298934:
27: 92832: loss: 0.2292645673:
27: 96032: loss: 0.2288536576:
27: 99232: loss: 0.2285885762:
27: 102432: loss: 0.2280722691:
27: 105632: loss: 0.2278400296:
27: 108832: loss: 0.2281425279:
27: 112032: loss: 0.2280630382:
27: 115232: loss: 0.2281801978:
27: 118432: loss: 0.2284448990:
27: 121632: loss: 0.2284139685:
27: 124832: loss: 0.2283382833:
27: 128032: loss: 0.2281771545:
27: 131232: loss: 0.2281683068:
27: 134432: loss: 0.2279774580:
Dev-Acc: 27: Accuracy: 0.9379960895: precision: 0.4558752998: recall: 0.3232443462: f1: 0.3782708188
Train-Acc: 27: Accuracy: 0.9160110950: precision: 0.7985206625: recall: 0.3264742620: f1: 0.4634624358
28: 3232: loss: 0.2285085698:
28: 6432: loss: 0.2321045038:
28: 9632: loss: 0.2342973288:
28: 12832: loss: 0.2322686219:
28: 16032: loss: 0.2324505932:
28: 19232: loss: 0.2292812158:
28: 22432: loss: 0.2277254888:
28: 25632: loss: 0.2260981265:
28: 28832: loss: 0.2279179186:
28: 32032: loss: 0.2281619036:
28: 35232: loss: 0.2273607777:
28: 38432: loss: 0.2262278384:
28: 41632: loss: 0.2267468688:
28: 44832: loss: 0.2273814078:
28: 48032: loss: 0.2275411863:
28: 51232: loss: 0.2272803707:
28: 54432: loss: 0.2269074189:
28: 57632: loss: 0.2276830692:
28: 60832: loss: 0.2269862034:
28: 64032: loss: 0.2269831571:
28: 67232: loss: 0.2271899895:
28: 70432: loss: 0.2273943503:
28: 73632: loss: 0.2273468434:
28: 76832: loss: 0.2266872528:
28: 80032: loss: 0.2261635722:
28: 83232: loss: 0.2264571635:
28: 86432: loss: 0.2262201091:
28: 89632: loss: 0.2259430080:
28: 92832: loss: 0.2255759157:
28: 96032: loss: 0.2256810314:
28: 99232: loss: 0.2254184633:
28: 102432: loss: 0.2259050280:
28: 105632: loss: 0.2259267959:
28: 108832: loss: 0.2255464685:
28: 112032: loss: 0.2254755153:
28: 115232: loss: 0.2256438520:
28: 118432: loss: 0.2259238179:
28: 121632: loss: 0.2259249834:
28: 124832: loss: 0.2258025492:
28: 128032: loss: 0.2257205449:
28: 131232: loss: 0.2258092684:
28: 134432: loss: 0.2255195838:
Dev-Acc: 28: Accuracy: 0.9375396967: precision: 0.4513399154: recall: 0.3264750893: f1: 0.3788850518
Train-Acc: 28: Accuracy: 0.9170629382: precision: 0.7986681121: recall: 0.3390309644: f1: 0.4760014768
29: 3232: loss: 0.2168129270:
29: 6432: loss: 0.2217763747:
29: 9632: loss: 0.2273454118:
29: 12832: loss: 0.2245201113:
29: 16032: loss: 0.2240167598:
29: 19232: loss: 0.2239695021:
29: 22432: loss: 0.2248529843:
29: 25632: loss: 0.2253324257:
29: 28832: loss: 0.2249076339:
29: 32032: loss: 0.2242750758:
29: 35232: loss: 0.2238750861:
29: 38432: loss: 0.2246026432:
29: 41632: loss: 0.2248358276:
29: 44832: loss: 0.2245751201:
29: 48032: loss: 0.2256419638:
29: 51232: loss: 0.2262692770:
29: 54432: loss: 0.2254543873:
29: 57632: loss: 0.2255121095:
29: 60832: loss: 0.2261847835:
29: 64032: loss: 0.2259461467:
29: 67232: loss: 0.2256455869:
29: 70432: loss: 0.2252130580:
29: 73632: loss: 0.2248750300:
29: 76832: loss: 0.2241196894:
29: 80032: loss: 0.2236970108:
29: 83232: loss: 0.2239804516:
29: 86432: loss: 0.2237547365:
29: 89632: loss: 0.2234971321:
29: 92832: loss: 0.2239123493:
29: 96032: loss: 0.2240886776:
29: 99232: loss: 0.2244028674:
29: 102432: loss: 0.2237970814:
29: 105632: loss: 0.2237874350:
29: 108832: loss: 0.2235965304:
29: 112032: loss: 0.2233496516:
29: 115232: loss: 0.2231097849:
29: 118432: loss: 0.2223724959:
29: 121632: loss: 0.2223660583:
29: 124832: loss: 0.2224253828:
29: 128032: loss: 0.2226935446:
29: 131232: loss: 0.2229273711:
29: 134432: loss: 0.2229681578:
Dev-Acc: 29: Accuracy: 0.9374701977: precision: 0.4515535098: recall: 0.3336167318: f1: 0.3837277528
Train-Acc: 29: Accuracy: 0.9178153276: precision: 0.7974759615: recall: 0.3489579909: f1: 0.4854804043
30: 3232: loss: 0.2119038353:
30: 6432: loss: 0.2194455973:
30: 9632: loss: 0.2231347088:
30: 12832: loss: 0.2240958113:
30: 16032: loss: 0.2242503695:
30: 19232: loss: 0.2234661353:
30: 22432: loss: 0.2217817444:
30: 25632: loss: 0.2215156361:
30: 28832: loss: 0.2208981363:
30: 32032: loss: 0.2207232583:
30: 35232: loss: 0.2201801800:
30: 38432: loss: 0.2191834784:
30: 41632: loss: 0.2197641458:
30: 44832: loss: 0.2191615891:
30: 48032: loss: 0.2186529468:
30: 51232: loss: 0.2188699145:
30: 54432: loss: 0.2204816144:
30: 57632: loss: 0.2203702938:
30: 60832: loss: 0.2206834935:
30: 64032: loss: 0.2201363377:
30: 67232: loss: 0.2202060463:
30: 70432: loss: 0.2202978452:
30: 73632: loss: 0.2200886687:
30: 76832: loss: 0.2200012068:
30: 80032: loss: 0.2197356274:
30: 83232: loss: 0.2195188982:
30: 86432: loss: 0.2197144414:
30: 89632: loss: 0.2193550077:
30: 92832: loss: 0.2188626513:
30: 96032: loss: 0.2189236197:
30: 99232: loss: 0.2195793482:
30: 102432: loss: 0.2199632878:
30: 105632: loss: 0.2198718998:
30: 108832: loss: 0.2197092679:
30: 112032: loss: 0.2199265653:
30: 115232: loss: 0.2201838021:
30: 118432: loss: 0.2203273657:
30: 121632: loss: 0.2208717428:
30: 124832: loss: 0.2208248511:
30: 128032: loss: 0.2206139850:
30: 131232: loss: 0.2206965549:
30: 134432: loss: 0.2205679740:
Dev-Acc: 30: Accuracy: 0.9374007583: precision: 0.4520609319: recall: 0.3431389220: f1: 0.3901401643
Train-Acc: 30: Accuracy: 0.9188160896: precision: 0.7976173180: recall: 0.3609230162: f1: 0.4969675025
31: 3232: loss: 0.2246587920:
31: 6432: loss: 0.2217106117:
31: 9632: loss: 0.2195518933:
31: 12832: loss: 0.2203617775:
31: 16032: loss: 0.2213607015:
31: 19232: loss: 0.2216025671:
31: 22432: loss: 0.2209744956:
31: 25632: loss: 0.2219879314:
31: 28832: loss: 0.2224528121:
31: 32032: loss: 0.2219965326:
31: 35232: loss: 0.2211991221:
31: 38432: loss: 0.2225694395:
31: 41632: loss: 0.2216882250:
31: 44832: loss: 0.2215455511:
31: 48032: loss: 0.2213551426:
31: 51232: loss: 0.2213190722:
31: 54432: loss: 0.2219317453:
31: 57632: loss: 0.2219644149:
31: 60832: loss: 0.2230164721:
31: 64032: loss: 0.2233810521:
31: 67232: loss: 0.2231944564:
31: 70432: loss: 0.2226201846:
31: 73632: loss: 0.2222406789:
31: 76832: loss: 0.2223570266:
31: 80032: loss: 0.2222077090:
31: 83232: loss: 0.2219125361:
31: 86432: loss: 0.2213794537:
31: 89632: loss: 0.2209866180:
31: 92832: loss: 0.2206235257:
31: 96032: loss: 0.2206709886:
31: 99232: loss: 0.2208515788:
31: 102432: loss: 0.2203277001:
31: 105632: loss: 0.2201224976:
31: 108832: loss: 0.2201615592:
31: 112032: loss: 0.2199265282:
31: 115232: loss: 0.2197919001:
31: 118432: loss: 0.2197702007:
31: 121632: loss: 0.2195528149:
31: 124832: loss: 0.2192127869:
31: 128032: loss: 0.2191902589:
31: 131232: loss: 0.2195581276:
31: 134432: loss: 0.2190675852:
Dev-Acc: 31: Accuracy: 0.9367061853: precision: 0.4456331878: recall: 0.3470498215: f1: 0.3902112609
Train-Acc: 31: Accuracy: 0.9197218418: precision: 0.7994891443: recall: 0.3703898495: f1: 0.5062449456
32: 3232: loss: 0.2136610480:
32: 6432: loss: 0.2106399878:
32: 9632: loss: 0.2072519881:
32: 12832: loss: 0.2081172668:
32: 16032: loss: 0.2105452065:
32: 19232: loss: 0.2122565483:
32: 22432: loss: 0.2129156666:
32: 25632: loss: 0.2142737856:
32: 28832: loss: 0.2150645592:
32: 32032: loss: 0.2150186800:
32: 35232: loss: 0.2136622465:
32: 38432: loss: 0.2128492860:
32: 41632: loss: 0.2125455945:
32: 44832: loss: 0.2116162257:
32: 48032: loss: 0.2120568217:
32: 51232: loss: 0.2123952423:
32: 54432: loss: 0.2131486893:
32: 57632: loss: 0.2139575447:
32: 60832: loss: 0.2146031842:
32: 64032: loss: 0.2149630941:
32: 67232: loss: 0.2150640741:
32: 70432: loss: 0.2147682077:
32: 73632: loss: 0.2146668375:
32: 76832: loss: 0.2147318521:
32: 80032: loss: 0.2147137925:
32: 83232: loss: 0.2146601677:
32: 86432: loss: 0.2144776279:
32: 89632: loss: 0.2146397899:
32: 92832: loss: 0.2144217735:
32: 96032: loss: 0.2150224235:
32: 99232: loss: 0.2150903870:
32: 102432: loss: 0.2154618287:
32: 105632: loss: 0.2153795655:
32: 108832: loss: 0.2153681608:
32: 112032: loss: 0.2161348457:
32: 115232: loss: 0.2163580926:
32: 118432: loss: 0.2163778324:
32: 121632: loss: 0.2162714683:
32: 124832: loss: 0.2162499317:
32: 128032: loss: 0.2163375982:
32: 131232: loss: 0.2164640417:
32: 134432: loss: 0.2167321908:
Dev-Acc: 32: Accuracy: 0.9362696409: precision: 0.4422174840: recall: 0.3526611121: f1: 0.3923942863
Train-Acc: 32: Accuracy: 0.9202112556: precision: 0.7972823073: recall: 0.3780159095: f1: 0.5128662534
33: 3232: loss: 0.2147225253:
33: 6432: loss: 0.2207913079:
33: 9632: loss: 0.2171691755:
33: 12832: loss: 0.2170627029:
33: 16032: loss: 0.2182693375:
33: 19232: loss: 0.2179462237:
33: 22432: loss: 0.2181192953:
33: 25632: loss: 0.2184343712:
33: 28832: loss: 0.2170382302:
33: 32032: loss: 0.2178657808:
33: 35232: loss: 0.2179867732:
33: 38432: loss: 0.2173041174:
33: 41632: loss: 0.2175335396:
33: 44832: loss: 0.2174271552:
33: 48032: loss: 0.2176948122:
33: 51232: loss: 0.2178610337:
33: 54432: loss: 0.2176515011:
33: 57632: loss: 0.2173118883:
33: 60832: loss: 0.2169218904:
33: 64032: loss: 0.2169529889:
33: 67232: loss: 0.2168575348:
33: 70432: loss: 0.2165855794:
33: 73632: loss: 0.2168443894:
33: 76832: loss: 0.2167909317:
33: 80032: loss: 0.2167902787:
33: 83232: loss: 0.2169239009:
33: 86432: loss: 0.2163058254:
33: 89632: loss: 0.2159793044:
33: 92832: loss: 0.2157793279:
33: 96032: loss: 0.2158399961:
33: 99232: loss: 0.2156259454:
33: 102432: loss: 0.2157630504:
33: 105632: loss: 0.2157388858:
33: 108832: loss: 0.2157667335:
33: 112032: loss: 0.2155822184:
33: 115232: loss: 0.2157161309:
33: 118432: loss: 0.2153943097:
33: 121632: loss: 0.2149910995:
33: 124832: loss: 0.2146228042:
33: 128032: loss: 0.2149051502:
33: 131232: loss: 0.2146573970:
33: 134432: loss: 0.2146538844:
Dev-Acc: 33: Accuracy: 0.9359322786: precision: 0.4398496241: recall: 0.3581023635: f1: 0.3947886400
Train-Acc: 33: Accuracy: 0.9207518101: precision: 0.7972199509: recall: 0.3845900993: f1: 0.5188700164
34: 3232: loss: 0.2100080434:
34: 6432: loss: 0.2111283339:
34: 9632: loss: 0.2074692155:
34: 12832: loss: 0.2119918669:
34: 16032: loss: 0.2112599402:
34: 19232: loss: 0.2122272624:
34: 22432: loss: 0.2139336492:
34: 25632: loss: 0.2110124364:
34: 28832: loss: 0.2099760209:
34: 32032: loss: 0.2104350803:
34: 35232: loss: 0.2122231354:
34: 38432: loss: 0.2131621055:
34: 41632: loss: 0.2126146555:
34: 44832: loss: 0.2122709321:
34: 48032: loss: 0.2122269930:
34: 51232: loss: 0.2131225980:
34: 54432: loss: 0.2138465355:
34: 57632: loss: 0.2141712821:
34: 60832: loss: 0.2149126342:
34: 64032: loss: 0.2146780821:
34: 67232: loss: 0.2139545867:
34: 70432: loss: 0.2132660692:
34: 73632: loss: 0.2134101855:
34: 76832: loss: 0.2127684357:
34: 80032: loss: 0.2131074541:
34: 83232: loss: 0.2128849667:
34: 86432: loss: 0.2124714812:
34: 89632: loss: 0.2129027236:
34: 92832: loss: 0.2128675525:
34: 96032: loss: 0.2129681906:
34: 99232: loss: 0.2126854609:
34: 102432: loss: 0.2128209054:
34: 105632: loss: 0.2127588211:
34: 108832: loss: 0.2130029764:
34: 112032: loss: 0.2125900570:
34: 115232: loss: 0.2126978292:
34: 118432: loss: 0.2124775403:
34: 121632: loss: 0.2127202452:
34: 124832: loss: 0.2127565500:
34: 128032: loss: 0.2129462893:
34: 131232: loss: 0.2131052502:
34: 134432: loss: 0.2131183671:
Dev-Acc: 34: Accuracy: 0.9355750680: precision: 0.4369851730: recall: 0.3608229893: f1: 0.3952686970
Train-Acc: 34: Accuracy: 0.9212558270: precision: 0.7963879599: recall: 0.3913615147: f1: 0.5248170678
35: 3232: loss: 0.2186760454:
35: 6432: loss: 0.2132923397:
35: 9632: loss: 0.2104198110:
35: 12832: loss: 0.2083328646:
35: 16032: loss: 0.2067931580:
35: 19232: loss: 0.2084754772:
35: 22432: loss: 0.2083796706:
35: 25632: loss: 0.2102099972:
35: 28832: loss: 0.2109066385:
35: 32032: loss: 0.2098136669:
35: 35232: loss: 0.2095469438:
35: 38432: loss: 0.2103528101:
35: 41632: loss: 0.2109748576:
35: 44832: loss: 0.2115198843:
35: 48032: loss: 0.2113219904:
35: 51232: loss: 0.2117159910:
35: 54432: loss: 0.2124905533:
35: 57632: loss: 0.2127154044:
35: 60832: loss: 0.2121825815:
35: 64032: loss: 0.2121358391:
35: 67232: loss: 0.2121500173:
35: 70432: loss: 0.2124240150:
35: 73632: loss: 0.2121148106:
35: 76832: loss: 0.2118268293:
35: 80032: loss: 0.2119958282:
35: 83232: loss: 0.2122845009:
35: 86432: loss: 0.2120197622:
35: 89632: loss: 0.2121473963:
35: 92832: loss: 0.2118823196:
35: 96032: loss: 0.2117539255:
35: 99232: loss: 0.2118798113:
35: 102432: loss: 0.2121026226:
35: 105632: loss: 0.2122985425:
35: 108832: loss: 0.2126572172:
35: 112032: loss: 0.2122944571:
35: 115232: loss: 0.2120584905:
35: 118432: loss: 0.2121139290:
35: 121632: loss: 0.2120088718:
35: 124832: loss: 0.2118900854:
35: 128032: loss: 0.2116527154:
35: 131232: loss: 0.2115168555:
35: 134432: loss: 0.2112932945:
Dev-Acc: 35: Accuracy: 0.9352178574: precision: 0.4344660194: recall: 0.3652440061: f1: 0.3968591224
Train-Acc: 35: Accuracy: 0.9215188026: precision: 0.7957103138: recall: 0.3951088028: f1: 0.5280267088
36: 3232: loss: 0.2017091558:
36: 6432: loss: 0.2137028451:
36: 9632: loss: 0.2140774479:
36: 12832: loss: 0.2126293788:
36: 16032: loss: 0.2153191244:
36: 19232: loss: 0.2128115289:
36: 22432: loss: 0.2148519460:
36: 25632: loss: 0.2148168630:
36: 28832: loss: 0.2139431612:
36: 32032: loss: 0.2135833100:
36: 35232: loss: 0.2137156585:
36: 38432: loss: 0.2131580655:
36: 41632: loss: 0.2140115769:
36: 44832: loss: 0.2130794513:
36: 48032: loss: 0.2126469069:
36: 51232: loss: 0.2125211593:
36: 54432: loss: 0.2124225421:
36: 57632: loss: 0.2118067949:
36: 60832: loss: 0.2111367336:
36: 64032: loss: 0.2113165763:
36: 67232: loss: 0.2112375372:
36: 70432: loss: 0.2108516265:
36: 73632: loss: 0.2113114061:
36: 76832: loss: 0.2109332013:
36: 80032: loss: 0.2111082574:
36: 83232: loss: 0.2112164638:
36: 86432: loss: 0.2112322578:
36: 89632: loss: 0.2105855668:
36: 92832: loss: 0.2101519997:
36: 96032: loss: 0.2099370569:
36: 99232: loss: 0.2102937051:
36: 102432: loss: 0.2102528506:
36: 105632: loss: 0.2103677303:
36: 108832: loss: 0.2102671885:
36: 112032: loss: 0.2100064715:
36: 115232: loss: 0.2101351084:
36: 118432: loss: 0.2101621082:
36: 121632: loss: 0.2102440734:
36: 124832: loss: 0.2099321449:
36: 128032: loss: 0.2098717037:
36: 131232: loss: 0.2095915350:
36: 134432: loss: 0.2096793451:
Dev-Acc: 36: Accuracy: 0.9350491762: precision: 0.4337253339: recall: 0.3700051012: f1: 0.3993393283
Train-Acc: 36: Accuracy: 0.9219644070: precision: 0.7955613577: recall: 0.4006311222: f1: 0.5329019282
37: 3232: loss: 0.2096318029:
37: 6432: loss: 0.2068332081:
37: 9632: loss: 0.2054209798:
37: 12832: loss: 0.2037679005:
37: 16032: loss: 0.2044033305:
37: 19232: loss: 0.2072770565:
37: 22432: loss: 0.2077681717:
37: 25632: loss: 0.2069834977:
37: 28832: loss: 0.2082423930:
37: 32032: loss: 0.2083801043:
37: 35232: loss: 0.2084247390:
37: 38432: loss: 0.2084253571:
37: 41632: loss: 0.2079884951:
37: 44832: loss: 0.2078928447:
37: 48032: loss: 0.2090893298:
37: 51232: loss: 0.2088765471:
37: 54432: loss: 0.2094311358:
37: 57632: loss: 0.2095121854:
37: 60832: loss: 0.2092093104:
37: 64032: loss: 0.2087014607:
37: 67232: loss: 0.2087529169:
37: 70432: loss: 0.2087578852:
37: 73632: loss: 0.2086934668:
37: 76832: loss: 0.2083669891:
37: 80032: loss: 0.2086148056:
37: 83232: loss: 0.2084105878:
37: 86432: loss: 0.2082783059:
37: 89632: loss: 0.2076583153:
37: 92832: loss: 0.2074555088:
37: 96032: loss: 0.2073353173:
37: 99232: loss: 0.2069585159:
37: 102432: loss: 0.2067962385:
37: 105632: loss: 0.2068326885:
37: 108832: loss: 0.2071535591:
37: 112032: loss: 0.2073495975:
37: 115232: loss: 0.2071977646:
37: 118432: loss: 0.2076550532:
37: 121632: loss: 0.2079398835:
37: 124832: loss: 0.2079006922:
37: 128032: loss: 0.2077988294:
37: 131232: loss: 0.2081657620:
37: 134432: loss: 0.2081862773:
Dev-Acc: 37: Accuracy: 0.9346721768: precision: 0.4314146341: recall: 0.3759564700: f1: 0.4017808468
Train-Acc: 37: Accuracy: 0.9225122333: precision: 0.7966997551: recall: 0.4062849254: f1: 0.5381400209
38: 3232: loss: 0.2065719005:
38: 6432: loss: 0.2117328955:
38: 9632: loss: 0.2141122013:
38: 12832: loss: 0.2103878847:
38: 16032: loss: 0.2119048388:
38: 19232: loss: 0.2095467041:
38: 22432: loss: 0.2090756868:
38: 25632: loss: 0.2087077350:
38: 28832: loss: 0.2075983617:
38: 32032: loss: 0.2065535477:
38: 35232: loss: 0.2075456144:
38: 38432: loss: 0.2076987504:
38: 41632: loss: 0.2086146867:
38: 44832: loss: 0.2076654113:
38: 48032: loss: 0.2082144701:
38: 51232: loss: 0.2082503082:
38: 54432: loss: 0.2085734501:
38: 57632: loss: 0.2083341983:
38: 60832: loss: 0.2084974652:
38: 64032: loss: 0.2080219635:
38: 67232: loss: 0.2083062497:
38: 70432: loss: 0.2086576887:
38: 73632: loss: 0.2075948943:
38: 76832: loss: 0.2077476388:
38: 80032: loss: 0.2078024554:
38: 83232: loss: 0.2077265494:
38: 86432: loss: 0.2079193016:
38: 89632: loss: 0.2074865322:
38: 92832: loss: 0.2076420421:
38: 96032: loss: 0.2079160516:
38: 99232: loss: 0.2075472677:
38: 102432: loss: 0.2081012507:
38: 105632: loss: 0.2082844401:
38: 108832: loss: 0.2081693997:
38: 112032: loss: 0.2079938024:
38: 115232: loss: 0.2079760708:
38: 118432: loss: 0.2077313830:
38: 121632: loss: 0.2075657981:
38: 124832: loss: 0.2076299308:
38: 128032: loss: 0.2070147636:
38: 131232: loss: 0.2069871624:
38: 134432: loss: 0.2067027341:
Dev-Acc: 38: Accuracy: 0.9345233440: precision: 0.4309083911: recall: 0.3807175650: f1: 0.4042610815
Train-Acc: 38: Accuracy: 0.9229212999: precision: 0.7969407266: recall: 0.4110183420: f1: 0.5423317141
39: 3232: loss: 0.2019720681:
39: 6432: loss: 0.1966277113:
39: 9632: loss: 0.2022936508:
39: 12832: loss: 0.2011387137:
39: 16032: loss: 0.2016305780:
39: 19232: loss: 0.2022185246:
39: 22432: loss: 0.2020207850:
39: 25632: loss: 0.2023840999:
39: 28832: loss: 0.2025510176:
39: 32032: loss: 0.2019432722:
39: 35232: loss: 0.2029399025:
39: 38432: loss: 0.2036277809:
39: 41632: loss: 0.2043289860:
39: 44832: loss: 0.2048811413:
39: 48032: loss: 0.2033672035:
39: 51232: loss: 0.2034400649:
39: 54432: loss: 0.2029467930:
39: 57632: loss: 0.2032195518:
39: 60832: loss: 0.2037713850:
39: 64032: loss: 0.2036721673:
39: 67232: loss: 0.2039296049:
39: 70432: loss: 0.2036258279:
39: 73632: loss: 0.2039140344:
39: 76832: loss: 0.2044816321:
39: 80032: loss: 0.2044202943:
39: 83232: loss: 0.2050236255:
39: 86432: loss: 0.2045936272:
39: 89632: loss: 0.2044635200:
39: 92832: loss: 0.2039736453:
39: 96032: loss: 0.2040911032:
39: 99232: loss: 0.2038884928:
39: 102432: loss: 0.2038883531:
39: 105632: loss: 0.2039700680:
39: 108832: loss: 0.2041175083:
39: 112032: loss: 0.2041431457:
39: 115232: loss: 0.2043195218:
39: 118432: loss: 0.2044240929:
39: 121632: loss: 0.2044437917:
39: 124832: loss: 0.2047300812:
39: 128032: loss: 0.2048675105:
39: 131232: loss: 0.2052171857:
39: 134432: loss: 0.2054072994:
Dev-Acc: 39: Accuracy: 0.9342355728: precision: 0.4290058924: recall: 0.3837782690: f1: 0.4051337282
Train-Acc: 39: Accuracy: 0.9233961105: precision: 0.7972564812: recall: 0.4164749195: f1: 0.5471347757
40: 3232: loss: 0.2118481441:
40: 6432: loss: 0.2073329842:
40: 9632: loss: 0.2064239970:
40: 12832: loss: 0.2066150051:
40: 16032: loss: 0.2064917824:
40: 19232: loss: 0.2058041877:
40: 22432: loss: 0.2058209293:
40: 25632: loss: 0.2051662941:
40: 28832: loss: 0.2043493165:
40: 32032: loss: 0.2039141451:
40: 35232: loss: 0.2031316303:
40: 38432: loss: 0.2039692530:
40: 41632: loss: 0.2027110232:
40: 44832: loss: 0.2030418864:
40: 48032: loss: 0.2042273574:
40: 51232: loss: 0.2044234800:
40: 54432: loss: 0.2044778387:
40: 57632: loss: 0.2052516405:
40: 60832: loss: 0.2054120014:
40: 64032: loss: 0.2056289794:
40: 67232: loss: 0.2055144885:
40: 70432: loss: 0.2053176483:
40: 73632: loss: 0.2050679856:
40: 76832: loss: 0.2048433370:
40: 80032: loss: 0.2051268338:
40: 83232: loss: 0.2051068892:
40: 86432: loss: 0.2051113190:
40: 89632: loss: 0.2048401244:
40: 92832: loss: 0.2046710854:
40: 96032: loss: 0.2048177018:
40: 99232: loss: 0.2043726565:
40: 102432: loss: 0.2041501035:
40: 105632: loss: 0.2041906683:
40: 108832: loss: 0.2044437251:
40: 112032: loss: 0.2042932153:
40: 115232: loss: 0.2045147852:
40: 118432: loss: 0.2045489238:
40: 121632: loss: 0.2046298938:
40: 124832: loss: 0.2043689560:
40: 128032: loss: 0.2041574965:
40: 131232: loss: 0.2041480731:
40: 134432: loss: 0.2041361249:
Dev-Acc: 40: Accuracy: 0.9341462851: precision: 0.4291869614: recall: 0.3895595987: f1: 0.4084142972
Train-Acc: 40: Accuracy: 0.9237540364: precision: 0.7976799301: recall: 0.4204194333: f1: 0.5506285517
41: 3232: loss: 0.2220995017:
41: 6432: loss: 0.2100122175:
41: 9632: loss: 0.2055775391:
41: 12832: loss: 0.2059637995:
41: 16032: loss: 0.2062126651:
41: 19232: loss: 0.2054210807:
41: 22432: loss: 0.2056281863:
41: 25632: loss: 0.2067269888:
41: 28832: loss: 0.2054616160:
41: 32032: loss: 0.2057036353:
41: 35232: loss: 0.2045601870:
41: 38432: loss: 0.2032437249:
41: 41632: loss: 0.2042577532:
41: 44832: loss: 0.2048846170:
41: 48032: loss: 0.2047445467:
41: 51232: loss: 0.2037272166:
41: 54432: loss: 0.2035178496:
41: 57632: loss: 0.2032714073:
41: 60832: loss: 0.2031736979:
41: 64032: loss: 0.2032563173:
41: 67232: loss: 0.2034826661:
41: 70432: loss: 0.2037353463:
41: 73632: loss: 0.2041164216:
41: 76832: loss: 0.2048389927:
41: 80032: loss: 0.2042816845:
41: 83232: loss: 0.2040056279:
41: 86432: loss: 0.2037812612:
41: 89632: loss: 0.2038327484:
41: 92832: loss: 0.2035644634:
41: 96032: loss: 0.2034351727:
41: 99232: loss: 0.2034866897:
41: 102432: loss: 0.2033355502:
41: 105632: loss: 0.2030876672:
41: 108832: loss: 0.2029948202:
41: 112032: loss: 0.2032372422:
41: 115232: loss: 0.2032334154:
41: 118432: loss: 0.2030451427:
41: 121632: loss: 0.2027979690:
41: 124832: loss: 0.2027680337:
41: 128032: loss: 0.2024167182:
41: 131232: loss: 0.2028113328:
41: 134432: loss: 0.2028568435:
Dev-Acc: 41: Accuracy: 0.9339180589: precision: 0.4277767476: recall: 0.3922802245: f1: 0.4092602448
Train-Acc: 41: Accuracy: 0.9242069125: precision: 0.7979051140: recall: 0.4256787851: f1: 0.5551744834
42: 3232: loss: 0.1948450437:
42: 6432: loss: 0.1923248058:
42: 9632: loss: 0.2002795591:
42: 12832: loss: 0.2019358564:
42: 16032: loss: 0.2000928312:
42: 19232: loss: 0.2003707368:
42: 22432: loss: 0.2008009752:
42: 25632: loss: 0.2000759468:
42: 28832: loss: 0.2028725832:
42: 32032: loss: 0.2036427888:
42: 35232: loss: 0.2032252738:
42: 38432: loss: 0.2026457083:
42: 41632: loss: 0.2028078668:
42: 44832: loss: 0.2026942750:
42: 48032: loss: 0.2030542276:
42: 51232: loss: 0.2040001929:
42: 54432: loss: 0.2037505893:
42: 57632: loss: 0.2040462826:
42: 60832: loss: 0.2041537807:
42: 64032: loss: 0.2044684473:
42: 67232: loss: 0.2041853950:
42: 70432: loss: 0.2037234566:
42: 73632: loss: 0.2036088481:
42: 76832: loss: 0.2037218453:
42: 80032: loss: 0.2034823952:
42: 83232: loss: 0.2027935354:
42: 86432: loss: 0.2023536729:
42: 89632: loss: 0.2025755991:
42: 92832: loss: 0.2021050607:
42: 96032: loss: 0.2020381399:
42: 99232: loss: 0.2019004626:
42: 102432: loss: 0.2018184175:
42: 105632: loss: 0.2015987428:
42: 108832: loss: 0.2014465293:
42: 112032: loss: 0.2015262159:
42: 115232: loss: 0.2013636386:
42: 118432: loss: 0.2015156382:
42: 121632: loss: 0.2015811985:
42: 124832: loss: 0.2016845798:
42: 128032: loss: 0.2017309778:
42: 131232: loss: 0.2015781038:
42: 134432: loss: 0.2017249830:
Dev-Acc: 42: Accuracy: 0.9337791800: precision: 0.4272610530: recall: 0.3960210848: f1: 0.4110483586
Train-Acc: 42: Accuracy: 0.9245867729: precision: 0.7987529038: recall: 0.4294918151: f1: 0.5586147926
43: 3232: loss: 0.2081873593:
43: 6432: loss: 0.2122483615:
43: 9632: loss: 0.2102443572:
43: 12832: loss: 0.2061579446:
43: 16032: loss: 0.2051744090:
43: 19232: loss: 0.2040204571:
43: 22432: loss: 0.2015731930:
43: 25632: loss: 0.2023270206:
43: 28832: loss: 0.2025992373:
43: 32032: loss: 0.2024610385:
43: 35232: loss: 0.2022156858:
43: 38432: loss: 0.2022286119:
43: 41632: loss: 0.2012505281:
43: 44832: loss: 0.2018259017:
43: 48032: loss: 0.2011742636:
43: 51232: loss: 0.2009416679:
43: 54432: loss: 0.2004486236:
43: 57632: loss: 0.2002862543:
43: 60832: loss: 0.2006756055:
43: 64032: loss: 0.2002372401:
43: 67232: loss: 0.1999743071:
43: 70432: loss: 0.2003239675:
43: 73632: loss: 0.2000842156:
43: 76832: loss: 0.2005849216:
43: 80032: loss: 0.1999871677:
43: 83232: loss: 0.1997137337:
43: 86432: loss: 0.1996635946:
43: 89632: loss: 0.1998559227:
43: 92832: loss: 0.1993252642:
43: 96032: loss: 0.1993261483:
43: 99232: loss: 0.1995414770:
43: 102432: loss: 0.1999971002:
43: 105632: loss: 0.2003111551:
43: 108832: loss: 0.2002657981:
43: 112032: loss: 0.1997195719:
43: 115232: loss: 0.1998983862:
43: 118432: loss: 0.1999958742:
43: 121632: loss: 0.2001787514:
43: 124832: loss: 0.2002208569:
43: 128032: loss: 0.2002430010:
43: 131232: loss: 0.2002735446:
43: 134432: loss: 0.2002820869:
Dev-Acc: 43: Accuracy: 0.9335806966: precision: 0.4261312012: recall: 0.3987417106: f1: 0.4119817287
Train-Acc: 43: Accuracy: 0.9248350859: precision: 0.7990035241: recall: 0.4322529748: f1: 0.5610068259
44: 3232: loss: 0.1892024083:
44: 6432: loss: 0.1956920531:
44: 9632: loss: 0.1997805719:
44: 12832: loss: 0.2005750957:
44: 16032: loss: 0.2018293586:
44: 19232: loss: 0.2023180073:
44: 22432: loss: 0.2011258444:
44: 25632: loss: 0.2028356023:
44: 28832: loss: 0.2028543565:
44: 32032: loss: 0.2022709432:
44: 35232: loss: 0.2024156818:
44: 38432: loss: 0.2002660238:
44: 41632: loss: 0.1994767184:
44: 44832: loss: 0.1994886185:
44: 48032: loss: 0.2000541594:
44: 51232: loss: 0.1992768801:
44: 54432: loss: 0.1998482790:
44: 57632: loss: 0.1996034063:
44: 60832: loss: 0.1997845492:
44: 64032: loss: 0.1995810255:
44: 67232: loss: 0.1990873845:
44: 70432: loss: 0.1992330524:
44: 73632: loss: 0.1990916102:
44: 76832: loss: 0.1999824260:
44: 80032: loss: 0.1996218761:
44: 83232: loss: 0.1989739511:
44: 86432: loss: 0.1993063595:
44: 89632: loss: 0.1993636238:
44: 92832: loss: 0.1995514780:
44: 96032: loss: 0.1992232398:
44: 99232: loss: 0.1993186575:
44: 102432: loss: 0.1997932132:
44: 105632: loss: 0.1999228887:
44: 108832: loss: 0.1996792608:
44: 112032: loss: 0.1997346178:
44: 115232: loss: 0.1998480388:
44: 118432: loss: 0.1998960553:
44: 121632: loss: 0.1996881562:
44: 124832: loss: 0.1997358815:
44: 128032: loss: 0.1995034551:
44: 131232: loss: 0.1994839454:
44: 134432: loss: 0.1994057630:
Dev-Acc: 44: Accuracy: 0.9336005449: precision: 0.4271338724: recall: 0.4041829621: f1: 0.4153416041
Train-Acc: 44: Accuracy: 0.9251199961: precision: 0.7990834539: recall: 0.4356058116: f1: 0.5638429137
45: 3232: loss: 0.1867883100:
45: 6432: loss: 0.1913935146:
45: 9632: loss: 0.1961321798:
45: 12832: loss: 0.1998142107:
45: 16032: loss: 0.2006199393:
45: 19232: loss: 0.2013556036:
45: 22432: loss: 0.1994011135:
45: 25632: loss: 0.1994536035:
45: 28832: loss: 0.2005922160:
45: 32032: loss: 0.2004162890:
45: 35232: loss: 0.1993236096:
45: 38432: loss: 0.1993611508:
45: 41632: loss: 0.1993331668:
45: 44832: loss: 0.1983483448:
45: 48032: loss: 0.1978532184:
45: 51232: loss: 0.1975545092:
45: 54432: loss: 0.1976056438:
45: 57632: loss: 0.1981661679:
45: 60832: loss: 0.1982759273:
45: 64032: loss: 0.1984422867:
45: 67232: loss: 0.1990238940:
45: 70432: loss: 0.1992089663:
45: 73632: loss: 0.1992815207:
45: 76832: loss: 0.1988274763:
45: 80032: loss: 0.1996573907:
45: 83232: loss: 0.1994653203:
45: 86432: loss: 0.1993053391:
45: 89632: loss: 0.1995409071:
45: 92832: loss: 0.1992553463:
45: 96032: loss: 0.1990693529:
45: 99232: loss: 0.1992133665:
45: 102432: loss: 0.1993428779:
45: 105632: loss: 0.1996115433:
45: 108832: loss: 0.1995403927:
45: 112032: loss: 0.1992740350:
45: 115232: loss: 0.1988831267:
45: 118432: loss: 0.1988124739:
45: 121632: loss: 0.1989223867:
45: 124832: loss: 0.1989011599:
45: 128032: loss: 0.1988687909:
45: 131232: loss: 0.1986445404:
45: 134432: loss: 0.1983743781:
Dev-Acc: 45: Accuracy: 0.9334418178: precision: 0.4264627423: recall: 0.4077537834: f1: 0.4168984701
Train-Acc: 45: Accuracy: 0.9252952933: precision: 0.7987293215: recall: 0.4380382618: f1: 0.5657877977
46: 3232: loss: 0.2015713139:
46: 6432: loss: 0.2009974218:
46: 9632: loss: 0.1982060405:
46: 12832: loss: 0.2013412067:
46: 16032: loss: 0.1989560647:
46: 19232: loss: 0.1991971646:
46: 22432: loss: 0.1995492549:
46: 25632: loss: 0.1997133476:
46: 28832: loss: 0.1983890850:
46: 32032: loss: 0.1970732174:
46: 35232: loss: 0.1976755726:
46: 38432: loss: 0.1970572583:
46: 41632: loss: 0.1966412247:
46: 44832: loss: 0.1969286291:
46: 48032: loss: 0.1969404859:
46: 51232: loss: 0.1968883631:
46: 54432: loss: 0.1974826333:
46: 57632: loss: 0.1981168257:
46: 60832: loss: 0.1982733127:
46: 64032: loss: 0.1976929324:
46: 67232: loss: 0.1973773513:
46: 70432: loss: 0.1971579294:
46: 73632: loss: 0.1977264718:
46: 76832: loss: 0.1970355351:
46: 80032: loss: 0.1967314603:
46: 83232: loss: 0.1967722097:
46: 86432: loss: 0.1973582176:
46: 89632: loss: 0.1980084890:
46: 92832: loss: 0.1977153959:
46: 96032: loss: 0.1979555608:
46: 99232: loss: 0.1981935641:
46: 102432: loss: 0.1982110717:
46: 105632: loss: 0.1981063252:
46: 108832: loss: 0.1975172387:
46: 112032: loss: 0.1971843574:
46: 115232: loss: 0.1974743060:
46: 118432: loss: 0.1970638334:
46: 121632: loss: 0.1972001434:
46: 124832: loss: 0.1971602579:
46: 128032: loss: 0.1972148558:
46: 131232: loss: 0.1970525687:
46: 134432: loss: 0.1970199822:
Dev-Acc: 46: Accuracy: 0.9330449104: precision: 0.4238003164: recall: 0.4099642918: f1: 0.4167675022
Train-Acc: 46: Accuracy: 0.9255217314: precision: 0.7987608722: recall: 0.4407336796: f1: 0.5680393154
47: 3232: loss: 0.1795675121:
47: 6432: loss: 0.1875343446:
47: 9632: loss: 0.1918507610:
47: 12832: loss: 0.1921089469:
47: 16032: loss: 0.1925660748:
47: 19232: loss: 0.1934533678:
47: 22432: loss: 0.1933061997:
47: 25632: loss: 0.1928897608:
47: 28832: loss: 0.1942788129:
47: 32032: loss: 0.1932809508:
47: 35232: loss: 0.1933027088:
47: 38432: loss: 0.1921890713:
47: 41632: loss: 0.1927682623:
47: 44832: loss: 0.1924804839:
47: 48032: loss: 0.1930532379:
47: 51232: loss: 0.1935527519:
47: 54432: loss: 0.1932736742:
47: 57632: loss: 0.1937960244:
47: 60832: loss: 0.1941283199:
47: 64032: loss: 0.1945852492:
47: 67232: loss: 0.1942056635:
47: 70432: loss: 0.1953228906:
47: 73632: loss: 0.1954370631:
47: 76832: loss: 0.1959422366:
47: 80032: loss: 0.1957947423:
47: 83232: loss: 0.1953344944:
47: 86432: loss: 0.1957442681:
47: 89632: loss: 0.1958414412:
47: 92832: loss: 0.1957068187:
47: 96032: loss: 0.1957851686:
47: 99232: loss: 0.1954974095:
47: 102432: loss: 0.1953901981:
47: 105632: loss: 0.1951827985:
47: 108832: loss: 0.1955025422:
47: 112032: loss: 0.1955281850:
47: 115232: loss: 0.1959567423:
47: 118432: loss: 0.1957682892:
47: 121632: loss: 0.1963075065:
47: 124832: loss: 0.1963650104:
47: 128032: loss: 0.1961745277:
47: 131232: loss: 0.1960843549:
47: 134432: loss: 0.1959436878:
Dev-Acc: 47: Accuracy: 0.9330250621: precision: 0.4243426780: recall: 0.4143853086: f1: 0.4193048864
Train-Acc: 47: Accuracy: 0.9256824255: precision: 0.7983651226: recall: 0.4430346460: f1: 0.5698461018
48: 3232: loss: 0.1954350098:
48: 6432: loss: 0.1914874044:
48: 9632: loss: 0.1941995830:
48: 12832: loss: 0.1928006260:
48: 16032: loss: 0.1921001088:
48: 19232: loss: 0.1921190795:
48: 22432: loss: 0.1918075685:
48: 25632: loss: 0.1915336088:
48: 28832: loss: 0.1918517329:
48: 32032: loss: 0.1919063491:
48: 35232: loss: 0.1919684143:
48: 38432: loss: 0.1925083033:
48: 41632: loss: 0.1924131438:
48: 44832: loss: 0.1928663003:
48: 48032: loss: 0.1928767410:
48: 51232: loss: 0.1944582168:
48: 54432: loss: 0.1945224615:
48: 57632: loss: 0.1946333794:
48: 60832: loss: 0.1948828846:
48: 64032: loss: 0.1950726896:
48: 67232: loss: 0.1957027454:
48: 70432: loss: 0.1947287316:
48: 73632: loss: 0.1949459755:
48: 76832: loss: 0.1951364573:
48: 80032: loss: 0.1950085039:
48: 83232: loss: 0.1951520421:
48: 86432: loss: 0.1947013772:
48: 89632: loss: 0.1948618658:
48: 92832: loss: 0.1951179872:
48: 96032: loss: 0.1957104137:
48: 99232: loss: 0.1959130802:
48: 102432: loss: 0.1957528713:
48: 105632: loss: 0.1954850897:
48: 108832: loss: 0.1959559219:
48: 112032: loss: 0.1957395927:
48: 115232: loss: 0.1956829130:
48: 118432: loss: 0.1954067649:
48: 121632: loss: 0.1954285114:
48: 124832: loss: 0.1950470771:
48: 128032: loss: 0.1949147323:
48: 131232: loss: 0.1951011731:
48: 134432: loss: 0.1951821792:
Dev-Acc: 48: Accuracy: 0.9328960776: precision: 0.4238341969: recall: 0.4172759735: f1: 0.4205295176
Train-Acc: 48: Accuracy: 0.9260330796: precision: 0.7988715176: recall: 0.4467819341: f1: 0.5730668690
49: 3232: loss: 0.1891687087:
49: 6432: loss: 0.1912099263:
49: 9632: loss: 0.1969810339:
49: 12832: loss: 0.1962353132:
49: 16032: loss: 0.1961846910:
49: 19232: loss: 0.1948818996:
49: 22432: loss: 0.1948917289:
49: 25632: loss: 0.1947909532:
49: 28832: loss: 0.1945397700:
49: 32032: loss: 0.1943982088:
49: 35232: loss: 0.1937411845:
49: 38432: loss: 0.1944980619:
49: 41632: loss: 0.1928012467:
49: 44832: loss: 0.1927772982:
49: 48032: loss: 0.1936879092:
49: 51232: loss: 0.1937988932:
49: 54432: loss: 0.1936239677:
49: 57632: loss: 0.1936557830:
49: 60832: loss: 0.1932564729:
49: 64032: loss: 0.1933237232:
49: 67232: loss: 0.1936704519:
49: 70432: loss: 0.1933829180:
49: 73632: loss: 0.1941565188:
49: 76832: loss: 0.1939536033:
49: 80032: loss: 0.1941357336:
49: 83232: loss: 0.1944198855:
49: 86432: loss: 0.1937043216:
49: 89632: loss: 0.1936125850:
49: 92832: loss: 0.1939907854:
49: 96032: loss: 0.1938257512:
49: 99232: loss: 0.1939116640:
49: 102432: loss: 0.1943164096:
49: 105632: loss: 0.1945513927:
49: 108832: loss: 0.1944804853:
49: 112032: loss: 0.1948715230:
49: 115232: loss: 0.1949238562:
49: 118432: loss: 0.1946635294:
49: 121632: loss: 0.1944684576:
49: 124832: loss: 0.1947435353:
49: 128032: loss: 0.1945545965:
49: 131232: loss: 0.1944367247:
49: 134432: loss: 0.1943840520:
Dev-Acc: 49: Accuracy: 0.9326381087: precision: 0.4221269297: recall: 0.4184662472: f1: 0.4202886175
Train-Acc: 49: Accuracy: 0.9264786839: precision: 0.7997437092: recall: 0.4513181250: f1: 0.5770119773
50: 3232: loss: 0.1957834001:
50: 6432: loss: 0.1922337350:
50: 9632: loss: 0.1908088011:
50: 12832: loss: 0.1911214649:
50: 16032: loss: 0.1926074560:
50: 19232: loss: 0.1948128907:
50: 22432: loss: 0.1934836213:
50: 25632: loss: 0.1915250140:
50: 28832: loss: 0.1921423567:
50: 32032: loss: 0.1927235174:
50: 35232: loss: 0.1931262818:
50: 38432: loss: 0.1925745792:
50: 41632: loss: 0.1931882714:
50: 44832: loss: 0.1936692229:
50: 48032: loss: 0.1930412433:
50: 51232: loss: 0.1936637246:
50: 54432: loss: 0.1936389663:
50: 57632: loss: 0.1934572019:
50: 60832: loss: 0.1929486054:
50: 64032: loss: 0.1933163375:
50: 67232: loss: 0.1933032929:
50: 70432: loss: 0.1928131484:
50: 73632: loss: 0.1919072381:
50: 76832: loss: 0.1921283532:
50: 80032: loss: 0.1924723022:
50: 83232: loss: 0.1919656246:
50: 86432: loss: 0.1919647789:
50: 89632: loss: 0.1918718461:
50: 92832: loss: 0.1914473714:
50: 96032: loss: 0.1916609058:
50: 99232: loss: 0.1917022584:
50: 102432: loss: 0.1919765403:
50: 105632: loss: 0.1916750236:
50: 108832: loss: 0.1921391240:
50: 112032: loss: 0.1927080289:
50: 115232: loss: 0.1932119610:
50: 118432: loss: 0.1933060387:
50: 121632: loss: 0.1932074888:
50: 124832: loss: 0.1931919637:
50: 128032: loss: 0.1935706127:
50: 131232: loss: 0.1935165655:
50: 134432: loss: 0.1934735745:
Dev-Acc: 50: Accuracy: 0.9324991703: precision: 0.4214918256: recall: 0.4208467948: f1: 0.4211690632
Train-Acc: 50: Accuracy: 0.9267854691: precision: 0.8000231321: recall: 0.4547367037: f1: 0.5798717358
