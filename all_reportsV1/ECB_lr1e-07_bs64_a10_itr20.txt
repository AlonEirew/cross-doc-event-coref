1: 6464: loss: 0.7103455919:
1: 12864: loss: 0.7084513560:
1: 19264: loss: 0.7076287494:
1: 25664: loss: 0.7067472759:
1: 32064: loss: 0.7056696334:
1: 38464: loss: 0.7046820866:
1: 44864: loss: 0.7036425604:
1: 51264: loss: 0.7024596646:
1: 57664: loss: 0.7012821661:
1: 64064: loss: 0.7002514358:
1: 70464: loss: 0.6991572657:
1: 76864: loss: 0.6980992384:
1: 83264: loss: 0.6970197784:
1: 89664: loss: 0.6960132023:
1: 96064: loss: 0.6949460652:
1: 102464: loss: 0.6939196550:
1: 108864: loss: 0.6928645955:
1: 115264: loss: 0.6918160071:
1: 121664: loss: 0.6908178183:
1: 128064: loss: 0.6897616465:
1: 134464: loss: 0.6886945021:
1: 140864: loss: 0.6875887389:
1: 147264: loss: 0.6865526988:
1: 153664: loss: 0.6855086809:
1: 160064: loss: 0.6844472415:
1: 166464: loss: 0.6833958245:
Dev-Acc: 1: Accuracy: 0.8635696173: precision: 0.0669235003: recall: 0.1033837783: f1: 0.0812508352
Train-Acc: 1: Accuracy: 0.8547701836: precision: 0.1655258703: recall: 0.1478535271: f1: 0.1561914022
2: 6464: loss: 0.6566570824:
2: 12864: loss: 0.6549714777:
2: 19264: loss: 0.6538878204:
2: 25664: loss: 0.6528686379:
2: 32064: loss: 0.6516770128:
2: 38464: loss: 0.6505542213:
2: 44864: loss: 0.6494580547:
2: 51264: loss: 0.6483878700:
2: 57664: loss: 0.6474376225:
2: 64064: loss: 0.6465518799:
2: 70464: loss: 0.6457055526:
2: 76864: loss: 0.6447446420:
2: 83264: loss: 0.6437779438:
2: 89664: loss: 0.6427896363:
2: 96064: loss: 0.6417983172:
2: 102464: loss: 0.6407979544:
2: 108864: loss: 0.6398904063:
2: 115264: loss: 0.6389208888:
2: 121664: loss: 0.6379448838:
2: 128064: loss: 0.6370579797:
2: 134464: loss: 0.6360934752:
2: 140864: loss: 0.6351026511:
2: 147264: loss: 0.6341144392:
2: 153664: loss: 0.6331524752:
2: 160064: loss: 0.6322175481:
2: 166464: loss: 0.6312339828:
Dev-Acc: 2: Accuracy: 0.9407941699: precision: 0.0865384615: recall: 0.0015303520: f1: 0.0030075188
Train-Acc: 2: Accuracy: 0.9083617926: precision: 0.2095238095: recall: 0.0028926435: f1: 0.0057065041
3: 6464: loss: 0.6051158625:
3: 12864: loss: 0.6044894278:
3: 19264: loss: 0.6032032041:
3: 25664: loss: 0.6024976173:
3: 32064: loss: 0.6016234738:
3: 38464: loss: 0.6005592682:
3: 44864: loss: 0.5996794677:
3: 51264: loss: 0.5987293579:
3: 57664: loss: 0.5976802001:
3: 64064: loss: 0.5966368869:
3: 70464: loss: 0.5959632313:
3: 76864: loss: 0.5951198086:
3: 83264: loss: 0.5941168025:
3: 89664: loss: 0.5932799840:
3: 96064: loss: 0.5924622261:
3: 102464: loss: 0.5914090621:
3: 108864: loss: 0.5906478286:
3: 115264: loss: 0.5896688745:
3: 121664: loss: 0.5888903081:
3: 128064: loss: 0.5879926365:
3: 134464: loss: 0.5871018437:
3: 140864: loss: 0.5862818873:
3: 147264: loss: 0.5854565494:
3: 153664: loss: 0.5845905132:
3: 160064: loss: 0.5836073377:
3: 166464: loss: 0.5826117846:
Dev-Acc: 3: Accuracy: 0.9416574240: precision: 1.0000000000: recall: 0.0001700391: f1: 0.0003400204
Train-Acc: 3: Accuracy: 0.9090968966: precision: 1.0000000000: recall: 0.0000657419: f1: 0.0001314752
4: 6464: loss: 0.5564049292:
4: 12864: loss: 0.5567048481:
4: 19264: loss: 0.5557817815:
4: 25664: loss: 0.5551203048:
4: 32064: loss: 0.5547234734:
4: 38464: loss: 0.5537974902:
4: 44864: loss: 0.5532264349:
4: 51264: loss: 0.5523483599:
4: 57664: loss: 0.5515233952:
4: 64064: loss: 0.5510507433:
4: 70464: loss: 0.5503184391:
4: 76864: loss: 0.5494456649:
4: 83264: loss: 0.5487068438:
4: 89664: loss: 0.5476320896:
4: 96064: loss: 0.5466526450:
4: 102464: loss: 0.5456965989:
4: 108864: loss: 0.5451312568:
4: 115264: loss: 0.5444121799:
4: 121664: loss: 0.5436728004:
4: 128064: loss: 0.5426074548:
4: 134464: loss: 0.5418181086:
4: 140864: loss: 0.5408347983:
4: 147264: loss: 0.5401285140:
4: 153664: loss: 0.5391068743:
4: 160064: loss: 0.5381784658:
4: 166464: loss: 0.5372281751:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 6464: loss: 0.5149609801:
5: 12864: loss: 0.5137985381:
5: 19264: loss: 0.5123490330:
5: 25664: loss: 0.5118648771:
5: 32064: loss: 0.5106138178:
5: 38464: loss: 0.5097203790:
5: 44864: loss: 0.5085794506:
5: 51264: loss: 0.5081507470:
5: 57664: loss: 0.5076836637:
5: 64064: loss: 0.5070102626:
5: 70464: loss: 0.5062051625:
5: 76864: loss: 0.5057783037:
5: 83264: loss: 0.5049042961:
5: 89664: loss: 0.5042176285:
5: 96064: loss: 0.5034574654:
5: 102464: loss: 0.5026793750:
5: 108864: loss: 0.5019998881:
5: 115264: loss: 0.5012215595:
5: 121664: loss: 0.5002562144:
5: 128064: loss: 0.4996010642:
5: 134464: loss: 0.4991436893:
5: 140864: loss: 0.4981693087:
5: 147264: loss: 0.4972910538:
5: 153664: loss: 0.4964591237:
5: 160064: loss: 0.4957664420:
5: 166464: loss: 0.4950926457:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 6464: loss: 0.4751521748:
6: 12864: loss: 0.4774927638:
6: 19264: loss: 0.4769695834:
6: 25664: loss: 0.4747963658:
6: 32064: loss: 0.4730085456:
6: 38464: loss: 0.4724928399:
6: 44864: loss: 0.4717028589:
6: 51264: loss: 0.4707498509:
6: 57664: loss: 0.4699207952:
6: 64064: loss: 0.4690014614:
6: 70464: loss: 0.4679773086:
6: 76864: loss: 0.4669673758:
6: 83264: loss: 0.4664574002:
6: 89664: loss: 0.4655393239:
6: 96064: loss: 0.4648955946:
6: 102464: loss: 0.4638465698:
6: 108864: loss: 0.4632393391:
6: 115264: loss: 0.4623240293:
6: 121664: loss: 0.4615222066:
6: 128064: loss: 0.4606548197:
6: 134464: loss: 0.4597258260:
6: 140864: loss: 0.4592177666:
6: 147264: loss: 0.4584817261:
6: 153664: loss: 0.4577744547:
6: 160064: loss: 0.4571633721:
6: 166464: loss: 0.4563978036:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 6464: loss: 0.4428767735:
7: 12864: loss: 0.4375998136:
7: 19264: loss: 0.4399290921:
7: 25664: loss: 0.4385883281:
7: 32064: loss: 0.4359189169:
7: 38464: loss: 0.4349619828:
7: 44864: loss: 0.4332035846:
7: 51264: loss: 0.4335888514:
7: 57664: loss: 0.4334608736:
7: 64064: loss: 0.4327599083:
7: 70464: loss: 0.4318595968:
7: 76864: loss: 0.4307970703:
7: 83264: loss: 0.4301949428:
7: 89664: loss: 0.4295131592:
7: 96064: loss: 0.4292460081:
7: 102464: loss: 0.4286299081:
7: 108864: loss: 0.4275409037:
7: 115264: loss: 0.4270241534:
7: 121664: loss: 0.4260097826:
7: 128064: loss: 0.4253403372:
7: 134464: loss: 0.4247967229:
7: 140864: loss: 0.4240012193:
7: 147264: loss: 0.4233946639:
7: 153664: loss: 0.4225231803:
7: 160064: loss: 0.4222220958:
7: 166464: loss: 0.4216304038:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 6464: loss: 0.4049591550:
8: 12864: loss: 0.4033327563:
8: 19264: loss: 0.4036033963:
8: 25664: loss: 0.4015514179:
8: 32064: loss: 0.4017812105:
8: 38464: loss: 0.4012382992:
8: 44864: loss: 0.4006171477:
8: 51264: loss: 0.4009699815:
8: 57664: loss: 0.4003037294:
8: 64064: loss: 0.3997752927:
8: 70464: loss: 0.3989483958:
8: 76864: loss: 0.3986558378:
8: 83264: loss: 0.3986122835:
8: 89664: loss: 0.3981826187:
8: 96064: loss: 0.3971225669:
8: 102464: loss: 0.3962745069:
8: 108864: loss: 0.3957228278:
8: 115264: loss: 0.3950391419:
8: 121664: loss: 0.3944984438:
8: 128064: loss: 0.3939775872:
8: 134464: loss: 0.3934188270:
8: 140864: loss: 0.3928315922:
8: 147264: loss: 0.3923972530:
8: 153664: loss: 0.3920528945:
8: 160064: loss: 0.3919260096:
8: 166464: loss: 0.3917013415:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 6464: loss: 0.3719277740:
9: 12864: loss: 0.3775726390:
9: 19264: loss: 0.3745405360:
9: 25664: loss: 0.3733900290:
9: 32064: loss: 0.3725252240:
9: 38464: loss: 0.3727424444:
9: 44864: loss: 0.3732873014:
9: 51264: loss: 0.3732935284:
9: 57664: loss: 0.3738848721:
9: 64064: loss: 0.3738211322:
9: 70464: loss: 0.3730441673:
9: 76864: loss: 0.3719638554:
9: 83264: loss: 0.3712253310:
9: 89664: loss: 0.3709756427:
9: 96064: loss: 0.3705057847:
9: 102464: loss: 0.3698965168:
9: 108864: loss: 0.3694967587:
9: 115264: loss: 0.3690635209:
9: 121664: loss: 0.3690676464:
9: 128064: loss: 0.3688542261:
9: 134464: loss: 0.3681570320:
9: 140864: loss: 0.3679109864:
9: 147264: loss: 0.3674116243:
9: 153664: loss: 0.3667722468:
9: 160064: loss: 0.3665597526:
9: 166464: loss: 0.3660700670:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 6464: loss: 0.3472563776:
10: 12864: loss: 0.3499901471:
10: 19264: loss: 0.3510222397:
10: 25664: loss: 0.3488184938:
10: 32064: loss: 0.3508327906:
10: 38464: loss: 0.3499298323:
10: 44864: loss: 0.3499962375:
10: 51264: loss: 0.3501044756:
10: 57664: loss: 0.3491038069:
10: 64064: loss: 0.3497383503:
10: 70464: loss: 0.3487424240:
10: 76864: loss: 0.3482613833:
10: 83264: loss: 0.3481637173:
10: 89664: loss: 0.3474353747:
10: 96064: loss: 0.3477036959:
10: 102464: loss: 0.3476916941:
10: 108864: loss: 0.3479862503:
10: 115264: loss: 0.3480295523:
10: 121664: loss: 0.3474106639:
10: 128064: loss: 0.3468409002:
10: 134464: loss: 0.3463796347:
10: 140864: loss: 0.3461498634:
10: 147264: loss: 0.3457069373:
10: 153664: loss: 0.3452270022:
10: 160064: loss: 0.3450479166:
10: 166464: loss: 0.3450239275:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 6464: loss: 0.3354729354:
11: 12864: loss: 0.3369998960:
11: 19264: loss: 0.3331095175:
11: 25664: loss: 0.3342922366:
11: 32064: loss: 0.3319954353:
11: 38464: loss: 0.3324398737:
11: 44864: loss: 0.3331717316:
11: 51264: loss: 0.3323472765:
11: 57664: loss: 0.3315712099:
11: 64064: loss: 0.3321924344:
11: 70464: loss: 0.3324099624:
11: 76864: loss: 0.3322689628:
11: 83264: loss: 0.3321015078:
11: 89664: loss: 0.3324261555:
11: 96064: loss: 0.3316875072:
11: 102464: loss: 0.3321459213:
11: 108864: loss: 0.3321014971:
11: 115264: loss: 0.3315276441:
11: 121664: loss: 0.3307538054:
11: 128064: loss: 0.3307985080:
11: 134464: loss: 0.3306167939:
11: 140864: loss: 0.3300363188:
11: 147264: loss: 0.3294433648:
11: 153664: loss: 0.3288262536:
11: 160064: loss: 0.3286243945:
11: 166464: loss: 0.3279110207:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 6464: loss: 0.3154646212:
12: 12864: loss: 0.3163886508:
12: 19264: loss: 0.3187063380:
12: 25664: loss: 0.3172530954:
12: 32064: loss: 0.3162692596:
12: 38464: loss: 0.3166158638:
12: 44864: loss: 0.3165836275:
12: 51264: loss: 0.3166468197:
12: 57664: loss: 0.3162936256:
12: 64064: loss: 0.3152570666:
12: 70464: loss: 0.3149834666:
12: 76864: loss: 0.3150088454:
12: 83264: loss: 0.3151835394:
12: 89664: loss: 0.3144782559:
12: 96064: loss: 0.3142330522:
12: 102464: loss: 0.3144428310:
12: 108864: loss: 0.3144614527:
12: 115264: loss: 0.3156834755:
12: 121664: loss: 0.3151251490:
12: 128064: loss: 0.3147922508:
12: 134464: loss: 0.3147561787:
12: 140864: loss: 0.3149353822:
12: 147264: loss: 0.3142575082:
12: 153664: loss: 0.3140743269:
12: 160064: loss: 0.3137926455:
12: 166464: loss: 0.3135572831:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 6464: loss: 0.3156759761:
13: 12864: loss: 0.3120733965:
13: 19264: loss: 0.3045553389:
13: 25664: loss: 0.3039333492:
13: 32064: loss: 0.3053009362:
13: 38464: loss: 0.3062910864:
13: 44864: loss: 0.3054236462:
13: 51264: loss: 0.3048537484:
13: 57664: loss: 0.3039931552:
13: 64064: loss: 0.3035246724:
13: 70464: loss: 0.3032057642:
13: 76864: loss: 0.3026623268:
13: 83264: loss: 0.3022188180:
13: 89664: loss: 0.3026049453:
13: 96064: loss: 0.3033432542:
13: 102464: loss: 0.3037036885:
13: 108864: loss: 0.3026864393:
13: 115264: loss: 0.3027086155:
13: 121664: loss: 0.3024298162:
13: 128064: loss: 0.3024690569:
13: 134464: loss: 0.3029438260:
13: 140864: loss: 0.3032046805:
13: 147264: loss: 0.3023670302:
13: 153664: loss: 0.3023767197:
13: 160064: loss: 0.3022572404:
13: 166464: loss: 0.3020647731:
Dev-Acc: 13: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 13: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
14: 6464: loss: 0.2942127433:
14: 12864: loss: 0.2955009933:
14: 19264: loss: 0.2944305598:
14: 25664: loss: 0.2961225289:
14: 32064: loss: 0.2962430214:
14: 38464: loss: 0.2960621880:
14: 44864: loss: 0.2959111923:
14: 51264: loss: 0.2955654662:
14: 57664: loss: 0.2952595863:
14: 64064: loss: 0.2939503056:
14: 70464: loss: 0.2936011617:
14: 76864: loss: 0.2939761667:
14: 83264: loss: 0.2933451881:
14: 89664: loss: 0.2938570203:
14: 96064: loss: 0.2939919285:
14: 102464: loss: 0.2936646112:
14: 108864: loss: 0.2928976588:
14: 115264: loss: 0.2931855368:
14: 121664: loss: 0.2932709632:
14: 128064: loss: 0.2934652758:
14: 134464: loss: 0.2931781014:
14: 140864: loss: 0.2931061065:
14: 147264: loss: 0.2929809763:
14: 153664: loss: 0.2929984774:
14: 160064: loss: 0.2925480090:
14: 166464: loss: 0.2922837061:
Dev-Acc: 14: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 14: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
15: 6464: loss: 0.2899284685:
15: 12864: loss: 0.2865861582:
15: 19264: loss: 0.2863846529:
15: 25664: loss: 0.2888140628:
15: 32064: loss: 0.2859882656:
15: 38464: loss: 0.2857428069:
15: 44864: loss: 0.2849001598:
15: 51264: loss: 0.2835940914:
15: 57664: loss: 0.2848019652:
15: 64064: loss: 0.2849980626:
15: 70464: loss: 0.2837726205:
15: 76864: loss: 0.2836723430:
15: 83264: loss: 0.2844323186:
15: 89664: loss: 0.2846484944:
15: 96064: loss: 0.2846887947:
15: 102464: loss: 0.2846173371:
15: 108864: loss: 0.2840933240:
15: 115264: loss: 0.2835464561:
15: 121664: loss: 0.2840339683:
15: 128064: loss: 0.2837155850:
15: 134464: loss: 0.2836057073:
15: 140864: loss: 0.2838297512:
15: 147264: loss: 0.2840431743:
15: 153664: loss: 0.2836406360:
15: 160064: loss: 0.2836464687:
15: 166464: loss: 0.2833140435:
Dev-Acc: 15: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 15: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
16: 6464: loss: 0.2811923152:
16: 12864: loss: 0.2821639995:
16: 19264: loss: 0.2819766227:
16: 25664: loss: 0.2799834382:
16: 32064: loss: 0.2813983724:
16: 38464: loss: 0.2804159270:
16: 44864: loss: 0.2820312536:
16: 51264: loss: 0.2790641994:
16: 57664: loss: 0.2781651874:
16: 64064: loss: 0.2782932249:
16: 70464: loss: 0.2775752273:
16: 76864: loss: 0.2767004663:
16: 83264: loss: 0.2765919901:
16: 89664: loss: 0.2767346617:
16: 96064: loss: 0.2764870155:
16: 102464: loss: 0.2765551337:
16: 108864: loss: 0.2757349659:
16: 115264: loss: 0.2755152679:
16: 121664: loss: 0.2755308682:
16: 128064: loss: 0.2755696879:
16: 134464: loss: 0.2752362064:
16: 140864: loss: 0.2753760469:
16: 147264: loss: 0.2756068281:
16: 153664: loss: 0.2755501163:
16: 160064: loss: 0.2760226131:
16: 166464: loss: 0.2761174002:
Dev-Acc: 16: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 16: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
17: 6464: loss: 0.2765798996:
17: 12864: loss: 0.2739765871:
17: 19264: loss: 0.2762933526:
17: 25664: loss: 0.2754389332:
17: 32064: loss: 0.2764580990:
17: 38464: loss: 0.2753039976:
17: 44864: loss: 0.2758864650:
17: 51264: loss: 0.2752525107:
17: 57664: loss: 0.2736124933:
17: 64064: loss: 0.2729729434:
17: 70464: loss: 0.2727182950:
17: 76864: loss: 0.2725722787:
17: 83264: loss: 0.2721462462:
17: 89664: loss: 0.2715309433:
17: 96064: loss: 0.2709996430:
17: 102464: loss: 0.2710251963:
17: 108864: loss: 0.2714767342:
17: 115264: loss: 0.2713671326:
17: 121664: loss: 0.2717910685:
17: 128064: loss: 0.2719485609:
17: 134464: loss: 0.2711013257:
17: 140864: loss: 0.2708510506:
17: 147264: loss: 0.2704747005:
17: 153664: loss: 0.2701419435:
17: 160064: loss: 0.2698626787:
17: 166464: loss: 0.2697559183:
Dev-Acc: 17: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 17: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
18: 6464: loss: 0.2658586164:
18: 12864: loss: 0.2651686342:
18: 19264: loss: 0.2657745286:
18: 25664: loss: 0.2663556908:
18: 32064: loss: 0.2653230055:
18: 38464: loss: 0.2654775761:
18: 44864: loss: 0.2660119920:
18: 51264: loss: 0.2643511117:
18: 57664: loss: 0.2654377787:
18: 64064: loss: 0.2638772097:
18: 70464: loss: 0.2640358009:
18: 76864: loss: 0.2633109293:
18: 83264: loss: 0.2633012501:
18: 89664: loss: 0.2634317539:
18: 96064: loss: 0.2643644014:
18: 102464: loss: 0.2646864238:
18: 108864: loss: 0.2642466164:
18: 115264: loss: 0.2641810779:
18: 121664: loss: 0.2636430879:
18: 128064: loss: 0.2632769228:
18: 134464: loss: 0.2637161698:
18: 140864: loss: 0.2634537076:
18: 147264: loss: 0.2630926009:
18: 153664: loss: 0.2634894599:
18: 160064: loss: 0.2636543037:
18: 166464: loss: 0.2638379219:
Dev-Acc: 18: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 18: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
19: 6464: loss: 0.2637501178:
19: 12864: loss: 0.2644402598:
19: 19264: loss: 0.2631451406:
19: 25664: loss: 0.2627117131:
19: 32064: loss: 0.2605744296:
19: 38464: loss: 0.2596886405:
19: 44864: loss: 0.2599035354:
19: 51264: loss: 0.2591867802:
19: 57664: loss: 0.2593096111:
19: 64064: loss: 0.2583402953:
19: 70464: loss: 0.2586645495:
19: 76864: loss: 0.2586138956:
19: 83264: loss: 0.2590544019:
19: 89664: loss: 0.2595931689:
19: 96064: loss: 0.2592316334:
19: 102464: loss: 0.2590338526:
19: 108864: loss: 0.2593101074:
19: 115264: loss: 0.2591589003:
19: 121664: loss: 0.2592352694:
19: 128064: loss: 0.2594852391:
19: 134464: loss: 0.2592029140:
19: 140864: loss: 0.2594332506:
19: 147264: loss: 0.2589684358:
19: 153664: loss: 0.2588307358:
19: 160064: loss: 0.2586279765:
19: 166464: loss: 0.2584433540:
Dev-Acc: 19: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 19: Accuracy: 0.9091267586: precision: 1.0000000000: recall: 0.0003944514: f1: 0.0007885917
20: 6464: loss: 0.2642231545:
20: 12864: loss: 0.2618745964:
20: 19264: loss: 0.2559662489:
20: 25664: loss: 0.2541673642:
20: 32064: loss: 0.2555151217:
20: 38464: loss: 0.2549035316:
20: 44864: loss: 0.2550032373:
20: 51264: loss: 0.2547298656:
20: 57664: loss: 0.2549081758:
20: 64064: loss: 0.2545258871:
20: 70464: loss: 0.2539687811:
20: 76864: loss: 0.2539776119:
20: 83264: loss: 0.2535329984:
20: 89664: loss: 0.2529329449:
20: 96064: loss: 0.2530313655:
20: 102464: loss: 0.2532582796:
20: 108864: loss: 0.2535137030:
20: 115264: loss: 0.2535697576:
20: 121664: loss: 0.2535185799:
20: 128064: loss: 0.2533910864:
20: 134464: loss: 0.2529662614:
20: 140864: loss: 0.2528764002:
20: 147264: loss: 0.2532066009:
20: 153664: loss: 0.2533153940:
20: 160064: loss: 0.2532771880:
20: 166464: loss: 0.2534952359:
Dev-Acc: 20: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 20: Accuracy: 0.9093897343: precision: 1.0000000000: recall: 0.0032870949: f1: 0.0065526505
