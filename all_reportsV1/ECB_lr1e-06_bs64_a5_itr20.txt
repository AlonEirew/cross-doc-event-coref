1: 6464: loss: 0.6981137902:
1: 12864: loss: 0.6900256482:
1: 19264: loss: 0.6821522979:
1: 25664: loss: 0.6745722565:
1: 32064: loss: 0.6668806336:
1: 38464: loss: 0.6597719553:
1: 44864: loss: 0.6529192663:
1: 51264: loss: 0.6457588327:
1: 57664: loss: 0.6388660459:
1: 64064: loss: 0.6320023940:
1: 70464: loss: 0.6252131496:
1: 76864: loss: 0.6187329496:
1: 83264: loss: 0.6120358021:
1: 89664: loss: 0.6053119632:
Dev-Acc: 1: Accuracy: 0.9416375756: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8333990574: precision: 1.0000000000: recall: 0.0003944514: f1: 0.0007885917
2: 6464: loss: 0.5058178377:
2: 12864: loss: 0.4992331386:
2: 19264: loss: 0.4918676664:
2: 25664: loss: 0.4864781780:
2: 32064: loss: 0.4807802405:
2: 38464: loss: 0.4750490294:
2: 44864: loss: 0.4705561014:
2: 51264: loss: 0.4671619530:
2: 57664: loss: 0.4620052378:
2: 64064: loss: 0.4573569141:
2: 70464: loss: 0.4526335526:
2: 76864: loss: 0.4478010851:
2: 83264: loss: 0.4440602892:
2: 89664: loss: 0.4405152830:
Dev-Acc: 2: Accuracy: 0.9425900578: precision: 0.5925925926: recall: 0.0516918891: f1: 0.0950891461
Train-Acc: 2: Accuracy: 0.8471062779: precision: 0.9105160026: recall: 0.0916442049: f1: 0.1665272966
3: 6464: loss: 0.3756901979:
3: 12864: loss: 0.3730476017:
3: 19264: loss: 0.3737998839:
3: 25664: loss: 0.3712803864:
3: 32064: loss: 0.3702001522:
3: 38464: loss: 0.3675459799:
3: 44864: loss: 0.3647005583:
3: 51264: loss: 0.3621768005:
3: 57664: loss: 0.3591772689:
3: 64064: loss: 0.3563124990:
3: 70464: loss: 0.3537821039:
3: 76864: loss: 0.3517017625:
3: 83264: loss: 0.3489207409:
3: 89664: loss: 0.3465480572:
Dev-Acc: 3: Accuracy: 0.9231425524: precision: 0.3619132238: recall: 0.4155755824: f1: 0.3868925123
Train-Acc: 3: Accuracy: 0.8832204938: precision: 0.8492100015: recall: 0.3639471435: f1: 0.5095260009
4: 6464: loss: 0.3099048653:
4: 12864: loss: 0.3098148908:
4: 19264: loss: 0.3107667167:
4: 25664: loss: 0.3089685502:
4: 32064: loss: 0.3049220094:
4: 38464: loss: 0.3038279624:
4: 44864: loss: 0.3020046873:
4: 51264: loss: 0.3011843346:
4: 57664: loss: 0.3003637334:
4: 64064: loss: 0.2985745142:
4: 70464: loss: 0.2971192176:
4: 76864: loss: 0.2951878987:
4: 83264: loss: 0.2940208462:
4: 89664: loss: 0.2926321366:
Dev-Acc: 4: Accuracy: 0.9070586562: precision: 0.3163329821: recall: 0.5104574052: f1: 0.3906056860
Train-Acc: 4: Accuracy: 0.9003681540: precision: 0.8420169946: recall: 0.4951022287: f1: 0.6235561995
5: 6464: loss: 0.2727023204:
5: 12864: loss: 0.2692995897:
5: 19264: loss: 0.2683344415:
5: 25664: loss: 0.2667561904:
5: 32064: loss: 0.2657867803:
5: 38464: loss: 0.2649286988:
5: 44864: loss: 0.2637035665:
5: 51264: loss: 0.2621231756:
5: 57664: loss: 0.2615144484:
5: 64064: loss: 0.2608621983:
5: 70464: loss: 0.2605393837:
5: 76864: loss: 0.2601285352:
5: 83264: loss: 0.2599266808:
5: 89664: loss: 0.2596020626:
Dev-Acc: 5: Accuracy: 0.8919273019: precision: 0.2830172339: recall: 0.5556878082: f1: 0.3750286895
Train-Acc: 5: Accuracy: 0.9081037641: precision: 0.8433286376: recall: 0.5509828414: f1: 0.6665076146
6: 6464: loss: 0.2487395102:
6: 12864: loss: 0.2466670424:
6: 19264: loss: 0.2428856867:
6: 25664: loss: 0.2452494125:
6: 32064: loss: 0.2441668641:
6: 38464: loss: 0.2426537326:
6: 44864: loss: 0.2424352527:
6: 51264: loss: 0.2418560390:
6: 57664: loss: 0.2410367119:
6: 64064: loss: 0.2397390590:
6: 70464: loss: 0.2393628153:
6: 76864: loss: 0.2398392601:
6: 83264: loss: 0.2389423952:
6: 89664: loss: 0.2377307176:
Dev-Acc: 6: Accuracy: 0.8792268634: precision: 0.2586881473: recall: 0.5733718755: f1: 0.3565235779
Train-Acc: 6: Accuracy: 0.9122674465: precision: 0.8419403835: recall: 0.5830648873: f1: 0.6889881530
7: 6464: loss: 0.2386692628:
7: 12864: loss: 0.2369869316:
7: 19264: loss: 0.2335021546:
7: 25664: loss: 0.2304150939:
7: 32064: loss: 0.2285680399:
7: 38464: loss: 0.2279760574:
7: 44864: loss: 0.2259010665:
7: 51264: loss: 0.2256665202:
7: 57664: loss: 0.2235658671:
7: 64064: loss: 0.2235233203:
7: 70464: loss: 0.2236109749:
7: 76864: loss: 0.2236556270:
7: 83264: loss: 0.2233548508:
7: 89664: loss: 0.2229738025:
Dev-Acc: 7: Accuracy: 0.8692351580: precision: 0.2427020166: recall: 0.5852746132: f1: 0.3431191746
Train-Acc: 7: Accuracy: 0.9166940451: precision: 0.8433832822: recall: 0.6142265466: f1: 0.7107915858
8: 6464: loss: 0.2091696303:
8: 12864: loss: 0.2101950999:
8: 19264: loss: 0.2104559518:
8: 25664: loss: 0.2089672567:
8: 32064: loss: 0.2104352302:
8: 38464: loss: 0.2111821054:
8: 44864: loss: 0.2111823455:
8: 51264: loss: 0.2117205082:
8: 57664: loss: 0.2121295540:
8: 64064: loss: 0.2111630379:
8: 70464: loss: 0.2106358301:
8: 76864: loss: 0.2116605542:
8: 83264: loss: 0.2114430381:
8: 89664: loss: 0.2111387877:
Dev-Acc: 8: Accuracy: 0.8601861596: precision: 0.2304307854: recall: 0.5966672335: f1: 0.3324648254
Train-Acc: 8: Accuracy: 0.9215041399: precision: 0.8478430016: recall: 0.6447307869: f1: 0.7324669505
9: 6464: loss: 0.2106382796:
9: 12864: loss: 0.2051991671:
9: 19264: loss: 0.2071839231:
9: 25664: loss: 0.2083909344:
9: 32064: loss: 0.2077010066:
9: 38464: loss: 0.2066679324:
9: 44864: loss: 0.2057047479:
9: 51264: loss: 0.2046472917:
9: 57664: loss: 0.2032339225:
9: 64064: loss: 0.2028522905:
9: 70464: loss: 0.2030088391:
9: 76864: loss: 0.2029286063:
9: 83264: loss: 0.2024245192:
9: 89664: loss: 0.2018480307:
Dev-Acc: 9: Accuracy: 0.8521789312: precision: 0.2204031008: recall: 0.6043189934: f1: 0.3230028174
Train-Acc: 9: Accuracy: 0.9262813926: precision: 0.8558603910: recall: 0.6706330945: f1: 0.7520088463
10: 6464: loss: 0.1813252406:
10: 12864: loss: 0.1934677051:
10: 19264: loss: 0.1939746923:
10: 25664: loss: 0.1932638409:
10: 32064: loss: 0.1914282679:
10: 38464: loss: 0.1933160215:
10: 44864: loss: 0.1935431933:
10: 51264: loss: 0.1942039647:
10: 57664: loss: 0.1946986209:
10: 64064: loss: 0.1949762508:
10: 70464: loss: 0.1955717589:
10: 76864: loss: 0.1952220824:
10: 83264: loss: 0.1949601559:
10: 89664: loss: 0.1942483203:
Dev-Acc: 10: Accuracy: 0.8428421021: precision: 0.2105232558: recall: 0.6157116137: f1: 0.3137645683
Train-Acc: 10: Accuracy: 0.9294480085: precision: 0.8602168200: recall: 0.6885806324: f1: 0.7648884507
11: 6464: loss: 0.1825248877:
11: 12864: loss: 0.1857954458:
11: 19264: loss: 0.1877308262:
11: 25664: loss: 0.1901692220:
11: 32064: loss: 0.1899055888:
11: 38464: loss: 0.1901330174:
11: 44864: loss: 0.1909190798:
11: 51264: loss: 0.1894163298:
11: 57664: loss: 0.1889602580:
11: 64064: loss: 0.1880403802:
11: 70464: loss: 0.1890330645:
11: 76864: loss: 0.1887853589:
11: 83264: loss: 0.1886391616:
11: 89664: loss: 0.1877278152:
Dev-Acc: 11: Accuracy: 0.8375138640: precision: 0.2039158156: recall: 0.6145213399: f1: 0.3062192849
Train-Acc: 11: Accuracy: 0.9327679276: precision: 0.8712263765: recall: 0.7000854645: f1: 0.7763359335
12: 6464: loss: 0.1953454308:
12: 12864: loss: 0.1916481755:
12: 19264: loss: 0.1883764262:
12: 25664: loss: 0.1869758562:
12: 32064: loss: 0.1869276084:
12: 38464: loss: 0.1867322943:
12: 44864: loss: 0.1866650556:
12: 51264: loss: 0.1858819120:
12: 57664: loss: 0.1852641084:
12: 64064: loss: 0.1841347342:
12: 70464: loss: 0.1841720470:
12: 76864: loss: 0.1843767563:
12: 83264: loss: 0.1837802800:
12: 89664: loss: 0.1830310854:
Dev-Acc: 12: Accuracy: 0.8302309513: precision: 0.1965299173: recall: 0.6182622003: f1: 0.2982528094
Train-Acc: 12: Accuracy: 0.9354304671: precision: 0.8748793048: recall: 0.7148116495: f1: 0.7867867868
13: 6464: loss: 0.1672694834:
13: 12864: loss: 0.1743585449:
13: 19264: loss: 0.1747135158:
13: 25664: loss: 0.1758928825:
13: 32064: loss: 0.1782177363:
13: 38464: loss: 0.1791297656:
13: 44864: loss: 0.1784286910:
13: 51264: loss: 0.1783271559:
13: 57664: loss: 0.1783406706:
13: 64064: loss: 0.1785409938:
13: 70464: loss: 0.1781673171:
13: 76864: loss: 0.1774090338:
13: 83264: loss: 0.1777851102:
13: 89664: loss: 0.1779628501:
Dev-Acc: 13: Accuracy: 0.8214994073: precision: 0.1883781975: recall: 0.6223431389: f1: 0.2892137495
Train-Acc: 13: Accuracy: 0.9372931719: precision: 0.8744277822: recall: 0.7283544803: f1: 0.7947347656
14: 6464: loss: 0.1715418039:
14: 12864: loss: 0.1726961978:
14: 19264: loss: 0.1743819908:
14: 25664: loss: 0.1760995650:
14: 32064: loss: 0.1767958594:
14: 38464: loss: 0.1771339515:
14: 44864: loss: 0.1766755399:
14: 51264: loss: 0.1769751914:
14: 57664: loss: 0.1766159136:
14: 64064: loss: 0.1763384559:
14: 70464: loss: 0.1750368626:
14: 76864: loss: 0.1750286349:
14: 83264: loss: 0.1743535487:
14: 89664: loss: 0.1738195865:
Dev-Acc: 14: Accuracy: 0.8144844174: precision: 0.1821743875: recall: 0.6245536473: f1: 0.2820719579
Train-Acc: 14: Accuracy: 0.9388271570: precision: 0.8759175387: recall: 0.7374268621: f1: 0.8007281294
15: 6464: loss: 0.1720494632:
15: 12864: loss: 0.1726523855:
15: 19264: loss: 0.1735704524:
15: 25664: loss: 0.1711307607:
15: 32064: loss: 0.1697093526:
15: 38464: loss: 0.1726306885:
15: 44864: loss: 0.1724368593:
15: 51264: loss: 0.1713439193:
15: 57664: loss: 0.1706362624:
15: 64064: loss: 0.1694263650:
15: 70464: loss: 0.1692057251:
15: 76864: loss: 0.1687782425:
15: 83264: loss: 0.1695013129:
15: 89664: loss: 0.1692913435:
Dev-Acc: 15: Accuracy: 0.8089478612: precision: 0.1771125060: recall: 0.6237034518: f1: 0.2758828175
Train-Acc: 15: Accuracy: 0.9404268861: precision: 0.8796612803: recall: 0.7443955033: f1: 0.8063953281
16: 6464: loss: 0.1737593599:
16: 12864: loss: 0.1666292022:
16: 19264: loss: 0.1675739537:
16: 25664: loss: 0.1697390251:
16: 32064: loss: 0.1696408622:
16: 38464: loss: 0.1676030840:
16: 44864: loss: 0.1669302954:
16: 51264: loss: 0.1668584902:
16: 57664: loss: 0.1664309470:
16: 64064: loss: 0.1659825652:
16: 70464: loss: 0.1665192936:
16: 76864: loss: 0.1672272318:
16: 83264: loss: 0.1664970479:
16: 89664: loss: 0.1659447258:
Dev-Acc: 16: Accuracy: 0.8054850101: precision: 0.1746408080: recall: 0.6262540384: f1: 0.2731182796
Train-Acc: 16: Accuracy: 0.9411938787: precision: 0.8811367508: recall: 0.7480770495: f1: 0.8091733333
17: 6464: loss: 0.1675728912:
17: 12864: loss: 0.1631326893:
17: 19264: loss: 0.1665879643:
17: 25664: loss: 0.1638815584:
17: 32064: loss: 0.1661281090:
17: 38464: loss: 0.1664738074:
17: 44864: loss: 0.1668258986:
17: 51264: loss: 0.1680195334:
17: 57664: loss: 0.1668453949:
17: 64064: loss: 0.1655291724:
17: 70464: loss: 0.1645094813:
17: 76864: loss: 0.1640962956:
17: 83264: loss: 0.1633641579:
17: 89664: loss: 0.1631042540:
Dev-Acc: 17: Accuracy: 0.8058620095: precision: 0.1717438235: recall: 0.6087400102: f1: 0.2679039138
Train-Acc: 17: Accuracy: 0.9428374171: precision: 0.8958960545: recall: 0.7434093748: f1: 0.8125606295
18: 6464: loss: 0.1672646672:
18: 12864: loss: 0.1648575932:
18: 19264: loss: 0.1629725445:
18: 25664: loss: 0.1613738030:
18: 32064: loss: 0.1582824886:
18: 38464: loss: 0.1597958666:
18: 44864: loss: 0.1603142580:
18: 51264: loss: 0.1595607357:
18: 57664: loss: 0.1608157362:
18: 64064: loss: 0.1614098896:
18: 70464: loss: 0.1610058639:
18: 76864: loss: 0.1610851079:
18: 83264: loss: 0.1609030326:
18: 89664: loss: 0.1602658012:
Dev-Acc: 18: Accuracy: 0.8000277877: precision: 0.1678380265: recall: 0.6131610270: f1: 0.2635386977
Train-Acc: 18: Accuracy: 0.9437797070: precision: 0.8947986840: recall: 0.7509696930: f1: 0.8165993495
19: 6464: loss: 0.1606306851:
19: 12864: loss: 0.1613363800:
19: 19264: loss: 0.1578669252:
19: 25664: loss: 0.1605007884:
19: 32064: loss: 0.1597083265:
19: 38464: loss: 0.1588812185:
19: 44864: loss: 0.1585397744:
19: 51264: loss: 0.1582771754:
19: 57664: loss: 0.1573470568:
19: 64064: loss: 0.1575173750:
19: 70464: loss: 0.1574340253:
19: 76864: loss: 0.1582078075:
19: 83264: loss: 0.1578190387:
19: 89664: loss: 0.1572041596:
Dev-Acc: 19: Accuracy: 0.7953444719: precision: 0.1661761376: recall: 0.6240435300: f1: 0.2624615605
Train-Acc: 19: Accuracy: 0.9443932772: precision: 0.8931730023: recall: 0.7568864637: f1: 0.8194014448
20: 6464: loss: 0.1593519357:
20: 12864: loss: 0.1591445997:
20: 19264: loss: 0.1578126497:
20: 25664: loss: 0.1584567945:
20: 32064: loss: 0.1583129511:
20: 38464: loss: 0.1558935052:
20: 44864: loss: 0.1556841623:
20: 51264: loss: 0.1556123592:
20: 57664: loss: 0.1548363835:
20: 64064: loss: 0.1547397160:
20: 70464: loss: 0.1552259195:
20: 76864: loss: 0.1552171773:
20: 83264: loss: 0.1546967163:
20: 89664: loss: 0.1545621093:
Dev-Acc: 20: Accuracy: 0.7948285341: precision: 0.1636586807: recall: 0.6121407924: f1: 0.2582681684
Train-Acc: 20: Accuracy: 0.9452917576: precision: 0.9014615747: recall: 0.7541910460: f1: 0.8212764434
