1: 3232: loss: 0.7051446211:
1: 6432: loss: 0.7053244862:
1: 9632: loss: 0.7047931949:
1: 12832: loss: 0.7043137985:
1: 16032: loss: 0.7044139210:
1: 19232: loss: 0.7042351708:
1: 22432: loss: 0.7040162448:
1: 25632: loss: 0.7038777278:
1: 28832: loss: 0.7038464281:
1: 32032: loss: 0.7036364856:
1: 35232: loss: 0.7035824814:
1: 38432: loss: 0.7034433238:
1: 41632: loss: 0.7033803899:
1: 44832: loss: 0.7033407214:
1: 48032: loss: 0.7032632616:
1: 51232: loss: 0.7031955132:
1: 54432: loss: 0.7031503437:
1: 57632: loss: 0.7030738108:
1: 60832: loss: 0.7030305382:
1: 64032: loss: 0.7028772633:
1: 67232: loss: 0.7028236288:
1: 70432: loss: 0.7027553913:
1: 73632: loss: 0.7027348183:
Dev-Acc: 1: Accuracy: 0.2907306552: precision: 0.0640252007: recall: 0.8190783880: f1: 0.1187667197
Train-Acc: 1: Accuracy: 0.3839458227: precision: 0.2208381120: recall: 0.8228255867: f1: 0.3482180119
2: 3232: loss: 0.7008530104:
2: 6432: loss: 0.7007356179:
2: 9632: loss: 0.7010879840:
2: 12832: loss: 0.7011983593:
2: 16032: loss: 0.7011767950:
2: 19232: loss: 0.7014161876:
2: 22432: loss: 0.7011408130:
2: 25632: loss: 0.7009931482:
2: 28832: loss: 0.7008670689:
2: 32032: loss: 0.7007986462:
2: 35232: loss: 0.7006912586:
2: 38432: loss: 0.7007353433:
2: 41632: loss: 0.7007059079:
2: 44832: loss: 0.7006771986:
2: 48032: loss: 0.7006349104:
2: 51232: loss: 0.7005313164:
2: 54432: loss: 0.7004556995:
2: 57632: loss: 0.7003288023:
2: 60832: loss: 0.7001692446:
2: 64032: loss: 0.7000596788:
2: 67232: loss: 0.6999532566:
2: 70432: loss: 0.6998057723:
2: 73632: loss: 0.6997401117:
Dev-Acc: 2: Accuracy: 0.3309057057: precision: 0.0653957495: recall: 0.7874511138: f1: 0.1207624909
Train-Acc: 2: Accuracy: 0.4198671877: precision: 0.2270023229: recall: 0.7902176057: f1: 0.3526891817
3: 3232: loss: 0.6984197026:
3: 6432: loss: 0.6977051714:
3: 9632: loss: 0.6976040840:
3: 12832: loss: 0.6974623893:
3: 16032: loss: 0.6976344712:
3: 19232: loss: 0.6976246271:
3: 22432: loss: 0.6975644803:
3: 25632: loss: 0.6974286396:
3: 28832: loss: 0.6973985171:
3: 32032: loss: 0.6973523751:
3: 35232: loss: 0.6972079144:
3: 38432: loss: 0.6971022270:
3: 41632: loss: 0.6969445825:
3: 44832: loss: 0.6969452198:
3: 48032: loss: 0.6969417843:
3: 51232: loss: 0.6968792371:
3: 54432: loss: 0.6967521509:
3: 57632: loss: 0.6966680531:
3: 60832: loss: 0.6966352160:
3: 64032: loss: 0.6965539274:
3: 67232: loss: 0.6964083368:
3: 70432: loss: 0.6963782433:
3: 73632: loss: 0.6962867091:
Dev-Acc: 3: Accuracy: 0.3777583838: precision: 0.0672656666: recall: 0.7510627444: f1: 0.1234730104
Train-Acc: 3: Accuracy: 0.4606271684: precision: 0.2352337772: recall: 0.7537965946: f1: 0.3585702223
4: 3232: loss: 0.6952602428:
4: 6432: loss: 0.6947331300:
4: 9632: loss: 0.6949815226:
4: 12832: loss: 0.6947493953:
4: 16032: loss: 0.6948261577:
4: 19232: loss: 0.6947177691:
4: 22432: loss: 0.6946049183:
4: 25632: loss: 0.6946514021:
4: 28832: loss: 0.6945237468:
4: 32032: loss: 0.6944980336:
4: 35232: loss: 0.6943168519:
4: 38432: loss: 0.6942480899:
4: 41632: loss: 0.6942113483:
4: 44832: loss: 0.6941921518:
4: 48032: loss: 0.6941768100:
4: 51232: loss: 0.6940674389:
4: 54432: loss: 0.6940322242:
4: 57632: loss: 0.6939123852:
4: 60832: loss: 0.6937566433:
4: 64032: loss: 0.6937123071:
4: 67232: loss: 0.6936402330:
4: 70432: loss: 0.6935523007:
4: 73632: loss: 0.6935077459:
Dev-Acc: 4: Accuracy: 0.4247499406: precision: 0.0681708914: recall: 0.6992008162: f1: 0.1242296073
Train-Acc: 4: Accuracy: 0.5044507384: precision: 0.2451011521: recall: 0.7104726842: f1: 0.3644672276
5: 3232: loss: 0.6913831204:
5: 6432: loss: 0.6907687315:
5: 9632: loss: 0.6907819223:
5: 12832: loss: 0.6907381271:
5: 16032: loss: 0.6907900779:
5: 19232: loss: 0.6909320302:
5: 22432: loss: 0.6909885007:
5: 25632: loss: 0.6910392194:
5: 28832: loss: 0.6908378405:
5: 32032: loss: 0.6908133374:
5: 35232: loss: 0.6907815046:
5: 38432: loss: 0.6905779198:
5: 41632: loss: 0.6905432892:
5: 44832: loss: 0.6903957097:
5: 48032: loss: 0.6902435920:
5: 51232: loss: 0.6902436220:
5: 54432: loss: 0.6902197199:
5: 57632: loss: 0.6901968606:
5: 60832: loss: 0.6901591805:
5: 64032: loss: 0.6901182956:
5: 67232: loss: 0.6901200984:
5: 70432: loss: 0.6901098353:
5: 73632: loss: 0.6900431939:
Dev-Acc: 5: Accuracy: 0.4732199609: precision: 0.0685747706: recall: 0.6379867369: f1: 0.1238386005
Train-Acc: 5: Accuracy: 0.5510222912: precision: 0.2568941611: recall: 0.6577476826: f1: 0.3694813228
6: 3232: loss: 0.6877778393:
6: 6432: loss: 0.6877057439:
6: 9632: loss: 0.6879597602:
6: 12832: loss: 0.6876079266:
6: 16032: loss: 0.6881431477:
6: 19232: loss: 0.6881688329:
6: 22432: loss: 0.6879993160:
6: 25632: loss: 0.6879092641:
6: 28832: loss: 0.6879686377:
6: 32032: loss: 0.6877674720:
6: 35232: loss: 0.6879084348:
6: 38432: loss: 0.6879023945:
6: 41632: loss: 0.6877291268:
6: 44832: loss: 0.6875883965:
6: 48032: loss: 0.6876034452:
6: 51232: loss: 0.6876333840:
6: 54432: loss: 0.6876301818:
6: 57632: loss: 0.6875290297:
6: 60832: loss: 0.6874842872:
6: 64032: loss: 0.6874452443:
6: 67232: loss: 0.6873808271:
6: 70432: loss: 0.6873151639:
6: 73632: loss: 0.6872527950:
Dev-Acc: 6: Accuracy: 0.5208663940: precision: 0.0689192486: recall: 0.5764325795: f1: 0.1231182698
Train-Acc: 6: Accuracy: 0.5973440409: precision: 0.2715170921: recall: 0.6020642956: f1: 0.3742541888
7: 3232: loss: 0.6849867004:
7: 6432: loss: 0.6853630525:
7: 9632: loss: 0.6850826933:
7: 12832: loss: 0.6851925351:
7: 16032: loss: 0.6852485292:
7: 19232: loss: 0.6851884897:
7: 22432: loss: 0.6852864592:
7: 25632: loss: 0.6852699269:
7: 28832: loss: 0.6851829218:
7: 32032: loss: 0.6851086296:
7: 35232: loss: 0.6850207130:
7: 38432: loss: 0.6849232323:
7: 41632: loss: 0.6848883397:
7: 44832: loss: 0.6848669904:
7: 48032: loss: 0.6847979362:
7: 51232: loss: 0.6847953941:
7: 54432: loss: 0.6846520168:
7: 57632: loss: 0.6846050424:
7: 60832: loss: 0.6845807477:
7: 64032: loss: 0.6845512137:
7: 67232: loss: 0.6844781215:
7: 70432: loss: 0.6843901560:
7: 73632: loss: 0.6843311849:
Dev-Acc: 7: Accuracy: 0.5711224079: precision: 0.0688970469: recall: 0.5073967012: f1: 0.1213205399
Train-Acc: 7: Accuracy: 0.6403786540: precision: 0.2899944644: recall: 0.5510485833: f1: 0.3800068004
8: 3232: loss: 0.6826461452:
8: 6432: loss: 0.6822982332:
8: 9632: loss: 0.6827840577:
8: 12832: loss: 0.6825101914:
8: 16032: loss: 0.6824746950:
8: 19232: loss: 0.6821329232:
8: 22432: loss: 0.6819975191:
8: 25632: loss: 0.6818902380:
8: 28832: loss: 0.6818791476:
8: 32032: loss: 0.6818716895:
8: 35232: loss: 0.6817513372:
8: 38432: loss: 0.6816663427:
8: 41632: loss: 0.6814995331:
8: 44832: loss: 0.6814426688:
8: 48032: loss: 0.6814072584:
8: 51232: loss: 0.6813046557:
8: 54432: loss: 0.6813261830:
8: 57632: loss: 0.6812107355:
8: 60832: loss: 0.6811074697:
8: 64032: loss: 0.6810957260:
8: 67232: loss: 0.6811048477:
8: 70432: loss: 0.6810576727:
8: 73632: loss: 0.6809936077:
Dev-Acc: 8: Accuracy: 0.6181139946: precision: 0.0705649941: recall: 0.4555347730: f1: 0.1222004288
Train-Acc: 8: Accuracy: 0.6786535978: precision: 0.3117375872: recall: 0.5023338374: f1: 0.3847238306
9: 3232: loss: 0.6793629330:
9: 6432: loss: 0.6794633856:
9: 9632: loss: 0.6794448028:
9: 12832: loss: 0.6794784495:
9: 16032: loss: 0.6791435915:
9: 19232: loss: 0.6790813925:
9: 22432: loss: 0.6791000568:
9: 25632: loss: 0.6793587824:
9: 28832: loss: 0.6792369207:
9: 32032: loss: 0.6790407103:
9: 35232: loss: 0.6790772542:
9: 38432: loss: 0.6789908681:
9: 41632: loss: 0.6788421565:
9: 44832: loss: 0.6787804326:
9: 48032: loss: 0.6787827329:
9: 51232: loss: 0.6787060412:
9: 54432: loss: 0.6786382816:
9: 57632: loss: 0.6786781658:
9: 60832: loss: 0.6786108434:
9: 64032: loss: 0.6785679170:
9: 67232: loss: 0.6785053878:
9: 70432: loss: 0.6783896733:
9: 73632: loss: 0.6782777992:
Dev-Acc: 9: Accuracy: 0.6605512500: precision: 0.0730861965: recall: 0.4123448393: f1: 0.1241647679
Train-Acc: 9: Accuracy: 0.7113536000: precision: 0.3384917593: recall: 0.4644665045: f1: 0.3915971510
10: 3232: loss: 0.6757260484:
10: 6432: loss: 0.6767174372:
10: 9632: loss: 0.6764676011:
10: 12832: loss: 0.6760667913:
10: 16032: loss: 0.6762330478:
10: 19232: loss: 0.6762160064:
10: 22432: loss: 0.6761766378:
10: 25632: loss: 0.6761285182:
10: 28832: loss: 0.6761268129:
10: 32032: loss: 0.6760507472:
10: 35232: loss: 0.6759052468:
10: 38432: loss: 0.6758266211:
10: 41632: loss: 0.6757640309:
10: 44832: loss: 0.6757413320:
10: 48032: loss: 0.6756091293:
10: 51232: loss: 0.6755198703:
10: 54432: loss: 0.6754494248:
10: 57632: loss: 0.6754096897:
10: 60832: loss: 0.6753850563:
10: 64032: loss: 0.6753651613:
10: 67232: loss: 0.6753420786:
10: 70432: loss: 0.6752847090:
10: 73632: loss: 0.6752045418:
Dev-Acc: 10: Accuracy: 0.7016887665: precision: 0.0766106443: recall: 0.3720455705: f1: 0.1270578671
Train-Acc: 10: Accuracy: 0.7397935390: precision: 0.3689692669: recall: 0.4238380120: f1: 0.3945049566
11: 3232: loss: 0.6736361367:
11: 6432: loss: 0.6737888789:
11: 9632: loss: 0.6739196211:
11: 12832: loss: 0.6738401875:
11: 16032: loss: 0.6739300320:
11: 19232: loss: 0.6738145391:
11: 22432: loss: 0.6735900975:
11: 25632: loss: 0.6735485344:
11: 28832: loss: 0.6737279049:
11: 32032: loss: 0.6735701024:
11: 35232: loss: 0.6735073595:
11: 38432: loss: 0.6732801466:
11: 41632: loss: 0.6731978230:
11: 44832: loss: 0.6731457454:
11: 48032: loss: 0.6730661443:
11: 51232: loss: 0.6730641633:
11: 54432: loss: 0.6729711423:
11: 57632: loss: 0.6729082544:
11: 60832: loss: 0.6728136512:
11: 64032: loss: 0.6727271608:
11: 67232: loss: 0.6726268662:
11: 70432: loss: 0.6725278597:
11: 73632: loss: 0.6724637344:
Dev-Acc: 11: Accuracy: 0.7363073230: precision: 0.0795237515: recall: 0.3327665363: f1: 0.1283699574
Train-Acc: 11: Accuracy: 0.7636841536: precision: 0.4057080432: recall: 0.3906383538: f1: 0.3980306126
12: 3232: loss: 0.6717298084:
12: 6432: loss: 0.6710838637:
12: 9632: loss: 0.6710350631:
12: 12832: loss: 0.6709767064:
12: 16032: loss: 0.6708226755:
12: 19232: loss: 0.6707641486:
12: 22432: loss: 0.6707660896:
12: 25632: loss: 0.6707959805:
12: 28832: loss: 0.6706514463:
12: 32032: loss: 0.6704641722:
12: 35232: loss: 0.6704153867:
12: 38432: loss: 0.6702831644:
12: 41632: loss: 0.6702276197:
12: 44832: loss: 0.6701412338:
12: 48032: loss: 0.6700616089:
12: 51232: loss: 0.6698931638:
12: 54432: loss: 0.6699114423:
12: 57632: loss: 0.6698228769:
12: 60832: loss: 0.6697501382:
12: 64032: loss: 0.6697087187:
12: 67232: loss: 0.6696221360:
12: 70432: loss: 0.6696108780:
12: 73632: loss: 0.6695304601:
Dev-Acc: 12: Accuracy: 0.7655481100: precision: 0.0842391304: recall: 0.3057303180: f1: 0.1320844812
Train-Acc: 12: Accuracy: 0.7819867134: precision: 0.4446330424: recall: 0.3616461771: f1: 0.3988688685
13: 3232: loss: 0.6672332221:
13: 6432: loss: 0.6675584880:
13: 9632: loss: 0.6677017415:
13: 12832: loss: 0.6672444196:
13: 16032: loss: 0.6675069610:
13: 19232: loss: 0.6672495396:
13: 22432: loss: 0.6671725237:
13: 25632: loss: 0.6671665789:
13: 28832: loss: 0.6670860691:
13: 32032: loss: 0.6669720640:
13: 35232: loss: 0.6669838249:
13: 38432: loss: 0.6669613717:
13: 41632: loss: 0.6670045986:
13: 44832: loss: 0.6670161301:
13: 48032: loss: 0.6669849003:
13: 51232: loss: 0.6668972609:
13: 54432: loss: 0.6668140903:
13: 57632: loss: 0.6667604633:
13: 60832: loss: 0.6666876716:
13: 64032: loss: 0.6666924160:
13: 67232: loss: 0.6665874277:
13: 70432: loss: 0.6664955608:
13: 73632: loss: 0.6663900906:
Dev-Acc: 13: Accuracy: 0.7907703519: precision: 0.0879132791: recall: 0.2758034348: f1: 0.1333278534
Train-Acc: 13: Accuracy: 0.7953585982: precision: 0.4832907318: recall: 0.3356123858: f1: 0.3961356406
14: 3232: loss: 0.6654151779:
14: 6432: loss: 0.6649866557:
14: 9632: loss: 0.6648617880:
14: 12832: loss: 0.6648166265:
14: 16032: loss: 0.6647767743:
14: 19232: loss: 0.6645221843:
14: 22432: loss: 0.6646063100:
14: 25632: loss: 0.6647319952:
14: 28832: loss: 0.6647209462:
14: 32032: loss: 0.6647663122:
14: 35232: loss: 0.6647168793:
14: 38432: loss: 0.6646244419:
14: 41632: loss: 0.6645682809:
14: 44832: loss: 0.6644458131:
14: 48032: loss: 0.6643909410:
14: 51232: loss: 0.6643365830:
14: 54432: loss: 0.6643398389:
14: 57632: loss: 0.6641547121:
14: 60832: loss: 0.6640374745:
14: 64032: loss: 0.6639451975:
14: 67232: loss: 0.6637767127:
14: 70432: loss: 0.6637036123:
14: 73632: loss: 0.6636700501:
Dev-Acc: 14: Accuracy: 0.8128175139: precision: 0.0929780564: recall: 0.2521679986: f1: 0.1358618478
Train-Acc: 14: Accuracy: 0.8067713976: precision: 0.5287356322: recall: 0.3114851095: f1: 0.3920238292
15: 3232: loss: 0.6622622436:
15: 6432: loss: 0.6621081606:
15: 9632: loss: 0.6624274909:
15: 12832: loss: 0.6621718672:
15: 16032: loss: 0.6624128927:
15: 19232: loss: 0.6625033589:
15: 22432: loss: 0.6623310594:
15: 25632: loss: 0.6622071940:
15: 28832: loss: 0.6621331225:
15: 32032: loss: 0.6620653890:
15: 35232: loss: 0.6619109370:
15: 38432: loss: 0.6619007609:
15: 41632: loss: 0.6618829829:
15: 44832: loss: 0.6617143123:
15: 48032: loss: 0.6615677271:
15: 51232: loss: 0.6614574637:
15: 54432: loss: 0.6613970800:
15: 57632: loss: 0.6614190855:
15: 60832: loss: 0.6613103186:
15: 64032: loss: 0.6612200560:
15: 67232: loss: 0.6610603836:
15: 70432: loss: 0.6610270666:
15: 73632: loss: 0.6609490574:
Dev-Acc: 15: Accuracy: 0.8319276571: precision: 0.0984749455: recall: 0.2305730318: f1: 0.1380082439
Train-Acc: 15: Accuracy: 0.8146867156: precision: 0.5726173449: recall: 0.2895273158: f1: 0.3845952319
16: 3232: loss: 0.6592754710:
16: 6432: loss: 0.6594632572:
16: 9632: loss: 0.6597977682:
16: 12832: loss: 0.6595352341:
16: 16032: loss: 0.6594692625:
16: 19232: loss: 0.6595757473:
16: 22432: loss: 0.6594488679:
16: 25632: loss: 0.6595236325:
16: 28832: loss: 0.6592817363:
16: 32032: loss: 0.6591534981:
16: 35232: loss: 0.6590046275:
16: 38432: loss: 0.6589718989:
16: 41632: loss: 0.6589070019:
16: 44832: loss: 0.6588540193:
16: 48032: loss: 0.6588576179:
16: 51232: loss: 0.6587085002:
16: 54432: loss: 0.6586136740:
16: 57632: loss: 0.6584872490:
16: 60832: loss: 0.6583566468:
16: 64032: loss: 0.6583297331:
16: 67232: loss: 0.6582383587:
16: 70432: loss: 0.6582161213:
16: 73632: loss: 0.6581411688:
Dev-Acc: 16: Accuracy: 0.8493014574: precision: 0.1054684188: recall: 0.2115286516: f1: 0.1407558271
Train-Acc: 16: Accuracy: 0.8200381398: precision: 0.6160170524: recall: 0.2659917165: f1: 0.3715505762
17: 3232: loss: 0.6566045475:
17: 6432: loss: 0.6572394013:
17: 9632: loss: 0.6571450452:
17: 12832: loss: 0.6564689551:
17: 16032: loss: 0.6561707293:
17: 19232: loss: 0.6563607718:
17: 22432: loss: 0.6561047059:
17: 25632: loss: 0.6560750826:
17: 28832: loss: 0.6560965474:
17: 32032: loss: 0.6558775595:
17: 35232: loss: 0.6558104183:
17: 38432: loss: 0.6558122800:
17: 41632: loss: 0.6556841767:
17: 44832: loss: 0.6557242283:
17: 48032: loss: 0.6556802499:
17: 51232: loss: 0.6556739864:
17: 54432: loss: 0.6556160812:
17: 57632: loss: 0.6555424576:
17: 60832: loss: 0.6554728777:
17: 64032: loss: 0.6553511408:
17: 67232: loss: 0.6552693726:
17: 70432: loss: 0.6551666636:
17: 73632: loss: 0.6551410846:
Dev-Acc: 17: Accuracy: 0.8644626141: precision: 0.1153960249: recall: 0.1984356402: f1: 0.1459297236
Train-Acc: 17: Accuracy: 0.8240089417: precision: 0.6636787379: recall: 0.2433765038: f1: 0.3561498870
18: 3232: loss: 0.6545031106:
18: 6432: loss: 0.6538817245:
18: 9632: loss: 0.6542017331:
18: 12832: loss: 0.6539111806:
18: 16032: loss: 0.6539205575:
18: 19232: loss: 0.6538329362:
18: 22432: loss: 0.6535761670:
18: 25632: loss: 0.6534183938:
18: 28832: loss: 0.6534148675:
18: 32032: loss: 0.6533597666:
18: 35232: loss: 0.6532487543:
18: 38432: loss: 0.6530396988:
18: 41632: loss: 0.6530433518:
18: 44832: loss: 0.6530066168:
18: 48032: loss: 0.6529952002:
18: 51232: loss: 0.6528479155:
18: 54432: loss: 0.6528404006:
18: 57632: loss: 0.6528201069:
18: 60832: loss: 0.6527877677:
18: 64032: loss: 0.6527332286:
18: 67232: loss: 0.6527592136:
18: 70432: loss: 0.6527462229:
18: 73632: loss: 0.6526492854:
Dev-Acc: 18: Accuracy: 0.8765478730: precision: 0.1235800344: recall: 0.1831321204: f1: 0.1475746780
Train-Acc: 18: Accuracy: 0.8263099194: precision: 0.7039755352: recall: 0.2270067714: f1: 0.3433088089
19: 3232: loss: 0.6508615673:
19: 6432: loss: 0.6505479011:
19: 9632: loss: 0.6501069321:
19: 12832: loss: 0.6503368479:
19: 16032: loss: 0.6505017793:
19: 19232: loss: 0.6506212234:
19: 22432: loss: 0.6506798456:
19: 25632: loss: 0.6505017085:
19: 28832: loss: 0.6502711957:
19: 32032: loss: 0.6502011955:
19: 35232: loss: 0.6501777243:
19: 38432: loss: 0.6501619750:
19: 41632: loss: 0.6500648637:
19: 44832: loss: 0.6500297339:
19: 48032: loss: 0.6498130236:
19: 51232: loss: 0.6497918253:
19: 54432: loss: 0.6498154193:
19: 57632: loss: 0.6498185587:
19: 60832: loss: 0.6498549565:
19: 64032: loss: 0.6497277769:
19: 67232: loss: 0.6496690804:
19: 70432: loss: 0.6496750114:
19: 73632: loss: 0.6496759369:
Dev-Acc: 19: Accuracy: 0.8884743452: precision: 0.1349952323: recall: 0.1685087570: f1: 0.1499016790
Train-Acc: 19: Accuracy: 0.8265728951: precision: 0.7344003711: recall: 0.2081388469: f1: 0.3243520131
20: 3232: loss: 0.6484879529:
20: 6432: loss: 0.6489727235:
20: 9632: loss: 0.6494841973:
20: 12832: loss: 0.6494288063:
20: 16032: loss: 0.6492001340:
20: 19232: loss: 0.6487752520:
20: 22432: loss: 0.6485679373:
20: 25632: loss: 0.6482506637:
20: 28832: loss: 0.6481747044:
20: 32032: loss: 0.6482041643:
20: 35232: loss: 0.6480667353:
20: 38432: loss: 0.6479349390:
20: 41632: loss: 0.6477001816:
20: 44832: loss: 0.6475979838:
20: 48032: loss: 0.6474544210:
20: 51232: loss: 0.6474360136:
20: 54432: loss: 0.6474967241:
20: 57632: loss: 0.6474488219:
20: 60832: loss: 0.6472983860:
20: 64032: loss: 0.6471631727:
20: 67232: loss: 0.6470804591:
20: 70432: loss: 0.6470238912:
20: 73632: loss: 0.6469177137:
Dev-Acc: 20: Accuracy: 0.8982080221: precision: 0.1457928803: recall: 0.1532052372: f1: 0.1494071802
Train-Acc: 20: Accuracy: 0.8268752694: precision: 0.7683823529: recall: 0.1923607915: f1: 0.3076923077
