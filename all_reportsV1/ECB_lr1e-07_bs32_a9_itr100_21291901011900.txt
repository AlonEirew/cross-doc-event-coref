1: 3232: loss: 0.7111244577:
1: 6432: loss: 0.7094125569:
1: 9632: loss: 0.7083820947:
1: 12832: loss: 0.7074148309:
1: 16032: loss: 0.7065620910:
1: 19232: loss: 0.7057542194:
1: 22432: loss: 0.7047478926:
1: 25632: loss: 0.7037866612:
1: 28832: loss: 0.7030865491:
1: 32032: loss: 0.7021649307:
1: 35232: loss: 0.7013409737:
1: 38432: loss: 0.7003973394:
1: 41632: loss: 0.6993758652:
1: 44832: loss: 0.6984379728:
1: 48032: loss: 0.6975267936:
1: 51232: loss: 0.6966352424:
1: 54432: loss: 0.6956952740:
1: 57632: loss: 0.6948251848:
1: 60832: loss: 0.6939174083:
1: 64032: loss: 0.6929972117:
1: 67232: loss: 0.6920330500:
1: 70432: loss: 0.6911362050:
1: 73632: loss: 0.6903389028:
1: 76832: loss: 0.6894300652:
1: 80032: loss: 0.6885198372:
1: 83232: loss: 0.6876142357:
1: 86432: loss: 0.6867421676:
1: 89632: loss: 0.6858704972:
1: 92832: loss: 0.6850314956:
1: 96032: loss: 0.6841437631:
1: 99232: loss: 0.6832517824:
1: 102432: loss: 0.6823782315:
1: 105632: loss: 0.6815177701:
1: 108832: loss: 0.6806982373:
1: 112032: loss: 0.6798611891:
1: 115232: loss: 0.6789700724:
1: 118432: loss: 0.6781358388:
1: 121632: loss: 0.6772824614:
1: 124832: loss: 0.6764577491:
1: 128032: loss: 0.6756301627:
1: 131232: loss: 0.6748143931:
1: 134432: loss: 0.6739744677:
1: 137632: loss: 0.6731360874:
1: 140832: loss: 0.6722692980:
1: 144032: loss: 0.6714017851:
1: 147232: loss: 0.6705698666:
1: 150432: loss: 0.6697083469:
Dev-Acc: 1: Accuracy: 0.9352873564: precision: 0.0709504685: recall: 0.0090120728: f1: 0.0159927580
Train-Acc: 1: Accuracy: 0.8952468634: precision: 0.2578700603: recall: 0.0253106305: f1: 0.0460967433
2: 3232: loss: 0.6287059605:
2: 6432: loss: 0.6281528375:
2: 9632: loss: 0.6269738352:
2: 12832: loss: 0.6262593848:
2: 16032: loss: 0.6258687074:
2: 19232: loss: 0.6248125014:
2: 22432: loss: 0.6238944697:
2: 25632: loss: 0.6227080821:
2: 28832: loss: 0.6222386575:
2: 32032: loss: 0.6213797609:
2: 35232: loss: 0.6208388061:
2: 38432: loss: 0.6202462053:
2: 41632: loss: 0.6197291995:
2: 44832: loss: 0.6189679228:
2: 48032: loss: 0.6182607930:
2: 51232: loss: 0.6175890736:
2: 54432: loss: 0.6167038232:
2: 57632: loss: 0.6157203862:
2: 60832: loss: 0.6149861505:
2: 64032: loss: 0.6141333520:
2: 67232: loss: 0.6133992926:
2: 70432: loss: 0.6127012249:
2: 73632: loss: 0.6119510170:
2: 76832: loss: 0.6112364980:
2: 80032: loss: 0.6103247424:
2: 83232: loss: 0.6094769344:
2: 86432: loss: 0.6086784151:
2: 89632: loss: 0.6079074214:
2: 92832: loss: 0.6071721792:
2: 96032: loss: 0.6064485412:
2: 99232: loss: 0.6056695262:
2: 102432: loss: 0.6049351234:
2: 105632: loss: 0.6040841557:
2: 108832: loss: 0.6032260604:
2: 112032: loss: 0.6023864001:
2: 115232: loss: 0.6016858016:
2: 118432: loss: 0.6009249317:
2: 121632: loss: 0.6001332131:
2: 124832: loss: 0.5992123716:
2: 128032: loss: 0.5984274664:
2: 131232: loss: 0.5976618746:
2: 134432: loss: 0.5968648896:
2: 137632: loss: 0.5960876519:
2: 140832: loss: 0.5952446495:
2: 144032: loss: 0.5944984645:
2: 147232: loss: 0.5937823924:
2: 150432: loss: 0.5930545751:
Dev-Acc: 2: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 2: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
3: 3232: loss: 0.5563245171:
3: 6432: loss: 0.5567201337:
3: 9632: loss: 0.5554723357:
3: 12832: loss: 0.5534426728:
3: 16032: loss: 0.5527211322:
3: 19232: loss: 0.5524475500:
3: 22432: loss: 0.5516392888:
3: 25632: loss: 0.5507649744:
3: 28832: loss: 0.5502144073:
3: 32032: loss: 0.5498154551:
3: 35232: loss: 0.5491912520:
3: 38432: loss: 0.5482335689:
3: 41632: loss: 0.5474092528:
3: 44832: loss: 0.5464804520:
3: 48032: loss: 0.5455288795:
3: 51232: loss: 0.5450033374:
3: 54432: loss: 0.5444944425:
3: 57632: loss: 0.5439750802:
3: 60832: loss: 0.5430826939:
3: 64032: loss: 0.5424505071:
3: 67232: loss: 0.5417839313:
3: 70432: loss: 0.5408909609:
3: 73632: loss: 0.5404338519:
3: 76832: loss: 0.5397704324:
3: 80032: loss: 0.5391170383:
3: 83232: loss: 0.5385127973:
3: 86432: loss: 0.5377712782:
3: 89632: loss: 0.5370539374:
3: 92832: loss: 0.5362980399:
3: 96032: loss: 0.5356231990:
3: 99232: loss: 0.5347942539:
3: 102432: loss: 0.5341206611:
3: 105632: loss: 0.5334577984:
3: 108832: loss: 0.5327515791:
3: 112032: loss: 0.5322165992:
3: 115232: loss: 0.5314452467:
3: 118432: loss: 0.5309004305:
3: 121632: loss: 0.5302616488:
3: 124832: loss: 0.5295418924:
3: 128032: loss: 0.5287446705:
3: 131232: loss: 0.5279492861:
3: 134432: loss: 0.5273709668:
3: 137632: loss: 0.5267129966:
3: 140832: loss: 0.5261198953:
3: 144032: loss: 0.5255486537:
3: 147232: loss: 0.5249408234:
3: 150432: loss: 0.5242064102:
Dev-Acc: 3: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 3: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
4: 3232: loss: 0.4913266388:
4: 6432: loss: 0.4904819937:
4: 9632: loss: 0.4900741268:
4: 12832: loss: 0.4892859878:
4: 16032: loss: 0.4889517115:
4: 19232: loss: 0.4879994324:
4: 22432: loss: 0.4882463528:
4: 25632: loss: 0.4873012979:
4: 28832: loss: 0.4876868527:
4: 32032: loss: 0.4868017220:
4: 35232: loss: 0.4863861971:
4: 38432: loss: 0.4856660393:
4: 41632: loss: 0.4851257729:
4: 44832: loss: 0.4840123196:
4: 48032: loss: 0.4833452420:
4: 51232: loss: 0.4825029410:
4: 54432: loss: 0.4816464439:
4: 57632: loss: 0.4810352353:
4: 60832: loss: 0.4802902945:
4: 64032: loss: 0.4799512610:
4: 67232: loss: 0.4794841904:
4: 70432: loss: 0.4790989842:
4: 73632: loss: 0.4784338160:
4: 76832: loss: 0.4775538240:
4: 80032: loss: 0.4767078689:
4: 83232: loss: 0.4759508113:
4: 86432: loss: 0.4752266710:
4: 89632: loss: 0.4748427138:
4: 92832: loss: 0.4741563484:
4: 96032: loss: 0.4736528850:
4: 99232: loss: 0.4728215964:
4: 102432: loss: 0.4723767934:
4: 105632: loss: 0.4716076901:
4: 108832: loss: 0.4710934661:
4: 112032: loss: 0.4703090848:
4: 115232: loss: 0.4697288640:
4: 118432: loss: 0.4689530546:
4: 121632: loss: 0.4683378826:
4: 124832: loss: 0.4677239979:
4: 128032: loss: 0.4670731129:
4: 131232: loss: 0.4667640255:
4: 134432: loss: 0.4662299574:
4: 137632: loss: 0.4657622840:
4: 140832: loss: 0.4653106958:
4: 144032: loss: 0.4648016637:
4: 147232: loss: 0.4641017948:
4: 150432: loss: 0.4636714168:
Dev-Acc: 4: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 4: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
5: 3232: loss: 0.4355729342:
5: 6432: loss: 0.4349409121:
5: 9632: loss: 0.4336597661:
5: 12832: loss: 0.4356011024:
5: 16032: loss: 0.4338588110:
5: 19232: loss: 0.4332160010:
5: 22432: loss: 0.4335530419:
5: 25632: loss: 0.4322853152:
5: 28832: loss: 0.4315384238:
5: 32032: loss: 0.4306270387:
5: 35232: loss: 0.4316708881:
5: 38432: loss: 0.4306295384:
5: 41632: loss: 0.4306276228:
5: 44832: loss: 0.4299086999:
5: 48032: loss: 0.4301309132:
5: 51232: loss: 0.4295664569:
5: 54432: loss: 0.4296146717:
5: 57632: loss: 0.4290034321:
5: 60832: loss: 0.4285042084:
5: 64032: loss: 0.4280729602:
5: 67232: loss: 0.4276458137:
5: 70432: loss: 0.4270683568:
5: 73632: loss: 0.4263049096:
5: 76832: loss: 0.4257095594:
5: 80032: loss: 0.4253398481:
5: 83232: loss: 0.4248018526:
5: 86432: loss: 0.4242744426:
5: 89632: loss: 0.4240041372:
5: 92832: loss: 0.4234627310:
5: 96032: loss: 0.4232178595:
5: 99232: loss: 0.4227228286:
5: 102432: loss: 0.4224543314:
5: 105632: loss: 0.4219554033:
5: 108832: loss: 0.4211990911:
5: 112032: loss: 0.4206348663:
5: 115232: loss: 0.4201463330:
5: 118432: loss: 0.4195248246:
5: 121632: loss: 0.4188151951:
5: 124832: loss: 0.4180038733:
5: 128032: loss: 0.4174258727:
5: 131232: loss: 0.4169535864:
5: 134432: loss: 0.4164034331:
5: 137632: loss: 0.4157973167:
5: 140832: loss: 0.4153193439:
5: 144032: loss: 0.4150163963:
5: 147232: loss: 0.4143594290:
5: 150432: loss: 0.4139559579:
Dev-Acc: 5: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 5: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
6: 3232: loss: 0.3870534724:
6: 6432: loss: 0.3858283243:
6: 9632: loss: 0.3880040434:
6: 12832: loss: 0.3911829364:
6: 16032: loss: 0.3904379705:
6: 19232: loss: 0.3904791996:
6: 22432: loss: 0.3912221542:
6: 25632: loss: 0.3906769877:
6: 28832: loss: 0.3893098807:
6: 32032: loss: 0.3887798075:
6: 35232: loss: 0.3888279248:
6: 38432: loss: 0.3882102260:
6: 41632: loss: 0.3874578896:
6: 44832: loss: 0.3873762195:
6: 48032: loss: 0.3877667080:
6: 51232: loss: 0.3875775048:
6: 54432: loss: 0.3875433847:
6: 57632: loss: 0.3875900075:
6: 60832: loss: 0.3868994794:
6: 64032: loss: 0.3863669859:
6: 67232: loss: 0.3856100938:
6: 70432: loss: 0.3846605896:
6: 73632: loss: 0.3841123354:
6: 76832: loss: 0.3839903802:
6: 80032: loss: 0.3837259664:
6: 83232: loss: 0.3833767582:
6: 86432: loss: 0.3831556223:
6: 89632: loss: 0.3826309029:
6: 92832: loss: 0.3820850027:
6: 96032: loss: 0.3818646604:
6: 99232: loss: 0.3813393564:
6: 102432: loss: 0.3803738394:
6: 105632: loss: 0.3802909010:
6: 108832: loss: 0.3797729670:
6: 112032: loss: 0.3794262857:
6: 115232: loss: 0.3790689380:
6: 118432: loss: 0.3789025076:
6: 121632: loss: 0.3784306674:
6: 124832: loss: 0.3782167321:
6: 128032: loss: 0.3776292483:
6: 131232: loss: 0.3773067410:
6: 134432: loss: 0.3766483202:
6: 137632: loss: 0.3762518825:
6: 140832: loss: 0.3758140733:
6: 144032: loss: 0.3754691248:
6: 147232: loss: 0.3751705095:
6: 150432: loss: 0.3748053116:
Dev-Acc: 6: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 6: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
7: 3232: loss: 0.3438250795:
7: 6432: loss: 0.3606323738:
7: 9632: loss: 0.3607528919:
7: 12832: loss: 0.3604182174:
7: 16032: loss: 0.3608775521:
7: 19232: loss: 0.3588995270:
7: 22432: loss: 0.3585622026:
7: 25632: loss: 0.3575903645:
7: 28832: loss: 0.3583907807:
7: 32032: loss: 0.3590011830:
7: 35232: loss: 0.3579084848:
7: 38432: loss: 0.3559903879:
7: 41632: loss: 0.3558013327:
7: 44832: loss: 0.3548727339:
7: 48032: loss: 0.3548455405:
7: 51232: loss: 0.3549205866:
7: 54432: loss: 0.3546092441:
7: 57632: loss: 0.3544438936:
7: 60832: loss: 0.3541005430:
7: 64032: loss: 0.3538793912:
7: 67232: loss: 0.3532390817:
7: 70432: loss: 0.3529138730:
7: 73632: loss: 0.3528403012:
7: 76832: loss: 0.3519573421:
7: 80032: loss: 0.3520395543:
7: 83232: loss: 0.3519882879:
7: 86432: loss: 0.3521106657:
7: 89632: loss: 0.3513621932:
7: 92832: loss: 0.3512728999:
7: 96032: loss: 0.3507967698:
7: 99232: loss: 0.3502152769:
7: 102432: loss: 0.3499930207:
7: 105632: loss: 0.3495499822:
7: 108832: loss: 0.3492310659:
7: 112032: loss: 0.3492743715:
7: 115232: loss: 0.3489429471:
7: 118432: loss: 0.3487784253:
7: 121632: loss: 0.3485139410:
7: 124832: loss: 0.3486293694:
7: 128032: loss: 0.3482315072:
7: 131232: loss: 0.3479570371:
7: 134432: loss: 0.3474480155:
7: 137632: loss: 0.3470585133:
7: 140832: loss: 0.3469573817:
7: 144032: loss: 0.3465027452:
7: 147232: loss: 0.3464266725:
7: 150432: loss: 0.3463399001:
Dev-Acc: 7: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 7: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
8: 3232: loss: 0.3402914742:
8: 6432: loss: 0.3408429311:
8: 9632: loss: 0.3381149892:
8: 12832: loss: 0.3368816010:
8: 16032: loss: 0.3382071004:
8: 19232: loss: 0.3354449291:
8: 22432: loss: 0.3348411033:
8: 25632: loss: 0.3341500592:
8: 28832: loss: 0.3345893618:
8: 32032: loss: 0.3348601366:
8: 35232: loss: 0.3356442361:
8: 38432: loss: 0.3356340561:
8: 41632: loss: 0.3360277096:
8: 44832: loss: 0.3359685918:
8: 48032: loss: 0.3356729834:
8: 51232: loss: 0.3348315252:
8: 54432: loss: 0.3335688860:
8: 57632: loss: 0.3328852866:
8: 60832: loss: 0.3324579335:
8: 64032: loss: 0.3321752055:
8: 67232: loss: 0.3315366790:
8: 70432: loss: 0.3305003279:
8: 73632: loss: 0.3303691184:
8: 76832: loss: 0.3302712013:
8: 80032: loss: 0.3300826797:
8: 83232: loss: 0.3298316133:
8: 86432: loss: 0.3295049081:
8: 89632: loss: 0.3287838688:
8: 92832: loss: 0.3285679857:
8: 96032: loss: 0.3283303474:
8: 99232: loss: 0.3278768979:
8: 102432: loss: 0.3272710729:
8: 105632: loss: 0.3274599965:
8: 108832: loss: 0.3275729912:
8: 112032: loss: 0.3272896419:
8: 115232: loss: 0.3272543317:
8: 118432: loss: 0.3269494619:
8: 121632: loss: 0.3270012538:
8: 124832: loss: 0.3265824091:
8: 128032: loss: 0.3261817861:
8: 131232: loss: 0.3259275464:
8: 134432: loss: 0.3257951654:
8: 137632: loss: 0.3251410086:
8: 140832: loss: 0.3253900287:
8: 144032: loss: 0.3254039521:
8: 147232: loss: 0.3255554894:
8: 150432: loss: 0.3251731733:
Dev-Acc: 8: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 8: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
9: 3232: loss: 0.3124183159:
9: 6432: loss: 0.3144356333:
9: 9632: loss: 0.3176171371:
9: 12832: loss: 0.3157209527:
9: 16032: loss: 0.3164894571:
9: 19232: loss: 0.3131324935:
9: 22432: loss: 0.3132864449:
9: 25632: loss: 0.3128764573:
9: 28832: loss: 0.3115157354:
9: 32032: loss: 0.3119829738:
9: 35232: loss: 0.3113781272:
9: 38432: loss: 0.3102617054:
9: 41632: loss: 0.3098571982:
9: 44832: loss: 0.3101988715:
9: 48032: loss: 0.3104433051:
9: 51232: loss: 0.3100427888:
9: 54432: loss: 0.3105441177:
9: 57632: loss: 0.3103633581:
9: 60832: loss: 0.3101933297:
9: 64032: loss: 0.3104872546:
9: 67232: loss: 0.3111016038:
9: 70432: loss: 0.3108378686:
9: 73632: loss: 0.3112826637:
9: 76832: loss: 0.3118280027:
9: 80032: loss: 0.3112811057:
9: 83232: loss: 0.3114561357:
9: 86432: loss: 0.3111984405:
9: 89632: loss: 0.3109063735:
9: 92832: loss: 0.3106059638:
9: 96032: loss: 0.3107021359:
9: 99232: loss: 0.3113881483:
9: 102432: loss: 0.3114812552:
9: 105632: loss: 0.3113340013:
9: 108832: loss: 0.3114450738:
9: 112032: loss: 0.3111627518:
9: 115232: loss: 0.3114373798:
9: 118432: loss: 0.3114046421:
9: 121632: loss: 0.3111898903:
9: 124832: loss: 0.3111552517:
9: 128032: loss: 0.3110230071:
9: 131232: loss: 0.3107881522:
9: 134432: loss: 0.3107739929:
9: 137632: loss: 0.3101921454:
9: 140832: loss: 0.3100451589:
9: 144032: loss: 0.3096932708:
9: 147232: loss: 0.3096000831:
9: 150432: loss: 0.3094847892:
Dev-Acc: 9: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 9: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
10: 3232: loss: 0.2917311358:
10: 6432: loss: 0.3051430292:
10: 9632: loss: 0.3029045116:
10: 12832: loss: 0.2988844875:
10: 16032: loss: 0.2956851780:
10: 19232: loss: 0.2985717031:
10: 22432: loss: 0.3000133289:
10: 25632: loss: 0.3012894201:
10: 28832: loss: 0.3013947346:
10: 32032: loss: 0.3022518711:
10: 35232: loss: 0.3008248667:
10: 38432: loss: 0.2995748415:
10: 41632: loss: 0.2983818144:
10: 44832: loss: 0.2988742240:
10: 48032: loss: 0.2987071459:
10: 51232: loss: 0.2989301325:
10: 54432: loss: 0.2990327109:
10: 57632: loss: 0.2990271806:
10: 60832: loss: 0.2990651213:
10: 64032: loss: 0.2989332847:
10: 67232: loss: 0.2989874891:
10: 70432: loss: 0.2990688933:
10: 73632: loss: 0.2983475343:
10: 76832: loss: 0.2976803058:
10: 80032: loss: 0.2975186271:
10: 83232: loss: 0.2982874227:
10: 86432: loss: 0.2986150682:
10: 89632: loss: 0.2983210550:
10: 92832: loss: 0.2987906282:
10: 96032: loss: 0.2986786515:
10: 99232: loss: 0.2983758808:
10: 102432: loss: 0.2979320936:
10: 105632: loss: 0.2980471565:
10: 108832: loss: 0.2978801825:
10: 112032: loss: 0.2973965167:
10: 115232: loss: 0.2975021831:
10: 118432: loss: 0.2972448163:
10: 121632: loss: 0.2972783582:
10: 124832: loss: 0.2971802640:
10: 128032: loss: 0.2970843292:
10: 131232: loss: 0.2969066683:
10: 134432: loss: 0.2969188223:
10: 137632: loss: 0.2967435673:
10: 140832: loss: 0.2963233908:
10: 144032: loss: 0.2964463405:
10: 147232: loss: 0.2965499415:
10: 150432: loss: 0.2966580023:
Dev-Acc: 10: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 10: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
11: 3232: loss: 0.2922160549:
11: 6432: loss: 0.2978521585:
11: 9632: loss: 0.2941921922:
11: 12832: loss: 0.2929243619:
11: 16032: loss: 0.2946414612:
11: 19232: loss: 0.2929871072:
11: 22432: loss: 0.2914759812:
11: 25632: loss: 0.2910312415:
11: 28832: loss: 0.2905834883:
11: 32032: loss: 0.2901246857:
11: 35232: loss: 0.2890931998:
11: 38432: loss: 0.2889766393:
11: 41632: loss: 0.2882968954:
11: 44832: loss: 0.2872743818:
11: 48032: loss: 0.2877662631:
11: 51232: loss: 0.2877825084:
11: 54432: loss: 0.2869865765:
11: 57632: loss: 0.2867231728:
11: 60832: loss: 0.2877226775:
11: 64032: loss: 0.2865390585:
11: 67232: loss: 0.2865505718:
11: 70432: loss: 0.2870612016:
11: 73632: loss: 0.2875339773:
11: 76832: loss: 0.2878269403:
11: 80032: loss: 0.2873952661:
11: 83232: loss: 0.2878718765:
11: 86432: loss: 0.2875314206:
11: 89632: loss: 0.2871905246:
11: 92832: loss: 0.2868564941:
11: 96032: loss: 0.2871262832:
11: 99232: loss: 0.2869141660:
11: 102432: loss: 0.2868509102:
11: 105632: loss: 0.2865797515:
11: 108832: loss: 0.2871461891:
11: 112032: loss: 0.2871668354:
11: 115232: loss: 0.2871721850:
11: 118432: loss: 0.2873976743:
11: 121632: loss: 0.2867607239:
11: 124832: loss: 0.2864696809:
11: 128032: loss: 0.2867967029:
11: 131232: loss: 0.2867141461:
11: 134432: loss: 0.2863841391:
11: 137632: loss: 0.2862115715:
11: 140832: loss: 0.2859508022:
11: 144032: loss: 0.2860572803:
11: 147232: loss: 0.2864137045:
11: 150432: loss: 0.2863606606:
Dev-Acc: 11: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 11: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
12: 3232: loss: 0.2812590869:
12: 6432: loss: 0.2895238905:
12: 9632: loss: 0.2829718986:
12: 12832: loss: 0.2821504930:
12: 16032: loss: 0.2818073618:
12: 19232: loss: 0.2826074572:
12: 22432: loss: 0.2827553565:
12: 25632: loss: 0.2823754240:
12: 28832: loss: 0.2825083807:
12: 32032: loss: 0.2821871806:
12: 35232: loss: 0.2828284562:
12: 38432: loss: 0.2822660418:
12: 41632: loss: 0.2819120634:
12: 44832: loss: 0.2820847602:
12: 48032: loss: 0.2816012771:
12: 51232: loss: 0.2806598491:
12: 54432: loss: 0.2805626340:
12: 57632: loss: 0.2804288840:
12: 60832: loss: 0.2807847059:
12: 64032: loss: 0.2806010726:
12: 67232: loss: 0.2797511138:
12: 70432: loss: 0.2783706083:
12: 73632: loss: 0.2786692158:
12: 76832: loss: 0.2792389571:
12: 80032: loss: 0.2794075184:
12: 83232: loss: 0.2796942107:
12: 86432: loss: 0.2794067545:
12: 89632: loss: 0.2790451973:
12: 92832: loss: 0.2785094886:
12: 96032: loss: 0.2788597408:
12: 99232: loss: 0.2791206823:
12: 102432: loss: 0.2789817606:
12: 105632: loss: 0.2788084020:
12: 108832: loss: 0.2788361523:
12: 112032: loss: 0.2782023850:
12: 115232: loss: 0.2782025643:
12: 118432: loss: 0.2785239330:
12: 121632: loss: 0.2783665680:
12: 124832: loss: 0.2782860792:
12: 128032: loss: 0.2782535201:
12: 131232: loss: 0.2781060658:
12: 134432: loss: 0.2779388142:
12: 137632: loss: 0.2779476300:
12: 140832: loss: 0.2781899295:
12: 144032: loss: 0.2779295231:
12: 147232: loss: 0.2778389598:
12: 150432: loss: 0.2776198754:
Dev-Acc: 12: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 12: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
13: 3232: loss: 0.2909319723:
13: 6432: loss: 0.2832344667:
13: 9632: loss: 0.2725583443:
13: 12832: loss: 0.2763910281:
13: 16032: loss: 0.2778423490:
13: 19232: loss: 0.2774065713:
13: 22432: loss: 0.2787256910:
13: 25632: loss: 0.2770683816:
13: 28832: loss: 0.2743068188:
13: 32032: loss: 0.2740117814:
13: 35232: loss: 0.2732500665:
13: 38432: loss: 0.2731575733:
13: 41632: loss: 0.2740110856:
13: 44832: loss: 0.2730424206:
13: 48032: loss: 0.2733571729:
13: 51232: loss: 0.2728871004:
13: 54432: loss: 0.2728543167:
13: 57632: loss: 0.2720750458:
13: 60832: loss: 0.2720237409:
13: 64032: loss: 0.2716620059:
13: 67232: loss: 0.2715882533:
13: 70432: loss: 0.2721655395:
13: 73632: loss: 0.2728692305:
13: 76832: loss: 0.2724331759:
13: 80032: loss: 0.2724994168:
13: 83232: loss: 0.2723037643:
13: 86432: loss: 0.2716219535:
13: 89632: loss: 0.2715420941:
13: 92832: loss: 0.2716445954:
13: 96032: loss: 0.2718004201:
13: 99232: loss: 0.2718710832:
13: 102432: loss: 0.2717380231:
13: 105632: loss: 0.2716678897:
13: 108832: loss: 0.2717803086:
13: 112032: loss: 0.2714744500:
13: 115232: loss: 0.2715163305:
13: 118432: loss: 0.2712046785:
13: 121632: loss: 0.2709387110:
13: 124832: loss: 0.2710109989:
13: 128032: loss: 0.2711268447:
13: 131232: loss: 0.2712587748:
13: 134432: loss: 0.2710896598:
13: 137632: loss: 0.2710006970:
13: 140832: loss: 0.2708458445:
13: 144032: loss: 0.2703629536:
13: 147232: loss: 0.2700486588:
13: 150432: loss: 0.2696808907:
Dev-Acc: 13: Accuracy: 0.9416772127: precision: 0.8000000000: recall: 0.0006801564: f1: 0.0013591573
Train-Acc: 13: Accuracy: 0.9003484249: precision: 1.0000000000: recall: 0.0034843206: f1: 0.0069444444
14: 3232: loss: 0.2697176825:
14: 6432: loss: 0.2739817545:
14: 9632: loss: 0.2705094926:
14: 12832: loss: 0.2704290625:
14: 16032: loss: 0.2664338295:
14: 19232: loss: 0.2677733593:
14: 22432: loss: 0.2670274546:
14: 25632: loss: 0.2667878072:
14: 28832: loss: 0.2667911119:
14: 32032: loss: 0.2680442902:
14: 35232: loss: 0.2673563329:
14: 38432: loss: 0.2672762421:
14: 41632: loss: 0.2669564128:
14: 44832: loss: 0.2670316639:
14: 48032: loss: 0.2660517046:
14: 51232: loss: 0.2670853156:
14: 54432: loss: 0.2671736895:
14: 57632: loss: 0.2657793370:
14: 60832: loss: 0.2665261244:
14: 64032: loss: 0.2664557096:
14: 67232: loss: 0.2661879096:
14: 70432: loss: 0.2654436451:
14: 73632: loss: 0.2659946834:
14: 76832: loss: 0.2650793081:
14: 80032: loss: 0.2650875855:
14: 83232: loss: 0.2656816511:
14: 86432: loss: 0.2656412710:
14: 89632: loss: 0.2653627500:
14: 92832: loss: 0.2654077407:
14: 96032: loss: 0.2655167488:
14: 99232: loss: 0.2658105388:
14: 102432: loss: 0.2649934244:
14: 105632: loss: 0.2646921381:
14: 108832: loss: 0.2648121517:
14: 112032: loss: 0.2647695847:
14: 115232: loss: 0.2648145519:
14: 118432: loss: 0.2643823522:
14: 121632: loss: 0.2641784138:
14: 124832: loss: 0.2637773205:
14: 128032: loss: 0.2636116885:
14: 131232: loss: 0.2633464267:
14: 134432: loss: 0.2634513824:
14: 137632: loss: 0.2634355675:
14: 140832: loss: 0.2631667561:
14: 144032: loss: 0.2629625356:
14: 147232: loss: 0.2625727073:
14: 150432: loss: 0.2623206675:
Dev-Acc: 14: Accuracy: 0.9418558478: precision: 0.6567164179: recall: 0.0074817208: f1: 0.0147948890
Train-Acc: 14: Accuracy: 0.9020182490: precision: 0.8139059305: recall: 0.0261652751: f1: 0.0507006369
15: 3232: loss: 0.2594810870:
15: 6432: loss: 0.2510575066:
15: 9632: loss: 0.2530396612:
15: 12832: loss: 0.2552531694:
15: 16032: loss: 0.2545790986:
15: 19232: loss: 0.2544009738:
15: 22432: loss: 0.2561039498:
15: 25632: loss: 0.2551279851:
15: 28832: loss: 0.2575299777:
15: 32032: loss: 0.2582708917:
15: 35232: loss: 0.2596429226:
15: 38432: loss: 0.2586580187:
15: 41632: loss: 0.2583548555:
15: 44832: loss: 0.2589657326:
15: 48032: loss: 0.2589357654:
15: 51232: loss: 0.2579927937:
15: 54432: loss: 0.2572903228:
15: 57632: loss: 0.2578773790:
15: 60832: loss: 0.2579754520:
15: 64032: loss: 0.2575893012:
15: 67232: loss: 0.2580820408:
15: 70432: loss: 0.2580382390:
15: 73632: loss: 0.2590609183:
15: 76832: loss: 0.2592365622:
15: 80032: loss: 0.2588956299:
15: 83232: loss: 0.2591817931:
15: 86432: loss: 0.2586780882:
15: 89632: loss: 0.2588391223:
15: 92832: loss: 0.2589744520:
15: 96032: loss: 0.2591239099:
15: 99232: loss: 0.2589316152:
15: 102432: loss: 0.2585411014:
15: 105632: loss: 0.2581902920:
15: 108832: loss: 0.2582112697:
15: 112032: loss: 0.2582262077:
15: 115232: loss: 0.2583135128:
15: 118432: loss: 0.2580276846:
15: 121632: loss: 0.2578004889:
15: 124832: loss: 0.2573052674:
15: 128032: loss: 0.2569088003:
15: 131232: loss: 0.2570705500:
15: 134432: loss: 0.2569800381:
15: 137632: loss: 0.2567535461:
15: 140832: loss: 0.2564772693:
15: 144032: loss: 0.2566966482:
15: 147232: loss: 0.2565856385:
15: 150432: loss: 0.2565672802:
Dev-Acc: 15: Accuracy: 0.9420741200: precision: 0.7009345794: recall: 0.0127529332: f1: 0.0250501002
Train-Acc: 15: Accuracy: 0.9028203487: precision: 0.8206278027: recall: 0.0360923016: f1: 0.0691435768
16: 3232: loss: 0.2523232310:
16: 6432: loss: 0.2522386058:
16: 9632: loss: 0.2486077096:
16: 12832: loss: 0.2491137001:
16: 16032: loss: 0.2490023393:
16: 19232: loss: 0.2479461659:
16: 22432: loss: 0.2492817701:
16: 25632: loss: 0.2484754295:
16: 28832: loss: 0.2507528292:
16: 32032: loss: 0.2513178902:
16: 35232: loss: 0.2505156306:
16: 38432: loss: 0.2513670192:
16: 41632: loss: 0.2521956561:
16: 44832: loss: 0.2525751979:
16: 48032: loss: 0.2517845232:
16: 51232: loss: 0.2514789244:
16: 54432: loss: 0.2510242340:
16: 57632: loss: 0.2506247085:
16: 60832: loss: 0.2499596029:
16: 64032: loss: 0.2508140456:
16: 67232: loss: 0.2507086190:
16: 70432: loss: 0.2508920058:
16: 73632: loss: 0.2516105115:
16: 76832: loss: 0.2514538363:
16: 80032: loss: 0.2516343612:
16: 83232: loss: 0.2519279603:
16: 86432: loss: 0.2521578342:
16: 89632: loss: 0.2528449661:
16: 92832: loss: 0.2526993234:
16: 96032: loss: 0.2526160915:
16: 99232: loss: 0.2522835991:
16: 102432: loss: 0.2519301801:
16: 105632: loss: 0.2519105758:
16: 108832: loss: 0.2519718049:
16: 112032: loss: 0.2517259055:
16: 115232: loss: 0.2515460238:
16: 118432: loss: 0.2515918528:
16: 121632: loss: 0.2514174878:
16: 124832: loss: 0.2514412645:
16: 128032: loss: 0.2514906759:
16: 131232: loss: 0.2509563351:
16: 134432: loss: 0.2509592686:
16: 137632: loss: 0.2508930373:
16: 140832: loss: 0.2504399951:
16: 144032: loss: 0.2505601649:
16: 147232: loss: 0.2506794846:
16: 150432: loss: 0.2506071977:
Dev-Acc: 16: Accuracy: 0.9422923923: precision: 0.5970149254: recall: 0.0340078218: f1: 0.0643500644
Train-Acc: 16: Accuracy: 0.9045953751: precision: 0.8569969356: recall: 0.0551574518: f1: 0.1036442248
17: 3232: loss: 0.2518353560:
17: 6432: loss: 0.2524212846:
17: 9632: loss: 0.2508920569:
17: 12832: loss: 0.2484515785:
17: 16032: loss: 0.2499036130:
17: 19232: loss: 0.2487451746:
17: 22432: loss: 0.2496983988:
17: 25632: loss: 0.2479685703:
17: 28832: loss: 0.2471754253:
17: 32032: loss: 0.2484202965:
17: 35232: loss: 0.2468294185:
17: 38432: loss: 0.2457813538:
17: 41632: loss: 0.2461675572:
17: 44832: loss: 0.2455284523:
17: 48032: loss: 0.2454770509:
17: 51232: loss: 0.2462221212:
17: 54432: loss: 0.2468618605:
17: 57632: loss: 0.2474507519:
17: 60832: loss: 0.2478288778:
17: 64032: loss: 0.2482047310:
17: 67232: loss: 0.2484852635:
17: 70432: loss: 0.2481756819:
17: 73632: loss: 0.2476629306:
17: 76832: loss: 0.2475460264:
17: 80032: loss: 0.2461989290:
17: 83232: loss: 0.2464599544:
17: 86432: loss: 0.2467847385:
17: 89632: loss: 0.2474100672:
17: 92832: loss: 0.2470671075:
17: 96032: loss: 0.2473674220:
17: 99232: loss: 0.2479342819:
17: 102432: loss: 0.2481103216:
17: 105632: loss: 0.2480761395:
17: 108832: loss: 0.2477330156:
17: 112032: loss: 0.2479787717:
17: 115232: loss: 0.2477434036:
17: 118432: loss: 0.2474929478:
17: 121632: loss: 0.2471311101:
17: 124832: loss: 0.2470343641:
17: 128032: loss: 0.2469932622:
17: 131232: loss: 0.2469379872:
17: 134432: loss: 0.2468033833:
17: 137632: loss: 0.2468747073:
17: 140832: loss: 0.2468230925:
17: 144032: loss: 0.2466577175:
17: 147232: loss: 0.2465293279:
17: 150432: loss: 0.2461082627:
Dev-Acc: 17: Accuracy: 0.9419848323: precision: 0.5371179039: recall: 0.0418296208: f1: 0.0776147657
Train-Acc: 17: Accuracy: 0.9059101939: precision: 0.8514464425: recall: 0.0715929262: f1: 0.1320800485
18: 3232: loss: 0.2556764055:
18: 6432: loss: 0.2430247768:
18: 9632: loss: 0.2396186005:
18: 12832: loss: 0.2352557714:
18: 16032: loss: 0.2401929872:
18: 19232: loss: 0.2381765901:
18: 22432: loss: 0.2400578482:
18: 25632: loss: 0.2405303601:
18: 28832: loss: 0.2403014672:
18: 32032: loss: 0.2411397774:
18: 35232: loss: 0.2412607509:
18: 38432: loss: 0.2407487559:
18: 41632: loss: 0.2413086864:
18: 44832: loss: 0.2412965959:
18: 48032: loss: 0.2416851109:
18: 51232: loss: 0.2411335787:
18: 54432: loss: 0.2418917503:
18: 57632: loss: 0.2417414208:
18: 60832: loss: 0.2423974004:
18: 64032: loss: 0.2417145046:
18: 67232: loss: 0.2410927887:
18: 70432: loss: 0.2419087831:
18: 73632: loss: 0.2418711420:
18: 76832: loss: 0.2421712684:
18: 80032: loss: 0.2419380983:
18: 83232: loss: 0.2421546735:
18: 86432: loss: 0.2426706665:
18: 89632: loss: 0.2424697876:
18: 92832: loss: 0.2429126461:
18: 96032: loss: 0.2431033647:
18: 99232: loss: 0.2428401479:
18: 102432: loss: 0.2427251213:
18: 105632: loss: 0.2422817656:
18: 108832: loss: 0.2423992179:
18: 112032: loss: 0.2420694744:
18: 115232: loss: 0.2416635099:
18: 118432: loss: 0.2418145469:
18: 121632: loss: 0.2419424525:
18: 124832: loss: 0.2419931901:
18: 128032: loss: 0.2418233935:
18: 131232: loss: 0.2418308215:
18: 134432: loss: 0.2415402450:
18: 137632: loss: 0.2413283495:
18: 140832: loss: 0.2409586606:
18: 144032: loss: 0.2410324361:
18: 147232: loss: 0.2413944574:
18: 150432: loss: 0.2415013423:
Dev-Acc: 18: Accuracy: 0.9430862069: precision: 0.5898389095: recall: 0.0809386159: f1: 0.1423444976
Train-Acc: 18: Accuracy: 0.9087173939: precision: 0.8583783784: recall: 0.1043981329: f1: 0.1861555595
19: 3232: loss: 0.2443658295:
19: 6432: loss: 0.2379428275:
19: 9632: loss: 0.2365239518:
19: 12832: loss: 0.2377484675:
19: 16032: loss: 0.2418329543:
19: 19232: loss: 0.2419608575:
19: 22432: loss: 0.2413305771:
19: 25632: loss: 0.2424351204:
19: 28832: loss: 0.2413084421:
19: 32032: loss: 0.2414234180:
19: 35232: loss: 0.2401091357:
19: 38432: loss: 0.2388867005:
19: 41632: loss: 0.2385704071:
19: 44832: loss: 0.2389484771:
19: 48032: loss: 0.2388330087:
19: 51232: loss: 0.2374571939:
19: 54432: loss: 0.2371117725:
19: 57632: loss: 0.2365431027:
19: 60832: loss: 0.2359350691:
19: 64032: loss: 0.2361636182:
19: 67232: loss: 0.2363567098:
19: 70432: loss: 0.2366809310:
19: 73632: loss: 0.2368930860:
19: 76832: loss: 0.2365998347:
19: 80032: loss: 0.2358980570:
19: 83232: loss: 0.2355716429:
19: 86432: loss: 0.2350489998:
19: 89632: loss: 0.2359366425:
19: 92832: loss: 0.2361904230:
19: 96032: loss: 0.2366994740:
19: 99232: loss: 0.2370055208:
19: 102432: loss: 0.2370133404:
19: 105632: loss: 0.2373117386:
19: 108832: loss: 0.2372716539:
19: 112032: loss: 0.2370222655:
19: 115232: loss: 0.2367729879:
19: 118432: loss: 0.2370251445:
19: 121632: loss: 0.2371466317:
19: 124832: loss: 0.2371660220:
19: 128032: loss: 0.2373325485:
19: 131232: loss: 0.2369716098:
19: 134432: loss: 0.2369426646:
19: 137632: loss: 0.2370964875:
19: 140832: loss: 0.2371597150:
19: 144032: loss: 0.2372268943:
19: 147232: loss: 0.2375020901:
19: 150432: loss: 0.2374059909:
Dev-Acc: 19: Accuracy: 0.9430762529: precision: 0.5637168142: recall: 0.1083149124: f1: 0.1817144487
Train-Acc: 19: Accuracy: 0.9115048051: precision: 0.8574346405: recall: 0.1379922425: f1: 0.2377258055
20: 3232: loss: 0.2360446853:
20: 6432: loss: 0.2356314659:
20: 9632: loss: 0.2317123758:
20: 12832: loss: 0.2334427912:
20: 16032: loss: 0.2327332486:
20: 19232: loss: 0.2340714235:
20: 22432: loss: 0.2335612131:
20: 25632: loss: 0.2323141309:
20: 28832: loss: 0.2327255907:
20: 32032: loss: 0.2329321877:
20: 35232: loss: 0.2331036033:
20: 38432: loss: 0.2328951588:
20: 41632: loss: 0.2339588816:
20: 44832: loss: 0.2326705435:
20: 48032: loss: 0.2336924844:
20: 51232: loss: 0.2340369934:
20: 54432: loss: 0.2344456003:
20: 57632: loss: 0.2350512889:
20: 60832: loss: 0.2347388039:
20: 64032: loss: 0.2345511156:
20: 67232: loss: 0.2342578355:
20: 70432: loss: 0.2352101915:
20: 73632: loss: 0.2355708827:
20: 76832: loss: 0.2353801104:
20: 80032: loss: 0.2351529284:
20: 83232: loss: 0.2350463314:
20: 86432: loss: 0.2345985378:
20: 89632: loss: 0.2348766422:
20: 92832: loss: 0.2348452570:
20: 96032: loss: 0.2346791057:
20: 99232: loss: 0.2343021296:
20: 102432: loss: 0.2343178464:
20: 105632: loss: 0.2348689141:
20: 108832: loss: 0.2350326315:
20: 112032: loss: 0.2344650692:
20: 115232: loss: 0.2343818508:
20: 118432: loss: 0.2338998561:
20: 121632: loss: 0.2335588371:
20: 124832: loss: 0.2334396631:
20: 128032: loss: 0.2332142760:
20: 131232: loss: 0.2331228605:
20: 134432: loss: 0.2331898331:
20: 137632: loss: 0.2332851036:
20: 140832: loss: 0.2327303476:
20: 144032: loss: 0.2325601405:
20: 147232: loss: 0.2329575371:
20: 150432: loss: 0.2329301085:
Dev-Acc: 20: Accuracy: 0.9433640242: precision: 0.5657794677: recall: 0.1265090971: f1: 0.2067815453
Train-Acc: 20: Accuracy: 0.9138189554: precision: 0.8401294498: recall: 0.1706659654: f1: 0.2837003442
21: 3232: loss: 0.2337836141:
21: 6432: loss: 0.2386746920:
21: 9632: loss: 0.2377593328:
21: 12832: loss: 0.2353358879:
21: 16032: loss: 0.2343451087:
21: 19232: loss: 0.2340132290:
21: 22432: loss: 0.2328468905:
21: 25632: loss: 0.2317755451:
21: 28832: loss: 0.2321173285:
21: 32032: loss: 0.2324474341:
21: 35232: loss: 0.2315308961:
21: 38432: loss: 0.2320670552:
21: 41632: loss: 0.2314572785:
21: 44832: loss: 0.2315393372:
21: 48032: loss: 0.2321349969:
21: 51232: loss: 0.2322809165:
21: 54432: loss: 0.2321895291:
21: 57632: loss: 0.2321971187:
21: 60832: loss: 0.2327853469:
21: 64032: loss: 0.2323477011:
21: 67232: loss: 0.2318420617:
21: 70432: loss: 0.2311274967:
21: 73632: loss: 0.2310699847:
21: 76832: loss: 0.2304490245:
21: 80032: loss: 0.2303516796:
21: 83232: loss: 0.2304939047:
21: 86432: loss: 0.2310108997:
21: 89632: loss: 0.2308060297:
21: 92832: loss: 0.2305352532:
21: 96032: loss: 0.2304385457:
21: 99232: loss: 0.2299283235:
21: 102432: loss: 0.2295643555:
21: 105632: loss: 0.2298401391:
21: 108832: loss: 0.2299286334:
21: 112032: loss: 0.2297865793:
21: 115232: loss: 0.2301007900:
21: 118432: loss: 0.2302970948:
21: 121632: loss: 0.2304719588:
21: 124832: loss: 0.2304591704:
21: 128032: loss: 0.2302266078:
21: 131232: loss: 0.2299141292:
21: 134432: loss: 0.2298402388:
21: 137632: loss: 0.2294847039:
21: 140832: loss: 0.2296586214:
21: 144032: loss: 0.2293896760:
21: 147232: loss: 0.2295363217:
21: 150432: loss: 0.2298602086:
Dev-Acc: 21: Accuracy: 0.9437906742: precision: 0.5661764706: recall: 0.1571161367: f1: 0.2459736457
Train-Acc: 21: Accuracy: 0.9158700705: precision: 0.8136694387: recall: 0.2058378805: f1: 0.3285586862
22: 3232: loss: 0.2233457481:
22: 6432: loss: 0.2216406908:
22: 9632: loss: 0.2236730734:
22: 12832: loss: 0.2209929599:
22: 16032: loss: 0.2257479863:
22: 19232: loss: 0.2253401735:
22: 22432: loss: 0.2260293515:
22: 25632: loss: 0.2248705652:
22: 28832: loss: 0.2264388156:
22: 32032: loss: 0.2264923263:
22: 35232: loss: 0.2267475805:
22: 38432: loss: 0.2270101828:
22: 41632: loss: 0.2273609858:
22: 44832: loss: 0.2280445918:
22: 48032: loss: 0.2274135853:
22: 51232: loss: 0.2270254821:
22: 54432: loss: 0.2273301298:
22: 57632: loss: 0.2269227043:
22: 60832: loss: 0.2273889739:
22: 64032: loss: 0.2273895854:
22: 67232: loss: 0.2268854530:
22: 70432: loss: 0.2268914964:
22: 73632: loss: 0.2264772987:
22: 76832: loss: 0.2261774592:
22: 80032: loss: 0.2261116705:
22: 83232: loss: 0.2254534208:
22: 86432: loss: 0.2258206177:
22: 89632: loss: 0.2257488836:
22: 92832: loss: 0.2253996099:
22: 96032: loss: 0.2256235307:
22: 99232: loss: 0.2260236559:
22: 102432: loss: 0.2261524145:
22: 105632: loss: 0.2264324827:
22: 108832: loss: 0.2257365713:
22: 112032: loss: 0.2258744367:
22: 115232: loss: 0.2253422073:
22: 118432: loss: 0.2254472377:
22: 121632: loss: 0.2260519858:
22: 124832: loss: 0.2256653425:
22: 128032: loss: 0.2257105148:
22: 131232: loss: 0.2257676648:
22: 134432: loss: 0.2255672001:
22: 137632: loss: 0.2262363127:
22: 140832: loss: 0.2262500564:
22: 144032: loss: 0.2262785291:
22: 147232: loss: 0.2263548607:
22: 150432: loss: 0.2263491705:
Dev-Acc: 22: Accuracy: 0.9424908757: precision: 0.5189816883: recall: 0.1975854447: f1: 0.2862068966
Train-Acc: 22: Accuracy: 0.9171192050: precision: 0.7937725632: recall: 0.2312799947: f1: 0.3581937586
23: 3232: loss: 0.2231176108:
23: 6432: loss: 0.2199385388:
23: 9632: loss: 0.2252349412:
23: 12832: loss: 0.2236154380:
23: 16032: loss: 0.2302138073:
23: 19232: loss: 0.2276900165:
23: 22432: loss: 0.2263576574:
23: 25632: loss: 0.2273506556:
23: 28832: loss: 0.2267418255:
23: 32032: loss: 0.2263929635:
23: 35232: loss: 0.2256034372:
23: 38432: loss: 0.2258061648:
23: 41632: loss: 0.2248737934:
23: 44832: loss: 0.2234791886:
23: 48032: loss: 0.2240987027:
23: 51232: loss: 0.2232433027:
23: 54432: loss: 0.2237095271:
23: 57632: loss: 0.2235094069:
23: 60832: loss: 0.2238203325:
23: 64032: loss: 0.2240185017:
23: 67232: loss: 0.2235598285:
23: 70432: loss: 0.2238681622:
23: 73632: loss: 0.2240478102:
23: 76832: loss: 0.2243466324:
23: 80032: loss: 0.2241726258:
23: 83232: loss: 0.2243991597:
23: 86432: loss: 0.2242306523:
23: 89632: loss: 0.2247666908:
23: 92832: loss: 0.2245192498:
23: 96032: loss: 0.2247039139:
23: 99232: loss: 0.2240453251:
23: 102432: loss: 0.2239375694:
23: 105632: loss: 0.2244758356:
23: 108832: loss: 0.2244360286:
23: 112032: loss: 0.2242068469:
23: 115232: loss: 0.2246162039:
23: 118432: loss: 0.2250067027:
23: 121632: loss: 0.2247898186:
23: 124832: loss: 0.2244508538:
23: 128032: loss: 0.2240698494:
23: 131232: loss: 0.2239555876:
23: 134432: loss: 0.2239748887:
23: 137632: loss: 0.2241041504:
23: 140832: loss: 0.2236702519:
23: 144032: loss: 0.2234630058:
23: 147232: loss: 0.2235651078:
23: 150432: loss: 0.2235186493:
Dev-Acc: 23: Accuracy: 0.9412902594: precision: 0.4939106901: recall: 0.2482570991: f1: 0.3304288786
Train-Acc: 23: Accuracy: 0.9187890291: precision: 0.7903291345: recall: 0.2557359805: f1: 0.3864302389
24: 3232: loss: 0.2242110323:
24: 6432: loss: 0.2233418079:
24: 9632: loss: 0.2281669070:
24: 12832: loss: 0.2255444998:
24: 16032: loss: 0.2240412159:
24: 19232: loss: 0.2215679478:
24: 22432: loss: 0.2193335355:
24: 25632: loss: 0.2195827497:
24: 28832: loss: 0.2191604240:
24: 32032: loss: 0.2188391445:
24: 35232: loss: 0.2185951378:
24: 38432: loss: 0.2194796974:
24: 41632: loss: 0.2189127701:
24: 44832: loss: 0.2198641649:
24: 48032: loss: 0.2193833806:
24: 51232: loss: 0.2207105413:
24: 54432: loss: 0.2217364879:
24: 57632: loss: 0.2213516215:
24: 60832: loss: 0.2216293012:
24: 64032: loss: 0.2210138405:
24: 67232: loss: 0.2207317675:
24: 70432: loss: 0.2219301272:
24: 73632: loss: 0.2218224193:
24: 76832: loss: 0.2214307814:
24: 80032: loss: 0.2219861011:
24: 83232: loss: 0.2216401883:
24: 86432: loss: 0.2218796335:
24: 89632: loss: 0.2217614519:
24: 92832: loss: 0.2216163445:
24: 96032: loss: 0.2209154370:
24: 99232: loss: 0.2210213527:
24: 102432: loss: 0.2210414763:
24: 105632: loss: 0.2214117919:
24: 108832: loss: 0.2209481612:
24: 112032: loss: 0.2205376625:
24: 115232: loss: 0.2202537985:
24: 118432: loss: 0.2205500625:
24: 121632: loss: 0.2205227857:
24: 124832: loss: 0.2205503272:
24: 128032: loss: 0.2207321854:
24: 131232: loss: 0.2207348402:
24: 134432: loss: 0.2206116707:
24: 137632: loss: 0.2206932491:
24: 140832: loss: 0.2204781264:
24: 144032: loss: 0.2204289913:
24: 147232: loss: 0.2205785492:
24: 150432: loss: 0.2204880115:
Dev-Acc: 24: Accuracy: 0.9407941699: precision: 0.4870793269: recall: 0.2756333957: f1: 0.3520469106
Train-Acc: 24: Accuracy: 0.9202287793: precision: 0.7939996178: recall: 0.2731575833: f1: 0.4064762277
25: 3232: loss: 0.2158234953:
25: 6432: loss: 0.2122583912:
25: 9632: loss: 0.2181054443:
25: 12832: loss: 0.2147078731:
25: 16032: loss: 0.2176981762:
25: 19232: loss: 0.2180585091:
25: 22432: loss: 0.2191713631:
25: 25632: loss: 0.2177482700:
25: 28832: loss: 0.2169236861:
25: 32032: loss: 0.2186927671:
25: 35232: loss: 0.2185297744:
25: 38432: loss: 0.2173117169:
25: 41632: loss: 0.2187347519:
25: 44832: loss: 0.2184358169:
25: 48032: loss: 0.2187273146:
25: 51232: loss: 0.2198895349:
25: 54432: loss: 0.2199552529:
25: 57632: loss: 0.2198299324:
25: 60832: loss: 0.2204758975:
25: 64032: loss: 0.2197384165:
25: 67232: loss: 0.2190302542:
25: 70432: loss: 0.2190783084:
25: 73632: loss: 0.2185276301:
25: 76832: loss: 0.2178730273:
25: 80032: loss: 0.2181470838:
25: 83232: loss: 0.2179533779:
25: 86432: loss: 0.2179606774:
25: 89632: loss: 0.2183056386:
25: 92832: loss: 0.2185480984:
25: 96032: loss: 0.2180014215:
25: 99232: loss: 0.2178095247:
25: 102432: loss: 0.2177958863:
25: 105632: loss: 0.2175186621:
25: 108832: loss: 0.2174932314:
25: 112032: loss: 0.2175513944:
25: 115232: loss: 0.2174545041:
25: 118432: loss: 0.2175907374:
25: 121632: loss: 0.2172526976:
25: 124832: loss: 0.2173402498:
25: 128032: loss: 0.2169712286:
25: 131232: loss: 0.2169791645:
25: 134432: loss: 0.2173947503:
25: 137632: loss: 0.2173625842:
25: 140832: loss: 0.2173384915:
25: 144032: loss: 0.2172620758:
25: 147232: loss: 0.2174762663:
25: 150432: loss: 0.2174187041:
Dev-Acc: 25: Accuracy: 0.9400698543: precision: 0.4777746715: recall: 0.2905968373: f1: 0.3613871855
Train-Acc: 25: Accuracy: 0.9211885929: precision: 0.7940156906: recall: 0.2861087371: f1: 0.4206456602
26: 3232: loss: 0.2086712680:
26: 6432: loss: 0.2150471903:
26: 9632: loss: 0.2180584651:
26: 12832: loss: 0.2152525196:
26: 16032: loss: 0.2166624555:
26: 19232: loss: 0.2145291297:
26: 22432: loss: 0.2142554290:
26: 25632: loss: 0.2163493359:
26: 28832: loss: 0.2173026544:
26: 32032: loss: 0.2175764432:
26: 35232: loss: 0.2176686133:
26: 38432: loss: 0.2179480318:
26: 41632: loss: 0.2176609167:
26: 44832: loss: 0.2175152198:
26: 48032: loss: 0.2170494000:
26: 51232: loss: 0.2168023069:
26: 54432: loss: 0.2165450319:
26: 57632: loss: 0.2161644618:
26: 60832: loss: 0.2159472059:
26: 64032: loss: 0.2161273125:
26: 67232: loss: 0.2162568253:
26: 70432: loss: 0.2160667723:
26: 73632: loss: 0.2164411424:
26: 76832: loss: 0.2161252543:
26: 80032: loss: 0.2161365920:
26: 83232: loss: 0.2161044855:
26: 86432: loss: 0.2159593265:
26: 89632: loss: 0.2161232158:
26: 92832: loss: 0.2159400788:
26: 96032: loss: 0.2156740826:
26: 99232: loss: 0.2161063627:
26: 102432: loss: 0.2162052073:
26: 105632: loss: 0.2162045725:
26: 108832: loss: 0.2162080174:
26: 112032: loss: 0.2161083443:
26: 115232: loss: 0.2161966057:
26: 118432: loss: 0.2160314512:
26: 121632: loss: 0.2162834477:
26: 124832: loss: 0.2162405094:
26: 128032: loss: 0.2162157468:
26: 131232: loss: 0.2160164943:
26: 134432: loss: 0.2157387373:
26: 137632: loss: 0.2156729553:
26: 140832: loss: 0.2155119641:
26: 144032: loss: 0.2155524805:
26: 147232: loss: 0.2155804711:
26: 150432: loss: 0.2150680300:
Dev-Acc: 26: Accuracy: 0.9396927953: precision: 0.4737263270: recall: 0.3019894576: f1: 0.3688473520
Train-Acc: 26: Accuracy: 0.9220038056: precision: 0.7909928708: recall: 0.2990598909: f1: 0.4340234710
27: 3232: loss: 0.2190107996:
27: 6432: loss: 0.2299556990:
27: 9632: loss: 0.2217115014:
27: 12832: loss: 0.2153061342:
27: 16032: loss: 0.2156175889:
27: 19232: loss: 0.2144947644:
27: 22432: loss: 0.2155848773:
27: 25632: loss: 0.2128982994:
27: 28832: loss: 0.2118986158:
27: 32032: loss: 0.2128378049:
27: 35232: loss: 0.2125495328:
27: 38432: loss: 0.2127155190:
27: 41632: loss: 0.2126225026:
27: 44832: loss: 0.2137857133:
27: 48032: loss: 0.2138486081:
27: 51232: loss: 0.2138138443:
27: 54432: loss: 0.2138829837:
27: 57632: loss: 0.2140105922:
27: 60832: loss: 0.2148112693:
27: 64032: loss: 0.2143596865:
27: 67232: loss: 0.2141688850:
27: 70432: loss: 0.2142224101:
27: 73632: loss: 0.2137448968:
27: 76832: loss: 0.2131800790:
27: 80032: loss: 0.2133194421:
27: 83232: loss: 0.2136944269:
27: 86432: loss: 0.2137956553:
27: 89632: loss: 0.2136534060:
27: 92832: loss: 0.2135475444:
27: 96032: loss: 0.2131453826:
27: 99232: loss: 0.2130491206:
27: 102432: loss: 0.2132170576:
27: 105632: loss: 0.2131926496:
27: 108832: loss: 0.2127402703:
27: 112032: loss: 0.2128206473:
27: 115232: loss: 0.2131717168:
27: 118432: loss: 0.2127638154:
27: 121632: loss: 0.2125350221:
27: 124832: loss: 0.2123565526:
27: 128032: loss: 0.2126251824:
27: 131232: loss: 0.2126838904:
27: 134432: loss: 0.2126409561:
27: 137632: loss: 0.2125981409:
27: 140832: loss: 0.2127682824:
27: 144032: loss: 0.2127752766:
27: 147232: loss: 0.2127903225:
27: 150432: loss: 0.2126571329:
Dev-Acc: 27: Accuracy: 0.9393058419: precision: 0.4696189495: recall: 0.3101513348: f1: 0.3735791091
Train-Acc: 27: Accuracy: 0.9229702353: precision: 0.7919451872: recall: 0.3115508514: f1: 0.4471809389
28: 3232: loss: 0.2064735034:
28: 6432: loss: 0.2180269147:
28: 9632: loss: 0.2200338703:
28: 12832: loss: 0.2156198309:
28: 16032: loss: 0.2162779139:
28: 19232: loss: 0.2143467267:
28: 22432: loss: 0.2133169538:
28: 25632: loss: 0.2123551139:
28: 28832: loss: 0.2117689268:
28: 32032: loss: 0.2125247502:
28: 35232: loss: 0.2129194674:
28: 38432: loss: 0.2126418687:
28: 41632: loss: 0.2125212239:
28: 44832: loss: 0.2118499495:
28: 48032: loss: 0.2121324815:
28: 51232: loss: 0.2126625513:
28: 54432: loss: 0.2124654179:
28: 57632: loss: 0.2127035635:
28: 60832: loss: 0.2123388817:
28: 64032: loss: 0.2120949527:
28: 67232: loss: 0.2124222189:
28: 70432: loss: 0.2119275703:
28: 73632: loss: 0.2116029493:
28: 76832: loss: 0.2111547370:
28: 80032: loss: 0.2106341414:
28: 83232: loss: 0.2109832265:
28: 86432: loss: 0.2106226324:
28: 89632: loss: 0.2109510960:
28: 92832: loss: 0.2104878500:
28: 96032: loss: 0.2104836279:
28: 99232: loss: 0.2106823950:
28: 102432: loss: 0.2105669724:
28: 105632: loss: 0.2103569293:
28: 108832: loss: 0.2104140132:
28: 112032: loss: 0.2104814132:
28: 115232: loss: 0.2104367836:
28: 118432: loss: 0.2105882878:
28: 121632: loss: 0.2102115101:
28: 124832: loss: 0.2104358415:
28: 128032: loss: 0.2106889527:
28: 131232: loss: 0.2107613874:
28: 134432: loss: 0.2109251511:
28: 137632: loss: 0.2107248172:
28: 140832: loss: 0.2107471404:
28: 144032: loss: 0.2108113103:
28: 147232: loss: 0.2106188079:
28: 150432: loss: 0.2104235755:
Dev-Acc: 28: Accuracy: 0.9390478730: precision: 0.4670688788: recall: 0.3159326645: f1: 0.3769144944
Train-Acc: 28: Accuracy: 0.9236999750: precision: 0.7894652321: recall: 0.3231871672: f1: 0.4586248717
29: 3232: loss: 0.2097356471:
29: 6432: loss: 0.2135244634:
29: 9632: loss: 0.2079803140:
29: 12832: loss: 0.2082102360:
29: 16032: loss: 0.2085751809:
29: 19232: loss: 0.2100642058:
29: 22432: loss: 0.2107831118:
29: 25632: loss: 0.2103365746:
29: 28832: loss: 0.2086349062:
29: 32032: loss: 0.2084519695:
29: 35232: loss: 0.2091938534:
29: 38432: loss: 0.2102823147:
29: 41632: loss: 0.2108922906:
29: 44832: loss: 0.2110930976:
29: 48032: loss: 0.2112188510:
29: 51232: loss: 0.2102942513:
29: 54432: loss: 0.2107021656:
29: 57632: loss: 0.2100971997:
29: 60832: loss: 0.2099324295:
29: 64032: loss: 0.2100322874:
29: 67232: loss: 0.2098607016:
29: 70432: loss: 0.2102945506:
29: 73632: loss: 0.2100535036:
29: 76832: loss: 0.2100466449:
29: 80032: loss: 0.2098129580:
29: 83232: loss: 0.2101650926:
29: 86432: loss: 0.2100009119:
29: 89632: loss: 0.2099096725:
29: 92832: loss: 0.2097490841:
29: 96032: loss: 0.2094794202:
29: 99232: loss: 0.2096893846:
29: 102432: loss: 0.2095353414:
29: 105632: loss: 0.2094520089:
29: 108832: loss: 0.2100794129:
29: 112032: loss: 0.2103241122:
29: 115232: loss: 0.2103853670:
29: 118432: loss: 0.2106628211:
29: 121632: loss: 0.2107470443:
29: 124832: loss: 0.2104530753:
29: 128032: loss: 0.2100937024:
29: 131232: loss: 0.2100405040:
29: 134432: loss: 0.2099511380:
29: 137632: loss: 0.2097066475:
29: 140832: loss: 0.2095609578:
29: 144032: loss: 0.2089374507:
29: 147232: loss: 0.2087882439:
29: 150432: loss: 0.2086842121:
Dev-Acc: 29: Accuracy: 0.9389287829: precision: 0.4664215686: recall: 0.3235844244: f1: 0.3820901516
Train-Acc: 29: Accuracy: 0.9244296551: precision: 0.7882407695: recall: 0.3340345802: f1: 0.4692247310
30: 3232: loss: 0.2013839352:
30: 6432: loss: 0.2019742136:
30: 9632: loss: 0.2075294150:
30: 12832: loss: 0.2066527970:
30: 16032: loss: 0.2068091050:
30: 19232: loss: 0.2047374053:
30: 22432: loss: 0.2040447184:
30: 25632: loss: 0.2046436905:
30: 28832: loss: 0.2043053527:
30: 32032: loss: 0.2053517842:
30: 35232: loss: 0.2064296765:
30: 38432: loss: 0.2065223380:
30: 41632: loss: 0.2074799260:
30: 44832: loss: 0.2076078515:
30: 48032: loss: 0.2084067556:
30: 51232: loss: 0.2077228969:
30: 54432: loss: 0.2071676639:
30: 57632: loss: 0.2071821950:
30: 60832: loss: 0.2062925395:
30: 64032: loss: 0.2066747976:
30: 67232: loss: 0.2063672691:
30: 70432: loss: 0.2060203730:
30: 73632: loss: 0.2058348830:
30: 76832: loss: 0.2061369124:
30: 80032: loss: 0.2061373020:
30: 83232: loss: 0.2063735700:
30: 86432: loss: 0.2065591545:
30: 89632: loss: 0.2065088698:
30: 92832: loss: 0.2064411494:
30: 96032: loss: 0.2063290682:
30: 99232: loss: 0.2064696341:
30: 102432: loss: 0.2066713753:
30: 105632: loss: 0.2069582799:
30: 108832: loss: 0.2073315443:
30: 112032: loss: 0.2078496016:
30: 115232: loss: 0.2074376705:
30: 118432: loss: 0.2074551560:
30: 121632: loss: 0.2073047197:
30: 124832: loss: 0.2072026240:
30: 128032: loss: 0.2070718256:
30: 131232: loss: 0.2067447595:
30: 134432: loss: 0.2067734025:
30: 137632: loss: 0.2069590142:
30: 140832: loss: 0.2069671557:
30: 144032: loss: 0.2069726614:
30: 147232: loss: 0.2068689892:
30: 150432: loss: 0.2067823180:
Dev-Acc: 30: Accuracy: 0.9385616779: precision: 0.4628257232: recall: 0.3291957150: f1: 0.3847376789
Train-Acc: 30: Accuracy: 0.9251593947: precision: 0.7870106495: recall: 0.3449477352: f1: 0.4796599324
31: 3232: loss: 0.1983412641:
31: 6432: loss: 0.2013952271:
31: 9632: loss: 0.2044347205:
31: 12832: loss: 0.2059273146:
31: 16032: loss: 0.2055846938:
31: 19232: loss: 0.2079555204:
31: 22432: loss: 0.2060195100:
31: 25632: loss: 0.2056370448:
31: 28832: loss: 0.2040019580:
31: 32032: loss: 0.2026751018:
31: 35232: loss: 0.2027615137:
31: 38432: loss: 0.2020604960:
31: 41632: loss: 0.2022901475:
31: 44832: loss: 0.2023769690:
31: 48032: loss: 0.2029093853:
31: 51232: loss: 0.2030337226:
31: 54432: loss: 0.2038891257:
31: 57632: loss: 0.2038209227:
31: 60832: loss: 0.2043370122:
31: 64032: loss: 0.2045655414:
31: 67232: loss: 0.2049986803:
31: 70432: loss: 0.2044257755:
31: 73632: loss: 0.2038078377:
31: 76832: loss: 0.2037208073:
31: 80032: loss: 0.2038124594:
31: 83232: loss: 0.2033122168:
31: 86432: loss: 0.2035757493:
31: 89632: loss: 0.2036888338:
31: 92832: loss: 0.2039603005:
31: 96032: loss: 0.2038315412:
31: 99232: loss: 0.2038765427:
31: 102432: loss: 0.2037669790:
31: 105632: loss: 0.2039455293:
31: 108832: loss: 0.2036354410:
31: 112032: loss: 0.2035941347:
31: 115232: loss: 0.2035856102:
31: 118432: loss: 0.2035124736:
31: 121632: loss: 0.2036112594:
31: 124832: loss: 0.2040858755:
31: 128032: loss: 0.2045312582:
31: 131232: loss: 0.2044485466:
31: 134432: loss: 0.2043947317:
31: 137632: loss: 0.2043244994:
31: 140832: loss: 0.2044208713:
31: 144032: loss: 0.2046679176:
31: 147232: loss: 0.2047194436:
31: 150432: loss: 0.2042565867:
Dev-Acc: 31: Accuracy: 0.9379960895: precision: 0.4571694600: recall: 0.3339568101: f1: 0.3859683600
Train-Acc: 31: Accuracy: 0.9258497357: precision: 0.7865889213: recall: 0.3547432779: f1: 0.4889674233
32: 3232: loss: 0.1976180552:
32: 6432: loss: 0.1980074168:
32: 9632: loss: 0.2042539403:
32: 12832: loss: 0.2042623606:
32: 16032: loss: 0.2076101527:
32: 19232: loss: 0.2092250392:
32: 22432: loss: 0.2067302648:
32: 25632: loss: 0.2066017632:
32: 28832: loss: 0.2046861289:
32: 32032: loss: 0.2053088528:
32: 35232: loss: 0.2041354975:
32: 38432: loss: 0.2035213080:
32: 41632: loss: 0.2036953997:
32: 44832: loss: 0.2027958043:
32: 48032: loss: 0.2031750456:
32: 51232: loss: 0.2026809673:
32: 54432: loss: 0.2034874583:
32: 57632: loss: 0.2047386802:
32: 60832: loss: 0.2051477458:
32: 64032: loss: 0.2050984801:
32: 67232: loss: 0.2049510414:
32: 70432: loss: 0.2047527789:
32: 73632: loss: 0.2048860114:
32: 76832: loss: 0.2045156436:
32: 80032: loss: 0.2045478905:
32: 83232: loss: 0.2046237595:
32: 86432: loss: 0.2045989593:
32: 89632: loss: 0.2050208245:
32: 92832: loss: 0.2049851790:
32: 96032: loss: 0.2046331961:
32: 99232: loss: 0.2046339457:
32: 102432: loss: 0.2049061532:
32: 105632: loss: 0.2049477672:
32: 108832: loss: 0.2043432634:
32: 112032: loss: 0.2042897103:
32: 115232: loss: 0.2040232742:
32: 118432: loss: 0.2039828346:
32: 121632: loss: 0.2034664534:
32: 124832: loss: 0.2032587111:
32: 128032: loss: 0.2032456278:
32: 131232: loss: 0.2032436575:
32: 134432: loss: 0.2032680754:
32: 137632: loss: 0.2031835467:
32: 140832: loss: 0.2029803268:
32: 144032: loss: 0.2027219012:
32: 147232: loss: 0.2029167249:
32: 150432: loss: 0.2029957688:
Dev-Acc: 32: Accuracy: 0.9374603033: precision: 0.4523486902: recall: 0.3405883353: f1: 0.3885924920
Train-Acc: 32: Accuracy: 0.9262375832: precision: 0.7847053788: recall: 0.3615804352: f1: 0.4950495050
33: 3232: loss: 0.1995846470:
33: 6432: loss: 0.2003884291:
33: 9632: loss: 0.2030352076:
33: 12832: loss: 0.2020781370:
33: 16032: loss: 0.2016239900:
33: 19232: loss: 0.2016904718:
33: 22432: loss: 0.2027147948:
33: 25632: loss: 0.2011254562:
33: 28832: loss: 0.2016856587:
33: 32032: loss: 0.2004966219:
33: 35232: loss: 0.1999716342:
33: 38432: loss: 0.2003833116:
33: 41632: loss: 0.2011296133:
33: 44832: loss: 0.2010562176:
33: 48032: loss: 0.2009321900:
33: 51232: loss: 0.2007866521:
33: 54432: loss: 0.2015229467:
33: 57632: loss: 0.2015963131:
33: 60832: loss: 0.2019652126:
33: 64032: loss: 0.2015296781:
33: 67232: loss: 0.2025062087:
33: 70432: loss: 0.2027635636:
33: 73632: loss: 0.2024158188:
33: 76832: loss: 0.2030675741:
33: 80032: loss: 0.2031479640:
33: 83232: loss: 0.2033368474:
33: 86432: loss: 0.2031182458:
33: 89632: loss: 0.2028955906:
33: 92832: loss: 0.2028486891:
33: 96032: loss: 0.2026260468:
33: 99232: loss: 0.2025291036:
33: 102432: loss: 0.2027769832:
33: 105632: loss: 0.2023780183:
33: 108832: loss: 0.2024742103:
33: 112032: loss: 0.2021439021:
33: 115232: loss: 0.2022194160:
33: 118432: loss: 0.2017870524:
33: 121632: loss: 0.2011978257:
33: 124832: loss: 0.2012809817:
33: 128032: loss: 0.2016990065:
33: 131232: loss: 0.2016740370:
33: 134432: loss: 0.2015888629:
33: 137632: loss: 0.2012843317:
33: 140832: loss: 0.2010573095:
33: 144032: loss: 0.2011479448:
33: 147232: loss: 0.2008574610:
33: 150432: loss: 0.2010893963:
Dev-Acc: 33: Accuracy: 0.9370336533: precision: 0.4485505643: recall: 0.3446692739: f1: 0.3898076923
Train-Acc: 33: Accuracy: 0.9267175198: precision: 0.7848331932: recall: 0.3680888830: f1: 0.5011411949
34: 3232: loss: 0.2048141437:
34: 6432: loss: 0.1994218520:
34: 9632: loss: 0.2008719228:
34: 12832: loss: 0.1989569019:
34: 16032: loss: 0.1984376270:
34: 19232: loss: 0.2004043342:
34: 22432: loss: 0.2007025213:
34: 25632: loss: 0.2007551928:
34: 28832: loss: 0.2028443077:
34: 32032: loss: 0.2026395656:
34: 35232: loss: 0.2026487329:
34: 38432: loss: 0.2021205017:
34: 41632: loss: 0.2020698466:
34: 44832: loss: 0.2011753873:
34: 48032: loss: 0.2018857503:
34: 51232: loss: 0.2008239501:
34: 54432: loss: 0.2000604280:
34: 57632: loss: 0.1995264993:
34: 60832: loss: 0.1994573248:
34: 64032: loss: 0.1995948303:
34: 67232: loss: 0.2004197604:
34: 70432: loss: 0.2005601078:
34: 73632: loss: 0.2005698469:
34: 76832: loss: 0.2010657430:
34: 80032: loss: 0.2010093711:
34: 83232: loss: 0.2004788251:
34: 86432: loss: 0.2005125118:
34: 89632: loss: 0.2001706545:
34: 92832: loss: 0.2004880750:
34: 96032: loss: 0.2004358495:
34: 99232: loss: 0.2003402730:
34: 102432: loss: 0.2002480933:
34: 105632: loss: 0.1995370473:
34: 108832: loss: 0.1993813045:
34: 112032: loss: 0.1996572550:
34: 115232: loss: 0.1991128751:
34: 118432: loss: 0.1998739977:
34: 121632: loss: 0.1998565865:
34: 124832: loss: 0.1999388880:
34: 128032: loss: 0.1999861990:
34: 131232: loss: 0.1998225164:
34: 134432: loss: 0.1997659241:
34: 137632: loss: 0.1998999531:
34: 140832: loss: 0.1997894002:
34: 144032: loss: 0.1998853026:
34: 147232: loss: 0.1999058341:
34: 150432: loss: 0.1997353840:
Dev-Acc: 34: Accuracy: 0.9367955327: precision: 0.4467436288: recall: 0.3487502125: f1: 0.3917112299
Train-Acc: 34: Accuracy: 0.9271908402: precision: 0.7835207019: recall: 0.3757149431: f1: 0.5078871362
35: 3232: loss: 0.1955801238:
35: 6432: loss: 0.1965591965:
35: 9632: loss: 0.1899372298:
35: 12832: loss: 0.1917954646:
35: 16032: loss: 0.1909494990:
35: 19232: loss: 0.1939442644:
35: 22432: loss: 0.1960384585:
35: 25632: loss: 0.1966945306:
35: 28832: loss: 0.1963920658:
35: 32032: loss: 0.1953098217:
35: 35232: loss: 0.1973274817:
35: 38432: loss: 0.1981909905:
35: 41632: loss: 0.1980362164:
35: 44832: loss: 0.1978204775:
35: 48032: loss: 0.1970781471:
35: 51232: loss: 0.1971442391:
35: 54432: loss: 0.1972778705:
35: 57632: loss: 0.1965389894:
35: 60832: loss: 0.1965040941:
35: 64032: loss: 0.1965401676:
35: 67232: loss: 0.1964162922:
35: 70432: loss: 0.1965556253:
35: 73632: loss: 0.1966312086:
35: 76832: loss: 0.1968776559:
35: 80032: loss: 0.1967738193:
35: 83232: loss: 0.1963469646:
35: 86432: loss: 0.1961025941:
35: 89632: loss: 0.1960019891:
35: 92832: loss: 0.1966872659:
35: 96032: loss: 0.1968658839:
35: 99232: loss: 0.1970620857:
35: 102432: loss: 0.1969807196:
35: 105632: loss: 0.1972297364:
35: 108832: loss: 0.1971254425:
35: 112032: loss: 0.1975616816:
35: 115232: loss: 0.1975243279:
35: 118432: loss: 0.1974830262:
35: 121632: loss: 0.1977118621:
35: 124832: loss: 0.1978634453:
35: 128032: loss: 0.1982779597:
35: 131232: loss: 0.1980908471:
35: 134432: loss: 0.1983329923:
35: 137632: loss: 0.1979724752:
35: 140832: loss: 0.1979047646:
35: 144032: loss: 0.1980048913:
35: 147232: loss: 0.1978338495:
35: 150432: loss: 0.1980154967:
Dev-Acc: 35: Accuracy: 0.9362002015: precision: 0.4414587332: recall: 0.3519809556: f1: 0.3916745506
Train-Acc: 35: Accuracy: 0.9276378751: precision: 0.7835176693: recall: 0.3818946815: f1: 0.5135027624
36: 3232: loss: 0.1895884220:
36: 6432: loss: 0.1918377475:
36: 9632: loss: 0.2001544366:
36: 12832: loss: 0.2004719572:
36: 16032: loss: 0.1963741121:
36: 19232: loss: 0.1973174042:
36: 22432: loss: 0.1971222045:
36: 25632: loss: 0.1938426108:
36: 28832: loss: 0.1939575702:
36: 32032: loss: 0.1939308321:
36: 35232: loss: 0.1932214458:
36: 38432: loss: 0.1938574683:
36: 41632: loss: 0.1955980412:
36: 44832: loss: 0.1956480553:
36: 48032: loss: 0.1955427750:
36: 51232: loss: 0.1957644118:
36: 54432: loss: 0.1966416000:
36: 57632: loss: 0.1958937961:
36: 60832: loss: 0.1956496373:
36: 64032: loss: 0.1956279361:
36: 67232: loss: 0.1954987891:
36: 70432: loss: 0.1956380487:
36: 73632: loss: 0.1959158199:
36: 76832: loss: 0.1957321508:
36: 80032: loss: 0.1957480277:
36: 83232: loss: 0.1953871373:
36: 86432: loss: 0.1953683549:
36: 89632: loss: 0.1955220071:
36: 92832: loss: 0.1955407412:
36: 96032: loss: 0.1957572746:
36: 99232: loss: 0.1961143331:
36: 102432: loss: 0.1963028278:
36: 105632: loss: 0.1961921340:
36: 108832: loss: 0.1962180421:
36: 112032: loss: 0.1965117286:
36: 115232: loss: 0.1963080089:
36: 118432: loss: 0.1961053197:
36: 121632: loss: 0.1960214490:
36: 124832: loss: 0.1961424367:
36: 128032: loss: 0.1961163532:
36: 131232: loss: 0.1961483194:
36: 134432: loss: 0.1965697958:
36: 137632: loss: 0.1963366179:
36: 140832: loss: 0.1964128317:
36: 144032: loss: 0.1967160907:
36: 147232: loss: 0.1967856763:
36: 150432: loss: 0.1967262578:
Dev-Acc: 36: Accuracy: 0.9359025359: precision: 0.4391167192: recall: 0.3550416596: f1: 0.3926288078
Train-Acc: 36: Accuracy: 0.9280126095: precision: 0.7816259088: recall: 0.3887318388: f1: 0.5192307692
37: 3232: loss: 0.1895314709:
37: 6432: loss: 0.1889792383:
37: 9632: loss: 0.1939111861:
37: 12832: loss: 0.1948891345:
37: 16032: loss: 0.1965723759:
37: 19232: loss: 0.1975325093:
37: 22432: loss: 0.1985444909:
37: 25632: loss: 0.1976669680:
37: 28832: loss: 0.1972171219:
37: 32032: loss: 0.1976913308:
37: 35232: loss: 0.1973181212:
37: 38432: loss: 0.1964876756:
37: 41632: loss: 0.1974199331:
37: 44832: loss: 0.1969667390:
37: 48032: loss: 0.1968804820:
37: 51232: loss: 0.1961153380:
37: 54432: loss: 0.1956233959:
37: 57632: loss: 0.1950090618:
37: 60832: loss: 0.1948584763:
37: 64032: loss: 0.1949929906:
37: 67232: loss: 0.1954926253:
37: 70432: loss: 0.1956174306:
37: 73632: loss: 0.1959623368:
37: 76832: loss: 0.1961123589:
37: 80032: loss: 0.1959743976:
37: 83232: loss: 0.1960594912:
37: 86432: loss: 0.1958999067:
37: 89632: loss: 0.1960741417:
37: 92832: loss: 0.1958894092:
37: 96032: loss: 0.1961455559:
37: 99232: loss: 0.1965328216:
37: 102432: loss: 0.1961077623:
37: 105632: loss: 0.1961584341:
37: 108832: loss: 0.1961960280:
37: 112032: loss: 0.1963718800:
37: 115232: loss: 0.1963146734:
37: 118432: loss: 0.1963699016:
37: 121632: loss: 0.1962204063:
37: 124832: loss: 0.1963222050:
37: 128032: loss: 0.1960684924:
37: 131232: loss: 0.1959497629:
37: 134432: loss: 0.1958173429:
37: 137632: loss: 0.1958182459:
37: 140832: loss: 0.1959199006:
37: 144032: loss: 0.1957548652:
37: 147232: loss: 0.1958335078:
37: 150432: loss: 0.1956939423:
Dev-Acc: 37: Accuracy: 0.9355850220: precision: 0.4366576819: recall: 0.3581023635: f1: 0.3934977578
Train-Acc: 37: Accuracy: 0.9283347726: precision: 0.7823637317: recall: 0.3925448688: f1: 0.5227859738
38: 3232: loss: 0.1825073593:
38: 6432: loss: 0.1845205593:
38: 9632: loss: 0.1909691681:
38: 12832: loss: 0.1950241197:
38: 16032: loss: 0.1953553569:
38: 19232: loss: 0.1963002423:
38: 22432: loss: 0.1954992224:
38: 25632: loss: 0.1959939025:
38: 28832: loss: 0.1976524295:
38: 32032: loss: 0.1973225217:
38: 35232: loss: 0.1971896219:
38: 38432: loss: 0.1970104443:
38: 41632: loss: 0.1971886999:
38: 44832: loss: 0.1970738821:
38: 48032: loss: 0.1968842303:
38: 51232: loss: 0.1964532836:
38: 54432: loss: 0.1964987248:
38: 57632: loss: 0.1953456421:
38: 60832: loss: 0.1959532034:
38: 64032: loss: 0.1962009912:
38: 67232: loss: 0.1962424277:
38: 70432: loss: 0.1962166119:
38: 73632: loss: 0.1958814106:
38: 76832: loss: 0.1963345871:
38: 80032: loss: 0.1958524730:
38: 83232: loss: 0.1962702263:
38: 86432: loss: 0.1961597454:
38: 89632: loss: 0.1958402243:
38: 92832: loss: 0.1954967477:
38: 96032: loss: 0.1950824538:
38: 99232: loss: 0.1954261928:
38: 102432: loss: 0.1955237847:
38: 105632: loss: 0.1955721744:
38: 108832: loss: 0.1958163640:
38: 112032: loss: 0.1957791176:
38: 115232: loss: 0.1954761207:
38: 118432: loss: 0.1953701466:
38: 121632: loss: 0.1953291978:
38: 124832: loss: 0.1947671128:
38: 128032: loss: 0.1945139181:
38: 131232: loss: 0.1946045384:
38: 134432: loss: 0.1942975553:
38: 137632: loss: 0.1942549677:
38: 140832: loss: 0.1942287788:
38: 144032: loss: 0.1943185827:
38: 147232: loss: 0.1939191965:
38: 150432: loss: 0.1941982000:
Dev-Acc: 38: Accuracy: 0.9354758859: precision: 0.4361658456: recall: 0.3613331066: f1: 0.3952385381
Train-Acc: 38: Accuracy: 0.9286174178: precision: 0.7816746473: recall: 0.3970810598: f1: 0.5266370215
39: 3232: loss: 0.2044032901:
39: 6432: loss: 0.1959825953:
39: 9632: loss: 0.1902540114:
39: 12832: loss: 0.1896461925:
39: 16032: loss: 0.1876802717:
39: 19232: loss: 0.1884311675:
39: 22432: loss: 0.1887241498:
39: 25632: loss: 0.1901862920:
39: 28832: loss: 0.1921478858:
39: 32032: loss: 0.1932088040:
39: 35232: loss: 0.1928793877:
39: 38432: loss: 0.1927385011:
39: 41632: loss: 0.1926741388:
39: 44832: loss: 0.1928990265:
39: 48032: loss: 0.1931957586:
39: 51232: loss: 0.1926889259:
39: 54432: loss: 0.1932421051:
39: 57632: loss: 0.1936307730:
39: 60832: loss: 0.1933673979:
39: 64032: loss: 0.1934168611:
39: 67232: loss: 0.1934937497:
39: 70432: loss: 0.1936001897:
39: 73632: loss: 0.1938990032:
39: 76832: loss: 0.1939064978:
39: 80032: loss: 0.1938232713:
39: 83232: loss: 0.1930387607:
39: 86432: loss: 0.1930974858:
39: 89632: loss: 0.1929393200:
39: 92832: loss: 0.1931773194:
39: 96032: loss: 0.1929608464:
39: 99232: loss: 0.1932597298:
39: 102432: loss: 0.1927533976:
39: 105632: loss: 0.1928578878:
39: 108832: loss: 0.1928620048:
39: 112032: loss: 0.1928567488:
39: 115232: loss: 0.1929732367:
39: 118432: loss: 0.1930626230:
39: 121632: loss: 0.1935882799:
39: 124832: loss: 0.1936755010:
39: 128032: loss: 0.1938303380:
39: 131232: loss: 0.1939641179:
39: 134432: loss: 0.1939120816:
39: 137632: loss: 0.1938829837:
39: 140832: loss: 0.1934815603:
39: 144032: loss: 0.1934647556:
39: 147232: loss: 0.1932543677:
39: 150432: loss: 0.1933823612:
Dev-Acc: 39: Accuracy: 0.9352575541: precision: 0.4350282486: recall: 0.3666043190: f1: 0.3978960967
Train-Acc: 39: Accuracy: 0.9290513396: precision: 0.7827255278: recall: 0.4021431859: f1: 0.5313124294
40: 3232: loss: 0.1975000756:
40: 6432: loss: 0.1881299182:
40: 9632: loss: 0.1857202582:
40: 12832: loss: 0.1845706817:
40: 16032: loss: 0.1868319271:
40: 19232: loss: 0.1866802893:
40: 22432: loss: 0.1862948525:
40: 25632: loss: 0.1878332792:
40: 28832: loss: 0.1889022007:
40: 32032: loss: 0.1888395493:
40: 35232: loss: 0.1887971110:
40: 38432: loss: 0.1891356086:
40: 41632: loss: 0.1896076831:
40: 44832: loss: 0.1894420578:
40: 48032: loss: 0.1901085567:
40: 51232: loss: 0.1907181149:
40: 54432: loss: 0.1923149084:
40: 57632: loss: 0.1919761850:
40: 60832: loss: 0.1927985837:
40: 64032: loss: 0.1932145115:
40: 67232: loss: 0.1934832255:
40: 70432: loss: 0.1928866610:
40: 73632: loss: 0.1928186305:
40: 76832: loss: 0.1923639906:
40: 80032: loss: 0.1926213155:
40: 83232: loss: 0.1923625378:
40: 86432: loss: 0.1921994951:
40: 89632: loss: 0.1922020518:
40: 92832: loss: 0.1925884873:
40: 96032: loss: 0.1925709530:
40: 99232: loss: 0.1928439433:
40: 102432: loss: 0.1927172751:
40: 105632: loss: 0.1925213036:
40: 108832: loss: 0.1922286349:
40: 112032: loss: 0.1918725586:
40: 115232: loss: 0.1921321238:
40: 118432: loss: 0.1920377018:
40: 121632: loss: 0.1921530413:
40: 124832: loss: 0.1918991193:
40: 128032: loss: 0.1920991061:
40: 131232: loss: 0.1919621227:
40: 134432: loss: 0.1918178299:
40: 137632: loss: 0.1917995393:
40: 140832: loss: 0.1918306728:
40: 144032: loss: 0.1921621018:
40: 147232: loss: 0.1920203541:
40: 150432: loss: 0.1918502441:
Dev-Acc: 40: Accuracy: 0.9348309040: precision: 0.4318857823: recall: 0.3703451794: f1: 0.3987550348
Train-Acc: 40: Accuracy: 0.9293997884: precision: 0.7825372757: recall: 0.4071395700: f1: 0.5356108108
41: 3232: loss: 0.1959706905:
41: 6432: loss: 0.1916873040:
41: 9632: loss: 0.1948816161:
41: 12832: loss: 0.1929587388:
41: 16032: loss: 0.1949758224:
41: 19232: loss: 0.1959186435:
41: 22432: loss: 0.1948768967:
41: 25632: loss: 0.1946545257:
41: 28832: loss: 0.1935992873:
41: 32032: loss: 0.1938499636:
41: 35232: loss: 0.1933199847:
41: 38432: loss: 0.1922075473:
41: 41632: loss: 0.1926546416:
41: 44832: loss: 0.1920056269:
41: 48032: loss: 0.1919208903:
41: 51232: loss: 0.1926483059:
41: 54432: loss: 0.1928146577:
41: 57632: loss: 0.1924393357:
41: 60832: loss: 0.1929398665:
41: 64032: loss: 0.1927811638:
41: 67232: loss: 0.1928366829:
41: 70432: loss: 0.1927394721:
41: 73632: loss: 0.1923133146:
41: 76832: loss: 0.1919201513:
41: 80032: loss: 0.1916423971:
41: 83232: loss: 0.1913242383:
41: 86432: loss: 0.1908365938:
41: 89632: loss: 0.1906737944:
41: 92832: loss: 0.1905475062:
41: 96032: loss: 0.1905650849:
41: 99232: loss: 0.1901829803:
41: 102432: loss: 0.1900435729:
41: 105632: loss: 0.1903084929:
41: 108832: loss: 0.1900627762:
41: 112032: loss: 0.1898947079:
41: 115232: loss: 0.1898978162:
41: 118432: loss: 0.1906090544:
41: 121632: loss: 0.1905088955:
41: 124832: loss: 0.1905333047:
41: 128032: loss: 0.1908096305:
41: 131232: loss: 0.1906144513:
41: 134432: loss: 0.1907552360:
41: 137632: loss: 0.1908002113:
41: 140832: loss: 0.1908941711:
41: 144032: loss: 0.1907577199:
41: 147232: loss: 0.1907180298:
41: 150432: loss: 0.1908072694:
Dev-Acc: 41: Accuracy: 0.9346126318: precision: 0.4304492839: recall: 0.3730658051: f1: 0.3997085079
Train-Acc: 41: Accuracy: 0.9295772910: precision: 0.7813633521: recall: 0.4106896325: f1: 0.5383952426
42: 3232: loss: 0.1945567998:
42: 6432: loss: 0.1898256886:
42: 9632: loss: 0.1945505055:
42: 12832: loss: 0.1952865464:
42: 16032: loss: 0.1951497152:
42: 19232: loss: 0.1931338669:
42: 22432: loss: 0.1925955740:
42: 25632: loss: 0.1910457528:
42: 28832: loss: 0.1912774695:
42: 32032: loss: 0.1917637108:
42: 35232: loss: 0.1930934566:
42: 38432: loss: 0.1917033702:
42: 41632: loss: 0.1923848200:
42: 44832: loss: 0.1921949883:
42: 48032: loss: 0.1917081937:
42: 51232: loss: 0.1910299391:
42: 54432: loss: 0.1906890132:
42: 57632: loss: 0.1898777953:
42: 60832: loss: 0.1897816341:
42: 64032: loss: 0.1896939401:
42: 67232: loss: 0.1894687941:
42: 70432: loss: 0.1895471341:
42: 73632: loss: 0.1892676124:
42: 76832: loss: 0.1896643097:
42: 80032: loss: 0.1897114848:
42: 83232: loss: 0.1898708238:
42: 86432: loss: 0.1896144002:
42: 89632: loss: 0.1894527435:
42: 92832: loss: 0.1899785927:
42: 96032: loss: 0.1900254198:
42: 99232: loss: 0.1904697693:
42: 102432: loss: 0.1905303004:
42: 105632: loss: 0.1904054786:
42: 108832: loss: 0.1902085427:
42: 112032: loss: 0.1901876350:
42: 115232: loss: 0.1902724152:
42: 118432: loss: 0.1899336017:
42: 121632: loss: 0.1898754492:
42: 124832: loss: 0.1896253223:
42: 128032: loss: 0.1898246496:
42: 131232: loss: 0.1898727263:
42: 134432: loss: 0.1897646110:
42: 137632: loss: 0.1899020364:
42: 140832: loss: 0.1899208597:
42: 144032: loss: 0.1897037979:
42: 147232: loss: 0.1896353519:
42: 150432: loss: 0.1894881742:
Dev-Acc: 42: Accuracy: 0.9344042540: precision: 0.4290711232: recall: 0.3754463527: f1: 0.4004715698
Train-Acc: 42: Accuracy: 0.9298468232: precision: 0.7814980159: recall: 0.4143054369: f1: 0.5415252417
43: 3232: loss: 0.1972610676:
43: 6432: loss: 0.1946690408:
43: 9632: loss: 0.1865555857:
43: 12832: loss: 0.1887266830:
43: 16032: loss: 0.1891932308:
43: 19232: loss: 0.1907308799:
43: 22432: loss: 0.1923960016:
43: 25632: loss: 0.1916006924:
43: 28832: loss: 0.1908978558:
43: 32032: loss: 0.1897086349:
43: 35232: loss: 0.1893023281:
43: 38432: loss: 0.1898283600:
43: 41632: loss: 0.1894850763:
43: 44832: loss: 0.1887745466:
43: 48032: loss: 0.1879200867:
43: 51232: loss: 0.1878900628:
43: 54432: loss: 0.1889966264:
43: 57632: loss: 0.1891876112:
43: 60832: loss: 0.1891417503:
43: 64032: loss: 0.1888793758:
43: 67232: loss: 0.1888378228:
43: 70432: loss: 0.1888205570:
43: 73632: loss: 0.1886259442:
43: 76832: loss: 0.1889226264:
43: 80032: loss: 0.1894870573:
43: 83232: loss: 0.1892260727:
43: 86432: loss: 0.1888446148:
43: 89632: loss: 0.1888469467:
43: 92832: loss: 0.1889712511:
43: 96032: loss: 0.1888129317:
43: 99232: loss: 0.1886536962:
43: 102432: loss: 0.1891831989:
43: 105632: loss: 0.1889661091:
43: 108832: loss: 0.1886926485:
43: 112032: loss: 0.1884241337:
43: 115232: loss: 0.1890262184:
43: 118432: loss: 0.1887961667:
43: 121632: loss: 0.1889334501:
43: 124832: loss: 0.1889037440:
43: 128032: loss: 0.1887879113:
43: 131232: loss: 0.1885950180:
43: 134432: loss: 0.1886380012:
43: 137632: loss: 0.1885290500:
43: 140832: loss: 0.1888722142:
43: 144032: loss: 0.1890200431:
43: 147232: loss: 0.1893836895:
43: 150432: loss: 0.1890958677:
Dev-Acc: 43: Accuracy: 0.9342355728: precision: 0.4280762565: recall: 0.3779969393: f1: 0.4014809464
Train-Acc: 43: Accuracy: 0.9300835133: precision: 0.7817040138: recall: 0.4173953060: f1: 0.5442077744
44: 3232: loss: 0.1839842739:
44: 6432: loss: 0.1890185209:
44: 9632: loss: 0.1913652107:
44: 12832: loss: 0.1914742860:
44: 16032: loss: 0.1905852065:
44: 19232: loss: 0.1918533745:
44: 22432: loss: 0.1917466218:
44: 25632: loss: 0.1899937438:
44: 28832: loss: 0.1889275548:
44: 32032: loss: 0.1885407766:
44: 35232: loss: 0.1888289044:
44: 38432: loss: 0.1876396305:
44: 41632: loss: 0.1871590429:
44: 44832: loss: 0.1880357054:
44: 48032: loss: 0.1882600273:
44: 51232: loss: 0.1890825319:
44: 54432: loss: 0.1896256608:
44: 57632: loss: 0.1893471871:
44: 60832: loss: 0.1891026442:
44: 64032: loss: 0.1885891499:
44: 67232: loss: 0.1887502203:
44: 70432: loss: 0.1887309270:
44: 73632: loss: 0.1888921456:
44: 76832: loss: 0.1888465512:
44: 80032: loss: 0.1885686007:
44: 83232: loss: 0.1884927282:
44: 86432: loss: 0.1882072857:
44: 89632: loss: 0.1880487370:
44: 92832: loss: 0.1883174401:
44: 96032: loss: 0.1884242517:
44: 99232: loss: 0.1887052091:
44: 102432: loss: 0.1887537710:
44: 105632: loss: 0.1888422272:
44: 108832: loss: 0.1884757727:
44: 112032: loss: 0.1880917688:
44: 115232: loss: 0.1881761607:
44: 118432: loss: 0.1877762121:
44: 121632: loss: 0.1874929844:
44: 124832: loss: 0.1873710847:
44: 128032: loss: 0.1875760313:
44: 131232: loss: 0.1874907682:
44: 134432: loss: 0.1873076632:
44: 137632: loss: 0.1875170566:
44: 140832: loss: 0.1874681070:
44: 144032: loss: 0.1874871167:
44: 147232: loss: 0.1879051832:
44: 150432: loss: 0.1878921043:
Dev-Acc: 44: Accuracy: 0.9341165423: precision: 0.4275348482: recall: 0.3807175650: f1: 0.4027702824
Train-Acc: 44: Accuracy: 0.9302873015: precision: 0.7812232939: recall: 0.4206824009: f1: 0.5468763354
45: 3232: loss: 0.1986803373:
45: 6432: loss: 0.1866147411:
45: 9632: loss: 0.1853182464:
45: 12832: loss: 0.1831168562:
45: 16032: loss: 0.1817214817:
45: 19232: loss: 0.1840497952:
45: 22432: loss: 0.1848393707:
45: 25632: loss: 0.1833164695:
45: 28832: loss: 0.1833733010:
45: 32032: loss: 0.1846760901:
45: 35232: loss: 0.1840328217:
45: 38432: loss: 0.1847776487:
45: 41632: loss: 0.1854860435:
45: 44832: loss: 0.1854445492:
45: 48032: loss: 0.1857493698:
45: 51232: loss: 0.1858230415:
45: 54432: loss: 0.1855538639:
45: 57632: loss: 0.1867270111:
45: 60832: loss: 0.1867603072:
45: 64032: loss: 0.1870850294:
45: 67232: loss: 0.1867171707:
45: 70432: loss: 0.1872451475:
45: 73632: loss: 0.1872433942:
45: 76832: loss: 0.1870783996:
45: 80032: loss: 0.1869469749:
45: 83232: loss: 0.1873684856:
45: 86432: loss: 0.1874964418:
45: 89632: loss: 0.1877496314:
45: 92832: loss: 0.1876473049:
45: 96032: loss: 0.1871859906:
45: 99232: loss: 0.1871554798:
45: 102432: loss: 0.1872601002:
45: 105632: loss: 0.1873708929:
45: 108832: loss: 0.1872527170:
45: 112032: loss: 0.1872547167:
45: 115232: loss: 0.1874292838:
45: 118432: loss: 0.1873693237:
45: 121632: loss: 0.1874188301:
45: 124832: loss: 0.1877643110:
45: 128032: loss: 0.1876271028:
45: 131232: loss: 0.1874615280:
45: 134432: loss: 0.1876485507:
45: 137632: loss: 0.1877192849:
45: 140832: loss: 0.1874587887:
45: 144032: loss: 0.1871965454:
45: 147232: loss: 0.1871459208:
45: 150432: loss: 0.1871497959:
Dev-Acc: 45: Accuracy: 0.9339776039: precision: 0.4271167264: recall: 0.3851385819: f1: 0.4050429185
Train-Acc: 45: Accuracy: 0.9304121733: precision: 0.7803636364: recall: 0.4232463349: f1: 0.5488257108
46: 3232: loss: 0.1977352859:
46: 6432: loss: 0.1905583683:
46: 9632: loss: 0.1915433888:
46: 12832: loss: 0.1865269294:
46: 16032: loss: 0.1841002389:
46: 19232: loss: 0.1875155036:
46: 22432: loss: 0.1868549492:
46: 25632: loss: 0.1850467295:
46: 28832: loss: 0.1865639024:
46: 32032: loss: 0.1860356636:
46: 35232: loss: 0.1859253280:
46: 38432: loss: 0.1851159272:
46: 41632: loss: 0.1851278733:
46: 44832: loss: 0.1858283050:
46: 48032: loss: 0.1868329156:
46: 51232: loss: 0.1868670296:
46: 54432: loss: 0.1878645984:
46: 57632: loss: 0.1880162490:
46: 60832: loss: 0.1878583312:
46: 64032: loss: 0.1880336560:
46: 67232: loss: 0.1883640776:
46: 70432: loss: 0.1880849101:
46: 73632: loss: 0.1883946912:
46: 76832: loss: 0.1878778913:
46: 80032: loss: 0.1870507682:
46: 83232: loss: 0.1870980410:
46: 86432: loss: 0.1870027142:
46: 89632: loss: 0.1865549833:
46: 92832: loss: 0.1861496798:
46: 96032: loss: 0.1856765140:
46: 99232: loss: 0.1855886350:
46: 102432: loss: 0.1856638456:
46: 105632: loss: 0.1850730023:
46: 108832: loss: 0.1849482397:
46: 112032: loss: 0.1846282390:
46: 115232: loss: 0.1847653755:
46: 118432: loss: 0.1847363504:
46: 121632: loss: 0.1846129526:
46: 124832: loss: 0.1852432959:
46: 128032: loss: 0.1854614141:
46: 131232: loss: 0.1857736325:
46: 134432: loss: 0.1857724204:
46: 137632: loss: 0.1857544093:
46: 140832: loss: 0.1860131924:
46: 144032: loss: 0.1859892772:
46: 147232: loss: 0.1857979777:
46: 150432: loss: 0.1856954459:
Dev-Acc: 46: Accuracy: 0.9339577556: precision: 0.4272573681: recall: 0.3870090121: f1: 0.4061384725
Train-Acc: 46: Accuracy: 0.9306488633: precision: 0.7806405008: recall: 0.4262704622: f1: 0.5514308798
47: 3232: loss: 0.1962563246:
47: 6432: loss: 0.1896139863:
47: 9632: loss: 0.1879081974:
47: 12832: loss: 0.1869171337:
47: 16032: loss: 0.1884250520:
47: 19232: loss: 0.1883176377:
47: 22432: loss: 0.1888328165:
47: 25632: loss: 0.1877808598:
47: 28832: loss: 0.1874723425:
47: 32032: loss: 0.1872073373:
47: 35232: loss: 0.1863135281:
47: 38432: loss: 0.1865572802:
47: 41632: loss: 0.1858332539:
47: 44832: loss: 0.1856494395:
47: 48032: loss: 0.1846310573:
47: 51232: loss: 0.1843499132:
47: 54432: loss: 0.1842292645:
47: 57632: loss: 0.1847682936:
47: 60832: loss: 0.1848573266:
47: 64032: loss: 0.1847437828:
47: 67232: loss: 0.1852215514:
47: 70432: loss: 0.1851126884:
47: 73632: loss: 0.1852040633:
47: 76832: loss: 0.1853171633:
47: 80032: loss: 0.1852969124:
47: 83232: loss: 0.1855941311:
47: 86432: loss: 0.1858673911:
47: 89632: loss: 0.1858731799:
47: 92832: loss: 0.1856843158:
47: 96032: loss: 0.1856261214:
47: 99232: loss: 0.1854141918:
47: 102432: loss: 0.1856906675:
47: 105632: loss: 0.1855020501:
47: 108832: loss: 0.1858083990:
47: 112032: loss: 0.1854520785:
47: 115232: loss: 0.1854263022:
47: 118432: loss: 0.1851300308:
47: 121632: loss: 0.1853440411:
47: 124832: loss: 0.1855178944:
47: 128032: loss: 0.1856453849:
47: 131232: loss: 0.1858956371:
47: 134432: loss: 0.1854901089:
47: 137632: loss: 0.1854228782:
47: 140832: loss: 0.1852220487:
47: 144032: loss: 0.1852692661:
47: 147232: loss: 0.1852581600:
47: 150432: loss: 0.1853484936:
Dev-Acc: 47: Accuracy: 0.9338188767: precision: 0.4263580362: recall: 0.3883693249: f1: 0.4064780210
Train-Acc: 47: Accuracy: 0.9309118390: precision: 0.7802813543: recall: 0.4302807179: f1: 0.5546845205
48: 3232: loss: 0.1797756241:
48: 6432: loss: 0.1927477129:
48: 9632: loss: 0.1912553225:
48: 12832: loss: 0.1900732231:
48: 16032: loss: 0.1896201621:
48: 19232: loss: 0.1888705104:
48: 22432: loss: 0.1879352759:
48: 25632: loss: 0.1870147281:
48: 28832: loss: 0.1890270603:
48: 32032: loss: 0.1887236603:
48: 35232: loss: 0.1889619210:
48: 38432: loss: 0.1888610839:
48: 41632: loss: 0.1895290399:
48: 44832: loss: 0.1891085657:
48: 48032: loss: 0.1878189987:
48: 51232: loss: 0.1880745086:
48: 54432: loss: 0.1881360889:
48: 57632: loss: 0.1875802377:
48: 60832: loss: 0.1871739782:
48: 64032: loss: 0.1870704496:
48: 67232: loss: 0.1864884158:
48: 70432: loss: 0.1865878146:
48: 73632: loss: 0.1860013827:
48: 76832: loss: 0.1861633501:
48: 80032: loss: 0.1867451041:
48: 83232: loss: 0.1873237067:
48: 86432: loss: 0.1868708534:
48: 89632: loss: 0.1867466109:
48: 92832: loss: 0.1866590859:
48: 96032: loss: 0.1867818835:
48: 99232: loss: 0.1864827013:
48: 102432: loss: 0.1866485290:
48: 105632: loss: 0.1867077493:
48: 108832: loss: 0.1862913720:
48: 112032: loss: 0.1860510575:
48: 115232: loss: 0.1861583043:
48: 118432: loss: 0.1861552960:
48: 121632: loss: 0.1861023348:
48: 124832: loss: 0.1858585884:
48: 128032: loss: 0.1854969521:
48: 131232: loss: 0.1857166750:
48: 134432: loss: 0.1853361934:
48: 137632: loss: 0.1851302174:
48: 140832: loss: 0.1848471001:
48: 144032: loss: 0.1847688339:
48: 147232: loss: 0.1847867397:
48: 150432: loss: 0.1848756126:
Dev-Acc: 48: Accuracy: 0.9336898327: precision: 0.4255200594: recall: 0.3895595987: f1: 0.4067465601
Train-Acc: 48: Accuracy: 0.9311156273: precision: 0.7805572021: recall: 0.4328446519: f1: 0.5568806563
49: 3232: loss: 0.1894543964:
49: 6432: loss: 0.1882509759:
49: 9632: loss: 0.1874548958:
49: 12832: loss: 0.1876359887:
49: 16032: loss: 0.1869355410:
49: 19232: loss: 0.1883866059:
49: 22432: loss: 0.1889629122:
49: 25632: loss: 0.1887813525:
49: 28832: loss: 0.1869910693:
49: 32032: loss: 0.1859690130:
49: 35232: loss: 0.1845876593:
49: 38432: loss: 0.1854886832:
49: 41632: loss: 0.1843186093:
49: 44832: loss: 0.1839285721:
49: 48032: loss: 0.1844671271:
49: 51232: loss: 0.1847070887:
49: 54432: loss: 0.1839351692:
49: 57632: loss: 0.1842918722:
49: 60832: loss: 0.1842187685:
49: 64032: loss: 0.1838193820:
49: 67232: loss: 0.1840682309:
49: 70432: loss: 0.1835195047:
49: 73632: loss: 0.1834005762:
49: 76832: loss: 0.1831588291:
49: 80032: loss: 0.1835722076:
49: 83232: loss: 0.1837918003:
49: 86432: loss: 0.1838263180:
49: 89632: loss: 0.1838151593:
49: 92832: loss: 0.1839067416:
49: 96032: loss: 0.1837378691:
49: 99232: loss: 0.1833265665:
49: 102432: loss: 0.1833268139:
49: 105632: loss: 0.1826207497:
49: 108832: loss: 0.1827111897:
49: 112032: loss: 0.1824371873:
49: 115232: loss: 0.1825087681:
49: 118432: loss: 0.1825032857:
49: 121632: loss: 0.1825811548:
49: 124832: loss: 0.1827119206:
49: 128032: loss: 0.1828420719:
49: 131232: loss: 0.1832078523:
49: 134432: loss: 0.1835367028:
49: 137632: loss: 0.1835283707:
49: 140832: loss: 0.1832658286:
49: 144032: loss: 0.1832863709:
49: 147232: loss: 0.1834745916:
49: 150432: loss: 0.1836067615:
Dev-Acc: 49: Accuracy: 0.9336799383: precision: 0.4257992977: recall: 0.3917701071: f1: 0.4080765143
Train-Acc: 49: Accuracy: 0.9314114451: precision: 0.7808605690: recall: 0.4366576819: f1: 0.5601045663
50: 3232: loss: 0.1778781995:
50: 6432: loss: 0.1786616529:
50: 9632: loss: 0.1782201944:
50: 12832: loss: 0.1779093089:
50: 16032: loss: 0.1761310828:
50: 19232: loss: 0.1791476399:
50: 22432: loss: 0.1804595060:
50: 25632: loss: 0.1807819371:
50: 28832: loss: 0.1800594747:
50: 32032: loss: 0.1794146177:
50: 35232: loss: 0.1793628920:
50: 38432: loss: 0.1803873942:
50: 41632: loss: 0.1806428017:
50: 44832: loss: 0.1812413332:
50: 48032: loss: 0.1813255363:
50: 51232: loss: 0.1825031098:
50: 54432: loss: 0.1824411364:
50: 57632: loss: 0.1818024030:
50: 60832: loss: 0.1824809856:
50: 64032: loss: 0.1831312106:
50: 67232: loss: 0.1831140584:
50: 70432: loss: 0.1831919324:
50: 73632: loss: 0.1829900521:
50: 76832: loss: 0.1834551079:
50: 80032: loss: 0.1827730558:
50: 83232: loss: 0.1830375562:
50: 86432: loss: 0.1830423984:
50: 89632: loss: 0.1827969104:
50: 92832: loss: 0.1830271443:
50: 96032: loss: 0.1835204757:
50: 99232: loss: 0.1832739074:
50: 102432: loss: 0.1831958534:
50: 105632: loss: 0.1829579073:
50: 108832: loss: 0.1831716610:
50: 112032: loss: 0.1829115448:
50: 115232: loss: 0.1831086671:
50: 118432: loss: 0.1829540778:
50: 121632: loss: 0.1829020150:
50: 124832: loss: 0.1828794207:
50: 128032: loss: 0.1828362588:
50: 131232: loss: 0.1831922416:
50: 134432: loss: 0.1830349715:
50: 137632: loss: 0.1833149376:
50: 140832: loss: 0.1834701696:
50: 144032: loss: 0.1833480550:
50: 147232: loss: 0.1832797591:
50: 150432: loss: 0.1827858349:
Dev-Acc: 50: Accuracy: 0.9336203933: precision: 0.4254377880: recall: 0.3924502636: f1: 0.4082787900
Train-Acc: 50: Accuracy: 0.9316481352: precision: 0.7809946299: recall: 0.4398132930: f1: 0.5627286874
51: 3232: loss: 0.1860911172:
51: 6432: loss: 0.1818336003:
51: 9632: loss: 0.1844973753:
51: 12832: loss: 0.1831356080:
51: 16032: loss: 0.1829443213:
51: 19232: loss: 0.1863915726:
51: 22432: loss: 0.1843700419:
51: 25632: loss: 0.1840419031:
51: 28832: loss: 0.1842871597:
51: 32032: loss: 0.1852827612:
51: 35232: loss: 0.1858207239:
51: 38432: loss: 0.1848433183:
51: 41632: loss: 0.1833574781:
51: 44832: loss: 0.1837218476:
51: 48032: loss: 0.1827579708:
51: 51232: loss: 0.1826357143:
51: 54432: loss: 0.1826712114:
51: 57632: loss: 0.1830419283:
51: 60832: loss: 0.1825072315:
51: 64032: loss: 0.1828044908:
51: 67232: loss: 0.1821776529:
51: 70432: loss: 0.1818632913:
51: 73632: loss: 0.1815034697:
51: 76832: loss: 0.1808426536:
51: 80032: loss: 0.1812843954:
51: 83232: loss: 0.1818153345:
51: 86432: loss: 0.1819889819:
51: 89632: loss: 0.1821085402:
51: 92832: loss: 0.1821685020:
51: 96032: loss: 0.1818451147:
51: 99232: loss: 0.1822146775:
51: 102432: loss: 0.1819683905:
51: 105632: loss: 0.1824602976:
51: 108832: loss: 0.1823168201:
51: 112032: loss: 0.1819097749:
51: 115232: loss: 0.1820950091:
51: 118432: loss: 0.1825355875:
51: 121632: loss: 0.1823150307:
51: 124832: loss: 0.1823670908:
51: 128032: loss: 0.1819405529:
51: 131232: loss: 0.1820938481:
51: 134432: loss: 0.1824638601:
51: 137632: loss: 0.1823427083:
51: 140832: loss: 0.1821435277:
51: 144032: loss: 0.1824244298:
51: 147232: loss: 0.1824298621:
51: 150432: loss: 0.1821123663:
Dev-Acc: 51: Accuracy: 0.9336501956: precision: 0.4260821717: recall: 0.3950008502: f1: 0.4099532339
Train-Acc: 51: Accuracy: 0.9319900274: precision: 0.7814669135: recall: 0.4440865163: f1: 0.5663382939
52: 3232: loss: 0.1667166169:
52: 6432: loss: 0.1657704817:
52: 9632: loss: 0.1714962013:
52: 12832: loss: 0.1736158907:
52: 16032: loss: 0.1771124808:
52: 19232: loss: 0.1783628021:
52: 22432: loss: 0.1789807378:
52: 25632: loss: 0.1791564459:
52: 28832: loss: 0.1804129586:
52: 32032: loss: 0.1809472297:
52: 35232: loss: 0.1830972536:
52: 38432: loss: 0.1841960108:
52: 41632: loss: 0.1845386084:
52: 44832: loss: 0.1841369259:
52: 48032: loss: 0.1844354370:
52: 51232: loss: 0.1837703505:
52: 54432: loss: 0.1837365173:
52: 57632: loss: 0.1835513352:
52: 60832: loss: 0.1839174642:
52: 64032: loss: 0.1835333658:
52: 67232: loss: 0.1829479408:
52: 70432: loss: 0.1828758395:
52: 73632: loss: 0.1831211231:
52: 76832: loss: 0.1830557069:
52: 80032: loss: 0.1826202457:
52: 83232: loss: 0.1827105144:
52: 86432: loss: 0.1824200361:
52: 89632: loss: 0.1823592158:
52: 92832: loss: 0.1826733281:
52: 96032: loss: 0.1823877300:
52: 99232: loss: 0.1821708027:
52: 102432: loss: 0.1819326188:
52: 105632: loss: 0.1822075737:
52: 108832: loss: 0.1816767461:
52: 112032: loss: 0.1816926355:
52: 115232: loss: 0.1821125905:
52: 118432: loss: 0.1819036761:
52: 121632: loss: 0.1822329555:
52: 124832: loss: 0.1819845987:
52: 128032: loss: 0.1819054741:
52: 131232: loss: 0.1816778389:
52: 134432: loss: 0.1817939319:
52: 137632: loss: 0.1814828526:
52: 140832: loss: 0.1815806819:
52: 144032: loss: 0.1814967547:
52: 147232: loss: 0.1817390589:
52: 150432: loss: 0.1816242381:
Dev-Acc: 52: Accuracy: 0.9335509539: precision: 0.4254385965: recall: 0.3958510457: f1: 0.4101118647
Train-Acc: 52: Accuracy: 0.9321806431: precision: 0.7816131630: recall: 0.4465847084: f1: 0.5684043176
53: 3232: loss: 0.1763349065:
53: 6432: loss: 0.1859252650:
53: 9632: loss: 0.1836407570:
53: 12832: loss: 0.1780405970:
53: 16032: loss: 0.1822526380:
53: 19232: loss: 0.1837952291:
53: 22432: loss: 0.1836176536:
53: 25632: loss: 0.1835924298:
53: 28832: loss: 0.1829712170:
53: 32032: loss: 0.1819096881:
53: 35232: loss: 0.1804461222:
53: 38432: loss: 0.1796971380:
53: 41632: loss: 0.1810790160:
53: 44832: loss: 0.1806928981:
53: 48032: loss: 0.1809804735:
53: 51232: loss: 0.1816914557:
53: 54432: loss: 0.1828988707:
53: 57632: loss: 0.1826071884:
53: 60832: loss: 0.1825125253:
53: 64032: loss: 0.1820727080:
53: 67232: loss: 0.1809511538:
53: 70432: loss: 0.1808638423:
53: 73632: loss: 0.1810473338:
53: 76832: loss: 0.1807426807:
53: 80032: loss: 0.1804238330:
53: 83232: loss: 0.1803986105:
53: 86432: loss: 0.1800363993:
53: 89632: loss: 0.1802774850:
53: 92832: loss: 0.1803992868:
53: 96032: loss: 0.1801889311:
53: 99232: loss: 0.1803999281:
53: 102432: loss: 0.1804736553:
53: 105632: loss: 0.1805480131:
53: 108832: loss: 0.1806423228:
53: 112032: loss: 0.1807535354:
53: 115232: loss: 0.1806553636:
53: 118432: loss: 0.1806513550:
53: 121632: loss: 0.1807325322:
53: 124832: loss: 0.1805838947:
53: 128032: loss: 0.1806835221:
53: 131232: loss: 0.1807725728:
53: 134432: loss: 0.1807274680:
53: 137632: loss: 0.1806948372:
53: 140832: loss: 0.1807466096:
53: 144032: loss: 0.1807311889:
53: 147232: loss: 0.1808284266:
53: 150432: loss: 0.1807805503:
Dev-Acc: 53: Accuracy: 0.9334418178: precision: 0.4246675169: recall: 0.3963611631: f1: 0.4100263852
Train-Acc: 53: Accuracy: 0.9324896336: precision: 0.7825937786: recall: 0.4498718033: f1: 0.5713212273
54: 3232: loss: 0.1795955098:
54: 6432: loss: 0.1771926661:
54: 9632: loss: 0.1737603033:
54: 12832: loss: 0.1764443764:
54: 16032: loss: 0.1763224329:
54: 19232: loss: 0.1768833550:
54: 22432: loss: 0.1749370436:
54: 25632: loss: 0.1742192996:
54: 28832: loss: 0.1748481553:
54: 32032: loss: 0.1751211870:
54: 35232: loss: 0.1765911496:
54: 38432: loss: 0.1777044171:
54: 41632: loss: 0.1778735118:
54: 44832: loss: 0.1784021915:
54: 48032: loss: 0.1780870605:
54: 51232: loss: 0.1779150832:
54: 54432: loss: 0.1780334693:
54: 57632: loss: 0.1790184019:
54: 60832: loss: 0.1785259171:
54: 64032: loss: 0.1784565205:
54: 67232: loss: 0.1784976545:
54: 70432: loss: 0.1785969684:
54: 73632: loss: 0.1792966111:
54: 76832: loss: 0.1796219518:
54: 80032: loss: 0.1796891175:
54: 83232: loss: 0.1793643494:
54: 86432: loss: 0.1793677654:
54: 89632: loss: 0.1789937326:
54: 92832: loss: 0.1789032373:
54: 96032: loss: 0.1788451941:
54: 99232: loss: 0.1786477846:
54: 102432: loss: 0.1785034401:
54: 105632: loss: 0.1783728702:
54: 108832: loss: 0.1787302946:
54: 112032: loss: 0.1786579451:
54: 115232: loss: 0.1790957467:
54: 118432: loss: 0.1788776297:
54: 121632: loss: 0.1788257351:
54: 124832: loss: 0.1791514492:
54: 128032: loss: 0.1791839647:
54: 131232: loss: 0.1794869088:
54: 134432: loss: 0.1795885843:
54: 137632: loss: 0.1797036941:
54: 140832: loss: 0.1795460512:
54: 144032: loss: 0.1794629701:
54: 147232: loss: 0.1798617024:
54: 150432: loss: 0.1799274848:
Dev-Acc: 54: Accuracy: 0.9332731366: precision: 0.4234119782: recall: 0.3967012413: f1: 0.4096216311
Train-Acc: 54: Accuracy: 0.9327000380: precision: 0.7826778813: recall: 0.4526987049: f1: 0.5736182265
55: 3232: loss: 0.1828359723:
55: 6432: loss: 0.1782003673:
55: 9632: loss: 0.1773361905:
55: 12832: loss: 0.1798949092:
55: 16032: loss: 0.1820174704:
55: 19232: loss: 0.1849113094:
55: 22432: loss: 0.1839912307:
55: 25632: loss: 0.1829488654:
55: 28832: loss: 0.1836872166:
55: 32032: loss: 0.1826877095:
55: 35232: loss: 0.1831567928:
55: 38432: loss: 0.1821068897:
55: 41632: loss: 0.1814058920:
55: 44832: loss: 0.1803698861:
55: 48032: loss: 0.1807178995:
55: 51232: loss: 0.1808854962:
55: 54432: loss: 0.1810890810:
55: 57632: loss: 0.1807437617:
55: 60832: loss: 0.1799440822:
55: 64032: loss: 0.1806478212:
55: 67232: loss: 0.1801072680:
55: 70432: loss: 0.1798367093:
55: 73632: loss: 0.1801212887:
55: 76832: loss: 0.1802751670:
55: 80032: loss: 0.1800588481:
55: 83232: loss: 0.1796741162:
55: 86432: loss: 0.1796884606:
55: 89632: loss: 0.1794116472:
55: 92832: loss: 0.1793148031:
55: 96032: loss: 0.1793312273:
55: 99232: loss: 0.1795765711:
55: 102432: loss: 0.1800061730:
55: 105632: loss: 0.1800198698:
55: 108832: loss: 0.1800886248:
55: 112032: loss: 0.1802775675:
55: 115232: loss: 0.1799011524:
55: 118432: loss: 0.1796247266:
55: 121632: loss: 0.1795641570:
55: 124832: loss: 0.1794529954:
55: 128032: loss: 0.1794500648:
55: 131232: loss: 0.1792925524:
55: 134432: loss: 0.1793203538:
55: 137632: loss: 0.1793633804:
55: 140832: loss: 0.1793896539:
55: 144032: loss: 0.1792077555:
55: 147232: loss: 0.1794329135:
55: 150432: loss: 0.1794526956:
Dev-Acc: 55: Accuracy: 0.9332036972: precision: 0.4229864253: recall: 0.3973813977: f1: 0.4097843240
Train-Acc: 55: Accuracy: 0.9329169393: precision: 0.7830412663: recall: 0.4553283808: f1: 0.5758230795
56: 3232: loss: 0.1800992528:
56: 6432: loss: 0.1822095475:
56: 9632: loss: 0.1824287828:
56: 12832: loss: 0.1820179976:
56: 16032: loss: 0.1861691995:
56: 19232: loss: 0.1838611646:
56: 22432: loss: 0.1816610677:
56: 25632: loss: 0.1807351226:
56: 28832: loss: 0.1811805601:
56: 32032: loss: 0.1806091315:
56: 35232: loss: 0.1796669018:
56: 38432: loss: 0.1786620258:
56: 41632: loss: 0.1787071794:
56: 44832: loss: 0.1783180127:
56: 48032: loss: 0.1793698642:
56: 51232: loss: 0.1787306287:
56: 54432: loss: 0.1785430114:
56: 57632: loss: 0.1782969327:
56: 60832: loss: 0.1784469537:
56: 64032: loss: 0.1784325532:
56: 67232: loss: 0.1787561120:
56: 70432: loss: 0.1785184515:
56: 73632: loss: 0.1779237846:
56: 76832: loss: 0.1778037021:
56: 80032: loss: 0.1776322367:
56: 83232: loss: 0.1776815632:
56: 86432: loss: 0.1778263257:
56: 89632: loss: 0.1783968666:
56: 92832: loss: 0.1783044502:
56: 96032: loss: 0.1783298956:
56: 99232: loss: 0.1788951100:
56: 102432: loss: 0.1791313430:
56: 105632: loss: 0.1788779037:
56: 108832: loss: 0.1787825146:
56: 112032: loss: 0.1788055845:
56: 115232: loss: 0.1786000196:
56: 118432: loss: 0.1785617760:
56: 121632: loss: 0.1783951040:
56: 124832: loss: 0.1781322395:
56: 128032: loss: 0.1781824402:
56: 131232: loss: 0.1784213819:
56: 134432: loss: 0.1782858978:
56: 137632: loss: 0.1783612197:
56: 140832: loss: 0.1782753611:
56: 144032: loss: 0.1781936398:
56: 147232: loss: 0.1783339776:
56: 150432: loss: 0.1783844161:
Dev-Acc: 56: Accuracy: 0.9331143498: precision: 0.4225504323: recall: 0.3989117497: f1: 0.4103909735
Train-Acc: 56: Accuracy: 0.9331141710: precision: 0.7832639748: recall: 0.4578265729: f1: 0.5778773546
57: 3232: loss: 0.1699341916:
57: 6432: loss: 0.1707185127:
57: 9632: loss: 0.1729504845:
57: 12832: loss: 0.1743192247:
57: 16032: loss: 0.1768175751:
57: 19232: loss: 0.1768232773:
57: 22432: loss: 0.1766870002:
57: 25632: loss: 0.1760117100:
57: 28832: loss: 0.1769383995:
57: 32032: loss: 0.1776984823:
57: 35232: loss: 0.1777882277:
57: 38432: loss: 0.1780729158:
57: 41632: loss: 0.1773568793:
57: 44832: loss: 0.1785292397:
57: 48032: loss: 0.1788972237:
57: 51232: loss: 0.1780950171:
57: 54432: loss: 0.1785689426:
57: 57632: loss: 0.1794056591:
57: 60832: loss: 0.1793275030:
57: 64032: loss: 0.1787478995:
57: 67232: loss: 0.1785915153:
57: 70432: loss: 0.1794904419:
57: 73632: loss: 0.1791460378:
57: 76832: loss: 0.1789772654:
57: 80032: loss: 0.1791788260:
57: 83232: loss: 0.1789148462:
57: 86432: loss: 0.1784374886:
57: 89632: loss: 0.1783243198:
57: 92832: loss: 0.1781370138:
57: 96032: loss: 0.1779681312:
57: 99232: loss: 0.1780519355:
57: 102432: loss: 0.1779077537:
57: 105632: loss: 0.1780271116:
57: 108832: loss: 0.1777062460:
57: 112032: loss: 0.1777332047:
57: 115232: loss: 0.1776229580:
57: 118432: loss: 0.1773654476:
57: 121632: loss: 0.1774385660:
57: 124832: loss: 0.1772008415:
57: 128032: loss: 0.1773935893:
57: 131232: loss: 0.1774603217:
57: 134432: loss: 0.1776534066:
57: 137632: loss: 0.1777728062:
57: 140832: loss: 0.1779670333:
57: 144032: loss: 0.1776263242:
57: 147232: loss: 0.1776797702:
57: 150432: loss: 0.1777624184:
Dev-Acc: 57: Accuracy: 0.9330449104: precision: 0.4223257481: recall: 0.4007821799: f1: 0.4112720293
Train-Acc: 57: Accuracy: 0.9332456589: precision: 0.7833053221: recall: 0.4596016041: f1: 0.5793006298
58: 3232: loss: 0.1782212878:
58: 6432: loss: 0.1805029349:
58: 9632: loss: 0.1774771600:
58: 12832: loss: 0.1783683773:
58: 16032: loss: 0.1817104427:
58: 19232: loss: 0.1803409813:
58: 22432: loss: 0.1790882777:
58: 25632: loss: 0.1785467826:
58: 28832: loss: 0.1774841072:
58: 32032: loss: 0.1784623022:
58: 35232: loss: 0.1788808400:
58: 38432: loss: 0.1786633154:
58: 41632: loss: 0.1785819508:
58: 44832: loss: 0.1791701490:
58: 48032: loss: 0.1788418386:
58: 51232: loss: 0.1783461341:
58: 54432: loss: 0.1789708004:
58: 57632: loss: 0.1787007218:
58: 60832: loss: 0.1787460869:
58: 64032: loss: 0.1786101200:
58: 67232: loss: 0.1786806311:
58: 70432: loss: 0.1785862569:
58: 73632: loss: 0.1787476867:
58: 76832: loss: 0.1786266975:
58: 80032: loss: 0.1789333607:
58: 83232: loss: 0.1791114250:
58: 86432: loss: 0.1789352055:
58: 89632: loss: 0.1786793219:
58: 92832: loss: 0.1783577688:
58: 96032: loss: 0.1777313316:
58: 99232: loss: 0.1783528303:
58: 102432: loss: 0.1784047933:
58: 105632: loss: 0.1783260663:
58: 108832: loss: 0.1783377633:
58: 112032: loss: 0.1781492424:
58: 115232: loss: 0.1781916679:
58: 118432: loss: 0.1782382241:
58: 121632: loss: 0.1780451731:
58: 124832: loss: 0.1780723999:
58: 128032: loss: 0.1776988943:
58: 131232: loss: 0.1775264917:
58: 134432: loss: 0.1778193138:
58: 137632: loss: 0.1777493818:
58: 140832: loss: 0.1773460175:
58: 144032: loss: 0.1768951264:
58: 147232: loss: 0.1770665045:
58: 150432: loss: 0.1772603354:
Dev-Acc: 58: Accuracy: 0.9330647588: precision: 0.4228367529: recall: 0.4029926883: f1: 0.4126763016
Train-Acc: 58: Accuracy: 0.9333574176: precision: 0.7827055939: recall: 0.4617710867: f1: 0.5808559024
59: 3232: loss: 0.1756423220:
59: 6432: loss: 0.1761434969:
59: 9632: loss: 0.1801132419:
59: 12832: loss: 0.1788731831:
59: 16032: loss: 0.1784795247:
59: 19232: loss: 0.1778773109:
59: 22432: loss: 0.1791874905:
59: 25632: loss: 0.1786879038:
59: 28832: loss: 0.1806117697:
59: 32032: loss: 0.1811672396:
59: 35232: loss: 0.1809301808:
59: 38432: loss: 0.1803051478:
59: 41632: loss: 0.1801329310:
59: 44832: loss: 0.1795484686:
59: 48032: loss: 0.1787308776:
59: 51232: loss: 0.1785409246:
59: 54432: loss: 0.1773204216:
59: 57632: loss: 0.1774244471:
59: 60832: loss: 0.1776640552:
59: 64032: loss: 0.1776744717:
59: 67232: loss: 0.1778739802:
59: 70432: loss: 0.1781740359:
59: 73632: loss: 0.1784726638:
59: 76832: loss: 0.1777541827:
59: 80032: loss: 0.1774574465:
59: 83232: loss: 0.1775798045:
59: 86432: loss: 0.1777781176:
59: 89632: loss: 0.1775161832:
59: 92832: loss: 0.1776085904:
59: 96032: loss: 0.1776679205:
59: 99232: loss: 0.1777702546:
59: 102432: loss: 0.1777151468:
59: 105632: loss: 0.1774658190:
59: 108832: loss: 0.1775687792:
59: 112032: loss: 0.1769475135:
59: 115232: loss: 0.1768966912:
59: 118432: loss: 0.1771875939:
59: 121632: loss: 0.1767570419:
59: 124832: loss: 0.1768737582:
59: 128032: loss: 0.1768330486:
59: 131232: loss: 0.1769386532:
59: 134432: loss: 0.1767178096:
59: 137632: loss: 0.1765139681:
59: 140832: loss: 0.1763970373:
59: 144032: loss: 0.1767870042:
59: 147232: loss: 0.1769496296:
59: 150432: loss: 0.1770096209:
Dev-Acc: 59: Accuracy: 0.9330151677: precision: 0.4226254002: recall: 0.4040129230: f1: 0.4131096236
Train-Acc: 59: Accuracy: 0.9335283637: precision: 0.7823920266: recall: 0.4644665045: f1: 0.5828967452
60: 3232: loss: 0.1659001774:
60: 6432: loss: 0.1748640273:
60: 9632: loss: 0.1793605794:
60: 12832: loss: 0.1782656535:
60: 16032: loss: 0.1779931480:
60: 19232: loss: 0.1786480103:
60: 22432: loss: 0.1806587367:
60: 25632: loss: 0.1808182231:
60: 28832: loss: 0.1795788892:
60: 32032: loss: 0.1790369620:
60: 35232: loss: 0.1781348920:
60: 38432: loss: 0.1767350288:
60: 41632: loss: 0.1765521590:
60: 44832: loss: 0.1752344409:
60: 48032: loss: 0.1759749886:
60: 51232: loss: 0.1763557785:
60: 54432: loss: 0.1769474996:
60: 57632: loss: 0.1772629230:
60: 60832: loss: 0.1771120300:
60: 64032: loss: 0.1771617814:
60: 67232: loss: 0.1773550934:
60: 70432: loss: 0.1773941333:
60: 73632: loss: 0.1771338155:
60: 76832: loss: 0.1767689343:
60: 80032: loss: 0.1770342291:
60: 83232: loss: 0.1764512093:
60: 86432: loss: 0.1762719533:
60: 89632: loss: 0.1757476787:
60: 92832: loss: 0.1758135713:
60: 96032: loss: 0.1759517130:
60: 99232: loss: 0.1758729062:
60: 102432: loss: 0.1759560594:
60: 105632: loss: 0.1760353802:
60: 108832: loss: 0.1759021982:
60: 112032: loss: 0.1759934111:
60: 115232: loss: 0.1759983387:
60: 118432: loss: 0.1756617792:
60: 121632: loss: 0.1754226819:
60: 124832: loss: 0.1756887813:
60: 128032: loss: 0.1756085502:
60: 131232: loss: 0.1757616559:
60: 134432: loss: 0.1756546054:
60: 137632: loss: 0.1759738083:
60: 140832: loss: 0.1758334890:
60: 144032: loss: 0.1760403688:
60: 147232: loss: 0.1760812891:
60: 150432: loss: 0.1761337322:
Dev-Acc: 60: Accuracy: 0.9328563809: precision: 0.4215929204: recall: 0.4050331576: f1: 0.4131471685
Train-Acc: 60: Accuracy: 0.9337255955: precision: 0.7826757769: recall: 0.4668989547: f1: 0.5848877908
61: 3232: loss: 0.1681038995:
61: 6432: loss: 0.1736998443:
61: 9632: loss: 0.1760902221:
61: 12832: loss: 0.1753293644:
61: 16032: loss: 0.1765259407:
61: 19232: loss: 0.1783455576:
61: 22432: loss: 0.1782880066:
61: 25632: loss: 0.1780448597:
61: 28832: loss: 0.1780950919:
61: 32032: loss: 0.1788187076:
61: 35232: loss: 0.1782837845:
61: 38432: loss: 0.1785285968:
61: 41632: loss: 0.1777145411:
61: 44832: loss: 0.1773067914:
61: 48032: loss: 0.1768526187:
61: 51232: loss: 0.1763905117:
61: 54432: loss: 0.1762275370:
61: 57632: loss: 0.1763834452:
61: 60832: loss: 0.1758978275:
61: 64032: loss: 0.1764641137:
61: 67232: loss: 0.1755897824:
61: 70432: loss: 0.1758216409:
61: 73632: loss: 0.1761177185:
61: 76832: loss: 0.1764249289:
61: 80032: loss: 0.1765398869:
61: 83232: loss: 0.1764671241:
61: 86432: loss: 0.1761266953:
61: 89632: loss: 0.1761941171:
61: 92832: loss: 0.1760197074:
61: 96032: loss: 0.1755971819:
61: 99232: loss: 0.1753904312:
61: 102432: loss: 0.1747759643:
61: 105632: loss: 0.1752356589:
61: 108832: loss: 0.1752760972:
61: 112032: loss: 0.1755731370:
61: 115232: loss: 0.1760077346:
61: 118432: loss: 0.1757571648:
61: 121632: loss: 0.1759685288:
61: 124832: loss: 0.1758174459:
61: 128032: loss: 0.1757100371:
61: 131232: loss: 0.1752126234:
61: 134432: loss: 0.1752707321:
61: 137632: loss: 0.1751152899:
61: 140832: loss: 0.1752684383:
61: 144032: loss: 0.1749916251:
61: 147232: loss: 0.1751102574:
61: 150432: loss: 0.1756183814:
Dev-Acc: 61: Accuracy: 0.9327571988: precision: 0.4210711769: recall: 0.4063934705: f1: 0.4136021459
Train-Acc: 61: Accuracy: 0.9337781668: precision: 0.7824318382: recall: 0.4678850832: f1: 0.5855926276
62: 3232: loss: 0.1658106834:
62: 6432: loss: 0.1682980579:
62: 9632: loss: 0.1664407939:
62: 12832: loss: 0.1665898970:
62: 16032: loss: 0.1683416310:
62: 19232: loss: 0.1684548789:
62: 22432: loss: 0.1703352359:
62: 25632: loss: 0.1715245772:
62: 28832: loss: 0.1705648728:
62: 32032: loss: 0.1719091527:
62: 35232: loss: 0.1718834216:
62: 38432: loss: 0.1725334281:
62: 41632: loss: 0.1735845578:
62: 44832: loss: 0.1747552415:
62: 48032: loss: 0.1740606585:
62: 51232: loss: 0.1743568856:
62: 54432: loss: 0.1750148403:
62: 57632: loss: 0.1743962787:
62: 60832: loss: 0.1745585606:
62: 64032: loss: 0.1746965303:
62: 67232: loss: 0.1747660051:
62: 70432: loss: 0.1745049001:
62: 73632: loss: 0.1745770507:
62: 76832: loss: 0.1750485606:
62: 80032: loss: 0.1748525190:
62: 83232: loss: 0.1749550643:
62: 86432: loss: 0.1751443459:
62: 89632: loss: 0.1749787024:
62: 92832: loss: 0.1748197096:
62: 96032: loss: 0.1746372523:
62: 99232: loss: 0.1745504754:
62: 102432: loss: 0.1746637230:
62: 105632: loss: 0.1747130917:
62: 108832: loss: 0.1748169416:
62: 112032: loss: 0.1750030216:
62: 115232: loss: 0.1747422966:
62: 118432: loss: 0.1747945379:
62: 121632: loss: 0.1747026293:
62: 124832: loss: 0.1745244583:
62: 128032: loss: 0.1747820975:
62: 131232: loss: 0.1752065209:
62: 134432: loss: 0.1748965507:
62: 137632: loss: 0.1746701448:
62: 140832: loss: 0.1745594002:
62: 144032: loss: 0.1746709983:
62: 147232: loss: 0.1746150914:
62: 150432: loss: 0.1747520001:
Dev-Acc: 62: Accuracy: 0.9326976538: precision: 0.4206544687: recall: 0.4065635096: f1: 0.4134889754
Train-Acc: 62: Accuracy: 0.9340016842: precision: 0.7827465559: recall: 0.4706462429: f1: 0.5878392249
63: 3232: loss: 0.1556493300:
63: 6432: loss: 0.1708291787:
63: 9632: loss: 0.1720113376:
63: 12832: loss: 0.1677966280:
63: 16032: loss: 0.1682189573:
63: 19232: loss: 0.1686313593:
63: 22432: loss: 0.1710975008:
63: 25632: loss: 0.1711591411:
63: 28832: loss: 0.1704134058:
63: 32032: loss: 0.1707917382:
63: 35232: loss: 0.1713126050:
63: 38432: loss: 0.1726388175:
63: 41632: loss: 0.1732028155:
63: 44832: loss: 0.1735927271:
63: 48032: loss: 0.1747359489:
63: 51232: loss: 0.1749880946:
63: 54432: loss: 0.1753005188:
63: 57632: loss: 0.1749165215:
63: 60832: loss: 0.1751705391:
63: 64032: loss: 0.1753526664:
63: 67232: loss: 0.1749963081:
63: 70432: loss: 0.1750342509:
63: 73632: loss: 0.1744442808:
63: 76832: loss: 0.1745682307:
63: 80032: loss: 0.1745968833:
63: 83232: loss: 0.1751860340:
63: 86432: loss: 0.1750170934:
63: 89632: loss: 0.1755387389:
63: 92832: loss: 0.1752150236:
63: 96032: loss: 0.1750820643:
63: 99232: loss: 0.1749180663:
63: 102432: loss: 0.1745354903:
63: 105632: loss: 0.1745884193:
63: 108832: loss: 0.1744138737:
63: 112032: loss: 0.1745789181:
63: 115232: loss: 0.1742589021:
63: 118432: loss: 0.1743285337:
63: 121632: loss: 0.1742058755:
63: 124832: loss: 0.1740185277:
63: 128032: loss: 0.1739183920:
63: 131232: loss: 0.1737514340:
63: 134432: loss: 0.1737116794:
63: 137632: loss: 0.1735869738:
63: 140832: loss: 0.1735132368:
63: 144032: loss: 0.1733672924:
63: 147232: loss: 0.1736497783:
63: 150432: loss: 0.1739664713:
Dev-Acc: 63: Accuracy: 0.9326381087: precision: 0.4204346302: recall: 0.4079238225: f1: 0.4140847502
Train-Acc: 63: Accuracy: 0.9341989160: precision: 0.7833950752: recall: 0.4726842417: f1: 0.5896100701
64: 3232: loss: 0.1730744451:
64: 6432: loss: 0.1777199643:
64: 9632: loss: 0.1786425057:
64: 12832: loss: 0.1781100740:
64: 16032: loss: 0.1776338245:
64: 19232: loss: 0.1777969165:
64: 22432: loss: 0.1763710960:
64: 25632: loss: 0.1752198021:
64: 28832: loss: 0.1748576621:
64: 32032: loss: 0.1750356727:
64: 35232: loss: 0.1736268786:
64: 38432: loss: 0.1731384823:
64: 41632: loss: 0.1729536483:
64: 44832: loss: 0.1738527131:
64: 48032: loss: 0.1732190095:
64: 51232: loss: 0.1736875251:
64: 54432: loss: 0.1741621153:
64: 57632: loss: 0.1742659207:
64: 60832: loss: 0.1744272623:
64: 64032: loss: 0.1748176971:
64: 67232: loss: 0.1748741645:
64: 70432: loss: 0.1747822325:
64: 73632: loss: 0.1747207366:
64: 76832: loss: 0.1746606921:
64: 80032: loss: 0.1750290015:
64: 83232: loss: 0.1751462727:
64: 86432: loss: 0.1756244740:
64: 89632: loss: 0.1754435703:
64: 92832: loss: 0.1757949945:
64: 96032: loss: 0.1754106629:
64: 99232: loss: 0.1751736503:
64: 102432: loss: 0.1746333650:
64: 105632: loss: 0.1750984986:
64: 108832: loss: 0.1750803156:
64: 112032: loss: 0.1752332273:
64: 115232: loss: 0.1752036969:
64: 118432: loss: 0.1751315271:
64: 121632: loss: 0.1752781786:
64: 124832: loss: 0.1750720851:
64: 128032: loss: 0.1748327355:
64: 131232: loss: 0.1744237846:
64: 134432: loss: 0.1742469501:
64: 137632: loss: 0.1739960815:
64: 140832: loss: 0.1743020771:
64: 144032: loss: 0.1740466543:
64: 147232: loss: 0.1741112390:
64: 150432: loss: 0.1741426150:
Dev-Acc: 64: Accuracy: 0.9325190187: precision: 0.4196927374: recall: 0.4087740180: f1: 0.4141614265
Train-Acc: 64: Accuracy: 0.9343895912: precision: 0.7842625802: recall: 0.4743935310: f1: 0.5911846633
65: 3232: loss: 0.1715065947:
65: 6432: loss: 0.1746904184:
65: 9632: loss: 0.1779659826:
65: 12832: loss: 0.1778954131:
65: 16032: loss: 0.1743102626:
65: 19232: loss: 0.1725997937:
65: 22432: loss: 0.1729631483:
65: 25632: loss: 0.1713670826:
65: 28832: loss: 0.1712007246:
65: 32032: loss: 0.1721038941:
65: 35232: loss: 0.1730183823:
65: 38432: loss: 0.1727871476:
65: 41632: loss: 0.1725001595:
65: 44832: loss: 0.1723204395:
65: 48032: loss: 0.1725853697:
65: 51232: loss: 0.1719379842:
65: 54432: loss: 0.1723543867:
65: 57632: loss: 0.1728266732:
65: 60832: loss: 0.1728264264:
65: 64032: loss: 0.1728699519:
65: 67232: loss: 0.1729061121:
65: 70432: loss: 0.1731746871:
65: 73632: loss: 0.1730463308:
65: 76832: loss: 0.1731919885:
65: 80032: loss: 0.1736016250:
65: 83232: loss: 0.1732761996:
65: 86432: loss: 0.1729901705:
65: 89632: loss: 0.1731601573:
65: 92832: loss: 0.1726009871:
65: 96032: loss: 0.1728160588:
65: 99232: loss: 0.1729369971:
65: 102432: loss: 0.1726686033:
65: 105632: loss: 0.1722795897:
65: 108832: loss: 0.1720548121:
65: 112032: loss: 0.1722025171:
65: 115232: loss: 0.1719116800:
65: 118432: loss: 0.1720235023:
65: 121632: loss: 0.1722305625:
65: 124832: loss: 0.1724448799:
65: 128032: loss: 0.1724016452:
65: 131232: loss: 0.1725768774:
65: 134432: loss: 0.1732661630:
65: 137632: loss: 0.1731189498:
65: 140832: loss: 0.1731162064:
65: 144032: loss: 0.1730341513:
65: 147232: loss: 0.1733088916:
65: 150432: loss: 0.1733903169:
Dev-Acc: 65: Accuracy: 0.9323900342: precision: 0.4190244749: recall: 0.4104744091: f1: 0.4147053771
Train-Acc: 65: Accuracy: 0.9346328378: precision: 0.7849415837: recall: 0.4770232069: f1: 0.5934164792
66: 3232: loss: 0.1670536270:
66: 6432: loss: 0.1698856482:
66: 9632: loss: 0.1703174984:
66: 12832: loss: 0.1728278754:
66: 16032: loss: 0.1717829079:
66: 19232: loss: 0.1719432136:
66: 22432: loss: 0.1721893825:
66: 25632: loss: 0.1721990951:
66: 28832: loss: 0.1725830181:
66: 32032: loss: 0.1730385935:
66: 35232: loss: 0.1731258654:
66: 38432: loss: 0.1728999204:
66: 41632: loss: 0.1715479851:
66: 44832: loss: 0.1715315355:
66: 48032: loss: 0.1725812313:
66: 51232: loss: 0.1729416371:
66: 54432: loss: 0.1730761774:
66: 57632: loss: 0.1738228688:
66: 60832: loss: 0.1751920969:
66: 64032: loss: 0.1750960384:
66: 67232: loss: 0.1742558112:
66: 70432: loss: 0.1743247907:
66: 73632: loss: 0.1743870040:
66: 76832: loss: 0.1741901921:
66: 80032: loss: 0.1744818648:
66: 83232: loss: 0.1746134023:
66: 86432: loss: 0.1743592941:
66: 89632: loss: 0.1746113687:
66: 92832: loss: 0.1744501081:
66: 96032: loss: 0.1738813057:
66: 99232: loss: 0.1733537572:
66: 102432: loss: 0.1729086739:
66: 105632: loss: 0.1728140365:
66: 108832: loss: 0.1729188483:
66: 112032: loss: 0.1727343665:
66: 115232: loss: 0.1726453682:
66: 118432: loss: 0.1729351581:
66: 121632: loss: 0.1729446698:
66: 124832: loss: 0.1725274286:
66: 128032: loss: 0.1725201337:
66: 131232: loss: 0.1724076758:
66: 134432: loss: 0.1719442509:
66: 137632: loss: 0.1718525167:
66: 140832: loss: 0.1720111462:
66: 144032: loss: 0.1719146980:
66: 147232: loss: 0.1722372731:
66: 150432: loss: 0.1721803980:
Dev-Acc: 66: Accuracy: 0.9323205948: precision: 0.4186006235: recall: 0.4109845264: f1: 0.4147576148
Train-Acc: 66: Accuracy: 0.9347116947: precision: 0.7848511006: recall: 0.4782065610: f1: 0.5943053229
67: 3232: loss: 0.1673034707:
67: 6432: loss: 0.1743352082:
67: 9632: loss: 0.1748019814:
67: 12832: loss: 0.1773177041:
67: 16032: loss: 0.1765594693:
67: 19232: loss: 0.1765314930:
67: 22432: loss: 0.1761594965:
67: 25632: loss: 0.1753880140:
67: 28832: loss: 0.1748139957:
67: 32032: loss: 0.1748604523:
67: 35232: loss: 0.1750147672:
67: 38432: loss: 0.1741708264:
67: 41632: loss: 0.1733982958:
67: 44832: loss: 0.1739306391:
67: 48032: loss: 0.1742261889:
67: 51232: loss: 0.1736170103:
67: 54432: loss: 0.1738956924:
67: 57632: loss: 0.1735039432:
67: 60832: loss: 0.1730446483:
67: 64032: loss: 0.1728982860:
67: 67232: loss: 0.1726644530:
67: 70432: loss: 0.1723350756:
67: 73632: loss: 0.1723500366:
67: 76832: loss: 0.1723223623:
67: 80032: loss: 0.1723330196:
67: 83232: loss: 0.1717266869:
67: 86432: loss: 0.1723569239:
67: 89632: loss: 0.1726860794:
67: 92832: loss: 0.1722837066:
67: 96032: loss: 0.1727704528:
67: 99232: loss: 0.1725530862:
67: 102432: loss: 0.1724336806:
67: 105632: loss: 0.1723584173:
67: 108832: loss: 0.1722672131:
67: 112032: loss: 0.1726281894:
67: 115232: loss: 0.1725629219:
67: 118432: loss: 0.1728501957:
67: 121632: loss: 0.1727946419:
67: 124832: loss: 0.1728404783:
67: 128032: loss: 0.1727015813:
67: 131232: loss: 0.1725546482:
67: 134432: loss: 0.1724715454:
67: 137632: loss: 0.1723154310:
67: 140832: loss: 0.1723158294:
67: 144032: loss: 0.1724444991:
67: 147232: loss: 0.1722079956:
67: 150432: loss: 0.1722547508:
Dev-Acc: 67: Accuracy: 0.9322908521: precision: 0.4183266932: recall: 0.4106444482: f1: 0.4144499743
Train-Acc: 67: Accuracy: 0.9349286556: precision: 0.7856758791: recall: 0.4803103018: f1: 0.5961648307
68: 3232: loss: 0.1610358082:
68: 6432: loss: 0.1692433609:
68: 9632: loss: 0.1705158593:
68: 12832: loss: 0.1667894041:
68: 16032: loss: 0.1680092759:
68: 19232: loss: 0.1696987106:
68: 22432: loss: 0.1691080986:
68: 25632: loss: 0.1710038874:
68: 28832: loss: 0.1707106215:
68: 32032: loss: 0.1709130138:
68: 35232: loss: 0.1703597054:
68: 38432: loss: 0.1692366641:
68: 41632: loss: 0.1682471268:
68: 44832: loss: 0.1695446707:
68: 48032: loss: 0.1696369734:
68: 51232: loss: 0.1697494295:
68: 54432: loss: 0.1707682387:
68: 57632: loss: 0.1703616225:
68: 60832: loss: 0.1707339235:
68: 64032: loss: 0.1706105228:
68: 67232: loss: 0.1712437503:
68: 70432: loss: 0.1712877548:
68: 73632: loss: 0.1719375012:
68: 76832: loss: 0.1717535487:
68: 80032: loss: 0.1713702059:
68: 83232: loss: 0.1712319807:
68: 86432: loss: 0.1709031891:
68: 89632: loss: 0.1713002974:
68: 92832: loss: 0.1713126827:
68: 96032: loss: 0.1711590351:
68: 99232: loss: 0.1709577259:
68: 102432: loss: 0.1709915515:
68: 105632: loss: 0.1711687276:
68: 108832: loss: 0.1716204150:
68: 112032: loss: 0.1715527828:
68: 115232: loss: 0.1716185551:
68: 118432: loss: 0.1715516862:
68: 121632: loss: 0.1714611651:
68: 124832: loss: 0.1710474473:
68: 128032: loss: 0.1707792733:
68: 131232: loss: 0.1709383395:
68: 134432: loss: 0.1710695291:
68: 137632: loss: 0.1710337178:
68: 140832: loss: 0.1711313258:
68: 144032: loss: 0.1710857645:
68: 147232: loss: 0.1713569840:
68: 150432: loss: 0.1714315346:
Dev-Acc: 68: Accuracy: 0.9321221709: precision: 0.4172984149: recall: 0.4118347220: f1: 0.4145485665
Train-Acc: 68: Accuracy: 0.9351324439: precision: 0.7860813704: recall: 0.4826770101: f1: 0.5981019103
69: 3232: loss: 0.1732724004:
69: 6432: loss: 0.1701792107:
69: 9632: loss: 0.1725857699:
69: 12832: loss: 0.1729829617:
69: 16032: loss: 0.1734106511:
69: 19232: loss: 0.1737292634:
69: 22432: loss: 0.1730075652:
69: 25632: loss: 0.1717722994:
69: 28832: loss: 0.1715530075:
69: 32032: loss: 0.1710766653:
69: 35232: loss: 0.1715894907:
69: 38432: loss: 0.1713973017:
69: 41632: loss: 0.1703510866:
69: 44832: loss: 0.1695528703:
69: 48032: loss: 0.1701446229:
69: 51232: loss: 0.1695296839:
69: 54432: loss: 0.1701186981:
69: 57632: loss: 0.1692066814:
69: 60832: loss: 0.1697920661:
69: 64032: loss: 0.1700212752:
69: 67232: loss: 0.1710054399:
69: 70432: loss: 0.1708937109:
69: 73632: loss: 0.1714317570:
69: 76832: loss: 0.1711302878:
69: 80032: loss: 0.1708958907:
69: 83232: loss: 0.1707877164:
69: 86432: loss: 0.1706941479:
69: 89632: loss: 0.1710293226:
69: 92832: loss: 0.1714850631:
69: 96032: loss: 0.1713649495:
69: 99232: loss: 0.1712050718:
69: 102432: loss: 0.1714601935:
69: 105632: loss: 0.1716271501:
69: 108832: loss: 0.1715962804:
69: 112032: loss: 0.1715364267:
69: 115232: loss: 0.1715519157:
69: 118432: loss: 0.1719236105:
69: 121632: loss: 0.1716958476:
69: 124832: loss: 0.1714974288:
69: 128032: loss: 0.1721077601:
69: 131232: loss: 0.1725420570:
69: 134432: loss: 0.1725120666:
69: 137632: loss: 0.1722181584:
69: 140832: loss: 0.1719069691:
69: 144032: loss: 0.1716730500:
69: 147232: loss: 0.1717390011:
69: 150432: loss: 0.1716008529:
Dev-Acc: 69: Accuracy: 0.9320427775: precision: 0.4169811321: recall: 0.4133650740: f1: 0.4151652293
Train-Acc: 69: Accuracy: 0.9352310896: precision: 0.7857523728: recall: 0.4843862994: f1: 0.5993167399
70: 3232: loss: 0.1755663726:
70: 6432: loss: 0.1719778951:
70: 9632: loss: 0.1765087935:
70: 12832: loss: 0.1755244455:
70: 16032: loss: 0.1757551652:
70: 19232: loss: 0.1752519689:
70: 22432: loss: 0.1746547065:
70: 25632: loss: 0.1732978858:
70: 28832: loss: 0.1710332962:
70: 32032: loss: 0.1712205036:
70: 35232: loss: 0.1712844862:
70: 38432: loss: 0.1704287779:
70: 41632: loss: 0.1710730220:
70: 44832: loss: 0.1703279334:
70: 48032: loss: 0.1706007823:
70: 51232: loss: 0.1709256521:
70: 54432: loss: 0.1707015504:
70: 57632: loss: 0.1698470416:
70: 60832: loss: 0.1699151007:
70: 64032: loss: 0.1704956188:
70: 67232: loss: 0.1705505988:
70: 70432: loss: 0.1710098176:
70: 73632: loss: 0.1704782492:
70: 76832: loss: 0.1702940938:
70: 80032: loss: 0.1703291515:
70: 83232: loss: 0.1705839645:
70: 86432: loss: 0.1704203806:
70: 89632: loss: 0.1702278212:
70: 92832: loss: 0.1701365616:
70: 96032: loss: 0.1695703472:
70: 99232: loss: 0.1695997408:
70: 102432: loss: 0.1696823494:
70: 105632: loss: 0.1694260788:
70: 108832: loss: 0.1692486624:
70: 112032: loss: 0.1692034495:
70: 115232: loss: 0.1694587736:
70: 118432: loss: 0.1696251727:
70: 121632: loss: 0.1696822035:
70: 124832: loss: 0.1700355867:
70: 128032: loss: 0.1698729607:
70: 131232: loss: 0.1703604342:
70: 134432: loss: 0.1703325598:
70: 137632: loss: 0.1701809801:
70: 140832: loss: 0.1698864667:
70: 144032: loss: 0.1698706789:
70: 147232: loss: 0.1699692090:
70: 150432: loss: 0.1701517046:
Dev-Acc: 70: Accuracy: 0.9318542480: precision: 0.4157130658: recall: 0.4138751913: f1: 0.4147920927
Train-Acc: 70: Accuracy: 0.9354611635: precision: 0.7861234882: recall: 0.4871474591: f1: 0.6015342777
71: 3232: loss: 0.1737770541:
71: 6432: loss: 0.1692443347:
71: 9632: loss: 0.1707827860:
71: 12832: loss: 0.1703576453:
71: 16032: loss: 0.1680445451:
71: 19232: loss: 0.1693282326:
71: 22432: loss: 0.1687940789:
71: 25632: loss: 0.1694684811:
71: 28832: loss: 0.1693889379:
71: 32032: loss: 0.1717605046:
71: 35232: loss: 0.1721211677:
71: 38432: loss: 0.1721563972:
71: 41632: loss: 0.1727001067:
71: 44832: loss: 0.1723121497:
71: 48032: loss: 0.1719926264:
71: 51232: loss: 0.1718821523:
71: 54432: loss: 0.1718750595:
71: 57632: loss: 0.1716842548:
71: 60832: loss: 0.1711199207:
71: 64032: loss: 0.1711818112:
71: 67232: loss: 0.1714940965:
71: 70432: loss: 0.1713271374:
71: 73632: loss: 0.1712175847:
71: 76832: loss: 0.1712205605:
71: 80032: loss: 0.1710698611:
71: 83232: loss: 0.1719343743:
71: 86432: loss: 0.1718594408:
71: 89632: loss: 0.1720658981:
71: 92832: loss: 0.1713959314:
71: 96032: loss: 0.1716262438:
71: 99232: loss: 0.1715079020:
71: 102432: loss: 0.1711991853:
71: 105632: loss: 0.1711829489:
71: 108832: loss: 0.1709734885:
71: 112032: loss: 0.1710828235:
71: 115232: loss: 0.1709465164:
71: 118432: loss: 0.1711168558:
71: 121632: loss: 0.1712880060:
71: 124832: loss: 0.1712995665:
71: 128032: loss: 0.1708336951:
71: 131232: loss: 0.1708002559:
71: 134432: loss: 0.1707411786:
71: 137632: loss: 0.1704732682:
71: 140832: loss: 0.1704443554:
71: 144032: loss: 0.1702808572:
71: 147232: loss: 0.1704784242:
71: 150432: loss: 0.1705867109:
Dev-Acc: 71: Accuracy: 0.9315863252: precision: 0.4138341264: recall: 0.4140452304: f1: 0.4139396515
Train-Acc: 71: Accuracy: 0.9356386662: precision: 0.7868557519: recall: 0.4887910065: f1: 0.6030008110
72: 3232: loss: 0.1678854183:
72: 6432: loss: 0.1712516091:
72: 9632: loss: 0.1697359210:
72: 12832: loss: 0.1680761285:
72: 16032: loss: 0.1686692996:
72: 19232: loss: 0.1727138550:
72: 22432: loss: 0.1719299169:
72: 25632: loss: 0.1716211177:
72: 28832: loss: 0.1707589657:
72: 32032: loss: 0.1688435461:
72: 35232: loss: 0.1677671846:
72: 38432: loss: 0.1677202615:
72: 41632: loss: 0.1665984980:
72: 44832: loss: 0.1667410974:
72: 48032: loss: 0.1666062020:
72: 51232: loss: 0.1671580149:
72: 54432: loss: 0.1676904525:
72: 57632: loss: 0.1672723285:
72: 60832: loss: 0.1680427409:
72: 64032: loss: 0.1679316667:
72: 67232: loss: 0.1679655199:
72: 70432: loss: 0.1686676418:
72: 73632: loss: 0.1687732846:
72: 76832: loss: 0.1682170383:
72: 80032: loss: 0.1682143903:
72: 83232: loss: 0.1682886994:
72: 86432: loss: 0.1687246422:
72: 89632: loss: 0.1694992888:
72: 92832: loss: 0.1698466298:
72: 96032: loss: 0.1699001761:
72: 99232: loss: 0.1702807610:
72: 102432: loss: 0.1706281106:
72: 105632: loss: 0.1708684154:
72: 108832: loss: 0.1703883106:
72: 112032: loss: 0.1705194570:
72: 115232: loss: 0.1702718029:
72: 118432: loss: 0.1704786110:
72: 121632: loss: 0.1707539487:
72: 124832: loss: 0.1707676259:
72: 128032: loss: 0.1704245945:
72: 131232: loss: 0.1700402919:
72: 134432: loss: 0.1698463534:
72: 137632: loss: 0.1700750133:
72: 140832: loss: 0.1701485133:
72: 144032: loss: 0.1697494115:
72: 147232: loss: 0.1697959754:
72: 150432: loss: 0.1698224033:
Dev-Acc: 72: Accuracy: 0.9314374924: precision: 0.4128704488: recall: 0.4145553477: f1: 0.4137111828
Train-Acc: 72: Accuracy: 0.9357898831: precision: 0.7872520051: recall: 0.4904345539: f1: 0.6043666707
73: 3232: loss: 0.1713305071:
73: 6432: loss: 0.1702719362:
73: 9632: loss: 0.1694283119:
73: 12832: loss: 0.1670422064:
73: 16032: loss: 0.1646119508:
73: 19232: loss: 0.1665065348:
73: 22432: loss: 0.1680810718:
73: 25632: loss: 0.1683110943:
73: 28832: loss: 0.1686972069:
73: 32032: loss: 0.1694430250:
73: 35232: loss: 0.1689791877:
73: 38432: loss: 0.1695377485:
73: 41632: loss: 0.1694395270:
73: 44832: loss: 0.1692493382:
73: 48032: loss: 0.1689416735:
73: 51232: loss: 0.1695144763:
73: 54432: loss: 0.1696759500:
73: 57632: loss: 0.1700340367:
73: 60832: loss: 0.1696546414:
73: 64032: loss: 0.1695903286:
73: 67232: loss: 0.1696575179:
73: 70432: loss: 0.1690985574:
73: 73632: loss: 0.1695090875:
73: 76832: loss: 0.1695880722:
73: 80032: loss: 0.1697072141:
73: 83232: loss: 0.1696176208:
73: 86432: loss: 0.1689798522:
73: 89632: loss: 0.1690712635:
73: 92832: loss: 0.1691622392:
73: 96032: loss: 0.1693639102:
73: 99232: loss: 0.1696204019:
73: 102432: loss: 0.1697561382:
73: 105632: loss: 0.1696243888:
73: 108832: loss: 0.1696197542:
73: 112032: loss: 0.1696820935:
73: 115232: loss: 0.1700971126:
73: 118432: loss: 0.1701631012:
73: 121632: loss: 0.1702178413:
73: 124832: loss: 0.1700486848:
73: 128032: loss: 0.1697124216:
73: 131232: loss: 0.1698256174:
73: 134432: loss: 0.1695430064:
73: 137632: loss: 0.1694727214:
73: 140832: loss: 0.1696091172:
73: 144032: loss: 0.1697637851:
73: 147232: loss: 0.1696575876:
73: 150432: loss: 0.1698087353:
Dev-Acc: 73: Accuracy: 0.9313283563: precision: 0.4122510969: recall: 0.4154055433: f1: 0.4138223088
Train-Acc: 73: Accuracy: 0.9358621836: precision: 0.7866526537: recall: 0.4920781014: f1: 0.6054355739
74: 3232: loss: 0.1455513564:
74: 6432: loss: 0.1611464596:
74: 9632: loss: 0.1672803532:
74: 12832: loss: 0.1692826889:
74: 16032: loss: 0.1650169067:
74: 19232: loss: 0.1651147677:
74: 22432: loss: 0.1672549422:
74: 25632: loss: 0.1686396718:
74: 28832: loss: 0.1721453459:
74: 32032: loss: 0.1714172755:
74: 35232: loss: 0.1715192354:
74: 38432: loss: 0.1710910982:
74: 41632: loss: 0.1704992097:
74: 44832: loss: 0.1703396860:
74: 48032: loss: 0.1701895533:
74: 51232: loss: 0.1696193899:
74: 54432: loss: 0.1701485111:
74: 57632: loss: 0.1697815250:
74: 60832: loss: 0.1694756234:
74: 64032: loss: 0.1698686113:
74: 67232: loss: 0.1694641448:
74: 70432: loss: 0.1696394001:
74: 73632: loss: 0.1689026068:
74: 76832: loss: 0.1693730919:
74: 80032: loss: 0.1696636865:
74: 83232: loss: 0.1697985644:
74: 86432: loss: 0.1698457311:
74: 89632: loss: 0.1688910786:
74: 92832: loss: 0.1687830993:
74: 96032: loss: 0.1685968205:
74: 99232: loss: 0.1687699468:
74: 102432: loss: 0.1685949517:
74: 105632: loss: 0.1691344447:
74: 108832: loss: 0.1690477491:
74: 112032: loss: 0.1693023005:
74: 115232: loss: 0.1688130434:
74: 118432: loss: 0.1686033824:
74: 121632: loss: 0.1682804532:
74: 124832: loss: 0.1680171747:
74: 128032: loss: 0.1682338372:
74: 131232: loss: 0.1682458291:
74: 134432: loss: 0.1684418992:
74: 137632: loss: 0.1684076628:
74: 140832: loss: 0.1688035328:
74: 144032: loss: 0.1686841444:
74: 147232: loss: 0.1688949843:
74: 150432: loss: 0.1689696539:
Dev-Acc: 74: Accuracy: 0.9312589169: precision: 0.4117944398: recall: 0.4155755824: f1: 0.4136763710
Train-Acc: 74: Accuracy: 0.9360199571: precision: 0.7873099109: recall: 0.4935244231: f1: 0.6067243191
75: 3232: loss: 0.1828219414:
75: 6432: loss: 0.1731265836:
75: 9632: loss: 0.1677785272:
75: 12832: loss: 0.1689496080:
75: 16032: loss: 0.1683276242:
75: 19232: loss: 0.1671706252:
75: 22432: loss: 0.1659788604:
75: 25632: loss: 0.1661314808:
75: 28832: loss: 0.1650249820:
75: 32032: loss: 0.1640921580:
75: 35232: loss: 0.1651561847:
75: 38432: loss: 0.1653633492:
75: 41632: loss: 0.1656616884:
75: 44832: loss: 0.1667726127:
75: 48032: loss: 0.1665205549:
75: 51232: loss: 0.1673434821:
75: 54432: loss: 0.1674353941:
75: 57632: loss: 0.1674992669:
75: 60832: loss: 0.1684658973:
75: 64032: loss: 0.1692180386:
75: 67232: loss: 0.1686463386:
75: 70432: loss: 0.1684081970:
75: 73632: loss: 0.1684436740:
75: 76832: loss: 0.1687966815:
75: 80032: loss: 0.1689755390:
75: 83232: loss: 0.1687367048:
75: 86432: loss: 0.1687410999:
75: 89632: loss: 0.1687340180:
75: 92832: loss: 0.1691083908:
75: 96032: loss: 0.1692328468:
75: 99232: loss: 0.1693065399:
75: 102432: loss: 0.1693282663:
75: 105632: loss: 0.1690504016:
75: 108832: loss: 0.1686101725:
75: 112032: loss: 0.1685609186:
75: 115232: loss: 0.1681662286:
75: 118432: loss: 0.1680187418:
75: 121632: loss: 0.1679981835:
75: 124832: loss: 0.1677938714:
75: 128032: loss: 0.1678910185:
75: 131232: loss: 0.1684034582:
75: 134432: loss: 0.1682626342:
75: 137632: loss: 0.1681620698:
75: 140832: loss: 0.1678995186:
75: 144032: loss: 0.1675426963:
75: 147232: loss: 0.1673271846:
75: 150432: loss: 0.1677135031:
Dev-Acc: 75: Accuracy: 0.9311795235: precision: 0.4115378165: recall: 0.4172759735: f1: 0.4143870314
Train-Acc: 75: Accuracy: 0.9361711740: precision: 0.7878819590: recall: 0.4949707449: f1: 0.6079864336
76: 3232: loss: 0.1732168086:
76: 6432: loss: 0.1713386255:
76: 9632: loss: 0.1715023893:
76: 12832: loss: 0.1711926636:
76: 16032: loss: 0.1680110038:
76: 19232: loss: 0.1657275709:
76: 22432: loss: 0.1683966338:
76: 25632: loss: 0.1657922169:
76: 28832: loss: 0.1653421638:
76: 32032: loss: 0.1648749717:
76: 35232: loss: 0.1652598509:
76: 38432: loss: 0.1645829597:
76: 41632: loss: 0.1642067767:
76: 44832: loss: 0.1655461335:
76: 48032: loss: 0.1652584202:
76: 51232: loss: 0.1649657942:
76: 54432: loss: 0.1654141964:
76: 57632: loss: 0.1658973740:
76: 60832: loss: 0.1657202872:
76: 64032: loss: 0.1668062770:
76: 67232: loss: 0.1672714467:
76: 70432: loss: 0.1675087641:
76: 73632: loss: 0.1677524997:
76: 76832: loss: 0.1680147871:
76: 80032: loss: 0.1680962980:
76: 83232: loss: 0.1683337671:
76: 86432: loss: 0.1681646074:
76: 89632: loss: 0.1681178531:
76: 92832: loss: 0.1680227872:
76: 96032: loss: 0.1678254949:
76: 99232: loss: 0.1676775670:
76: 102432: loss: 0.1672156899:
76: 105632: loss: 0.1671976534:
76: 108832: loss: 0.1669742757:
76: 112032: loss: 0.1670174566:
76: 115232: loss: 0.1672883053:
76: 118432: loss: 0.1670329364:
76: 121632: loss: 0.1669839711:
76: 124832: loss: 0.1671899856:
76: 128032: loss: 0.1672005428:
76: 131232: loss: 0.1672556923:
76: 134432: loss: 0.1670223618:
76: 137632: loss: 0.1669794954:
76: 140832: loss: 0.1669703452:
76: 144032: loss: 0.1672380011:
76: 147232: loss: 0.1673994714:
76: 150432: loss: 0.1674944617:
Dev-Acc: 76: Accuracy: 0.9310902357: precision: 0.4112445779: recall: 0.4191464037: f1: 0.4151578947
Train-Acc: 76: Accuracy: 0.9362698197: precision: 0.7880338311: recall: 0.4961540990: f1: 0.6089236727
77: 3232: loss: 0.1675854565:
77: 6432: loss: 0.1652116053:
77: 9632: loss: 0.1655322344:
77: 12832: loss: 0.1642314614:
77: 16032: loss: 0.1645040390:
77: 19232: loss: 0.1666314211:
77: 22432: loss: 0.1644894563:
77: 25632: loss: 0.1638277226:
77: 28832: loss: 0.1663871145:
77: 32032: loss: 0.1666829072:
77: 35232: loss: 0.1667375203:
77: 38432: loss: 0.1675040428:
77: 41632: loss: 0.1683747589:
77: 44832: loss: 0.1683439119:
77: 48032: loss: 0.1676644846:
77: 51232: loss: 0.1669615182:
77: 54432: loss: 0.1658515640:
77: 57632: loss: 0.1665167439:
77: 60832: loss: 0.1665120156:
77: 64032: loss: 0.1665325199:
77: 67232: loss: 0.1671710209:
77: 70432: loss: 0.1671344462:
77: 73632: loss: 0.1670677155:
77: 76832: loss: 0.1677562011:
77: 80032: loss: 0.1679101117:
77: 83232: loss: 0.1675770306:
77: 86432: loss: 0.1676280872:
77: 89632: loss: 0.1674288186:
77: 92832: loss: 0.1672984047:
77: 96032: loss: 0.1673770863:
77: 99232: loss: 0.1673243196:
77: 102432: loss: 0.1672482574:
77: 105632: loss: 0.1670967407:
77: 108832: loss: 0.1668169087:
77: 112032: loss: 0.1669175913:
77: 115232: loss: 0.1671688559:
77: 118432: loss: 0.1670423591:
77: 121632: loss: 0.1666881950:
77: 124832: loss: 0.1669030460:
77: 128032: loss: 0.1669683643:
77: 131232: loss: 0.1673610667:
77: 134432: loss: 0.1675716169:
77: 137632: loss: 0.1670250432:
77: 140832: loss: 0.1670501413:
77: 144032: loss: 0.1668907997:
77: 147232: loss: 0.1670087853:
77: 150432: loss: 0.1671738072:
Dev-Acc: 77: Accuracy: 0.9310803413: precision: 0.4113829256: recall: 0.4203366774: f1: 0.4158116064
Train-Acc: 77: Accuracy: 0.9363618493: precision: 0.7879229568: recall: 0.4975346789: f1: 0.6099290780
78: 3232: loss: 0.1685972556:
78: 6432: loss: 0.1659060523:
78: 9632: loss: 0.1648251973:
78: 12832: loss: 0.1697458332:
78: 16032: loss: 0.1729366810:
78: 19232: loss: 0.1718823628:
78: 22432: loss: 0.1711333213:
78: 25632: loss: 0.1711184992:
78: 28832: loss: 0.1714789644:
78: 32032: loss: 0.1715888625:
78: 35232: loss: 0.1703536543:
78: 38432: loss: 0.1697045340:
78: 41632: loss: 0.1697676497:
78: 44832: loss: 0.1694728645:
78: 48032: loss: 0.1699854771:
78: 51232: loss: 0.1702072947:
78: 54432: loss: 0.1699051577:
78: 57632: loss: 0.1700987261:
78: 60832: loss: 0.1697902620:
78: 64032: loss: 0.1685527013:
78: 67232: loss: 0.1683842404:
78: 70432: loss: 0.1681276910:
78: 73632: loss: 0.1678139263:
78: 76832: loss: 0.1679658576:
78: 80032: loss: 0.1676108802:
78: 83232: loss: 0.1673928086:
78: 86432: loss: 0.1672957223:
78: 89632: loss: 0.1669922842:
78: 92832: loss: 0.1665297427:
78: 96032: loss: 0.1670405920:
78: 99232: loss: 0.1666984032:
78: 102432: loss: 0.1665914428:
78: 105632: loss: 0.1664215001:
78: 108832: loss: 0.1665539742:
78: 112032: loss: 0.1666085526:
78: 115232: loss: 0.1666692666:
78: 118432: loss: 0.1664484025:
78: 121632: loss: 0.1663465933:
78: 124832: loss: 0.1663666762:
78: 128032: loss: 0.1664916437:
78: 131232: loss: 0.1665315173:
78: 134432: loss: 0.1667841264:
78: 137632: loss: 0.1669733330:
78: 140832: loss: 0.1671792248:
78: 144032: loss: 0.1669958753:
78: 147232: loss: 0.1668240988:
78: 150432: loss: 0.1666432198:
Dev-Acc: 78: Accuracy: 0.9309909940: precision: 0.4107676969: recall: 0.4203366774: f1: 0.4154971006
Train-Acc: 78: Accuracy: 0.9365130663: precision: 0.7888495943: recall: 0.4985865492: f1: 0.6109969789
79: 3232: loss: 0.1706008073:
79: 6432: loss: 0.1705708794:
79: 9632: loss: 0.1645864590:
79: 12832: loss: 0.1635923646:
79: 16032: loss: 0.1648665445:
79: 19232: loss: 0.1660921726:
79: 22432: loss: 0.1660023072:
79: 25632: loss: 0.1652550253:
79: 28832: loss: 0.1671889833:
79: 32032: loss: 0.1679509703:
79: 35232: loss: 0.1685771597:
79: 38432: loss: 0.1686614561:
79: 41632: loss: 0.1678866592:
79: 44832: loss: 0.1677629208:
79: 48032: loss: 0.1683222804:
79: 51232: loss: 0.1676143486:
79: 54432: loss: 0.1673835597:
79: 57632: loss: 0.1675715665:
79: 60832: loss: 0.1681578996:
79: 64032: loss: 0.1672788564:
79: 67232: loss: 0.1670041547:
79: 70432: loss: 0.1664671578:
79: 73632: loss: 0.1661365776:
79: 76832: loss: 0.1661417418:
79: 80032: loss: 0.1660468561:
79: 83232: loss: 0.1655449157:
79: 86432: loss: 0.1655666122:
79: 89632: loss: 0.1653502190:
79: 92832: loss: 0.1652009885:
79: 96032: loss: 0.1655148357:
79: 99232: loss: 0.1656313610:
79: 102432: loss: 0.1659985352:
79: 105632: loss: 0.1661816095:
79: 108832: loss: 0.1662039410:
79: 112032: loss: 0.1660304958:
79: 115232: loss: 0.1663152576:
79: 118432: loss: 0.1662360875:
79: 121632: loss: 0.1664219161:
79: 124832: loss: 0.1662375723:
79: 128032: loss: 0.1666338151:
79: 131232: loss: 0.1670238239:
79: 134432: loss: 0.1668280380:
79: 137632: loss: 0.1669508576:
79: 140832: loss: 0.1668805352:
79: 144032: loss: 0.1667309918:
79: 147232: loss: 0.1666820666:
79: 150432: loss: 0.1663926917:
Dev-Acc: 79: Accuracy: 0.9308620095: precision: 0.4100016559: recall: 0.4210168339: f1: 0.4154362416
Train-Acc: 79: Accuracy: 0.9366905689: precision: 0.7893808981: recall: 0.5004273223: f1: 0.6125372173
80: 3232: loss: 0.1493528226:
80: 6432: loss: 0.1678985563:
80: 9632: loss: 0.1605752057:
80: 12832: loss: 0.1623940988:
80: 16032: loss: 0.1647795289:
80: 19232: loss: 0.1640420913:
80: 22432: loss: 0.1633222426:
80: 25632: loss: 0.1623415336:
80: 28832: loss: 0.1628971424:
80: 32032: loss: 0.1633896724:
80: 35232: loss: 0.1631408180:
80: 38432: loss: 0.1644182883:
80: 41632: loss: 0.1648969917:
80: 44832: loss: 0.1652782707:
80: 48032: loss: 0.1657158075:
80: 51232: loss: 0.1661870720:
80: 54432: loss: 0.1659224422:
80: 57632: loss: 0.1658347182:
80: 60832: loss: 0.1663320603:
80: 64032: loss: 0.1662605781:
80: 67232: loss: 0.1663487386:
80: 70432: loss: 0.1655547900:
80: 73632: loss: 0.1660963561:
80: 76832: loss: 0.1666972595:
80: 80032: loss: 0.1667618556:
80: 83232: loss: 0.1670519493:
80: 86432: loss: 0.1671020670:
80: 89632: loss: 0.1669028247:
80: 92832: loss: 0.1668581237:
80: 96032: loss: 0.1664394467:
80: 99232: loss: 0.1659706519:
80: 102432: loss: 0.1659509017:
80: 105632: loss: 0.1660199720:
80: 108832: loss: 0.1655348476:
80: 112032: loss: 0.1653607906:
80: 115232: loss: 0.1656575033:
80: 118432: loss: 0.1659703015:
80: 121632: loss: 0.1661813145:
80: 124832: loss: 0.1658683445:
80: 128032: loss: 0.1660301977:
80: 131232: loss: 0.1659890107:
80: 134432: loss: 0.1659353693:
80: 137632: loss: 0.1658206352:
80: 140832: loss: 0.1659302980:
80: 144032: loss: 0.1659581471:
80: 147232: loss: 0.1659393961:
80: 150432: loss: 0.1660965516:
Dev-Acc: 80: Accuracy: 0.9307231307: precision: 0.4092033647: recall: 0.4218670294: f1: 0.4154387140
Train-Acc: 80: Accuracy: 0.9367694259: precision: 0.7895227249: recall: 0.5013477089: f1: 0.6132689988
81: 3232: loss: 0.1794307484:
81: 6432: loss: 0.1772855150:
81: 9632: loss: 0.1720448456:
81: 12832: loss: 0.1692483316:
81: 16032: loss: 0.1670481033:
81: 19232: loss: 0.1680536404:
81: 22432: loss: 0.1670910911:
81: 25632: loss: 0.1678240437:
81: 28832: loss: 0.1663349859:
81: 32032: loss: 0.1663160092:
81: 35232: loss: 0.1666843969:
81: 38432: loss: 0.1660617919:
81: 41632: loss: 0.1664443392:
81: 44832: loss: 0.1662355203:
81: 48032: loss: 0.1665559576:
81: 51232: loss: 0.1648571194:
81: 54432: loss: 0.1650927423:
81: 57632: loss: 0.1648827979:
81: 60832: loss: 0.1649656734:
81: 64032: loss: 0.1649874910:
81: 67232: loss: 0.1644281501:
81: 70432: loss: 0.1653862809:
81: 73632: loss: 0.1648723127:
81: 76832: loss: 0.1652637595:
81: 80032: loss: 0.1648777772:
81: 83232: loss: 0.1649027731:
81: 86432: loss: 0.1647505706:
81: 89632: loss: 0.1644200829:
81: 92832: loss: 0.1642143252:
81: 96032: loss: 0.1641901301:
81: 99232: loss: 0.1644535657:
81: 102432: loss: 0.1648526876:
81: 105632: loss: 0.1647059143:
81: 108832: loss: 0.1646661883:
81: 112032: loss: 0.1646749141:
81: 115232: loss: 0.1648146048:
81: 118432: loss: 0.1650321980:
81: 121632: loss: 0.1649252620:
81: 124832: loss: 0.1651117766:
81: 128032: loss: 0.1653260829:
81: 131232: loss: 0.1653748983:
81: 134432: loss: 0.1655696075:
81: 137632: loss: 0.1653672044:
81: 140832: loss: 0.1653645455:
81: 144032: loss: 0.1654941941:
81: 147232: loss: 0.1657541471:
81: 150432: loss: 0.1656589691:
Dev-Acc: 81: Accuracy: 0.9307528734: precision: 0.4094656992: recall: 0.4222071076: f1: 0.4157388028
Train-Acc: 81: Accuracy: 0.9369338155: precision: 0.7903659293: recall: 0.5026625468: f1: 0.6145067310
82: 3232: loss: 0.1683302996:
82: 6432: loss: 0.1615716438:
82: 9632: loss: 0.1571399437:
82: 12832: loss: 0.1553476415:
82: 16032: loss: 0.1578930872:
82: 19232: loss: 0.1567319910:
82: 22432: loss: 0.1597021994:
82: 25632: loss: 0.1620723948:
82: 28832: loss: 0.1620839447:
82: 32032: loss: 0.1626909394:
82: 35232: loss: 0.1609491668:
82: 38432: loss: 0.1609108782:
82: 41632: loss: 0.1613422262:
82: 44832: loss: 0.1623714349:
82: 48032: loss: 0.1633842475:
82: 51232: loss: 0.1634983663:
82: 54432: loss: 0.1636426130:
82: 57632: loss: 0.1629074869:
82: 60832: loss: 0.1633470505:
82: 64032: loss: 0.1633561360:
82: 67232: loss: 0.1641577399:
82: 70432: loss: 0.1638243427:
82: 73632: loss: 0.1644827047:
82: 76832: loss: 0.1647133286:
82: 80032: loss: 0.1646494453:
82: 83232: loss: 0.1647432802:
82: 86432: loss: 0.1650813319:
82: 89632: loss: 0.1649215964:
82: 92832: loss: 0.1650344091:
82: 96032: loss: 0.1646072114:
82: 99232: loss: 0.1647559100:
82: 102432: loss: 0.1644406199:
82: 105632: loss: 0.1646124616:
82: 108832: loss: 0.1644139191:
82: 112032: loss: 0.1640223207:
82: 115232: loss: 0.1638261390:
82: 118432: loss: 0.1641834763:
82: 121632: loss: 0.1642379666:
82: 124832: loss: 0.1643024534:
82: 128032: loss: 0.1644362603:
82: 131232: loss: 0.1643589378:
82: 134432: loss: 0.1645811225:
82: 137632: loss: 0.1647284578:
82: 140832: loss: 0.1648352758:
82: 144032: loss: 0.1648274972:
82: 147232: loss: 0.1649063918:
82: 150432: loss: 0.1647631919:
Dev-Acc: 82: Accuracy: 0.9307131767: precision: 0.4093153390: recall: 0.4228872641: f1: 0.4159906331
Train-Acc: 82: Accuracy: 0.9369732141: precision: 0.7900165017: recall: 0.5035829334: f1: 0.6150881278
83: 3232: loss: 0.1840667044:
83: 6432: loss: 0.1745017893:
83: 9632: loss: 0.1761805050:
83: 12832: loss: 0.1739532478:
83: 16032: loss: 0.1709100415:
83: 19232: loss: 0.1691604657:
83: 22432: loss: 0.1676188285:
83: 25632: loss: 0.1657574061:
83: 28832: loss: 0.1656478201:
83: 32032: loss: 0.1647372105:
83: 35232: loss: 0.1629240688:
83: 38432: loss: 0.1634264842:
83: 41632: loss: 0.1632750061:
83: 44832: loss: 0.1635488697:
83: 48032: loss: 0.1634541753:
83: 51232: loss: 0.1644910856:
83: 54432: loss: 0.1641493913:
83: 57632: loss: 0.1645807446:
83: 60832: loss: 0.1646261593:
83: 64032: loss: 0.1649194528:
83: 67232: loss: 0.1641544325:
83: 70432: loss: 0.1644258025:
83: 73632: loss: 0.1652393769:
83: 76832: loss: 0.1648990564:
83: 80032: loss: 0.1652658474:
83: 83232: loss: 0.1654352754:
83: 86432: loss: 0.1655661608:
83: 89632: loss: 0.1651346597:
83: 92832: loss: 0.1651112599:
83: 96032: loss: 0.1652782163:
83: 99232: loss: 0.1654106834:
83: 102432: loss: 0.1651436828:
83: 105632: loss: 0.1647338255:
83: 108832: loss: 0.1649761183:
83: 112032: loss: 0.1650219012:
83: 115232: loss: 0.1650772641:
83: 118432: loss: 0.1651564021:
83: 121632: loss: 0.1650948137:
83: 124832: loss: 0.1653393738:
83: 128032: loss: 0.1649859723:
83: 131232: loss: 0.1648054987:
83: 134432: loss: 0.1649416998:
83: 137632: loss: 0.1651748754:
83: 140832: loss: 0.1652944169:
83: 144032: loss: 0.1652753711:
83: 147232: loss: 0.1650915970:
83: 150432: loss: 0.1648070174:
Dev-Acc: 83: Accuracy: 0.9305444956: precision: 0.4082937223: recall: 0.4235674205: f1: 0.4157903522
Train-Acc: 83: Accuracy: 0.9371836185: precision: 0.7907670162: recall: 0.5056209322: f1: 0.6168344227
84: 3232: loss: 0.1570744618:
84: 6432: loss: 0.1598198205:
84: 9632: loss: 0.1633126036:
84: 12832: loss: 0.1657893653:
84: 16032: loss: 0.1652578900:
84: 19232: loss: 0.1643481871:
84: 22432: loss: 0.1674296607:
84: 25632: loss: 0.1681338109:
84: 28832: loss: 0.1686585911:
84: 32032: loss: 0.1686948093:
84: 35232: loss: 0.1681383621:
84: 38432: loss: 0.1681126180:
84: 41632: loss: 0.1673763954:
84: 44832: loss: 0.1666218614:
84: 48032: loss: 0.1670366280:
84: 51232: loss: 0.1662948232:
84: 54432: loss: 0.1652784826:
84: 57632: loss: 0.1655958202:
84: 60832: loss: 0.1650799330:
84: 64032: loss: 0.1647954953:
84: 67232: loss: 0.1644054037:
84: 70432: loss: 0.1639435064:
84: 73632: loss: 0.1634923988:
84: 76832: loss: 0.1635589464:
84: 80032: loss: 0.1635435814:
84: 83232: loss: 0.1637923773:
84: 86432: loss: 0.1638481502:
84: 89632: loss: 0.1637155209:
84: 92832: loss: 0.1635476135:
84: 96032: loss: 0.1637402007:
84: 99232: loss: 0.1636230202:
84: 102432: loss: 0.1636235802:
84: 105632: loss: 0.1635941844:
84: 108832: loss: 0.1630445234:
84: 112032: loss: 0.1631852686:
84: 115232: loss: 0.1634987503:
84: 118432: loss: 0.1636832936:
84: 121632: loss: 0.1638467554:
84: 124832: loss: 0.1636657326:
84: 128032: loss: 0.1636794302:
84: 131232: loss: 0.1637660881:
84: 134432: loss: 0.1638504892:
84: 137632: loss: 0.1640693648:
84: 140832: loss: 0.1640875795:
84: 144032: loss: 0.1642479517:
84: 147232: loss: 0.1644064448:
84: 150432: loss: 0.1640751559:
Dev-Acc: 84: Accuracy: 0.9303560257: precision: 0.4069349035: recall: 0.4230573032: f1: 0.4148395165
Train-Acc: 84: Accuracy: 0.9374926090: precision: 0.7924315455: recall: 0.5079876405: f1: 0.6191010336
85: 3232: loss: 0.1588222665:
85: 6432: loss: 0.1628982118:
85: 9632: loss: 0.1641954526:
85: 12832: loss: 0.1686743962:
85: 16032: loss: 0.1709515326:
85: 19232: loss: 0.1687092927:
85: 22432: loss: 0.1681669824:
85: 25632: loss: 0.1677256867:
85: 28832: loss: 0.1676141502:
85: 32032: loss: 0.1671268535:
85: 35232: loss: 0.1672760503:
85: 38432: loss: 0.1667841988:
85: 41632: loss: 0.1669632358:
85: 44832: loss: 0.1673092010:
85: 48032: loss: 0.1665377763:
85: 51232: loss: 0.1677233069:
85: 54432: loss: 0.1670877379:
85: 57632: loss: 0.1663967828:
85: 60832: loss: 0.1659333679:
85: 64032: loss: 0.1662908515:
85: 67232: loss: 0.1657061398:
85: 70432: loss: 0.1656492843:
85: 73632: loss: 0.1652127553:
85: 76832: loss: 0.1656412964:
85: 80032: loss: 0.1655358759:
85: 83232: loss: 0.1655626923:
85: 86432: loss: 0.1647502006:
85: 89632: loss: 0.1645967364:
85: 92832: loss: 0.1647093632:
85: 96032: loss: 0.1646953321:
85: 99232: loss: 0.1641885620:
85: 102432: loss: 0.1642480057:
85: 105632: loss: 0.1639141806:
85: 108832: loss: 0.1639927418:
85: 112032: loss: 0.1637947827:
85: 115232: loss: 0.1638472192:
85: 118432: loss: 0.1636272533:
85: 121632: loss: 0.1633313264:
85: 124832: loss: 0.1635778778:
85: 128032: loss: 0.1638305118:
85: 131232: loss: 0.1638097371:
85: 134432: loss: 0.1636651144:
85: 137632: loss: 0.1634645108:
85: 140832: loss: 0.1633732870:
85: 144032: loss: 0.1633663670:
85: 147232: loss: 0.1634332795:
85: 150432: loss: 0.1635980224:
Dev-Acc: 85: Accuracy: 0.9301277995: precision: 0.4055329536: recall: 0.4237374596: f1: 0.4144353900
Train-Acc: 85: Accuracy: 0.9377489686: precision: 0.7931985294: recall: 0.5106173164: f1: 0.6212854457
86: 3232: loss: 0.1679076844:
86: 6432: loss: 0.1639671311:
86: 9632: loss: 0.1629463623:
86: 12832: loss: 0.1687036039:
86: 16032: loss: 0.1689398806:
86: 19232: loss: 0.1672294446:
86: 22432: loss: 0.1654351158:
86: 25632: loss: 0.1652943029:
86: 28832: loss: 0.1644211283:
86: 32032: loss: 0.1643270774:
86: 35232: loss: 0.1645026450:
86: 38432: loss: 0.1649137819:
86: 41632: loss: 0.1651338205:
86: 44832: loss: 0.1646499282:
86: 48032: loss: 0.1645214680:
86: 51232: loss: 0.1646577252:
86: 54432: loss: 0.1654212377:
86: 57632: loss: 0.1651046117:
86: 60832: loss: 0.1652942472:
86: 64032: loss: 0.1650663228:
86: 67232: loss: 0.1650613576:
86: 70432: loss: 0.1653048303:
86: 73632: loss: 0.1650483928:
86: 76832: loss: 0.1649917108:
86: 80032: loss: 0.1652050709:
86: 83232: loss: 0.1652252732:
86: 86432: loss: 0.1649304226:
86: 89632: loss: 0.1643311118:
86: 92832: loss: 0.1639000346:
86: 96032: loss: 0.1636982228:
86: 99232: loss: 0.1635472020:
86: 102432: loss: 0.1628840015:
86: 105632: loss: 0.1632819740:
86: 108832: loss: 0.1635718568:
86: 112032: loss: 0.1630506912:
86: 115232: loss: 0.1635921095:
86: 118432: loss: 0.1637423119:
86: 121632: loss: 0.1635115910:
86: 124832: loss: 0.1634927942:
86: 128032: loss: 0.1635782316:
86: 131232: loss: 0.1634051535:
86: 134432: loss: 0.1635181005:
86: 137632: loss: 0.1633821737:
86: 140832: loss: 0.1635583185:
86: 144032: loss: 0.1632342386:
86: 147232: loss: 0.1632335309:
86: 150432: loss: 0.1631602980:
Dev-Acc: 86: Accuracy: 0.9300781488: precision: 0.4052032520: recall: 0.4237374596: f1: 0.4142631535
Train-Acc: 86: Accuracy: 0.9380251169: precision: 0.7953431373: recall: 0.5119978963: f1: 0.6229652442
87: 3232: loss: 0.1803876857:
87: 6432: loss: 0.1682995447:
87: 9632: loss: 0.1672040608:
87: 12832: loss: 0.1668234260:
87: 16032: loss: 0.1655065258:
87: 19232: loss: 0.1662377466:
87: 22432: loss: 0.1659010496:
87: 25632: loss: 0.1645459842:
87: 28832: loss: 0.1650051874:
87: 32032: loss: 0.1652821828:
87: 35232: loss: 0.1638379405:
87: 38432: loss: 0.1630503578:
87: 41632: loss: 0.1640291655:
87: 44832: loss: 0.1647413634:
87: 48032: loss: 0.1643243517:
87: 51232: loss: 0.1649905020:
87: 54432: loss: 0.1651276323:
87: 57632: loss: 0.1657956717:
87: 60832: loss: 0.1656406979:
87: 64032: loss: 0.1656206326:
87: 67232: loss: 0.1653849422:
87: 70432: loss: 0.1652228202:
87: 73632: loss: 0.1649973222:
87: 76832: loss: 0.1648306723:
87: 80032: loss: 0.1649494730:
87: 83232: loss: 0.1644952620:
87: 86432: loss: 0.1640505180:
87: 89632: loss: 0.1639293343:
87: 92832: loss: 0.1640792108:
87: 96032: loss: 0.1639404843:
87: 99232: loss: 0.1637414926:
87: 102432: loss: 0.1635547980:
87: 105632: loss: 0.1637181965:
87: 108832: loss: 0.1633827977:
87: 112032: loss: 0.1629395624:
87: 115232: loss: 0.1632254949:
87: 118432: loss: 0.1634804300:
87: 121632: loss: 0.1633316168:
87: 124832: loss: 0.1632348460:
87: 128032: loss: 0.1631855066:
87: 131232: loss: 0.1632598753:
87: 134432: loss: 0.1632922876:
87: 137632: loss: 0.1628324520:
87: 140832: loss: 0.1630606842:
87: 144032: loss: 0.1629903917:
87: 147232: loss: 0.1630017706:
87: 150432: loss: 0.1626485179:
Dev-Acc: 87: Accuracy: 0.9299690127: precision: 0.4046654787: recall: 0.4247576943: f1: 0.4144682263
Train-Acc: 87: Accuracy: 0.9382157326: precision: 0.7960680452: recall: 0.5137729275: f1: 0.6245005594
88: 3232: loss: 0.1690677620:
88: 6432: loss: 0.1639033885:
88: 9632: loss: 0.1605917582:
88: 12832: loss: 0.1631959242:
88: 16032: loss: 0.1627886969:
88: 19232: loss: 0.1626616195:
88: 22432: loss: 0.1637969309:
88: 25632: loss: 0.1626599116:
88: 28832: loss: 0.1610584432:
88: 32032: loss: 0.1613169485:
88: 35232: loss: 0.1606131948:
88: 38432: loss: 0.1609830827:
88: 41632: loss: 0.1612467553:
88: 44832: loss: 0.1601034214:
88: 48032: loss: 0.1607933147:
88: 51232: loss: 0.1608715134:
88: 54432: loss: 0.1607085168:
88: 57632: loss: 0.1607392322:
88: 60832: loss: 0.1607955546:
88: 64032: loss: 0.1608027141:
88: 67232: loss: 0.1609623783:
88: 70432: loss: 0.1615193850:
88: 73632: loss: 0.1618624730:
88: 76832: loss: 0.1614169373:
88: 80032: loss: 0.1611841862:
88: 83232: loss: 0.1609006866:
88: 86432: loss: 0.1609317224:
88: 89632: loss: 0.1611279550:
88: 92832: loss: 0.1612532412:
88: 96032: loss: 0.1612688927:
88: 99232: loss: 0.1619856975:
88: 102432: loss: 0.1621492477:
88: 105632: loss: 0.1618032005:
88: 108832: loss: 0.1620560462:
88: 112032: loss: 0.1619123180:
88: 115232: loss: 0.1618706224:
88: 118432: loss: 0.1618440590:
88: 121632: loss: 0.1620385796:
88: 124832: loss: 0.1616676837:
88: 128032: loss: 0.1619387530:
88: 131232: loss: 0.1619426146:
88: 134432: loss: 0.1615120894:
88: 137632: loss: 0.1615654254:
88: 140832: loss: 0.1614324716:
88: 144032: loss: 0.1615877833:
88: 147232: loss: 0.1616724677:
88: 150432: loss: 0.1619843546:
Dev-Acc: 88: Accuracy: 0.9298797250: precision: 0.4041383770: recall: 0.4250977725: f1: 0.4143531947
Train-Acc: 88: Accuracy: 0.9384195805: precision: 0.7968305567: recall: 0.5156794425: f1: 0.6261424865
89: 3232: loss: 0.1556146256:
89: 6432: loss: 0.1550030624:
89: 9632: loss: 0.1583630657:
89: 12832: loss: 0.1576876735:
89: 16032: loss: 0.1593860904:
89: 19232: loss: 0.1632903934:
89: 22432: loss: 0.1644670314:
89: 25632: loss: 0.1645564503:
89: 28832: loss: 0.1645633594:
89: 32032: loss: 0.1644535927:
89: 35232: loss: 0.1643027799:
89: 38432: loss: 0.1653616744:
89: 41632: loss: 0.1648410533:
89: 44832: loss: 0.1644625125:
89: 48032: loss: 0.1641958858:
89: 51232: loss: 0.1641183595:
89: 54432: loss: 0.1643095257:
89: 57632: loss: 0.1637987014:
89: 60832: loss: 0.1641478487:
89: 64032: loss: 0.1645141457:
89: 67232: loss: 0.1640554007:
89: 70432: loss: 0.1637717699:
89: 73632: loss: 0.1634781359:
89: 76832: loss: 0.1632982992:
89: 80032: loss: 0.1631602341:
89: 83232: loss: 0.1628154023:
89: 86432: loss: 0.1622389301:
89: 89632: loss: 0.1619423500:
89: 92832: loss: 0.1620058011:
89: 96032: loss: 0.1618433915:
89: 99232: loss: 0.1622692427:
89: 102432: loss: 0.1619851132:
89: 105632: loss: 0.1618269996:
89: 108832: loss: 0.1614403448:
89: 112032: loss: 0.1613944324:
89: 115232: loss: 0.1614917407:
89: 118432: loss: 0.1617238998:
89: 121632: loss: 0.1616598345:
89: 124832: loss: 0.1615411666:
89: 128032: loss: 0.1614829726:
89: 131232: loss: 0.1614054634:
89: 134432: loss: 0.1616033858:
89: 137632: loss: 0.1615295417:
89: 140832: loss: 0.1614629722:
89: 144032: loss: 0.1616409761:
89: 147232: loss: 0.1616389223:
89: 150432: loss: 0.1617969190:
Dev-Acc: 89: Accuracy: 0.9297308922: precision: 0.4031607805: recall: 0.4250977725: f1: 0.4138387684
Train-Acc: 89: Accuracy: 0.9385839105: precision: 0.7975261077: recall: 0.5171257642: f1: 0.6274228284
90: 3232: loss: 0.1623388335:
90: 6432: loss: 0.1581365659:
90: 9632: loss: 0.1585510195:
90: 12832: loss: 0.1564488206:
90: 16032: loss: 0.1562792050:
90: 19232: loss: 0.1581352162:
90: 22432: loss: 0.1569941968:
90: 25632: loss: 0.1573857323:
90: 28832: loss: 0.1584635387:
90: 32032: loss: 0.1585728385:
90: 35232: loss: 0.1589146176:
90: 38432: loss: 0.1593978700:
90: 41632: loss: 0.1598527571:
90: 44832: loss: 0.1598051487:
90: 48032: loss: 0.1592382444:
90: 51232: loss: 0.1586654299:
90: 54432: loss: 0.1585442883:
90: 57632: loss: 0.1591804265:
90: 60832: loss: 0.1588115312:
90: 64032: loss: 0.1591206554:
90: 67232: loss: 0.1592700998:
90: 70432: loss: 0.1593585648:
90: 73632: loss: 0.1591955900:
90: 76832: loss: 0.1589718500:
90: 80032: loss: 0.1595407188:
90: 83232: loss: 0.1598046385:
90: 86432: loss: 0.1596358488:
90: 89632: loss: 0.1602025894:
90: 92832: loss: 0.1606628074:
90: 96032: loss: 0.1606212287:
90: 99232: loss: 0.1606479673:
90: 102432: loss: 0.1610531156:
90: 105632: loss: 0.1605841106:
90: 108832: loss: 0.1604862155:
90: 112032: loss: 0.1603714502:
90: 115232: loss: 0.1604959328:
90: 118432: loss: 0.1608044393:
90: 121632: loss: 0.1612772728:
90: 124832: loss: 0.1611280597:
90: 128032: loss: 0.1610423119:
90: 131232: loss: 0.1614534871:
90: 134432: loss: 0.1614009228:
90: 137632: loss: 0.1617043977:
90: 140832: loss: 0.1617124469:
90: 144032: loss: 0.1616543892:
90: 147232: loss: 0.1615810128:
90: 150432: loss: 0.1618084358:
Dev-Acc: 90: Accuracy: 0.9294431806: precision: 0.4013157895: recall: 0.4252678116: f1: 0.4129447701
Train-Acc: 90: Accuracy: 0.9388074279: precision: 0.7985837127: recall: 0.5189665374: f1: 0.6291042397
91: 3232: loss: 0.1530448769:
91: 6432: loss: 0.1570833297:
91: 9632: loss: 0.1616466630:
91: 12832: loss: 0.1632667198:
91: 16032: loss: 0.1627383308:
91: 19232: loss: 0.1612566239:
91: 22432: loss: 0.1617507952:
91: 25632: loss: 0.1600909540:
91: 28832: loss: 0.1601376062:
91: 32032: loss: 0.1606276358:
91: 35232: loss: 0.1631734174:
91: 38432: loss: 0.1634733411:
91: 41632: loss: 0.1632209194:
91: 44832: loss: 0.1624840307:
91: 48032: loss: 0.1625958219:
91: 51232: loss: 0.1624962605:
91: 54432: loss: 0.1623234443:
91: 57632: loss: 0.1629675634:
91: 60832: loss: 0.1625781005:
91: 64032: loss: 0.1627718688:
91: 67232: loss: 0.1633621596:
91: 70432: loss: 0.1627748190:
91: 73632: loss: 0.1629320805:
91: 76832: loss: 0.1630287496:
91: 80032: loss: 0.1628693116:
91: 83232: loss: 0.1626776230:
91: 86432: loss: 0.1623218164:
91: 89632: loss: 0.1619082962:
91: 92832: loss: 0.1618539613:
91: 96032: loss: 0.1615872754:
91: 99232: loss: 0.1614463656:
91: 102432: loss: 0.1610607278:
91: 105632: loss: 0.1615280831:
91: 108832: loss: 0.1611724311:
91: 112032: loss: 0.1613037585:
91: 115232: loss: 0.1607974023:
91: 118432: loss: 0.1607597332:
91: 121632: loss: 0.1610195158:
91: 124832: loss: 0.1608130076:
91: 128032: loss: 0.1608383611:
91: 131232: loss: 0.1607564451:
91: 134432: loss: 0.1610731290:
91: 137632: loss: 0.1607404995:
91: 140832: loss: 0.1606862900:
91: 144032: loss: 0.1609567126:
91: 147232: loss: 0.1608181653:
91: 150432: loss: 0.1608699487:
Dev-Acc: 91: Accuracy: 0.9295026660: precision: 0.4018286814: recall: 0.4259479680: f1: 0.4135369377
Train-Acc: 91: Accuracy: 0.9390046597: precision: 0.7999797755: recall: 0.5200841496: f1: 0.6303585657
92: 3232: loss: 0.1559694523:
92: 6432: loss: 0.1559134497:
92: 9632: loss: 0.1612868147:
92: 12832: loss: 0.1625296477:
92: 16032: loss: 0.1609574787:
92: 19232: loss: 0.1608693850:
92: 22432: loss: 0.1620867748:
92: 25632: loss: 0.1627390509:
92: 28832: loss: 0.1618029491:
92: 32032: loss: 0.1628549058:
92: 35232: loss: 0.1627863734:
92: 38432: loss: 0.1637075603:
92: 41632: loss: 0.1635128623:
92: 44832: loss: 0.1631725542:
92: 48032: loss: 0.1629304639:
92: 51232: loss: 0.1623076120:
92: 54432: loss: 0.1624502265:
92: 57632: loss: 0.1626131419:
92: 60832: loss: 0.1626543453:
92: 64032: loss: 0.1625165520:
92: 67232: loss: 0.1625020273:
92: 70432: loss: 0.1624411228:
92: 73632: loss: 0.1622864904:
92: 76832: loss: 0.1618964755:
92: 80032: loss: 0.1616945462:
92: 83232: loss: 0.1615580992:
92: 86432: loss: 0.1620719451:
92: 89632: loss: 0.1621557629:
92: 92832: loss: 0.1617737240:
92: 96032: loss: 0.1616943205:
92: 99232: loss: 0.1615773710:
92: 102432: loss: 0.1612956941:
92: 105632: loss: 0.1609214513:
92: 108832: loss: 0.1609654021:
92: 112032: loss: 0.1609713423:
92: 115232: loss: 0.1607164674:
92: 118432: loss: 0.1609565174:
92: 121632: loss: 0.1609070489:
92: 124832: loss: 0.1610254367:
92: 128032: loss: 0.1611774480:
92: 131232: loss: 0.1611753445:
92: 134432: loss: 0.1613027028:
92: 137632: loss: 0.1612699207:
92: 140832: loss: 0.1614562147:
92: 144032: loss: 0.1610820105:
92: 147232: loss: 0.1608575951:
92: 150432: loss: 0.1609834128:
Dev-Acc: 92: Accuracy: 0.9293736815: precision: 0.4009607686: recall: 0.4257779289: f1: 0.4129968662
Train-Acc: 92: Accuracy: 0.9391887188: precision: 0.8005445195: recall: 0.5219249228: f1: 0.6318847501
93: 3232: loss: 0.1609138287:
93: 6432: loss: 0.1613691206:
93: 9632: loss: 0.1575420612:
93: 12832: loss: 0.1574705209:
93: 16032: loss: 0.1596812129:
93: 19232: loss: 0.1585353486:
93: 22432: loss: 0.1579442257:
93: 25632: loss: 0.1574098256:
93: 28832: loss: 0.1594864835:
93: 32032: loss: 0.1589727801:
93: 35232: loss: 0.1595190187:
93: 38432: loss: 0.1596272734:
93: 41632: loss: 0.1589587566:
93: 44832: loss: 0.1592757814:
93: 48032: loss: 0.1605039691:
93: 51232: loss: 0.1607766394:
93: 54432: loss: 0.1604824174:
93: 57632: loss: 0.1600552028:
93: 60832: loss: 0.1604781660:
93: 64032: loss: 0.1603700257:
93: 67232: loss: 0.1606425058:
93: 70432: loss: 0.1611920231:
93: 73632: loss: 0.1610995201:
93: 76832: loss: 0.1604852178:
93: 80032: loss: 0.1605792962:
93: 83232: loss: 0.1607383350:
93: 86432: loss: 0.1606260263:
93: 89632: loss: 0.1606285098:
93: 92832: loss: 0.1606449705:
93: 96032: loss: 0.1602538193:
93: 99232: loss: 0.1602775745:
93: 102432: loss: 0.1603229317:
93: 105632: loss: 0.1600306713:
93: 108832: loss: 0.1601715521:
93: 112032: loss: 0.1601221381:
93: 115232: loss: 0.1604030566:
93: 118432: loss: 0.1605515310:
93: 121632: loss: 0.1602250338:
93: 124832: loss: 0.1603450475:
93: 128032: loss: 0.1607341786:
93: 131232: loss: 0.1604578000:
93: 134432: loss: 0.1605815750:
93: 137632: loss: 0.1606519816:
93: 140832: loss: 0.1607464106:
93: 144032: loss: 0.1610517819:
93: 147232: loss: 0.1607967533:
93: 150432: loss: 0.1605532750:
Dev-Acc: 93: Accuracy: 0.9291554093: precision: 0.3996492906: recall: 0.4262880463: f1: 0.4125390818
Train-Acc: 93: Accuracy: 0.9394582510: precision: 0.8022155086: recall: 0.5236999540: f1: 0.6337058987
94: 3232: loss: 0.1496962534:
94: 6432: loss: 0.1553122056:
94: 9632: loss: 0.1555993167:
94: 12832: loss: 0.1568265964:
94: 16032: loss: 0.1585055634:
94: 19232: loss: 0.1574302785:
94: 22432: loss: 0.1564091335:
94: 25632: loss: 0.1562371657:
94: 28832: loss: 0.1563371594:
94: 32032: loss: 0.1573983799:
94: 35232: loss: 0.1570847937:
94: 38432: loss: 0.1575837195:
94: 41632: loss: 0.1590694660:
94: 44832: loss: 0.1595570085:
94: 48032: loss: 0.1598945122:
94: 51232: loss: 0.1596029107:
94: 54432: loss: 0.1595372418:
94: 57632: loss: 0.1596659023:
94: 60832: loss: 0.1591511716:
94: 64032: loss: 0.1592113941:
94: 67232: loss: 0.1591161012:
94: 70432: loss: 0.1597929872:
94: 73632: loss: 0.1601492433:
94: 76832: loss: 0.1605297588:
94: 80032: loss: 0.1609533872:
94: 83232: loss: 0.1608411014:
94: 86432: loss: 0.1607098697:
94: 89632: loss: 0.1607365301:
94: 92832: loss: 0.1608276271:
94: 96032: loss: 0.1603490539:
94: 99232: loss: 0.1602327434:
94: 102432: loss: 0.1602103841:
94: 105632: loss: 0.1601729575:
94: 108832: loss: 0.1602050119:
94: 112032: loss: 0.1605637408:
94: 115232: loss: 0.1602611025:
94: 118432: loss: 0.1602423567:
94: 121632: loss: 0.1601881420:
94: 124832: loss: 0.1605139131:
94: 128032: loss: 0.1606093169:
94: 131232: loss: 0.1604041225:
94: 134432: loss: 0.1597159174:
94: 137632: loss: 0.1599053015:
94: 140832: loss: 0.1600561679:
94: 144032: loss: 0.1601979186:
94: 147232: loss: 0.1601545776:
94: 150432: loss: 0.1600640142:
Dev-Acc: 94: Accuracy: 0.9289867282: precision: 0.3986336193: recall: 0.4266281245: f1: 0.4121560575
Train-Acc: 94: Accuracy: 0.9397804141: precision: 0.8036126443: recall: 0.5264611137: f1: 0.6361614236
95: 3232: loss: 0.1560340944:
95: 6432: loss: 0.1557199086:
95: 9632: loss: 0.1546220191:
95: 12832: loss: 0.1538941205:
95: 16032: loss: 0.1552621007:
95: 19232: loss: 0.1557614295:
95: 22432: loss: 0.1562530192:
95: 25632: loss: 0.1572083308:
95: 28832: loss: 0.1584623784:
95: 32032: loss: 0.1582040307:
95: 35232: loss: 0.1589413995:
95: 38432: loss: 0.1590367320:
95: 41632: loss: 0.1589612756:
95: 44832: loss: 0.1586065531:
95: 48032: loss: 0.1600346439:
95: 51232: loss: 0.1599738746:
95: 54432: loss: 0.1608369436:
95: 57632: loss: 0.1604811367:
95: 60832: loss: 0.1608271942:
95: 64032: loss: 0.1607176692:
95: 67232: loss: 0.1604843044:
95: 70432: loss: 0.1603068385:
95: 73632: loss: 0.1607144009:
95: 76832: loss: 0.1607579287:
95: 80032: loss: 0.1606794748:
95: 83232: loss: 0.1609621553:
95: 86432: loss: 0.1610645795:
95: 89632: loss: 0.1608262138:
95: 92832: loss: 0.1606381603:
95: 96032: loss: 0.1604191140:
95: 99232: loss: 0.1600711171:
95: 102432: loss: 0.1601851057:
95: 105632: loss: 0.1602414235:
95: 108832: loss: 0.1601294812:
95: 112032: loss: 0.1600267589:
95: 115232: loss: 0.1600588640:
95: 118432: loss: 0.1604138585:
95: 121632: loss: 0.1603362840:
95: 124832: loss: 0.1599885436:
95: 128032: loss: 0.1599231199:
95: 131232: loss: 0.1599454791:
95: 134432: loss: 0.1604813624:
95: 137632: loss: 0.1603812230:
95: 140832: loss: 0.1601315406:
95: 144032: loss: 0.1601997678:
95: 147232: loss: 0.1601373654:
95: 150432: loss: 0.1596760972:
Dev-Acc: 95: Accuracy: 0.9289470315: precision: 0.3983481576: recall: 0.4264580854: f1: 0.4119241192
Train-Acc: 95: Accuracy: 0.9398329854: precision: 0.8041972086: recall: 0.5265268556: f1: 0.6363925308
96: 3232: loss: 0.1599930204:
96: 6432: loss: 0.1658061749:
96: 9632: loss: 0.1641547005:
96: 12832: loss: 0.1624464442:
96: 16032: loss: 0.1611468316:
96: 19232: loss: 0.1613511877:
96: 22432: loss: 0.1611832323:
96: 25632: loss: 0.1638577853:
96: 28832: loss: 0.1626825516:
96: 32032: loss: 0.1635754283:
96: 35232: loss: 0.1633203925:
96: 38432: loss: 0.1630879316:
96: 41632: loss: 0.1630640075:
96: 44832: loss: 0.1624614034:
96: 48032: loss: 0.1618537034:
96: 51232: loss: 0.1610118388:
96: 54432: loss: 0.1610468459:
96: 57632: loss: 0.1606681748:
96: 60832: loss: 0.1609152209:
96: 64032: loss: 0.1608531755:
96: 67232: loss: 0.1602637284:
96: 70432: loss: 0.1594202898:
96: 73632: loss: 0.1594205361:
96: 76832: loss: 0.1593866744:
96: 80032: loss: 0.1591089779:
96: 83232: loss: 0.1594973300:
96: 86432: loss: 0.1592343508:
96: 89632: loss: 0.1592736976:
96: 92832: loss: 0.1588294819:
96: 96032: loss: 0.1587601664:
96: 99232: loss: 0.1586627333:
96: 102432: loss: 0.1590118411:
96: 105632: loss: 0.1589798988:
96: 108832: loss: 0.1592471731:
96: 112032: loss: 0.1593172144:
96: 115232: loss: 0.1592207058:
96: 118432: loss: 0.1592964773:
96: 121632: loss: 0.1588932504:
96: 124832: loss: 0.1586161363:
96: 128032: loss: 0.1586685581:
96: 131232: loss: 0.1588740017:
96: 134432: loss: 0.1591967715:
96: 137632: loss: 0.1590445610:
96: 140832: loss: 0.1591793317:
96: 144032: loss: 0.1591559163:
96: 147232: loss: 0.1591899545:
96: 150432: loss: 0.1593474527:
Dev-Acc: 96: Accuracy: 0.9288280010: precision: 0.3976876782: recall: 0.4269682027: f1: 0.4118081181
Train-Acc: 96: Accuracy: 0.9400236607: precision: 0.8049489080: recall: 0.5282361449: f1: 0.6378756004
97: 3232: loss: 0.1557333576:
97: 6432: loss: 0.1554078508:
97: 9632: loss: 0.1560963754:
97: 12832: loss: 0.1567611916:
97: 16032: loss: 0.1606726196:
97: 19232: loss: 0.1586676961:
97: 22432: loss: 0.1573559797:
97: 25632: loss: 0.1582648432:
97: 28832: loss: 0.1585312640:
97: 32032: loss: 0.1572110802:
97: 35232: loss: 0.1576062284:
97: 38432: loss: 0.1580410829:
97: 41632: loss: 0.1582933340:
97: 44832: loss: 0.1574780567:
97: 48032: loss: 0.1585809366:
97: 51232: loss: 0.1590150168:
97: 54432: loss: 0.1587509343:
97: 57632: loss: 0.1595450880:
97: 60832: loss: 0.1598361205:
97: 64032: loss: 0.1603733630:
97: 67232: loss: 0.1601026346:
97: 70432: loss: 0.1602560370:
97: 73632: loss: 0.1606192941:
97: 76832: loss: 0.1597895901:
97: 80032: loss: 0.1596633790:
97: 83232: loss: 0.1595497124:
97: 86432: loss: 0.1598283665:
97: 89632: loss: 0.1600972106:
97: 92832: loss: 0.1604553229:
97: 96032: loss: 0.1604634653:
97: 99232: loss: 0.1603604999:
97: 102432: loss: 0.1602838754:
97: 105632: loss: 0.1602469552:
97: 108832: loss: 0.1599465239:
97: 112032: loss: 0.1599222393:
97: 115232: loss: 0.1598659164:
97: 118432: loss: 0.1592293925:
97: 121632: loss: 0.1595352745:
97: 124832: loss: 0.1592300594:
97: 128032: loss: 0.1592146649:
97: 131232: loss: 0.1592485881:
97: 134432: loss: 0.1589643507:
97: 137632: loss: 0.1590127173:
97: 140832: loss: 0.1589604199:
97: 144032: loss: 0.1589923254:
97: 147232: loss: 0.1588639196:
97: 150432: loss: 0.1589328097:
Dev-Acc: 97: Accuracy: 0.9285303354: precision: 0.3961997487: recall: 0.4290086720: f1: 0.4119519961
Train-Acc: 97: Accuracy: 0.9401879907: precision: 0.8054972514: recall: 0.5298139504: f1: 0.6391973350
98: 3232: loss: 0.1606461471:
98: 6432: loss: 0.1631008959:
98: 9632: loss: 0.1631694545:
98: 12832: loss: 0.1600944192:
98: 16032: loss: 0.1629206085:
98: 19232: loss: 0.1606165706:
98: 22432: loss: 0.1611945023:
98: 25632: loss: 0.1604744915:
98: 28832: loss: 0.1597721595:
98: 32032: loss: 0.1599238341:
98: 35232: loss: 0.1594961169:
98: 38432: loss: 0.1593553959:
98: 41632: loss: 0.1583630624:
98: 44832: loss: 0.1584511661:
98: 48032: loss: 0.1584522336:
98: 51232: loss: 0.1584745960:
98: 54432: loss: 0.1589384792:
98: 57632: loss: 0.1586030333:
98: 60832: loss: 0.1577738939:
98: 64032: loss: 0.1577530704:
98: 67232: loss: 0.1577984830:
98: 70432: loss: 0.1576345212:
98: 73632: loss: 0.1575312202:
98: 76832: loss: 0.1574146344:
98: 80032: loss: 0.1573709574:
98: 83232: loss: 0.1571323095:
98: 86432: loss: 0.1572932250:
98: 89632: loss: 0.1572444784:
98: 92832: loss: 0.1569973842:
98: 96032: loss: 0.1571140314:
98: 99232: loss: 0.1573997957:
98: 102432: loss: 0.1572663417:
98: 105632: loss: 0.1569703636:
98: 108832: loss: 0.1571737373:
98: 112032: loss: 0.1574710773:
98: 115232: loss: 0.1574798180:
98: 118432: loss: 0.1574541881:
98: 121632: loss: 0.1575702433:
98: 124832: loss: 0.1578865489:
98: 128032: loss: 0.1577277352:
98: 131232: loss: 0.1581834142:
98: 134432: loss: 0.1579661651:
98: 137632: loss: 0.1581294734:
98: 140832: loss: 0.1581696799:
98: 144032: loss: 0.1583379326:
98: 147232: loss: 0.1584628223:
98: 150432: loss: 0.1587103645:
Dev-Acc: 98: Accuracy: 0.9283318520: precision: 0.3950578668: recall: 0.4295187893: f1: 0.4115682281
Train-Acc: 98: Accuracy: 0.9403392076: precision: 0.8059433586: recall: 0.5313260141: f1: 0.6404374183
99: 3232: loss: 0.1569804592:
99: 6432: loss: 0.1615472765:
99: 9632: loss: 0.1610302841:
99: 12832: loss: 0.1581893437:
99: 16032: loss: 0.1603732245:
99: 19232: loss: 0.1600944136:
99: 22432: loss: 0.1596257269:
99: 25632: loss: 0.1579371464:
99: 28832: loss: 0.1578044487:
99: 32032: loss: 0.1571804346:
99: 35232: loss: 0.1568793481:
99: 38432: loss: 0.1574711406:
99: 41632: loss: 0.1576787937:
99: 44832: loss: 0.1578690818:
99: 48032: loss: 0.1579783722:
99: 51232: loss: 0.1583632940:
99: 54432: loss: 0.1575605117:
99: 57632: loss: 0.1573814161:
99: 60832: loss: 0.1571173443:
99: 64032: loss: 0.1572739879:
99: 67232: loss: 0.1580366523:
99: 70432: loss: 0.1581492562:
99: 73632: loss: 0.1583827754:
99: 76832: loss: 0.1580932662:
99: 80032: loss: 0.1586148997:
99: 83232: loss: 0.1582591010:
99: 86432: loss: 0.1586947374:
99: 89632: loss: 0.1590879269:
99: 92832: loss: 0.1591306458:
99: 96032: loss: 0.1592408802:
99: 99232: loss: 0.1590141403:
99: 102432: loss: 0.1588062166:
99: 105632: loss: 0.1584067178:
99: 108832: loss: 0.1580708974:
99: 112032: loss: 0.1578160013:
99: 115232: loss: 0.1580987200:
99: 118432: loss: 0.1583069549:
99: 121632: loss: 0.1586184279:
99: 124832: loss: 0.1583837823:
99: 128032: loss: 0.1582386492:
99: 131232: loss: 0.1584313472:
99: 134432: loss: 0.1584849043:
99: 137632: loss: 0.1585321435:
99: 140832: loss: 0.1585222298:
99: 144032: loss: 0.1585197091:
99: 147232: loss: 0.1583289398:
99: 150432: loss: 0.1580824460:
Dev-Acc: 99: Accuracy: 0.9282326698: precision: 0.3942758836: recall: 0.4286685938: f1: 0.4107535642
Train-Acc: 99: Accuracy: 0.9405233264: precision: 0.8070332736: recall: 0.5325751101: f1: 0.6416887798
100: 3232: loss: 0.1597432566:
100: 6432: loss: 0.1600876929:
100: 9632: loss: 0.1586338733:
100: 12832: loss: 0.1583726984:
100: 16032: loss: 0.1621469920:
100: 19232: loss: 0.1599719965:
100: 22432: loss: 0.1595362397:
100: 25632: loss: 0.1587052131:
100: 28832: loss: 0.1593811999:
100: 32032: loss: 0.1579089546:
100: 35232: loss: 0.1582233499:
100: 38432: loss: 0.1580238790:
100: 41632: loss: 0.1579808935:
100: 44832: loss: 0.1590852550:
100: 48032: loss: 0.1586232860:
100: 51232: loss: 0.1587741293:
100: 54432: loss: 0.1581299808:
100: 57632: loss: 0.1581843200:
100: 60832: loss: 0.1585337365:
100: 64032: loss: 0.1584375284:
100: 67232: loss: 0.1578135970:
100: 70432: loss: 0.1584498016:
100: 73632: loss: 0.1578948291:
100: 76832: loss: 0.1584547828:
100: 80032: loss: 0.1584071084:
100: 83232: loss: 0.1583832827:
100: 86432: loss: 0.1575967883:
100: 89632: loss: 0.1578265894:
100: 92832: loss: 0.1574568452:
100: 96032: loss: 0.1575231451:
100: 99232: loss: 0.1576382992:
100: 102432: loss: 0.1578121344:
100: 105632: loss: 0.1581844816:
100: 108832: loss: 0.1577089006:
100: 112032: loss: 0.1579045099:
100: 115232: loss: 0.1577448967:
100: 118432: loss: 0.1580105492:
100: 121632: loss: 0.1579663034:
100: 124832: loss: 0.1577923716:
100: 128032: loss: 0.1580792367:
100: 131232: loss: 0.1581625572:
100: 134432: loss: 0.1582160871:
100: 137632: loss: 0.1581471485:
100: 140832: loss: 0.1578318999:
100: 144032: loss: 0.1578125983:
100: 147232: loss: 0.1578715896:
100: 150432: loss: 0.1580608587:
Dev-Acc: 100: Accuracy: 0.9280937314: precision: 0.3934477379: recall: 0.4288386329: f1: 0.4103815800
Train-Acc: 100: Accuracy: 0.9406942129: precision: 0.8078989256: recall: 0.5338899481: f1: 0.6429165182
