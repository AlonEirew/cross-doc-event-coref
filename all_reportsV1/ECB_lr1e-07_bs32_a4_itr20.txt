1: 3232: loss: 0.7045299852:
1: 6432: loss: 0.7041092575:
1: 9632: loss: 0.7029984909:
1: 12832: loss: 0.7019424649:
1: 16032: loss: 0.7014034075:
1: 19232: loss: 0.7006393986:
1: 22432: loss: 0.6998181824:
1: 25632: loss: 0.6991017572:
1: 28832: loss: 0.6984780310:
1: 32032: loss: 0.6977093069:
1: 35232: loss: 0.6970573233:
1: 38432: loss: 0.6963204585:
1: 41632: loss: 0.6956434956:
1: 44832: loss: 0.6950198927:
1: 48032: loss: 0.6943871288:
1: 51232: loss: 0.6937350506:
1: 54432: loss: 0.6930738553:
1: 57632: loss: 0.6924550497:
1: 60832: loss: 0.6918058539:
1: 64032: loss: 0.6911040581:
1: 67232: loss: 0.6904563056:
1: 70432: loss: 0.6898447490:
1: 73632: loss: 0.6892613199:
Dev-Acc: 1: Accuracy: 0.7001210451: precision: 0.0767492002: recall: 0.3752763136: f1: 0.1274359789
Train-Acc: 1: Accuracy: 0.7387285233: precision: 0.3677789127: recall: 0.4260732365: f1: 0.3947857339
2: 3232: loss: 0.6732047427:
2: 6432: loss: 0.6726218605:
2: 9632: loss: 0.6724162350:
2: 12832: loss: 0.6716883251:
2: 16032: loss: 0.6708621615:
2: 19232: loss: 0.6704745127:
2: 22432: loss: 0.6699131870:
2: 25632: loss: 0.6691593411:
2: 28832: loss: 0.6685762493:
2: 32032: loss: 0.6680177008:
2: 35232: loss: 0.6673386407:
2: 38432: loss: 0.6667290858:
2: 41632: loss: 0.6662849286:
2: 44832: loss: 0.6657877439:
2: 48032: loss: 0.6651122757:
2: 51232: loss: 0.6645827563:
2: 54432: loss: 0.6640314700:
2: 57632: loss: 0.6632854083:
2: 60832: loss: 0.6626373933:
2: 64032: loss: 0.6620028027:
2: 67232: loss: 0.6614435229:
2: 70432: loss: 0.6608181830:
2: 73632: loss: 0.6602215545:
Dev-Acc: 2: Accuracy: 0.8963426352: precision: 0.1439488459: recall: 0.1569460976: f1: 0.1501667616
Train-Acc: 2: Accuracy: 0.8271250725: precision: 0.7634738186: recall: 0.1965025311: f1: 0.3125588205
3: 3232: loss: 0.6472557306:
3: 6432: loss: 0.6456708804:
3: 9632: loss: 0.6449890021:
3: 12832: loss: 0.6440336731:
3: 16032: loss: 0.6433838871:
3: 19232: loss: 0.6426093402:
3: 22432: loss: 0.6420190668:
3: 25632: loss: 0.6413607188:
3: 28832: loss: 0.6410135269:
3: 32032: loss: 0.6405077932:
3: 35232: loss: 0.6397713727:
3: 38432: loss: 0.6391685005:
3: 41632: loss: 0.6386029547:
3: 44832: loss: 0.6379703511:
3: 48032: loss: 0.6373095674:
3: 51232: loss: 0.6368093669:
3: 54432: loss: 0.6361799238:
3: 57632: loss: 0.6354962600:
3: 60832: loss: 0.6349519003:
3: 64032: loss: 0.6344029575:
3: 67232: loss: 0.6337870239:
3: 70432: loss: 0.6331554248:
3: 73632: loss: 0.6325977621:
Dev-Acc: 3: Accuracy: 0.9349797368: precision: 0.2529411765: recall: 0.0584934535: f1: 0.0950145008
Train-Acc: 3: Accuracy: 0.8184340000: precision: 0.9128386337: recall: 0.1018999408: f1: 0.1833343190
4: 3232: loss: 0.6155273914:
4: 6432: loss: 0.6151436314:
4: 9632: loss: 0.6152541113:
4: 12832: loss: 0.6148347113:
4: 16032: loss: 0.6147008104:
4: 19232: loss: 0.6143417062:
4: 22432: loss: 0.6138083260:
4: 25632: loss: 0.6133472288:
4: 28832: loss: 0.6127367842:
4: 32032: loss: 0.6124388847:
4: 35232: loss: 0.6119977054:
4: 38432: loss: 0.6119494790:
4: 41632: loss: 0.6114771204:
4: 44832: loss: 0.6108090821:
4: 48032: loss: 0.6102974534:
4: 51232: loss: 0.6097235884:
4: 54432: loss: 0.6091914651:
4: 57632: loss: 0.6087699104:
4: 60832: loss: 0.6083410407:
4: 64032: loss: 0.6078650260:
4: 67232: loss: 0.6073689973:
4: 70432: loss: 0.6069259799:
4: 73632: loss: 0.6064902428:
Dev-Acc: 4: Accuracy: 0.9406056404: precision: 0.3825503356: recall: 0.0290766876: f1: 0.0540455120
Train-Acc: 4: Accuracy: 0.8129116893: precision: 0.9132996633: recall: 0.0713299586: f1: 0.1323251418
5: 3232: loss: 0.5991388750:
5: 6432: loss: 0.5927962029:
5: 9632: loss: 0.5929392485:
5: 12832: loss: 0.5919502449:
5: 16032: loss: 0.5917019378:
5: 19232: loss: 0.5912295092:
5: 22432: loss: 0.5909882532:
5: 25632: loss: 0.5899551798:
5: 28832: loss: 0.5892547009:
5: 32032: loss: 0.5886953701:
5: 35232: loss: 0.5883495761:
5: 38432: loss: 0.5876113346:
5: 41632: loss: 0.5871996671:
5: 44832: loss: 0.5864142708:
5: 48032: loss: 0.5858901589:
5: 51232: loss: 0.5853724771:
5: 54432: loss: 0.5847762703:
5: 57632: loss: 0.5842061639:
5: 60832: loss: 0.5837830831:
5: 64032: loss: 0.5831368945:
5: 67232: loss: 0.5825594506:
5: 70432: loss: 0.5818487124:
5: 73632: loss: 0.5812642705:
Dev-Acc: 5: Accuracy: 0.9415978789: precision: 0.4890829694: recall: 0.0190443802: f1: 0.0366612111
Train-Acc: 5: Accuracy: 0.8100059032: precision: 0.8943005181: recall: 0.0567352574: f1: 0.1067012859
6: 3232: loss: 0.5659784019:
6: 6432: loss: 0.5673479694:
6: 9632: loss: 0.5669661500:
6: 12832: loss: 0.5658483870:
6: 16032: loss: 0.5654043045:
6: 19232: loss: 0.5654556645:
6: 22432: loss: 0.5652258367:
6: 25632: loss: 0.5647069360:
6: 28832: loss: 0.5639910959:
6: 32032: loss: 0.5632910409:
6: 35232: loss: 0.5625818741:
6: 38432: loss: 0.5626070155:
6: 41632: loss: 0.5619127462:
6: 44832: loss: 0.5617330292:
6: 48032: loss: 0.5612754313:
6: 51232: loss: 0.5611455045:
6: 54432: loss: 0.5605565437:
6: 57632: loss: 0.5596096081:
6: 60832: loss: 0.5591067332:
6: 64032: loss: 0.5588753654:
6: 67232: loss: 0.5581870688:
6: 70432: loss: 0.5575501872:
6: 73632: loss: 0.5570423890:
Dev-Acc: 6: Accuracy: 0.9419054389: precision: 0.5730337079: recall: 0.0173439891: f1: 0.0336689223
Train-Acc: 6: Accuracy: 0.8104529381: precision: 0.9068577277: recall: 0.0582473210: f1: 0.1094638003
7: 3232: loss: 0.5415109167:
7: 6432: loss: 0.5442129482:
7: 9632: loss: 0.5430020851:
7: 12832: loss: 0.5426726092:
7: 16032: loss: 0.5435735393:
7: 19232: loss: 0.5424319259:
7: 22432: loss: 0.5424965949:
7: 25632: loss: 0.5414263975:
7: 28832: loss: 0.5414698976:
7: 32032: loss: 0.5405799525:
7: 35232: loss: 0.5399599324:
7: 38432: loss: 0.5398357499:
7: 41632: loss: 0.5393960655:
7: 44832: loss: 0.5387945762:
7: 48032: loss: 0.5380286188:
7: 51232: loss: 0.5377086289:
7: 54432: loss: 0.5372752962:
7: 57632: loss: 0.5369258526:
7: 60832: loss: 0.5361778480:
7: 64032: loss: 0.5357895419:
7: 67232: loss: 0.5349189823:
7: 70432: loss: 0.5345371847:
7: 73632: loss: 0.5340461720:
Dev-Acc: 7: Accuracy: 0.9419748783: precision: 0.5882352941: recall: 0.0187043020: f1: 0.0362557680
Train-Acc: 7: Accuracy: 0.8125041127: precision: 0.9272237197: recall: 0.0678456380: f1: 0.1264395981
8: 3232: loss: 0.5220807403:
8: 6432: loss: 0.5207307050:
8: 9632: loss: 0.5207764402:
8: 12832: loss: 0.5203567302:
8: 16032: loss: 0.5201828523:
8: 19232: loss: 0.5188765364:
8: 22432: loss: 0.5190126324:
8: 25632: loss: 0.5185354206:
8: 28832: loss: 0.5182899889:
8: 32032: loss: 0.5174013318:
8: 35232: loss: 0.5165366459:
8: 38432: loss: 0.5161987596:
8: 41632: loss: 0.5161914807:
8: 44832: loss: 0.5156244006:
8: 48032: loss: 0.5153691971:
8: 51232: loss: 0.5146763811:
8: 54432: loss: 0.5140426796:
8: 57632: loss: 0.5136093313:
8: 60832: loss: 0.5130518095:
8: 64032: loss: 0.5126655871:
8: 67232: loss: 0.5124485121:
8: 70432: loss: 0.5122680044:
8: 73632: loss: 0.5116967208:
Dev-Acc: 8: Accuracy: 0.9419153929: precision: 0.5569620253: recall: 0.0224451624: f1: 0.0431513567
Train-Acc: 8: Accuracy: 0.8141476512: precision: 0.9297124601: recall: 0.0765235685: f1: 0.1414080058
9: 3232: loss: 0.5012966013:
9: 6432: loss: 0.5038429569:
9: 9632: loss: 0.5009623546:
9: 12832: loss: 0.5003840441:
9: 16032: loss: 0.5003415358:
9: 19232: loss: 0.4990233115:
9: 22432: loss: 0.4980687264:
9: 25632: loss: 0.4978006899:
9: 28832: loss: 0.4974924192:
9: 32032: loss: 0.4965348617:
9: 35232: loss: 0.4965052785:
9: 38432: loss: 0.4958572168:
9: 41632: loss: 0.4951019137:
9: 44832: loss: 0.4949955765:
9: 48032: loss: 0.4943888191:
9: 51232: loss: 0.4944097255:
9: 54432: loss: 0.4936561513:
9: 57632: loss: 0.4933500179:
9: 60832: loss: 0.4932275502:
9: 64032: loss: 0.4929542226:
9: 67232: loss: 0.4922253770:
9: 70432: loss: 0.4916580738:
9: 73632: loss: 0.4910896506:
Dev-Acc: 9: Accuracy: 0.9418955445: precision: 0.5338753388: recall: 0.0334977045: f1: 0.0630400000
Train-Acc: 9: Accuracy: 0.8170402646: precision: 0.9372469636: recall: 0.0913154954: f1: 0.1664170610
10: 3232: loss: 0.4751734942:
10: 6432: loss: 0.4785818182:
10: 9632: loss: 0.4799566133:
10: 12832: loss: 0.4783537363:
10: 16032: loss: 0.4785869126:
10: 19232: loss: 0.4787368366:
10: 22432: loss: 0.4784091765:
10: 25632: loss: 0.4784815765:
10: 28832: loss: 0.4782025313:
10: 32032: loss: 0.4773204027:
10: 35232: loss: 0.4767068074:
10: 38432: loss: 0.4760298142:
10: 41632: loss: 0.4756679650:
10: 44832: loss: 0.4752478213:
10: 48032: loss: 0.4745133056:
10: 51232: loss: 0.4738266260:
10: 54432: loss: 0.4732697660:
10: 57632: loss: 0.4732724117:
10: 60832: loss: 0.4728310418:
10: 64032: loss: 0.4724933782:
10: 67232: loss: 0.4720879310:
10: 70432: loss: 0.4714202270:
10: 73632: loss: 0.4709285592:
Dev-Acc: 10: Accuracy: 0.9415581822: precision: 0.4928909953: recall: 0.0530522020: f1: 0.0957936752
Train-Acc: 10: Accuracy: 0.8205114603: precision: 0.9452054795: recall: 0.1088685819: f1: 0.1952484820
11: 3232: loss: 0.4611239153:
11: 6432: loss: 0.4599799752:
11: 9632: loss: 0.4596164803:
11: 12832: loss: 0.4587637883:
11: 16032: loss: 0.4580232744:
11: 19232: loss: 0.4587291579:
11: 22432: loss: 0.4575314675:
11: 25632: loss: 0.4579370189:
11: 28832: loss: 0.4587704776:
11: 32032: loss: 0.4584100083:
11: 35232: loss: 0.4576339200:
11: 38432: loss: 0.4566348289:
11: 41632: loss: 0.4558744032:
11: 44832: loss: 0.4554045056:
11: 48032: loss: 0.4547656698:
11: 51232: loss: 0.4546538964:
11: 54432: loss: 0.4539593518:
11: 57632: loss: 0.4538184494:
11: 60832: loss: 0.4536515633:
11: 64032: loss: 0.4531851821:
11: 67232: loss: 0.4527622109:
11: 70432: loss: 0.4523443804:
11: 73632: loss: 0.4518481519:
Dev-Acc: 11: Accuracy: 0.9406552315: precision: 0.4454148472: recall: 0.0693759565: f1: 0.1200529645
Train-Acc: 11: Accuracy: 0.8242324591: precision: 0.9501709819: recall: 0.1278679903: f1: 0.2254027118
12: 3232: loss: 0.4436496323:
12: 6432: loss: 0.4401849130:
12: 9632: loss: 0.4397053542:
12: 12832: loss: 0.4397250308:
12: 16032: loss: 0.4372619545:
12: 19232: loss: 0.4385229073:
12: 22432: loss: 0.4388361981:
12: 25632: loss: 0.4387792369:
12: 28832: loss: 0.4387006546:
12: 32032: loss: 0.4384278030:
12: 35232: loss: 0.4382108268:
12: 38432: loss: 0.4377715248:
12: 41632: loss: 0.4379356640:
12: 44832: loss: 0.4378233621:
12: 48032: loss: 0.4372220106:
12: 51232: loss: 0.4365101612:
12: 54432: loss: 0.4367699028:
12: 57632: loss: 0.4363616222:
12: 60832: loss: 0.4356862883:
12: 64032: loss: 0.4352081030:
12: 67232: loss: 0.4348017582:
12: 70432: loss: 0.4349844440:
12: 73632: loss: 0.4344575211:
Dev-Acc: 12: Accuracy: 0.9399408698: precision: 0.4317460317: recall: 0.0925012753: f1: 0.1523596135
Train-Acc: 12: Accuracy: 0.8297810555: precision: 0.9583164711: recall: 0.1556768128: f1: 0.2678430042
13: 3232: loss: 0.4234428224:
13: 6432: loss: 0.4251670700:
13: 9632: loss: 0.4216350868:
13: 12832: loss: 0.4208894759:
13: 16032: loss: 0.4221918795:
13: 19232: loss: 0.4232803034:
13: 22432: loss: 0.4233961489:
13: 25632: loss: 0.4223263834:
13: 28832: loss: 0.4214789989:
13: 32032: loss: 0.4203616560:
13: 35232: loss: 0.4207385525:
13: 38432: loss: 0.4203907706:
13: 41632: loss: 0.4209720372:
13: 44832: loss: 0.4210289447:
13: 48032: loss: 0.4203592959:
13: 51232: loss: 0.4198582649:
13: 54432: loss: 0.4194308839:
13: 57632: loss: 0.4192183659:
13: 60832: loss: 0.4197900459:
13: 64032: loss: 0.4194765390:
13: 67232: loss: 0.4187640349:
13: 70432: loss: 0.4183730703:
13: 73632: loss: 0.4177588160:
Dev-Acc: 13: Accuracy: 0.9394943714: precision: 0.4418853776: recall: 0.1402822649: f1: 0.2129581828
Train-Acc: 13: Accuracy: 0.8376438022: precision: 0.9658314351: recall: 0.1951219512: f1: 0.3246554364
14: 3232: loss: 0.4061866418:
14: 6432: loss: 0.4020968159:
14: 9632: loss: 0.4023310805:
14: 12832: loss: 0.4039936035:
14: 16032: loss: 0.4054473677:
14: 19232: loss: 0.4052086851:
14: 22432: loss: 0.4065020489:
14: 25632: loss: 0.4073326590:
14: 28832: loss: 0.4082730118:
14: 32032: loss: 0.4085123625:
14: 35232: loss: 0.4084878492:
14: 38432: loss: 0.4083923536:
14: 41632: loss: 0.4077329089:
14: 44832: loss: 0.4063660608:
14: 48032: loss: 0.4059416649:
14: 51232: loss: 0.4064008213:
14: 54432: loss: 0.4068368557:
14: 57632: loss: 0.4060485912:
14: 60832: loss: 0.4049218987:
14: 64032: loss: 0.4044201972:
14: 67232: loss: 0.4040584571:
14: 70432: loss: 0.4033542046:
14: 73632: loss: 0.4027295226:
Dev-Acc: 14: Accuracy: 0.9387601018: precision: 0.4389425094: recall: 0.1778609080: f1: 0.2531461762
Train-Acc: 14: Accuracy: 0.8437578082: precision: 0.9697910785: recall: 0.2258234173: f1: 0.3663413854
15: 3232: loss: 0.4029760900:
15: 6432: loss: 0.3978451283:
15: 9632: loss: 0.3985179584:
15: 12832: loss: 0.3947639581:
15: 16032: loss: 0.3971168209:
15: 19232: loss: 0.3963147536:
15: 22432: loss: 0.3965319438:
15: 25632: loss: 0.3950445953:
15: 28832: loss: 0.3939606904:
15: 32032: loss: 0.3935766671:
15: 35232: loss: 0.3929844188:
15: 38432: loss: 0.3932213353:
15: 41632: loss: 0.3930177350:
15: 44832: loss: 0.3925740186:
15: 48032: loss: 0.3918718194:
15: 51232: loss: 0.3910927408:
15: 54432: loss: 0.3908133738:
15: 57632: loss: 0.3905951043:
15: 60832: loss: 0.3902699417:
15: 64032: loss: 0.3896625075:
15: 67232: loss: 0.3889429923:
15: 70432: loss: 0.3887096226:
15: 73632: loss: 0.3880117817:
Dev-Acc: 15: Accuracy: 0.9374603033: precision: 0.4310457516: recall: 0.2242815848: f1: 0.2950452969
Train-Acc: 15: Accuracy: 0.8521859050: precision: 0.9741935484: recall: 0.2680297153: f1: 0.4203959579
16: 3232: loss: 0.3848172179:
16: 6432: loss: 0.3813819031:
16: 9632: loss: 0.3828118946:
16: 12832: loss: 0.3834517610:
16: 16032: loss: 0.3811198361:
16: 19232: loss: 0.3814531675:
16: 22432: loss: 0.3807483032:
16: 25632: loss: 0.3827572861:
16: 28832: loss: 0.3810104354:
16: 32032: loss: 0.3800251974:
16: 35232: loss: 0.3797481595:
16: 38432: loss: 0.3795348636:
16: 41632: loss: 0.3788393957:
16: 44832: loss: 0.3786672869:
16: 48032: loss: 0.3793226954:
16: 51232: loss: 0.3782015485:
16: 54432: loss: 0.3771681107:
16: 57632: loss: 0.3769840128:
16: 60832: loss: 0.3760919136:
16: 64032: loss: 0.3762684821:
16: 67232: loss: 0.3758313045:
16: 70432: loss: 0.3757525345:
16: 73632: loss: 0.3753678961:
Dev-Acc: 16: Accuracy: 0.9349995852: precision: 0.4189254598: recall: 0.2943376977: f1: 0.3457505243
Train-Acc: 16: Accuracy: 0.8586417437: precision: 0.9769033362: recall: 0.3003089869: f1: 0.4593955851
17: 3232: loss: 0.3745927481:
17: 6432: loss: 0.3735975596:
17: 9632: loss: 0.3690825712:
17: 12832: loss: 0.3683265345:
17: 16032: loss: 0.3668822812:
17: 19232: loss: 0.3684184516:
17: 22432: loss: 0.3677056704:
17: 25632: loss: 0.3663602610:
17: 28832: loss: 0.3662658050:
17: 32032: loss: 0.3653909306:
17: 35232: loss: 0.3648257034:
17: 38432: loss: 0.3640930910:
17: 41632: loss: 0.3636083068:
17: 44832: loss: 0.3642014180:
17: 48032: loss: 0.3638259588:
17: 51232: loss: 0.3638410912:
17: 54432: loss: 0.3637851123:
17: 57632: loss: 0.3634010175:
17: 60832: loss: 0.3632982864:
17: 64032: loss: 0.3624568150:
17: 67232: loss: 0.3623202472:
17: 70432: loss: 0.3621136948:
17: 73632: loss: 0.3623185459:
Dev-Acc: 17: Accuracy: 0.9320130348: precision: 0.4044479433: recall: 0.3494303690: f1: 0.3749315818
Train-Acc: 17: Accuracy: 0.8663992882: precision: 0.9782196970: recall: 0.3395568996: f1: 0.5041237617
18: 3232: loss: 0.3674266034:
18: 6432: loss: 0.3609008494:
18: 9632: loss: 0.3617002230:
18: 12832: loss: 0.3576005810:
18: 16032: loss: 0.3570515396:
18: 19232: loss: 0.3557574288:
18: 22432: loss: 0.3540053924:
18: 25632: loss: 0.3532580885:
18: 28832: loss: 0.3537650273:
18: 32032: loss: 0.3535803474:
18: 35232: loss: 0.3539427632:
18: 38432: loss: 0.3529399275:
18: 41632: loss: 0.3527078132:
18: 44832: loss: 0.3526006324:
18: 48032: loss: 0.3524730739:
18: 51232: loss: 0.3517975011:
18: 54432: loss: 0.3521324343:
18: 57632: loss: 0.3524219331:
18: 60832: loss: 0.3517823997:
18: 64032: loss: 0.3513670558:
18: 67232: loss: 0.3516909758:
18: 70432: loss: 0.3512238319:
18: 73632: loss: 0.3513060519:
Dev-Acc: 18: Accuracy: 0.9210886359: precision: 0.3535067873: recall: 0.4250977725: f1: 0.3860109627
Train-Acc: 18: Accuracy: 0.8756425977: precision: 0.9542081162: recall: 0.3972782855: f1: 0.5609914593
19: 3232: loss: 0.3445899649:
19: 6432: loss: 0.3416265397:
19: 9632: loss: 0.3376171165:
19: 12832: loss: 0.3394196301:
19: 16032: loss: 0.3417427451:
19: 19232: loss: 0.3438705281:
19: 22432: loss: 0.3430975782:
19: 25632: loss: 0.3412609567:
19: 28832: loss: 0.3411706256:
19: 32032: loss: 0.3411961425:
19: 35232: loss: 0.3405390456:
19: 38432: loss: 0.3413936988:
19: 41632: loss: 0.3415036013:
19: 44832: loss: 0.3414758577:
19: 48032: loss: 0.3402089484:
19: 51232: loss: 0.3403637602:
19: 54432: loss: 0.3404255293:
19: 57632: loss: 0.3404533689:
19: 60832: loss: 0.3405806702:
19: 64032: loss: 0.3396584278:
19: 67232: loss: 0.3393266763:
19: 70432: loss: 0.3393901071:
19: 73632: loss: 0.3397695238:
Dev-Acc: 19: Accuracy: 0.9107000828: precision: 0.3208089165: recall: 0.4747491923: f1: 0.3828853538
Train-Acc: 19: Accuracy: 0.8797843456: precision: 0.9050734312: recall: 0.4456643219: f1: 0.5972424122
20: 3232: loss: 0.3285494763:
20: 6432: loss: 0.3350893898:
20: 9632: loss: 0.3375929162:
20: 12832: loss: 0.3385744903:
20: 16032: loss: 0.3393995653:
20: 19232: loss: 0.3385224388:
20: 22432: loss: 0.3384240671:
20: 25632: loss: 0.3366737086:
20: 28832: loss: 0.3361264041:
20: 32032: loss: 0.3358356412:
20: 35232: loss: 0.3352937938:
20: 38432: loss: 0.3344482285:
20: 41632: loss: 0.3338271452:
20: 44832: loss: 0.3334945552:
20: 48032: loss: 0.3327138936:
20: 51232: loss: 0.3326133081:
20: 54432: loss: 0.3326179707:
20: 57632: loss: 0.3324195294:
20: 60832: loss: 0.3320196912:
20: 64032: loss: 0.3315773589:
20: 67232: loss: 0.3308916203:
20: 70432: loss: 0.3305843285:
20: 73632: loss: 0.3297685180:
Dev-Acc: 20: Accuracy: 0.9035362601: precision: 0.3005918389: recall: 0.4922632205: f1: 0.3732594121
Train-Acc: 20: Accuracy: 0.8832555413: precision: 0.9014709612: recall: 0.4673591480: f1: 0.6155777807
