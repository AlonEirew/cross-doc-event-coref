1: 6464: loss: 0.7008759242:
1: 12864: loss: 0.6904677752:
1: 19264: loss: 0.6807627285:
1: 25664: loss: 0.6709724483:
1: 32064: loss: 0.6616445707:
1: 38464: loss: 0.6523873326:
1: 44864: loss: 0.6435491943:
1: 51264: loss: 0.6344239315:
1: 57664: loss: 0.6254920697:
1: 64064: loss: 0.6162089209:
1: 70464: loss: 0.6075299078:
1: 76864: loss: 0.5988485303:
1: 83264: loss: 0.5909141709:
1: 89664: loss: 0.5825323643:
1: 96064: loss: 0.5745017576:
1: 102464: loss: 0.5666071771:
1: 108864: loss: 0.5588314016:
1: 115264: loss: 0.5511299502:
1: 121664: loss: 0.5439459553:
1: 128064: loss: 0.5367452570:
1: 134464: loss: 0.5301620355:
1: 140864: loss: 0.5232847624:
1: 147264: loss: 0.5165515167:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.8999999762: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 6464: loss: 0.3622369285:
2: 12864: loss: 0.3563471796:
2: 19264: loss: 0.3525448110:
2: 25664: loss: 0.3470957889:
2: 32064: loss: 0.3456113277:
2: 38464: loss: 0.3451546925:
2: 44864: loss: 0.3433603198:
2: 51264: loss: 0.3415171685:
2: 57664: loss: 0.3377031573:
2: 64064: loss: 0.3348801466:
2: 70464: loss: 0.3329122035:
2: 76864: loss: 0.3311662701:
2: 83264: loss: 0.3283521939:
2: 89664: loss: 0.3262754044:
2: 96064: loss: 0.3244970599:
2: 102464: loss: 0.3223667886:
2: 108864: loss: 0.3196624422:
2: 115264: loss: 0.3177486303:
2: 121664: loss: 0.3158520917:
2: 128064: loss: 0.3135761383:
2: 134464: loss: 0.3115734232:
2: 140864: loss: 0.3096666749:
2: 147264: loss: 0.3080537202:
Dev-Acc: 2: Accuracy: 0.9416772127: precision: 0.6666666667: recall: 0.0010202347: f1: 0.0020373514
Train-Acc: 2: Accuracy: 0.9007823467: precision: 0.9407407407: recall: 0.0083492210: f1: 0.0165515444
3: 6464: loss: 0.2743717730:
3: 12864: loss: 0.2674183364:
3: 19264: loss: 0.2686126666:
3: 25664: loss: 0.2660177427:
3: 32064: loss: 0.2662169539:
3: 38464: loss: 0.2641484386:
3: 44864: loss: 0.2630684911:
3: 51264: loss: 0.2622691625:
3: 57664: loss: 0.2617618690:
3: 64064: loss: 0.2600967545:
3: 70464: loss: 0.2585082737:
3: 76864: loss: 0.2583392614:
3: 83264: loss: 0.2579397267:
3: 89664: loss: 0.2569801965:
3: 96064: loss: 0.2558520809:
3: 102464: loss: 0.2548170676:
3: 108864: loss: 0.2540646777:
3: 115264: loss: 0.2535156383:
3: 121664: loss: 0.2534484490:
3: 128064: loss: 0.2520587946:
3: 134464: loss: 0.2514382485:
3: 140864: loss: 0.2509484878:
3: 147264: loss: 0.2503117909:
Dev-Acc: 3: Accuracy: 0.9436418414: precision: 0.5623062616: recall: 0.1542254719: f1: 0.2420603149
Train-Acc: 3: Accuracy: 0.9153770208: precision: 0.8189255522: recall: 0.1974229176: f1: 0.3181481089
4: 6464: loss: 0.2313988673:
4: 12864: loss: 0.2322437107:
4: 19264: loss: 0.2324315730:
4: 25664: loss: 0.2323813169:
4: 32064: loss: 0.2314796352:
4: 38464: loss: 0.2304048796:
4: 44864: loss: 0.2290717914:
4: 51264: loss: 0.2279056819:
4: 57664: loss: 0.2275962042:
4: 64064: loss: 0.2277513882:
4: 70464: loss: 0.2283039361:
4: 76864: loss: 0.2268456476:
4: 83264: loss: 0.2256453806:
4: 89664: loss: 0.2249749363:
4: 96064: loss: 0.2248806360:
4: 102464: loss: 0.2244377476:
4: 108864: loss: 0.2240899226:
4: 115264: loss: 0.2237276643:
4: 121664: loss: 0.2229643666:
4: 128064: loss: 0.2223261544:
4: 134464: loss: 0.2224097628:
4: 140864: loss: 0.2222202876:
4: 147264: loss: 0.2216657943:
Dev-Acc: 4: Accuracy: 0.9389585257: precision: 0.4662347371: recall: 0.3181431729: f1: 0.3782090156
Train-Acc: 4: Accuracy: 0.9237130880: precision: 0.7887910328: recall: 0.3238445862: f1: 0.4591722595
5: 6464: loss: 0.2145007887:
5: 12864: loss: 0.2100266013:
5: 19264: loss: 0.2079762363:
5: 25664: loss: 0.2072713914:
5: 32064: loss: 0.2078608024:
5: 38464: loss: 0.2094789083:
5: 44864: loss: 0.2088115783:
5: 51264: loss: 0.2098085957:
5: 57664: loss: 0.2098209609:
5: 64064: loss: 0.2098301459:
5: 70464: loss: 0.2090645212:
5: 76864: loss: 0.2080905226:
5: 83264: loss: 0.2081089101:
5: 89664: loss: 0.2082932202:
5: 96064: loss: 0.2083711410:
5: 102464: loss: 0.2086465092:
5: 108864: loss: 0.2082377398:
5: 115264: loss: 0.2075345493:
5: 121664: loss: 0.2070325729:
5: 128064: loss: 0.2062856745:
5: 134464: loss: 0.2061125146:
5: 140864: loss: 0.2058295080:
5: 147264: loss: 0.2056211821:
Dev-Acc: 5: Accuracy: 0.9361108541: precision: 0.4403080873: recall: 0.3499404863: f1: 0.3899573662
Train-Acc: 5: Accuracy: 0.9275063872: precision: 0.7830086580: recall: 0.3805141016: f1: 0.5121444056
6: 6464: loss: 0.2026321195:
6: 12864: loss: 0.2020106829:
6: 19264: loss: 0.2008162425:
6: 25664: loss: 0.2013293035:
6: 32064: loss: 0.1995992821:
6: 38464: loss: 0.1997192204:
6: 44864: loss: 0.1976707526:
6: 51264: loss: 0.1983463540:
6: 57664: loss: 0.1990115398:
6: 64064: loss: 0.1980605934:
6: 70464: loss: 0.1969055035:
6: 76864: loss: 0.1967276046:
6: 83264: loss: 0.1964186181:
6: 89664: loss: 0.1963308721:
6: 96064: loss: 0.1964547734:
6: 102464: loss: 0.1957416430:
6: 108864: loss: 0.1957139405:
6: 115264: loss: 0.1956221929:
6: 121664: loss: 0.1958091787:
6: 128064: loss: 0.1955191660:
6: 134464: loss: 0.1953246344:
6: 140864: loss: 0.1949550816:
6: 147264: loss: 0.1952631614:
Dev-Acc: 6: Accuracy: 0.9341165423: precision: 0.4265246854: recall: 0.3745961571: f1: 0.3988774217
Train-Acc: 6: Accuracy: 0.9298205376: precision: 0.7820193982: recall: 0.4134507922: f1: 0.5409194513
7: 6464: loss: 0.1920974795:
7: 12864: loss: 0.1901429554:
7: 19264: loss: 0.1892912324:
7: 25664: loss: 0.1882760379:
7: 32064: loss: 0.1906607429:
7: 38464: loss: 0.1902178232:
7: 44864: loss: 0.1892993096:
7: 51264: loss: 0.1903810814:
7: 57664: loss: 0.1897641174:
7: 64064: loss: 0.1894459135:
7: 70464: loss: 0.1893771184:
7: 76864: loss: 0.1893249259:
7: 83264: loss: 0.1896107299:
7: 89664: loss: 0.1889978794:
7: 96064: loss: 0.1890225873:
7: 102464: loss: 0.1886436589:
7: 108864: loss: 0.1884983475:
7: 115264: loss: 0.1885663567:
7: 121664: loss: 0.1881637900:
7: 128064: loss: 0.1881605783:
7: 134464: loss: 0.1878180385:
7: 140864: loss: 0.1876346806:
7: 147264: loss: 0.1877414838:
Dev-Acc: 7: Accuracy: 0.9336700439: precision: 0.4249439881: recall: 0.3870090121: f1: 0.4050903266
Train-Acc: 7: Accuracy: 0.9314311743: precision: 0.7838062448: recall: 0.4340280060: f1: 0.5586866379
8: 6464: loss: 0.1869376804:
8: 12864: loss: 0.1822579892:
8: 19264: loss: 0.1835642006:
8: 25664: loss: 0.1842498744:
8: 32064: loss: 0.1841501644:
8: 38464: loss: 0.1845885894:
8: 44864: loss: 0.1852366034:
8: 51264: loss: 0.1840494795:
8: 57664: loss: 0.1835081696:
8: 64064: loss: 0.1834786701:
8: 70464: loss: 0.1833975850:
8: 76864: loss: 0.1834289843:
8: 83264: loss: 0.1827428881:
8: 89664: loss: 0.1821036530:
8: 96064: loss: 0.1821453447:
8: 102464: loss: 0.1811960715:
8: 108864: loss: 0.1820340147:
8: 115264: loss: 0.1820330922:
8: 121664: loss: 0.1823795002:
8: 128064: loss: 0.1816942217:
8: 134464: loss: 0.1813928191:
8: 140864: loss: 0.1812956114:
8: 147264: loss: 0.1817570452:
Dev-Acc: 8: Accuracy: 0.9329853654: precision: 0.4214786832: recall: 0.3984016324: f1: 0.4096153846
Train-Acc: 8: Accuracy: 0.9331141710: precision: 0.7838386115: recall: 0.4572348958: f1: 0.5775618668
9: 6464: loss: 0.1739468215:
9: 12864: loss: 0.1794416819:
9: 19264: loss: 0.1774250733:
9: 25664: loss: 0.1785426156:
9: 32064: loss: 0.1783849479:
9: 38464: loss: 0.1768157485:
9: 44864: loss: 0.1767028637:
9: 51264: loss: 0.1774357756:
9: 57664: loss: 0.1773454926:
9: 64064: loss: 0.1778344577:
9: 70464: loss: 0.1783719381:
9: 76864: loss: 0.1786445820:
9: 83264: loss: 0.1781406631:
9: 89664: loss: 0.1778280241:
9: 96064: loss: 0.1777841477:
9: 102464: loss: 0.1781671519:
9: 108864: loss: 0.1782681887:
9: 115264: loss: 0.1782777607:
9: 121664: loss: 0.1778861605:
9: 128064: loss: 0.1776590641:
9: 134464: loss: 0.1776137513:
9: 140864: loss: 0.1774511050:
9: 147264: loss: 0.1774796340:
Dev-Acc: 9: Accuracy: 0.9322313070: precision: 0.4168856192: recall: 0.4046930794: f1: 0.4106988783
Train-Acc: 9: Accuracy: 0.9342120886: precision: 0.7857456622: recall: 0.4703832753: f1: 0.5884771970
10: 6464: loss: 0.1757793796:
10: 12864: loss: 0.1705350404:
10: 19264: loss: 0.1728246219:
10: 25664: loss: 0.1766228525:
10: 32064: loss: 0.1773195798:
10: 38464: loss: 0.1748870402:
10: 44864: loss: 0.1742486809:
10: 51264: loss: 0.1740220974:
10: 57664: loss: 0.1741918959:
10: 64064: loss: 0.1738042984:
10: 70464: loss: 0.1743163306:
10: 76864: loss: 0.1734526876:
10: 83264: loss: 0.1734427317:
10: 89664: loss: 0.1735274218:
10: 96064: loss: 0.1741495755:
10: 102464: loss: 0.1735031872:
10: 108864: loss: 0.1738073501:
10: 115264: loss: 0.1736742767:
10: 121664: loss: 0.1734407362:
10: 128064: loss: 0.1733258083:
10: 134464: loss: 0.1732617580:
10: 140864: loss: 0.1729337323:
10: 147264: loss: 0.1731445863:
Dev-Acc: 10: Accuracy: 0.9314374924: precision: 0.4126337239: recall: 0.4131950349: f1: 0.4129141886
Train-Acc: 10: Accuracy: 0.9355334640: precision: 0.7881437253: recall: 0.4859641049: f1: 0.6012200081
11: 6464: loss: 0.1680097425:
11: 12864: loss: 0.1671953901:
11: 19264: loss: 0.1708948812:
11: 25664: loss: 0.1679230906:
11: 32064: loss: 0.1680435182:
11: 38464: loss: 0.1677853467:
11: 44864: loss: 0.1664409239:
11: 51264: loss: 0.1678830870:
11: 57664: loss: 0.1676163271:
11: 64064: loss: 0.1676432242:
11: 70464: loss: 0.1681565198:
11: 76864: loss: 0.1691570379:
11: 83264: loss: 0.1694301982:
11: 89664: loss: 0.1690705039:
11: 96064: loss: 0.1686208220:
11: 102464: loss: 0.1685972180:
11: 108864: loss: 0.1687779145:
11: 115264: loss: 0.1688503295:
11: 121664: loss: 0.1687580100:
11: 128064: loss: 0.1691351163:
11: 134464: loss: 0.1692600212:
11: 140864: loss: 0.1690418930:
11: 147264: loss: 0.1693326991:
Dev-Acc: 11: Accuracy: 0.9304056168: precision: 0.4071767983: recall: 0.4225471859: f1: 0.4147196262
Train-Acc: 11: Accuracy: 0.9365064502: precision: 0.7885274862: recall: 0.4988495168: f1: 0.6110976887
12: 6464: loss: 0.1702216575:
12: 12864: loss: 0.1662736602:
12: 19264: loss: 0.1640736489:
12: 25664: loss: 0.1643544086:
12: 32064: loss: 0.1658487325:
12: 38464: loss: 0.1657188854:
12: 44864: loss: 0.1661337466:
12: 51264: loss: 0.1649967569:
12: 57664: loss: 0.1651655901:
12: 64064: loss: 0.1651221073:
12: 70464: loss: 0.1638224629:
12: 76864: loss: 0.1648364820:
12: 83264: loss: 0.1655879623:
12: 89664: loss: 0.1652194676:
12: 96064: loss: 0.1654163846:
12: 102464: loss: 0.1659814080:
12: 108864: loss: 0.1658109653:
12: 115264: loss: 0.1656432418:
12: 121664: loss: 0.1657784431:
12: 128064: loss: 0.1657129946:
12: 134464: loss: 0.1656839696:
12: 140864: loss: 0.1660880428:
12: 147264: loss: 0.1659412071:
Dev-Acc: 12: Accuracy: 0.9296514988: precision: 0.4026413271: recall: 0.4250977725: f1: 0.4135649297
Train-Acc: 12: Accuracy: 0.9377753139: precision: 0.7934627171: recall: 0.5106830583: f1: 0.6214151434
13: 6464: loss: 0.1777090760:
13: 12864: loss: 0.1691801130:
13: 19264: loss: 0.1684424311:
13: 25664: loss: 0.1694156775:
13: 32064: loss: 0.1673425020:
13: 38464: loss: 0.1668699751:
13: 44864: loss: 0.1670660838:
13: 51264: loss: 0.1665780440:
13: 57664: loss: 0.1656869711:
13: 64064: loss: 0.1654313175:
13: 70464: loss: 0.1654248088:
13: 76864: loss: 0.1654227983:
13: 83264: loss: 0.1649669447:
13: 89664: loss: 0.1645532356:
13: 96064: loss: 0.1646803566:
13: 102464: loss: 0.1644006657:
13: 108864: loss: 0.1641947159:
13: 115264: loss: 0.1640126420:
13: 121664: loss: 0.1636372754:
13: 128064: loss: 0.1636325160:
13: 134464: loss: 0.1640441542:
13: 140864: loss: 0.1639291718:
13: 147264: loss: 0.1636111323:
Dev-Acc: 13: Accuracy: 0.9293141365: precision: 0.4001285554: recall: 0.4233973814: f1: 0.4114342366
Train-Acc: 13: Accuracy: 0.9391032457: precision: 0.8052761240: recall: 0.5157451844: f1: 0.6287821104
14: 6464: loss: 0.1662436671:
14: 12864: loss: 0.1652138733:
14: 19264: loss: 0.1632680830:
14: 25664: loss: 0.1634209431:
14: 32064: loss: 0.1641267624:
14: 38464: loss: 0.1631965530:
14: 44864: loss: 0.1621823627:
14: 51264: loss: 0.1633224128:
14: 57664: loss: 0.1630251600:
14: 64064: loss: 0.1637831113:
14: 70464: loss: 0.1631490060:
14: 76864: loss: 0.1622132033:
14: 83264: loss: 0.1624356229:
14: 89664: loss: 0.1623545339:
14: 96064: loss: 0.1620581616:
14: 102464: loss: 0.1617139074:
14: 108864: loss: 0.1618785678:
14: 115264: loss: 0.1620942793:
14: 121664: loss: 0.1617228237:
14: 128064: loss: 0.1613933149:
14: 134464: loss: 0.1615069736:
14: 140864: loss: 0.1610066158:
14: 147264: loss: 0.1603216336:
Dev-Acc: 14: Accuracy: 0.9279250503: precision: 0.3918686474: recall: 0.4261180071: f1: 0.4082763115
Train-Acc: 14: Accuracy: 0.9403063655: precision: 0.8103046867: recall: 0.5262638880: f1: 0.6381028298
15: 6464: loss: 0.1492152875:
15: 12864: loss: 0.1512561761:
15: 19264: loss: 0.1508024194:
15: 25664: loss: 0.1541959398:
15: 32064: loss: 0.1559051568:
15: 38464: loss: 0.1559684115:
15: 44864: loss: 0.1564812724:
15: 51264: loss: 0.1568493289:
15: 57664: loss: 0.1565370269:
15: 64064: loss: 0.1563405222:
15: 70464: loss: 0.1566410686:
15: 76864: loss: 0.1575666837:
15: 83264: loss: 0.1580796731:
15: 89664: loss: 0.1579076356:
15: 96064: loss: 0.1584884824:
15: 102464: loss: 0.1581881955:
15: 108864: loss: 0.1580466028:
15: 115264: loss: 0.1585692371:
15: 121664: loss: 0.1581525764:
15: 128064: loss: 0.1574434404:
15: 134464: loss: 0.1576939904:
15: 140864: loss: 0.1574862524:
15: 147264: loss: 0.1579148326:
Dev-Acc: 15: Accuracy: 0.9264664650: precision: 0.3840909091: recall: 0.4310491413: f1: 0.4062174505
Train-Acc: 15: Accuracy: 0.9417724013: precision: 0.8138707765: recall: 0.5415817500: f1: 0.6503769786
16: 6464: loss: 0.1488479018:
16: 12864: loss: 0.1518606755:
16: 19264: loss: 0.1490757010:
16: 25664: loss: 0.1509593495:
16: 32064: loss: 0.1536648664:
16: 38464: loss: 0.1540955793:
16: 44864: loss: 0.1548335127:
16: 51264: loss: 0.1543259436:
16: 57664: loss: 0.1533004844:
16: 64064: loss: 0.1533540005:
16: 70464: loss: 0.1534029078:
16: 76864: loss: 0.1543163228:
16: 83264: loss: 0.1551201490:
16: 89664: loss: 0.1556240401:
16: 96064: loss: 0.1559274898:
16: 102464: loss: 0.1559423316:
16: 108864: loss: 0.1557726572:
16: 115264: loss: 0.1554307973:
16: 121664: loss: 0.1555907767:
16: 128064: loss: 0.1557112951:
16: 134464: loss: 0.1554270617:
16: 140864: loss: 0.1552761112:
16: 147264: loss: 0.1554449768:
Dev-Acc: 16: Accuracy: 0.9256925583: precision: 0.3795325142: recall: 0.4307090631: f1: 0.4035045798
Train-Acc: 16: Accuracy: 0.9427256584: precision: 0.8177373619: recall: 0.5497994872: f1: 0.6575202453
17: 6464: loss: 0.1565039441:
17: 12864: loss: 0.1559962685:
17: 19264: loss: 0.1567757304:
17: 25664: loss: 0.1559443464:
17: 32064: loss: 0.1548469339:
17: 38464: loss: 0.1524721321:
17: 44864: loss: 0.1527440091:
17: 51264: loss: 0.1528957749:
17: 57664: loss: 0.1536806304:
17: 64064: loss: 0.1547657790:
17: 70464: loss: 0.1549563672:
17: 76864: loss: 0.1548854111:
17: 83264: loss: 0.1535866966:
17: 89664: loss: 0.1547624634:
17: 96064: loss: 0.1549683588:
17: 102464: loss: 0.1556209794:
17: 108864: loss: 0.1546623269:
17: 115264: loss: 0.1546431615:
17: 121664: loss: 0.1541462926:
17: 128064: loss: 0.1538331366:
17: 134464: loss: 0.1540762960:
17: 140864: loss: 0.1538578991:
17: 147264: loss: 0.1538802514:
Dev-Acc: 17: Accuracy: 0.9251468182: precision: 0.3760620063: recall: 0.4290086720: f1: 0.4007942812
Train-Acc: 17: Accuracy: 0.9433173537: precision: 0.8212579230: recall: 0.5536782592: f1: 0.6614309275
18: 6464: loss: 0.1526522321:
18: 12864: loss: 0.1471091023:
18: 19264: loss: 0.1499070341:
18: 25664: loss: 0.1511757569:
18: 32064: loss: 0.1518298285:
18: 38464: loss: 0.1517829239:
18: 44864: loss: 0.1516620491:
18: 51264: loss: 0.1510693370:
18: 57664: loss: 0.1512352418:
18: 64064: loss: 0.1518100437:
18: 70464: loss: 0.1515180199:
18: 76864: loss: 0.1517859224:
18: 83264: loss: 0.1515099356:
18: 89664: loss: 0.1518383738:
18: 96064: loss: 0.1525482229:
18: 102464: loss: 0.1522278377:
18: 108864: loss: 0.1522722400:
18: 115264: loss: 0.1516684234:
18: 121664: loss: 0.1518255896:
18: 128064: loss: 0.1516521132:
18: 134464: loss: 0.1517016553:
18: 140864: loss: 0.1512904569:
18: 147264: loss: 0.1514721720:
Dev-Acc: 18: Accuracy: 0.9236287475: precision: 0.3689754690: recall: 0.4347900017: f1: 0.3991881976
Train-Acc: 18: Accuracy: 0.9444481134: precision: 0.8215542661: recall: 0.5678127671: f1: 0.6715129840
19: 6464: loss: 0.1458140893:
19: 12864: loss: 0.1431310434:
19: 19264: loss: 0.1483953324:
19: 25664: loss: 0.1511599927:
19: 32064: loss: 0.1506666715:
19: 38464: loss: 0.1489461144:
19: 44864: loss: 0.1494653009:
19: 51264: loss: 0.1488244011:
19: 57664: loss: 0.1488253556:
19: 64064: loss: 0.1482669266:
19: 70464: loss: 0.1483158032:
19: 76864: loss: 0.1483893724:
19: 83264: loss: 0.1477641780:
19: 89664: loss: 0.1481796185:
19: 96064: loss: 0.1485646946:
19: 102464: loss: 0.1488667949:
19: 108864: loss: 0.1489968241:
19: 115264: loss: 0.1487451564:
19: 121664: loss: 0.1489752500:
19: 128064: loss: 0.1489081066:
19: 134464: loss: 0.1490748523:
19: 140864: loss: 0.1491578503:
19: 147264: loss: 0.1495380800:
Dev-Acc: 19: Accuracy: 0.9224976301: precision: 0.3631205674: recall: 0.4353001190: f1: 0.3959477225
Train-Acc: 19: Accuracy: 0.9450923800: precision: 0.8222305741: recall: 0.5753073434: f1: 0.6769552100
20: 6464: loss: 0.1489125318:
20: 12864: loss: 0.1445962295:
20: 19264: loss: 0.1468580678:
20: 25664: loss: 0.1468018576:
20: 32064: loss: 0.1477592049:
20: 38464: loss: 0.1478534200:
20: 44864: loss: 0.1472199758:
20: 51264: loss: 0.1477912013:
20: 57664: loss: 0.1488073971:
20: 64064: loss: 0.1485699143:
20: 70464: loss: 0.1487753093:
20: 76864: loss: 0.1487606515:
20: 83264: loss: 0.1483101631:
20: 89664: loss: 0.1483511273:
20: 96064: loss: 0.1479843935:
20: 102464: loss: 0.1479137869:
20: 108864: loss: 0.1486193544:
20: 115264: loss: 0.1481106180:
20: 121664: loss: 0.1475482990:
20: 128064: loss: 0.1474687476:
20: 134464: loss: 0.1474470424:
20: 140864: loss: 0.1470572010:
20: 147264: loss: 0.1471611037:
Dev-Acc: 20: Accuracy: 0.9227456450: precision: 0.3638702301: recall: 0.4329195715: f1: 0.3954030129
Train-Acc: 20: Accuracy: 0.9458878636: precision: 0.8333970195: recall: 0.5735323121: f1: 0.6794657113
