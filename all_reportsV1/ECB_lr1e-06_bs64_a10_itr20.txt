1: 6464: loss: 0.7003099084:
1: 12864: loss: 0.6891972101:
1: 19264: loss: 0.6788906439:
1: 25664: loss: 0.6693166433:
1: 32064: loss: 0.6597629586:
1: 38464: loss: 0.6503552084:
1: 44864: loss: 0.6409998129:
1: 51264: loss: 0.6313123555:
1: 57664: loss: 0.6218962847:
1: 64064: loss: 0.6127555434:
1: 70464: loss: 0.6037623211:
1: 76864: loss: 0.5947674070:
1: 83264: loss: 0.5860269049:
1: 89664: loss: 0.5778633230:
1: 96064: loss: 0.5692241460:
1: 102464: loss: 0.5609148041:
1: 108864: loss: 0.5528290821:
1: 115264: loss: 0.5451239493:
1: 121664: loss: 0.5374306945:
1: 128064: loss: 0.5298358110:
1: 134464: loss: 0.5227499948:
1: 140864: loss: 0.5154916327:
1: 147264: loss: 0.5085175194:
1: 153664: loss: 0.5018978581:
1: 160064: loss: 0.4954770032:
1: 166464: loss: 0.4893019684:
Dev-Acc: 1: Accuracy: 0.9416474700: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
Train-Acc: 1: Accuracy: 0.9090909362: precision: 0.0000000000: recall: 0.0000000000: f1: 0.0000000000
2: 6464: loss: 0.3383405261:
2: 12864: loss: 0.3308587916:
2: 19264: loss: 0.3282122150:
2: 25664: loss: 0.3270955025:
2: 32064: loss: 0.3224783925:
2: 38464: loss: 0.3195889473:
2: 44864: loss: 0.3168031365:
2: 51264: loss: 0.3144952069:
2: 57664: loss: 0.3128026620:
2: 64064: loss: 0.3114810143:
2: 70464: loss: 0.3102834418:
2: 76864: loss: 0.3084410581:
2: 83264: loss: 0.3058877371:
2: 89664: loss: 0.3037538207:
2: 96064: loss: 0.3019512912:
2: 102464: loss: 0.2999939442:
2: 108864: loss: 0.2983278304:
2: 115264: loss: 0.2964290404:
2: 121664: loss: 0.2946493737:
2: 128064: loss: 0.2932884169:
2: 134464: loss: 0.2917103303:
2: 140864: loss: 0.2900955067:
2: 147264: loss: 0.2880715618:
2: 153664: loss: 0.2868246016:
2: 160064: loss: 0.2856298251:
2: 166464: loss: 0.2840814040:
Dev-Acc: 2: Accuracy: 0.9416276217: precision: 0.4285714286: recall: 0.0010202347: f1: 0.0020356234
Train-Acc: 2: Accuracy: 0.9101068974: precision: 0.9126213592: recall: 0.0123594767: f1: 0.0243886619
3: 6464: loss: 0.2546761245:
3: 12864: loss: 0.2556055314:
3: 19264: loss: 0.2513129820:
3: 25664: loss: 0.2514921296:
3: 32064: loss: 0.2494693303:
3: 38464: loss: 0.2455144650:
3: 44864: loss: 0.2452780924:
3: 51264: loss: 0.2438733203:
3: 57664: loss: 0.2429462362:
3: 64064: loss: 0.2420366471:
3: 70464: loss: 0.2423458039:
3: 76864: loss: 0.2413488174:
3: 83264: loss: 0.2403564557:
3: 89664: loss: 0.2398782167:
3: 96064: loss: 0.2393572267:
3: 102464: loss: 0.2385629283:
3: 108864: loss: 0.2384575365:
3: 115264: loss: 0.2376445248:
3: 121664: loss: 0.2374698084:
3: 128064: loss: 0.2367096503:
3: 134464: loss: 0.2359292782:
3: 140864: loss: 0.2355777236:
3: 147264: loss: 0.2354797697:
3: 153664: loss: 0.2351651845:
3: 160064: loss: 0.2342328992:
3: 166464: loss: 0.2332144384:
Dev-Acc: 3: Accuracy: 0.9438502192: precision: 0.5740987984: recall: 0.1462336337: f1: 0.2330939152
Train-Acc: 3: Accuracy: 0.9217253327: precision: 0.7891137856: recall: 0.1896653737: f1: 0.3058249854
4: 6464: loss: 0.2128338098:
4: 12864: loss: 0.2167075222:
4: 19264: loss: 0.2142243119:
4: 25664: loss: 0.2138614261:
4: 32064: loss: 0.2154981583:
4: 38464: loss: 0.2151363716:
4: 44864: loss: 0.2147798438:
4: 51264: loss: 0.2141259357:
4: 57664: loss: 0.2141310250:
4: 64064: loss: 0.2139705446:
4: 70464: loss: 0.2142859785:
4: 76864: loss: 0.2141411033:
4: 83264: loss: 0.2149380584:
4: 89664: loss: 0.2139673182:
4: 96064: loss: 0.2132094464:
4: 102464: loss: 0.2120450635:
4: 108864: loss: 0.2124654730:
4: 115264: loss: 0.2125217149:
4: 121664: loss: 0.2126466393:
4: 128064: loss: 0.2121340476:
4: 134464: loss: 0.2118239073:
4: 140864: loss: 0.2114412269:
4: 147264: loss: 0.2115400591:
4: 153664: loss: 0.2109208827:
4: 160064: loss: 0.2103400405:
4: 166464: loss: 0.2097885913:
Dev-Acc: 4: Accuracy: 0.9397523403: precision: 0.4751883606: recall: 0.3110015304: f1: 0.3759506680
Train-Acc: 4: Accuracy: 0.9278452992: precision: 0.7562887945: recall: 0.3043849846: f1: 0.4340692823
5: 6464: loss: 0.2005553547:
5: 12864: loss: 0.2019640730:
5: 19264: loss: 0.1991791958:
5: 25664: loss: 0.1986776272:
5: 32064: loss: 0.1986224505:
5: 38464: loss: 0.1981346169:
5: 44864: loss: 0.1968753956:
5: 51264: loss: 0.1980624629:
5: 57664: loss: 0.1982104513:
5: 64064: loss: 0.1977975320:
5: 70464: loss: 0.1974902577:
5: 76864: loss: 0.1980683520:
5: 83264: loss: 0.1978614688:
5: 89664: loss: 0.1979038638:
5: 96064: loss: 0.1976596241:
5: 102464: loss: 0.1974736160:
5: 108864: loss: 0.1979705845:
5: 115264: loss: 0.1978744332:
5: 121664: loss: 0.1972050052:
5: 128064: loss: 0.1972662318:
5: 134464: loss: 0.1976631282:
5: 140864: loss: 0.1972141485:
5: 147264: loss: 0.1967355643:
5: 153664: loss: 0.1963911916:
5: 160064: loss: 0.1962821752:
5: 166464: loss: 0.1964114226:
Dev-Acc: 5: Accuracy: 0.9380457401: precision: 0.4591308264: recall: 0.3467097432: f1: 0.3950784732
Train-Acc: 5: Accuracy: 0.9308335781: precision: 0.7456116662: recall: 0.3630267570: f1: 0.4883052571
6: 6464: loss: 0.1956220353:
6: 12864: loss: 0.2019589875:
6: 19264: loss: 0.2011085025:
6: 25664: loss: 0.1980239009:
6: 32064: loss: 0.1965844866:
6: 38464: loss: 0.1951004592:
6: 44864: loss: 0.1943808758:
6: 51264: loss: 0.1945260057:
6: 57664: loss: 0.1939801712:
6: 64064: loss: 0.1931592355:
6: 70464: loss: 0.1918008826:
6: 76864: loss: 0.1906182578:
6: 83264: loss: 0.1914298600:
6: 89664: loss: 0.1913540876:
6: 96064: loss: 0.1909284377:
6: 102464: loss: 0.1903111577:
6: 108864: loss: 0.1902154239:
6: 115264: loss: 0.1897116902:
6: 121664: loss: 0.1894113576:
6: 128064: loss: 0.1886517302:
6: 134464: loss: 0.1882076289:
6: 140864: loss: 0.1882923917:
6: 147264: loss: 0.1879635488:
6: 153664: loss: 0.1877098681:
6: 160064: loss: 0.1877033262:
6: 166464: loss: 0.1873812826:
Dev-Acc: 6: Accuracy: 0.9363787770: precision: 0.4444676846: recall: 0.3613331066: f1: 0.3986118927
Train-Acc: 6: Accuracy: 0.9325906634: precision: 0.7462424850: recall: 0.3916902242: f1: 0.5137314076
7: 6464: loss: 0.1832481588:
7: 12864: loss: 0.1788549151:
7: 19264: loss: 0.1861220825:
7: 25664: loss: 0.1854052193:
7: 32064: loss: 0.1824000955:
7: 38464: loss: 0.1834565539:
7: 44864: loss: 0.1819361060:
7: 51264: loss: 0.1831312129:
7: 57664: loss: 0.1841599353:
7: 64064: loss: 0.1836372743:
7: 70464: loss: 0.1829732415:
7: 76864: loss: 0.1824811433:
7: 83264: loss: 0.1823531142:
7: 89664: loss: 0.1825808696:
7: 96064: loss: 0.1827942034:
7: 102464: loss: 0.1830119969:
7: 108864: loss: 0.1828165949:
7: 115264: loss: 0.1828599168:
7: 121664: loss: 0.1821469261:
7: 128064: loss: 0.1818738331:
7: 134464: loss: 0.1817054744:
7: 140864: loss: 0.1817317419:
7: 147264: loss: 0.1813243632:
7: 153664: loss: 0.1811506213:
7: 160064: loss: 0.1811666539:
7: 166464: loss: 0.1809440092:
Dev-Acc: 7: Accuracy: 0.9359322786: precision: 0.4420989144: recall: 0.3739160007: f1: 0.4051589129
Train-Acc: 7: Accuracy: 0.9342342019: precision: 0.7500297159: recall: 0.4148313720: f1: 0.5342025059
8: 6464: loss: 0.1804825114:
8: 12864: loss: 0.1789460761:
8: 19264: loss: 0.1790924311:
8: 25664: loss: 0.1776468576:
8: 32064: loss: 0.1791303071:
8: 38464: loss: 0.1781964148:
8: 44864: loss: 0.1778741394:
8: 51264: loss: 0.1783783434:
8: 57664: loss: 0.1780714831:
8: 64064: loss: 0.1782409665:
8: 70464: loss: 0.1776414175:
8: 76864: loss: 0.1776941322:
8: 83264: loss: 0.1784602086:
8: 89664: loss: 0.1785677887:
8: 96064: loss: 0.1775094072:
8: 102464: loss: 0.1771698206:
8: 108864: loss: 0.1770908255:
8: 115264: loss: 0.1767283631:
8: 121664: loss: 0.1769875568:
8: 128064: loss: 0.1768605769:
8: 134464: loss: 0.1766245120:
8: 140864: loss: 0.1763263708:
8: 147264: loss: 0.1764045644:
8: 153664: loss: 0.1762713968:
8: 160064: loss: 0.1760114365:
8: 166464: loss: 0.1762987961:
Dev-Acc: 8: Accuracy: 0.9352178574: precision: 0.4373792037: recall: 0.3847985037: f1: 0.4094075079
Train-Acc: 8: Accuracy: 0.9355610013: precision: 0.7496336377: recall: 0.4371836171: f1: 0.5522797110
9: 6464: loss: 0.1684849207:
9: 12864: loss: 0.1742106127:
9: 19264: loss: 0.1699682838:
9: 25664: loss: 0.1696068490:
9: 32064: loss: 0.1695166989:
9: 38464: loss: 0.1703558747:
9: 44864: loss: 0.1717751074:
9: 51264: loss: 0.1725535959:
9: 57664: loss: 0.1736699775:
9: 64064: loss: 0.1732956523:
9: 70464: loss: 0.1729798624:
9: 76864: loss: 0.1723438091:
9: 83264: loss: 0.1719341779:
9: 89664: loss: 0.1720014165:
9: 96064: loss: 0.1719761260:
9: 102464: loss: 0.1714003197:
9: 108864: loss: 0.1714111531:
9: 115264: loss: 0.1713517114:
9: 121664: loss: 0.1720832549:
9: 128064: loss: 0.1722551692:
9: 134464: loss: 0.1718850004:
9: 140864: loss: 0.1717906320:
9: 147264: loss: 0.1718142571:
9: 153664: loss: 0.1716882928:
9: 160064: loss: 0.1716496948:
9: 166464: loss: 0.1718332507:
Dev-Acc: 9: Accuracy: 0.9349400401: precision: 0.4352490421: recall: 0.3863288556: f1: 0.4093324926
Train-Acc: 9: Accuracy: 0.9367742538: precision: 0.7559116022: recall: 0.4497403195: f1: 0.5639503730
10: 6464: loss: 0.1614556055:
10: 12864: loss: 0.1644722026:
10: 19264: loss: 0.1668443299:
10: 25664: loss: 0.1641582793:
10: 32064: loss: 0.1670439331:
10: 38464: loss: 0.1676001689:
10: 44864: loss: 0.1688269738:
10: 51264: loss: 0.1694697366:
10: 57664: loss: 0.1686060340:
10: 64064: loss: 0.1690304602:
10: 70464: loss: 0.1678473720:
10: 76864: loss: 0.1673832939:
10: 83264: loss: 0.1677339624:
10: 89664: loss: 0.1672890926:
10: 96064: loss: 0.1670478428:
10: 102464: loss: 0.1676507933:
10: 108864: loss: 0.1683202809:
10: 115264: loss: 0.1688183452:
10: 121664: loss: 0.1684765895:
10: 128064: loss: 0.1686208550:
10: 134464: loss: 0.1685374395:
10: 140864: loss: 0.1683376057:
10: 147264: loss: 0.1680459146:
10: 153664: loss: 0.1680959990:
10: 160064: loss: 0.1680395251:
10: 166464: loss: 0.1682771495:
Dev-Acc: 10: Accuracy: 0.9345431924: precision: 0.4329336830: recall: 0.3929603809: f1: 0.4119796773
Train-Acc: 10: Accuracy: 0.9377304912: precision: 0.7583009918: recall: 0.4624285057: f1: 0.5745089231
11: 6464: loss: 0.1606294188:
11: 12864: loss: 0.1691967991:
11: 19264: loss: 0.1667485512:
11: 25664: loss: 0.1665184227:
11: 32064: loss: 0.1655844134:
11: 38464: loss: 0.1659044438:
11: 44864: loss: 0.1657988658:
11: 51264: loss: 0.1647129823:
11: 57664: loss: 0.1640283039:
11: 64064: loss: 0.1647393107:
11: 70464: loss: 0.1647605627:
11: 76864: loss: 0.1648702748:
11: 83264: loss: 0.1648705271:
11: 89664: loss: 0.1650574588:
11: 96064: loss: 0.1648227744:
11: 102464: loss: 0.1655550327:
11: 108864: loss: 0.1663960855:
11: 115264: loss: 0.1659149296:
11: 121664: loss: 0.1656883815:
11: 128064: loss: 0.1655370751:
11: 134464: loss: 0.1659050271:
11: 140864: loss: 0.1655609743:
11: 147264: loss: 0.1655665294:
11: 153664: loss: 0.1654514204:
11: 160064: loss: 0.1652664220:
11: 166464: loss: 0.1650792488:
Dev-Acc: 11: Accuracy: 0.9345828295: precision: 0.4331831832: recall: 0.3924502636: f1: 0.4118119368
Train-Acc: 11: Accuracy: 0.9386030436: precision: 0.7647437272: recall: 0.4688712116: f1: 0.5813261605
12: 6464: loss: 0.1632925119:
12: 12864: loss: 0.1649821625:
12: 19264: loss: 0.1638003602:
12: 25664: loss: 0.1629255343:
12: 32064: loss: 0.1617180685:
12: 38464: loss: 0.1620174337:
12: 44864: loss: 0.1620140376:
12: 51264: loss: 0.1621624512:
12: 57664: loss: 0.1621382423:
12: 64064: loss: 0.1622865160:
12: 70464: loss: 0.1624327843:
12: 76864: loss: 0.1623117982:
12: 83264: loss: 0.1618696425:
12: 89664: loss: 0.1616752918:
12: 96064: loss: 0.1613580393:
12: 102464: loss: 0.1615074531:
12: 108864: loss: 0.1617698179:
12: 115264: loss: 0.1628530350:
12: 121664: loss: 0.1621865579:
12: 128064: loss: 0.1622584889:
12: 134464: loss: 0.1621662729:
12: 140864: loss: 0.1625759147:
12: 147264: loss: 0.1621201308:
12: 153664: loss: 0.1621599130:
12: 160064: loss: 0.1620427586:
12: 166464: loss: 0.1620961623:
Dev-Acc: 12: Accuracy: 0.9334318638: precision: 0.4259921344: recall: 0.4052031967: f1: 0.4153376906
Train-Acc: 12: Accuracy: 0.9395772219: precision: 0.7635630877: recall: 0.4857668792: f1: 0.5937801350
13: 6464: loss: 0.1638218202:
13: 12864: loss: 0.1587498914:
13: 19264: loss: 0.1567692733:
13: 25664: loss: 0.1586798469:
13: 32064: loss: 0.1595042745:
13: 38464: loss: 0.1606242482:
13: 44864: loss: 0.1602180800:
13: 51264: loss: 0.1595065367:
13: 57664: loss: 0.1594892388:
13: 64064: loss: 0.1592256955:
13: 70464: loss: 0.1593854298:
13: 76864: loss: 0.1590063127:
13: 83264: loss: 0.1583596253:
13: 89664: loss: 0.1585125558:
13: 96064: loss: 0.1593114604:
13: 102464: loss: 0.1596631368:
13: 108864: loss: 0.1594469693:
13: 115264: loss: 0.1594459500:
13: 121664: loss: 0.1593475437:
13: 128064: loss: 0.1591874728:
13: 134464: loss: 0.1596165629:
13: 140864: loss: 0.1598103436:
13: 147264: loss: 0.1592491175:
13: 153664: loss: 0.1594158301:
13: 160064: loss: 0.1593581947:
13: 166464: loss: 0.1592375813:
Dev-Acc: 13: Accuracy: 0.9332731366: precision: 0.4248843005: recall: 0.4058833532: f1: 0.4151665362
Train-Acc: 13: Accuracy: 0.9405095577: precision: 0.7708397733: recall: 0.4918151338: f1: 0.6004976722
14: 6464: loss: 0.1538866082:
14: 12864: loss: 0.1528188142:
14: 19264: loss: 0.1528547932:
14: 25664: loss: 0.1555234202:
14: 32064: loss: 0.1563552584:
14: 38464: loss: 0.1558652339:
14: 44864: loss: 0.1569952124:
14: 51264: loss: 0.1570233369:
14: 57664: loss: 0.1564581074:
14: 64064: loss: 0.1557504402:
14: 70464: loss: 0.1557378736:
14: 76864: loss: 0.1558069747:
14: 83264: loss: 0.1563393900:
14: 89664: loss: 0.1572317328:
14: 96064: loss: 0.1569101695:
14: 102464: loss: 0.1570950802:
14: 108864: loss: 0.1565511382:
14: 115264: loss: 0.1570057518:
14: 121664: loss: 0.1569541291:
14: 128064: loss: 0.1572801905:
14: 134464: loss: 0.1572043946:
14: 140864: loss: 0.1572179174:
14: 147264: loss: 0.1574517904:
14: 153664: loss: 0.1573286752:
14: 160064: loss: 0.1569803725:
14: 166464: loss: 0.1568125204:
Dev-Acc: 14: Accuracy: 0.9328861833: precision: 0.4226930485: recall: 0.4104744091: f1: 0.4164941339
Train-Acc: 14: Accuracy: 0.9415076375: precision: 0.7773006135: recall: 0.4997699034: f1: 0.6083790164
15: 6464: loss: 0.1508142750:
15: 12864: loss: 0.1514633084:
15: 19264: loss: 0.1512082761:
15: 25664: loss: 0.1538020719:
15: 32064: loss: 0.1527227913:
15: 38464: loss: 0.1527360606:
15: 44864: loss: 0.1535894957:
15: 51264: loss: 0.1534142541:
15: 57664: loss: 0.1542075338:
15: 64064: loss: 0.1542509413:
15: 70464: loss: 0.1536728704:
15: 76864: loss: 0.1539571416:
15: 83264: loss: 0.1548914788:
15: 89664: loss: 0.1552929456:
15: 96064: loss: 0.1556431014:
15: 102464: loss: 0.1559184363:
15: 108864: loss: 0.1553346993:
15: 115264: loss: 0.1547472750:
15: 121664: loss: 0.1550769002:
15: 128064: loss: 0.1547495275:
15: 134464: loss: 0.1546099824:
15: 140864: loss: 0.1547200850:
15: 147264: loss: 0.1548841944:
15: 153664: loss: 0.1548161201:
15: 160064: loss: 0.1547321775:
15: 166464: loss: 0.1544874924:
Dev-Acc: 15: Accuracy: 0.9322015047: precision: 0.4183533448: recall: 0.4147253868: f1: 0.4165314661
Train-Acc: 15: Accuracy: 0.9421949387: precision: 0.7792116141: recall: 0.5081191243: f1: 0.6151213689
16: 6464: loss: 0.1590627443:
16: 12864: loss: 0.1584440183:
16: 19264: loss: 0.1576309821:
16: 25664: loss: 0.1570435267:
16: 32064: loss: 0.1570903644:
16: 38464: loss: 0.1550295938:
16: 44864: loss: 0.1561888884:
16: 51264: loss: 0.1547271157:
16: 57664: loss: 0.1539080588:
16: 64064: loss: 0.1540507831:
16: 70464: loss: 0.1538288124:
16: 76864: loss: 0.1530750394:
16: 83264: loss: 0.1528403828:
16: 89664: loss: 0.1527720212:
16: 96064: loss: 0.1526222817:
16: 102464: loss: 0.1529766091:
16: 108864: loss: 0.1521513380:
16: 115264: loss: 0.1518503026:
16: 121664: loss: 0.1516482473:
16: 128064: loss: 0.1518429064:
16: 134464: loss: 0.1516588762:
16: 140864: loss: 0.1517410777:
16: 147264: loss: 0.1516929103:
16: 153664: loss: 0.1518723708:
16: 160064: loss: 0.1521507894:
16: 166464: loss: 0.1521574294:
Dev-Acc: 16: Accuracy: 0.9319534898: precision: 0.4170768970: recall: 0.4177860908: f1: 0.4174311927
Train-Acc: 16: Accuracy: 0.9432109594: precision: 0.7853643907: recall: 0.5164683453: f1: 0.6231458713
17: 6464: loss: 0.1574645979:
17: 12864: loss: 0.1529832233:
17: 19264: loss: 0.1566698974:
17: 25664: loss: 0.1553911215:
17: 32064: loss: 0.1559836101:
17: 38464: loss: 0.1550052590:
17: 44864: loss: 0.1555154777:
17: 51264: loss: 0.1553870248:
17: 57664: loss: 0.1529601585:
17: 64064: loss: 0.1526141710:
17: 70464: loss: 0.1523786373:
17: 76864: loss: 0.1522676741:
17: 83264: loss: 0.1519277792:
17: 89664: loss: 0.1513870692:
17: 96064: loss: 0.1507307165:
17: 102464: loss: 0.1509243461:
17: 108864: loss: 0.1512851683:
17: 115264: loss: 0.1509939012:
17: 121664: loss: 0.1514183337:
17: 128064: loss: 0.1514253596:
17: 134464: loss: 0.1510650159:
17: 140864: loss: 0.1508533259:
17: 147264: loss: 0.1503519304:
17: 153664: loss: 0.1502880512:
17: 160064: loss: 0.1500677602:
17: 166464: loss: 0.1501356821:
Dev-Acc: 17: Accuracy: 0.9319236875: precision: 0.4172297297: recall: 0.4199965992: f1: 0.4186085925
Train-Acc: 17: Accuracy: 0.9438205957: precision: 0.7897676274: recall: 0.5206100848: f1: 0.6275457643
18: 6464: loss: 0.1518654712:
18: 12864: loss: 0.1491677368:
18: 19264: loss: 0.1483223516:
18: 25664: loss: 0.1491509760:
18: 32064: loss: 0.1484653825:
18: 38464: loss: 0.1482674442:
18: 44864: loss: 0.1476483860:
18: 51264: loss: 0.1469672777:
18: 57664: loss: 0.1483364692:
18: 64064: loss: 0.1476271181:
18: 70464: loss: 0.1477817859:
18: 76864: loss: 0.1474653207:
18: 83264: loss: 0.1477107968:
18: 89664: loss: 0.1475457127:
18: 96064: loss: 0.1478984214:
18: 102464: loss: 0.1478758698:
18: 108864: loss: 0.1477412478:
18: 115264: loss: 0.1477431614:
18: 121664: loss: 0.1476504562:
18: 128064: loss: 0.1474306734:
18: 134464: loss: 0.1476340411:
18: 140864: loss: 0.1474871786:
18: 147264: loss: 0.1471018174:
18: 153664: loss: 0.1475303904:
18: 160064: loss: 0.1480303057:
18: 166464: loss: 0.1482834417:
Dev-Acc: 18: Accuracy: 0.9309513569: precision: 0.4108501489: recall: 0.4223771467: f1: 0.4165339146
Train-Acc: 18: Accuracy: 0.9448485374: precision: 0.7919391041: recall: 0.5334954967: f1: 0.6375206222
19: 6464: loss: 0.1485873531:
19: 12864: loss: 0.1472547899:
19: 19264: loss: 0.1477737958:
19: 25664: loss: 0.1473775295:
19: 32064: loss: 0.1467562810:
19: 38464: loss: 0.1460784411:
19: 44864: loss: 0.1461718476:
19: 51264: loss: 0.1456371407:
19: 57664: loss: 0.1461669576:
19: 64064: loss: 0.1453184428:
19: 70464: loss: 0.1450751029:
19: 76864: loss: 0.1452159539:
19: 83264: loss: 0.1454561851:
19: 89664: loss: 0.1458639706:
19: 96064: loss: 0.1455941543:
19: 102464: loss: 0.1457865220:
19: 108864: loss: 0.1459379731:
19: 115264: loss: 0.1460673469:
19: 121664: loss: 0.1462645785:
19: 128064: loss: 0.1466555733:
19: 134464: loss: 0.1470724396:
19: 140864: loss: 0.1470494590:
19: 147264: loss: 0.1471506851:
19: 153664: loss: 0.1470547676:
19: 160064: loss: 0.1468877211:
19: 166464: loss: 0.1467729274:
Dev-Acc: 19: Accuracy: 0.9311795235: precision: 0.4120686781: recall: 0.4203366774: f1: 0.4161616162
Train-Acc: 19: Accuracy: 0.9454342127: precision: 0.7991146090: recall: 0.5340214319: f1: 0.6402112232
20: 6464: loss: 0.1536608848:
20: 12864: loss: 0.1487934035:
20: 19264: loss: 0.1432499952:
20: 25664: loss: 0.1434602859:
20: 32064: loss: 0.1442674015:
20: 38464: loss: 0.1448755536:
20: 44864: loss: 0.1446532998:
20: 51264: loss: 0.1442602773:
20: 57664: loss: 0.1451491395:
20: 64064: loss: 0.1454707846:
20: 70464: loss: 0.1451458985:
20: 76864: loss: 0.1450899370:
20: 83264: loss: 0.1444688025:
20: 89664: loss: 0.1436994088:
20: 96064: loss: 0.1439076358:
20: 102464: loss: 0.1439457546:
20: 108864: loss: 0.1441617937:
20: 115264: loss: 0.1445041422:
20: 121664: loss: 0.1448713552:
20: 128064: loss: 0.1447314330:
20: 134464: loss: 0.1445162429:
20: 140864: loss: 0.1445597363:
20: 147264: loss: 0.1447053247:
20: 153664: loss: 0.1447585858:
20: 160064: loss: 0.1444847724:
20: 166464: loss: 0.1447004879:
Dev-Acc: 20: Accuracy: 0.9302269816: precision: 0.4058871627: recall: 0.4220370685: f1: 0.4138046015
Train-Acc: 20: Accuracy: 0.9461573958: precision: 0.7984600577: recall: 0.5453947801: f1: 0.6480996836
