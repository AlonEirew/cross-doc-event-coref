1: 6464: loss: 0.6946794635:
1: 12864: loss: 0.6875292683:
1: 19264: loss: 0.6811869327:
1: 25664: loss: 0.6748032215:
1: 32064: loss: 0.6684374667:
1: 38464: loss: 0.6622086576:
1: 44864: loss: 0.6555770898:
1: 51264: loss: 0.6490566801:
1: 57664: loss: 0.6429199160:
Dev-Acc: 1: Accuracy: 0.9242339730: precision: 0.2970159611: recall: 0.2183302159: f1: 0.2516660133
Train-Acc: 1: Accuracy: 0.8180922270: precision: 0.9997587455: recall: 0.2724344225: f1: 0.4281876421
2: 6464: loss: 0.5758636624:
2: 12864: loss: 0.5691023704:
2: 19264: loss: 0.5619201295:
2: 25664: loss: 0.5551968219:
2: 32064: loss: 0.5483548664:
2: 38464: loss: 0.5419602430:
2: 44864: loss: 0.5354988128:
2: 51264: loss: 0.5289571728:
2: 57664: loss: 0.5231301725:
Dev-Acc: 2: Accuracy: 0.9123372436: precision: 0.2973380900: recall: 0.3684747492: f1: 0.3291062343
Train-Acc: 2: Accuracy: 0.8451121449: precision: 0.9996546365: recall: 0.3805798435: f1: 0.5512808304
3: 6464: loss: 0.4560969189:
3: 12864: loss: 0.4479925355:
3: 19264: loss: 0.4443500098:
3: 25664: loss: 0.4397078808:
3: 32064: loss: 0.4348902525:
3: 38464: loss: 0.4287814643:
3: 44864: loss: 0.4238581321:
3: 51264: loss: 0.4180596761:
3: 57664: loss: 0.4125372379:
Dev-Acc: 3: Accuracy: 0.8501547575: precision: 0.2086571880: recall: 0.5614691379: f1: 0.3042476735
Train-Acc: 3: Accuracy: 0.8789198995: precision: 0.9470025074: recall: 0.5462494248: f1: 0.6928496977
4: 6464: loss: 0.3555618495:
4: 12864: loss: 0.3529707409:
4: 19264: loss: 0.3515950753:
4: 25664: loss: 0.3459128371:
4: 32064: loss: 0.3427448951:
4: 38464: loss: 0.3379644374:
4: 44864: loss: 0.3351712109:
4: 51264: loss: 0.3323705916:
4: 57664: loss: 0.3286599878:
Dev-Acc: 4: Accuracy: 0.7907505035: precision: 0.1615329832: recall: 0.6170719265: f1: 0.2560412037
Train-Acc: 4: Accuracy: 0.8990040421: precision: 0.9495239984: recall: 0.6294786668: f1: 0.7570666140
5: 6464: loss: 0.3006081790:
5: 12864: loss: 0.2886954490:
5: 19264: loss: 0.2877781728:
5: 25664: loss: 0.2839614502:
5: 32064: loss: 0.2830097565:
5: 38464: loss: 0.2806420055:
5: 44864: loss: 0.2789252447:
5: 51264: loss: 0.2772879784:
5: 57664: loss: 0.2747882749:
Dev-Acc: 5: Accuracy: 0.7381727099: precision: 0.1361297420: recall: 0.6522700221: f1: 0.2252495596
Train-Acc: 5: Accuracy: 0.9153901935: precision: 0.9497631179: recall: 0.6985076589: f1: 0.8049852262
6: 6464: loss: 0.2560880882:
6: 12864: loss: 0.2501811765:
6: 19264: loss: 0.2503511918:
6: 25664: loss: 0.2468408089:
6: 32064: loss: 0.2464279140:
6: 38464: loss: 0.2450699465:
6: 44864: loss: 0.2436201999:
6: 51264: loss: 0.2413965467:
6: 57664: loss: 0.2392371742:
Dev-Acc: 6: Accuracy: 0.6914589405: precision: 0.1201072709: recall: 0.6777758885: f1: 0.2040544691
Train-Acc: 6: Accuracy: 0.9285550117: precision: 0.9519886836: recall: 0.7521530471: f1: 0.8403540343
7: 6464: loss: 0.2196898826:
7: 12864: loss: 0.2212151448:
7: 19264: loss: 0.2190524040:
7: 25664: loss: 0.2178171340:
7: 32064: loss: 0.2180788719:
7: 38464: loss: 0.2171397714:
7: 44864: loss: 0.2161212727:
7: 51264: loss: 0.2144428298:
7: 57664: loss: 0.2128235577:
Dev-Acc: 7: Accuracy: 0.6434553266: precision: 0.1084911805: recall: 0.7080428499: f1: 0.1881523655
Train-Acc: 7: Accuracy: 0.9367399216: precision: 0.9539715519: recall: 0.7848267701: f1: 0.8611722272
8: 6464: loss: 0.1960248701:
8: 12864: loss: 0.1944080310:
8: 19264: loss: 0.1953588916:
8: 25664: loss: 0.1969594171:
8: 32064: loss: 0.1963954066:
8: 38464: loss: 0.1949775205:
8: 44864: loss: 0.1946075829:
8: 51264: loss: 0.1936872496:
8: 57664: loss: 0.1931403069:
Dev-Acc: 8: Accuracy: 0.6058005095: precision: 0.1019006398: recall: 0.7366094202: f1: 0.1790341578
Train-Acc: 8: Accuracy: 0.9412432313: precision: 0.9540346496: recall: 0.8036946946: f1: 0.8724353256
9: 6464: loss: 0.1929085932:
9: 12864: loss: 0.1857981351:
9: 19264: loss: 0.1834446750:
9: 25664: loss: 0.1849405567:
9: 32064: loss: 0.1841063338:
9: 38464: loss: 0.1833451261:
9: 44864: loss: 0.1825607028:
9: 51264: loss: 0.1806882859:
9: 57664: loss: 0.1794703692:
Dev-Acc: 9: Accuracy: 0.5826123357: precision: 0.0982033800: recall: 0.7519129400: f1: 0.1737183265
Train-Acc: 9: Accuracy: 0.9442673326: precision: 0.9521805662: recall: 0.8181579120: f1: 0.8800961776
10: 6464: loss: 0.1642678531:
10: 12864: loss: 0.1742291968:
10: 19264: loss: 0.1732809447:
10: 25664: loss: 0.1718155316:
10: 32064: loss: 0.1712434864:
10: 38464: loss: 0.1714647765:
10: 44864: loss: 0.1697791685:
10: 51264: loss: 0.1685515511:
10: 57664: loss: 0.1691357303:
Dev-Acc: 10: Accuracy: 0.5692768693: precision: 0.0958800853: recall: 0.7570141132: f1: 0.1702030049
Train-Acc: 10: Accuracy: 0.9463875294: precision: 0.9512803082: recall: 0.8279534547: f1: 0.8853427065
11: 6464: loss: 0.1525250443:
11: 12864: loss: 0.1565560820:
11: 19264: loss: 0.1586809310:
11: 25664: loss: 0.1587983765:
11: 32064: loss: 0.1588800005:
11: 38464: loss: 0.1593353657:
11: 44864: loss: 0.1587279065:
11: 51264: loss: 0.1589843920:
11: 57664: loss: 0.1592525696:
Dev-Acc: 11: Accuracy: 0.5540165305: precision: 0.0937883420: recall: 0.7668763816: f1: 0.1671360806
Train-Acc: 11: Accuracy: 0.9492965937: precision: 0.9489115948: recall: 0.8425481559: f1: 0.8925723439
12: 6464: loss: 0.1638555677:
12: 12864: loss: 0.1577259138:
12: 19264: loss: 0.1570959320:
12: 25664: loss: 0.1585907914:
12: 32064: loss: 0.1563038566:
12: 38464: loss: 0.1540863664:
12: 44864: loss: 0.1533006229:
12: 51264: loss: 0.1538239944:
12: 57664: loss: 0.1537807879:
Dev-Acc: 12: Accuracy: 0.5466939211: precision: 0.0926210214: recall: 0.7694269682: f1: 0.1653390821
Train-Acc: 12: Accuracy: 0.9518440962: precision: 0.9496887587: recall: 0.8525409243: f1: 0.8984965011
13: 6464: loss: 0.1464297422:
13: 12864: loss: 0.1472452136:
13: 19264: loss: 0.1464809494:
13: 25664: loss: 0.1475553718:
13: 32064: loss: 0.1476079711:
13: 38464: loss: 0.1478375774:
13: 44864: loss: 0.1487349050:
13: 51264: loss: 0.1478279612:
13: 57664: loss: 0.1471027488:
Dev-Acc: 13: Accuracy: 0.5391133428: precision: 0.0915406456: recall: 0.7729977895: f1: 0.1636959418
Train-Acc: 13: Accuracy: 0.9527973533: precision: 0.9487888267: recall: 0.8574715666: f1: 0.9008218800
14: 6464: loss: 0.1426838745:
14: 12864: loss: 0.1391656879:
14: 19264: loss: 0.1396295597:
14: 25664: loss: 0.1408977202:
14: 32064: loss: 0.1411695919:
14: 38464: loss: 0.1402871715:
14: 44864: loss: 0.1410242911:
14: 51264: loss: 0.1420639892:
14: 57664: loss: 0.1421043216:
Dev-Acc: 14: Accuracy: 0.5271173716: precision: 0.0900259067: recall: 0.7799693930: f1: 0.1614203016
Train-Acc: 14: Accuracy: 0.9543258548: precision: 0.9455913978: recall: 0.8672013674: f1: 0.9047014849
15: 6464: loss: 0.1456339655:
15: 12864: loss: 0.1420506191:
15: 19264: loss: 0.1409597545:
15: 25664: loss: 0.1421949329:
15: 32064: loss: 0.1412218753:
15: 38464: loss: 0.1401517547:
15: 44864: loss: 0.1387639145:
15: 51264: loss: 0.1372640107:
15: 57664: loss: 0.1372851160:
Dev-Acc: 15: Accuracy: 0.5195566416: precision: 0.0894773412: recall: 0.7883013093: f1: 0.1607127381
Train-Acc: 15: Accuracy: 0.9557721615: precision: 0.9473985134: recall: 0.8714745908: f1: 0.9078519330
16: 6464: loss: 0.1364926250:
16: 12864: loss: 0.1340865766:
16: 19264: loss: 0.1351379131:
16: 25664: loss: 0.1349609734:
16: 32064: loss: 0.1339251230:
16: 38464: loss: 0.1360565060:
16: 44864: loss: 0.1348153282:
16: 51264: loss: 0.1352111297:
16: 57664: loss: 0.1339512858:
Dev-Acc: 16: Accuracy: 0.5145459771: precision: 0.0888494088: recall: 0.7908518959: f1: 0.1597513224
Train-Acc: 16: Accuracy: 0.9567090273: precision: 0.9476119297: recall: 0.8752218789: f1: 0.9099794942
17: 6464: loss: 0.1337810140:
17: 12864: loss: 0.1284945407:
17: 19264: loss: 0.1307565345:
17: 25664: loss: 0.1307276535:
17: 32064: loss: 0.1296316973:
17: 38464: loss: 0.1299854333:
17: 44864: loss: 0.1288012507:
17: 51264: loss: 0.1293942586:
17: 57664: loss: 0.1291550866:
Dev-Acc: 17: Accuracy: 0.5065784454: precision: 0.0879721857: recall: 0.7959530692: f1: 0.1584336024
Train-Acc: 17: Accuracy: 0.9581224322: precision: 0.9472977746: recall: 0.8815331010: f1: 0.9132329905
18: 6464: loss: 0.1248755551:
18: 12864: loss: 0.1262417061:
18: 19264: loss: 0.1274241183:
18: 25664: loss: 0.1249390072:
18: 32064: loss: 0.1256963085:
18: 38464: loss: 0.1275243692:
18: 44864: loss: 0.1277577118:
18: 51264: loss: 0.1270114717:
18: 57664: loss: 0.1270201972:
Dev-Acc: 18: Accuracy: 0.5038895011: precision: 0.0873566659: recall: 0.7940826390: f1: 0.1573980452
Train-Acc: 18: Accuracy: 0.9587470293: precision: 0.9501665840: recall: 0.8812043916: f1: 0.9143870660
19: 6464: loss: 0.1281153632:
19: 12864: loss: 0.1238253926:
19: 19264: loss: 0.1241770655:
19: 25664: loss: 0.1241176356:
19: 32064: loss: 0.1233867143:
19: 38464: loss: 0.1231789865:
19: 44864: loss: 0.1234604221:
19: 51264: loss: 0.1231348211:
19: 57664: loss: 0.1236846634:
Dev-Acc: 19: Accuracy: 0.4969340265: precision: 0.0867600959: recall: 0.8000340078: f1: 0.1565437274
Train-Acc: 19: Accuracy: 0.9600618482: precision: 0.9498134722: recall: 0.8871211623: f1: 0.9173975117
20: 6464: loss: 0.1141391821:
20: 12864: loss: 0.1183047010:
20: 19264: loss: 0.1182260029:
20: 25664: loss: 0.1196497174:
20: 32064: loss: 0.1209908925:
20: 38464: loss: 0.1210192248:
20: 44864: loss: 0.1207686615:
20: 51264: loss: 0.1209182452:
20: 57664: loss: 0.1207824079:
Dev-Acc: 20: Accuracy: 0.4941062033: precision: 0.0861698809: recall: 0.7985036558: f1: 0.1555533472
Train-Acc: 20: Accuracy: 0.9607356787: precision: 0.9526902980: recall: 0.8869896785: f1: 0.9186668028
